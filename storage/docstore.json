{"docstore/metadata": {"7de0154e-3b9d-4e45-8673-b023febe3c30": {"doc_hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3"}, "c576fc2c-b1db-4abd-83dc-d89ed3c81b11": {"doc_hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4"}, "5d6f3674-6d30-4bb7-bc38-f7b9ee628037": {"doc_hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41"}, "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4": {"doc_hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3"}, "6e50c860-0859-4310-8a53-f13a2d30632f": {"doc_hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a"}, "8a609cc0-d7fd-4e96-b9c9-7465edf4238c": {"doc_hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd"}, "eff895a0-e850-4669-b7d2-48b335bdca65": {"doc_hash": "056c51382730c37dbbfe690e81c966e78be27ad528e41055ab105eeb65435923", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "c699bb0e-f289-4ab5-9893-8dcc4c8f274e": {"doc_hash": "90362e25d1512e912e0dcdb4a613a70b5354e14f99c0e280565f270c792a3435", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "81de830d-7199-44f7-a9ba-fdb5b8850d6c": {"doc_hash": "c8d136425d9cd33f77c568219fdd33a37f5bd23f8b51325c78ebd4fa9beba58a", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "a1b51596-326b-4263-bb39-1e7639bc8bd0": {"doc_hash": "20e5dd52ae94028ddb4d4f56a0a6df090a356e49496b80b940eedb9147e15b35", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "41a14587-a647-48dc-9c32-d55bcd87c0f6": {"doc_hash": "b4d5f1079c4db53f264c5ccdf1402605fcb72b8100cf19cf1eab2ddce17ec081", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7": {"doc_hash": "ca62a5020e3b2a7e0ca3af0719715fed974b6cf120df765fa0b710a1474405e5", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83": {"doc_hash": "fdebeaefea2fc7bb1358f3c22d33ed4d92ac222c247f4781b0391c0535b7bae8", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "70b70c13-c34b-457d-89f1-98a9e3fa80a7": {"doc_hash": "9ff2aab2a143bc4672c177354f6d41d647520cffaed35ddd3c5289a820cb9336", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "77511500-900f-4415-a51a-104e7eb78c48": {"doc_hash": "4ac12856a621a390ae70d39d1ad242aa5f956c41cf19839df1877a9b5cd26ac8", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "763e5e0c-65e2-40d3-bb3e-096d0642a311": {"doc_hash": "1f8d72000a663935d62121d03714ba07e54c1f105a058bb6875c5c967f1dffe2", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "1ce3fa9b-5ed1-4f91-a503-5c565a396da0": {"doc_hash": "e7674ffb0453cc772221a1f5c45b55c7195fcceb9953173fa053309be19b0b03", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "e93ba833-a8de-4521-9662-43f8bc39be9b": {"doc_hash": "13eebc0bbf6e26d35296757d82fb30df2f621986a56c41e90a55efa4d35b15f5", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "80b100ce-8a82-409e-992b-833934deabd3": {"doc_hash": "d9cdfc7bb422c9e430ef0255f59747d0ac10f8d9b3ad5e5c20a7a9265964a20d", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "71f4e635-597c-4b3b-9c7c-a829195b1702": {"doc_hash": "0906929d027cc4af28b7ecc0d4dffb23efe916476bcda7c805879ec5afd82219", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "637ac51d-49ef-4ea9-89f7-016aac301edf": {"doc_hash": "94a24a2157fcff8aaba1e12081c081d26a6612953326cc1c05d806e86fb834f5", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "53545215-b49d-424b-84e0-55ab74d1a0de": {"doc_hash": "d0ebc3e3b03cd9e9e7191d4cb0eeea3230cdec7ec87182e8cc563d560ec4fbd4", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8": {"doc_hash": "25ec511c80adf345248a8327dd9679955b2ffdfa93d26195a64ab63ec503f698", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "4bdba251-a594-4ffa-959a-2a743dd494d6": {"doc_hash": "f440a3b85433e9c19588dbdd8174dfa9af2c0614d76aa954fc931bba262d737c", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "bfea95c2-215c-4221-a48d-0879f40c672c": {"doc_hash": "8826ee93a4b270c3009a6735699018ea892a34ba91d0a90b26378e5176a47e3a", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "f3a42e64-3ba1-41cd-9470-45adc356732e": {"doc_hash": "5d298b7b669a5fcfa94ff96517f4eb749f86b0f2c7113fcee587f08a3db352f9", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "398a91e3-ed3e-4591-9aad-7cceb071fad8": {"doc_hash": "dba6d7ade29ae9654e448726443d5d55bc6135246ce066acd4bbf059c477ad19", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "ba967449-fac4-4684-ab97-fc63b86b4365": {"doc_hash": "41e0f419686b91316bac02954b6114ae55df7d1043e9bf1122849bd6759e2a28", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "9bee0209-beda-4748-a654-f48e4b65bf84": {"doc_hash": "106b6874d3108d922ec10a85600e9af740a66136e5b3d764296d067250242534", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "f8acc35b-9d69-45bb-b855-e4dd0758f9b5": {"doc_hash": "4245d61edf77efc4a29f53ccd88749b98c6ea0fe9a00807eebd2be1736173ca9", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "637394f8-896a-4457-8886-44109cb1d276": {"doc_hash": "0b5c6ff1ef3321e2ba3964e034a69cbb68b2d559ba3978f226bb78064d069a0d", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90": {"doc_hash": "394b6b699077c9759188a7bb91d4730cebac8c68706452f5e8642dc18142ef69", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67": {"doc_hash": "dda25f32a901567aa1660219ed56bc3491fec483afd7f6db030dece6618170f1", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "e6745567-0530-4736-bc4a-f8591739b70a": {"doc_hash": "4a57467ec42fb38ce94eb8245f6351a4b038ca5c6f51591d0ae0e7760cfb49e9", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18": {"doc_hash": "9b0a958ff6dfbe45ed957837708e7fea5ac5d0527e74182d5dee5b7e98867097", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "5ead539f-bd15-416d-b2cb-80f7ab905fee": {"doc_hash": "25bf0c43511c70988f79591aa2220069b2dd6b418b5ebad0e4c0e6fa30fde00a", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "ede4c553-e278-4f1b-a25f-fa2274f66eb6": {"doc_hash": "6395fbdaf7c734e9030c49407940d4b539e829b2f3f62047a0c00f44ad1e3dfc", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "d355577d-2f90-45cc-a7d8-4a6439c352e0": {"doc_hash": "dacebab5e9f89b0ab77dbe6f2ad45807df8838fb70c3689668874ded19f621f3", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c": {"doc_hash": "55ae7afcc15421788bb7464faced8fa6298ea29c19400fe777ec072e3d5051c3", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70": {"doc_hash": "914ef9c01bc51704f5a58d2a3bedb507dce35c8921cd76bf91128ffb2b0aa55b", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "2fba4ce2-c81b-40d2-bce0-7a12ab75567f": {"doc_hash": "5a3c7722bc4516c573098dc3eb6ea188d76394095f92e90800b2b429778a2427", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "835bcdd2-168a-495a-a464-179f8aa2ca70": {"doc_hash": "a49cf37e2f632f0621f83cd5e8683942d469ff0ce81345d34430c532593129cf", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "0f694c65-1411-4846-a5d0-2b8fc3803278": {"doc_hash": "6e4ba7ed4dbfa5c6526649080ef64711328af926fd9c2da1b65669541e80b86c", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "6c1e9b43-ad5b-43cd-b871-6d87f691462e": {"doc_hash": "bcfaf364408ef93edf966d7d8b1e0d8665219beeec1e9215cb7733da71f0bfb2", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "12cc2c74-f75c-4799-baec-68c537b8d667": {"doc_hash": "d6863d048006dc802080578d6c9e611b7ef127792bc0f97d770f9ce6a0dd4238", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c": {"doc_hash": "349f349e686de28a9ab5d6071020c5af9652b845baa6b5fc7fb72932298b9961", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "e5da99ce-899c-4c85-85f4-6ce88e48424f": {"doc_hash": "469a91c05734349ad97720be12534cca6c02486804b6077f40aa5ad1bb800085", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e": {"doc_hash": "6f7f6918191fe435f8ee24c31d26c4aedd5b9d8a3aa5170a448e5e371071d0b9", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "831572d5-f925-4b58-9df1-ae938b74acb8": {"doc_hash": "b1bf54de2f8cf18b46709b253bdb62c01796b1e7c1800ebea504ab6ec5221460", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab": {"doc_hash": "7a57456badc9d133dc720d75a0428d8550c4f01f9042410af645da28cd35df6c", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "8051ccc9-38f8-4dab-ad84-13473e3afae7": {"doc_hash": "69be3f4271d1758666eca7e86ac66372e7c0738ece2ec5b5d139837e536984cc", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "d47c9834-6c8d-4c58-855d-7d57a3a96b92": {"doc_hash": "083ea72d607a26d83f5d898462e92e9a141c42d7ed175c80050017a965485da8", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "b0798139-ad4f-4676-ac66-15df2f5a5c5a": {"doc_hash": "87213dddfbceac1c6dc93317fdb25c507f9ea57a5028ff559620890f8bddcfb1", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "1a09560f-5496-4496-af00-6992d8cf3e4c": {"doc_hash": "3db64f0a8fb941bad08cb569aa819620e710050fc4cdced7646d89ef999a7202", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "b32b8215-cbce-447e-9449-f2a2b77d1b43": {"doc_hash": "da95dbeb877e74ceed47b3d4680dab6d33c0f95ccd36e0bde95215ea0e02d85f", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "0789d1e5-6e94-4900-b20d-bd61e8d690cb": {"doc_hash": "eb217489a433bdbd6d70a2b828633640fc96cd991aa3a67b1b36f95374843490", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "ffa9ed11-530a-43e5-8fcf-51f72cb5971f": {"doc_hash": "4c6cfffd0aeca46699a7bd4b73f51cdb625dfed60ead45bdb3fb41cbbac1590f", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "12a8665f-284a-4a7c-ada3-a1defdb80a41": {"doc_hash": "cff27fb46b46d000a4cf455f5a9aae8f3ca23a3daa01e0755176bf43d9bd4d6c", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "61f8bcea-bf4c-4328-8997-98e046bc8743": {"doc_hash": "aa553bfabeeee3c2aa51b6797abf183dcc4477f9889c4b6e666e4f26d160bfed", "ref_doc_id": "7de0154e-3b9d-4e45-8673-b023febe3c30"}, "807817cb-06dc-461a-a0b0-18851665bf43": {"doc_hash": "ecafb7efa3eaf58fa01a0f8a1a7ee9b21217083f31c365c980c8853060b30825", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "80d52e56-f761-47e4-95c6-74daf7d289c8": {"doc_hash": "a2bc84120bf267e244b72955f56c175b0d96b1e6bb6f91ffee7e2626c399c366", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "0c0c2524-768d-4301-aee4-c96ce01c46e3": {"doc_hash": "08dafd41c56b10fcab8ccea68148f55c23da093d7ad036aa7f62bca96f0f2513", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "5492fa06-d59a-4118-a7d5-461627d53796": {"doc_hash": "91225692bcabb644efb90564b5e9640de3a951da9255b5bc8bea13e4fd80106d", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "a91af3da-3a51-441f-a882-bd1e031f01fd": {"doc_hash": "9ef3bc5d6662b2a98e8eef8b8b3fa8b8e1eb7d7275ba7aa1382cb5563546574e", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "1dac587b-7d06-4aa2-b503-ad4beb38d907": {"doc_hash": "f92bc28c2f24ef179af5de0639ac1b52dd196be3fc893ace3e0c37672570f7a1", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "e018b138-e891-4624-901f-4b7839b40ed3": {"doc_hash": "b51e69df7e75c11b50ed2a89168eed5f939a0d393b7b81662234da77e001f65a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca": {"doc_hash": "a72b973ebc539c817f798dc46ad1168f0bf2aa24b4942e85d0bdf2eeb73ccd4d", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "3394e636-ae71-4ee0-be0c-39ac6beae551": {"doc_hash": "2bd6a4433cf7720dffae6ebee5225c0e67bb55bdc7768bf93e13f233d6a880ce", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "56cc79fe-e4cc-40a8-9600-57a8aac86724": {"doc_hash": "d735bd28589f9fa92b3bd997083f3c633cd144187dc5583fbf7f66f47f7c6bcb", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "19a64d02-ccbd-488c-ac40-7d4d15925208": {"doc_hash": "c24581e03d3412ee98d6b111de7d8069d58aa8738b7c694195fc4f9cf469424a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "4e81a266-9cd2-4387-a302-42ec2229acf7": {"doc_hash": "ea4200f1a0ab93918fc81749a0c61fd2ed28b22f86d6483f4468f5cfb0938bd7", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "1c83d6f9-7574-40ec-a53c-053389189571": {"doc_hash": "16ed67e92ff4abf37af318793f7eae09e9c77f49fb45d9b8eeca484eccb9ead3", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "42023691-822f-45cb-9f16-85775099ad3b": {"doc_hash": "ccd1c5c6839d8a45e0dbd76e4572e7239c8b6c649b97724dc392bf940bf28213", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b": {"doc_hash": "5128534e5a3b92e01e10a2b86457defd09bfb40020fa6abe994b6daf4b2460a1", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "5f474dc3-3a36-4237-848f-f9f3e1335b76": {"doc_hash": "04be8e094153af4fc9d852d0005669fbbf652c79576e73ac2656c84b91bb64b2", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "eba54af1-14d5-4f93-b21f-701e0f055df4": {"doc_hash": "a0a1c81c1657ee457e3c71f43c4411ae85d9cf0bde8160adcfe71da4f773ed65", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "7daa55ce-8eb3-49d9-a087-5f95ae19a941": {"doc_hash": "4858e33b455592c901dc474a8d6caf2685f3f2a95811ab60341bd13ef00d7abc", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30": {"doc_hash": "015d060cca7f034525cdcf3f1e826f38fa794140f318930f82ceea206f7858d2", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "62a87b18-e78e-4e3f-bbff-666b8c5b90b1": {"doc_hash": "3917942ffd48a3dac9b02de13db216653a5b0582853efd8250c8272bc1820956", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "04e5d2e3-d9ff-4df3-83b4-1658b4418f65": {"doc_hash": "30d58db4eb8bf6ba63c5df09838e02fbc8f81cf7bc6a55b10b677836ef2850b8", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "a65d2ead-61e3-4984-bac3-34586d385cb0": {"doc_hash": "6e16cee6ed57e56c7671fbcd42784b332c44bcd727b25a2b53b505cdf97c8e67", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760": {"doc_hash": "5d6f31c392fdb7f39098550e5dc246c7902c734f74ed6973297c4a218df21626", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "69fc89a8-e561-4c89-af30-89b83d8b87dd": {"doc_hash": "aa90ee3c4190d7dc563111f1f4ef26d0fc5cb8619cbccdfecfe699e8f5b209e7", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "7ffad171-3c3a-4d6e-90d4-1a54625db300": {"doc_hash": "eb0eee39bd4ec7418c91dd5e0606b28601b66e395b33fc45ca1fee46ec92d5a7", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "78aeb346-49d9-498e-a664-4e573eacfc1c": {"doc_hash": "2f571bb1b2140fd6ecf3dc25eabf6e24c96b73f0cc84182fb912571e90b0f7a1", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "d3202f94-b234-4111-9e1f-776cb8811b1d": {"doc_hash": "89226258b634f26114c353b68a451985cdc0340ae32f08ac6de69e7aa19d5d6b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "01d7d455-ed37-441c-84e7-a882638530d9": {"doc_hash": "0941eb21c2ef8d84112e6d170e5d3d554375d1121d13cd2efc459f1ac6794643", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "02bf118c-0051-4dd4-9753-43aa4c243006": {"doc_hash": "d286cba52fb648260f754a3cd1e6b23867bf0c8013d9a1909b5ce0e1e44a88ee", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "ae4554b2-0506-4b96-893d-6795e5473bb7": {"doc_hash": "66ab5bd60f94acb17a2435bf6f0041017bfc623cb030347974558592df94a9d2", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794": {"doc_hash": "d5f0dc9fdd05f0e21c7cd2d3e4b2faa36bfcd20a3186ebf81e410a997b75ee39", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8": {"doc_hash": "0f55004fd1cad0765fe8a161685bc04a26e7fa3a654eb8a01ad14b07ee8e26db", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "0a945e53-ea82-43dd-983c-f40f259f8e51": {"doc_hash": "35929ce9ee239361f443e4013b48243db747c91be4a47c33f4473418c034af17", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "6d3efe32-8bcf-4a43-b02e-62941a1a24a5": {"doc_hash": "46c94063a3e2bdc044b6bdfa6bef18ab16c912991a189550c8eea2190e479721", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "52bc1c89-f422-4dcc-b5a1-94697de969b7": {"doc_hash": "d5189d8dab8200dc9b036e8751d2863d1d18114fe59d94bfb67f46fa5ea5ac0a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "4670e407-2330-46b9-973c-f4f8520c614e": {"doc_hash": "12d91eddf3ad6bb3e64f25ca590349779ed2ad6794d3ee1e2c0d840b6e3e3f44", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf": {"doc_hash": "48a01b8d5af9b50d6ad9a91886421eddb0dec072a43ce9f4542d8b28db44f611", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "ec13eaeb-f179-444f-867e-fa8a0db4ec90": {"doc_hash": "bdff93d39f8a3bacb4f6aa12c89be40603bb0daf63847f115e6bb9b37e5c4546", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c": {"doc_hash": "bd1bb7a6f03357d260ee5dda11fbbe730ef82346369e56f3c6efa957905860b3", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "298d4478-3c0d-4bbd-a846-66f38b725174": {"doc_hash": "679e4a66a07b4e33cc0bfea9ae0e2c20e5a6cfc69bd2d2cef9a6f30e63a06a19", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "26bb97a1-8644-4569-9ba3-62f71311f640": {"doc_hash": "c3f2063ae3613a51c532319d2465480beeb0b85303b2089bf0e21cd9ffd54066", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "33b67e10-82f4-471e-b856-8851eb320b99": {"doc_hash": "384460b2a88847e027d68850420b5ab45efa003cd44d5460e156bbaf3efadb7b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "08f218c0-c59d-4827-b971-6cbc7c6bc241": {"doc_hash": "35c3f566b3e3781a612e88ff7a12acad9297c4868d1940ae38e693ca45d10610", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee": {"doc_hash": "bfbda187454dea7da5b9ba82882f7ff00e57fe7035795a1a30e10878001da7c9", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "893295f1-1612-4b49-9e50-ce5125bd1d90": {"doc_hash": "ddd038a4b9144a028653585c4169a3fc864fa1cefd96873a7f97836f72122db7", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "bfee077b-5da4-454c-957f-a2eaf0143bb8": {"doc_hash": "8d92a81af4dcff8ab68e34b6e253e353867f0474d90dce0570f9a86e90d4af78", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "e2f62c5c-2687-4c48-a1fd-0601b9724538": {"doc_hash": "6d588ec2863f030a6a731e86df7e3708764a9bd536995195b51a0c5e420d3940", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "b3d2596e-25b0-4833-b25f-ad7b320b13e9": {"doc_hash": "d9562d862dd78a451c24efa6764e8c2b2ed95669ff9d364b21e1f809f44c725a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "5ef2b830-7fc0-461d-a950-0ad96c57422e": {"doc_hash": "5e43f60f3c0fdb007ea621faee912f52c82e5d6587926a13665adf13ba89d88e", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3": {"doc_hash": "1f16092993ea233a0c1cee122c70d5a74b143884f0da46e00d3b41dd5ef5b451", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "fc146b52-8de7-4fda-bde9-475c39f32dfa": {"doc_hash": "6fb5c48cc147e650258b15d90b7809f7b9dc97e45e870ace3d456e7609708140", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "112bfe04-34ae-4fbd-a20e-4b86b3af5abb": {"doc_hash": "381b94f05920dabe1c9ef8e7f8591757077bede79590386803a4b0c1e193e1e1", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "4aa21f4b-0f43-408f-b6d3-31c66bd09e48": {"doc_hash": "417172d8df7f3038e349ede25cda67bb3976b3243fd10c6ed5beab3fe29c0209", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "7391ae77-6860-4ab5-8df8-594b1eb5a3c9": {"doc_hash": "36bf3e6c32c973776d7ed032ade00659b4f5f280eb1c89c7a8278ebf77e34942", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73": {"doc_hash": "48db986b35b25625bc2b4830c84140659b1334e0a13734bd9a20812881724a09", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1": {"doc_hash": "e7ba41e3d8d8f31b32250cd0105475357d0d75e31ebc71ac9acf7de44e294799", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "868a1957-514b-4ae2-80af-eaf25ff46dfe": {"doc_hash": "d6dc8f4a1224a91f9379028fa8f4d331cbcf0f2c5cb4d69f7f0cb59df340cade", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "1538348b-94e5-4bef-a6f1-a5c236b5838e": {"doc_hash": "76a724e2a8c8b724e200654765e5e448e885923ef6ae4c46844e4c4dc26115b6", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "80c9a6d7-42b6-4ece-8645-a826a45fc6ef": {"doc_hash": "cd1b9a950559d97999f04d48c130c0fe6099a9bf12df671f21e43f7cce1abcd4", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "b54e53d8-8a5f-422b-9662-f108c080a21b": {"doc_hash": "0ca14ad1fc1071dece6fd3a8aae61522c785a4fce5b36dfb6286515f4f486df4", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "d79be4ed-9127-443f-b239-50c0fda52944": {"doc_hash": "6b7b99a605c44e7c229d34dcc2f5453729cde5580a464ea2951ea1d374871b39", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9": {"doc_hash": "7327032e7a2eb3acd235e738db3436bce2a36cadc573b2c43785bd855c4f5e3a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "72af5b13-bcb0-4db0-859c-820e87512bba": {"doc_hash": "4cf9bcc28091c520534a365c9d023156a634585bd32224c330fca24748673f25", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "eec4cbf2-1349-48ae-a4fc-67c36bfa852a": {"doc_hash": "94b1247a1d60dc200deed39842250903ba155e0a280d651604246f9a92850505", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "078958b9-b94b-4ac0-8784-66e7fd942152": {"doc_hash": "a3a03d8bf5d693d3d2fc49e32e01b6d54e7be1c10d9a343552889cb9d65acfac", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "9afb965b-4378-42d1-874c-eec20e725d4b": {"doc_hash": "a09badaf06703e9a43033dec93b380e3376210ec993ff4cbadf01cf8546b5414", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7": {"doc_hash": "90d127e6a7d7d582d28617a9e6f1319cd523a67d40d4fe4f06656189e46dc710", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "66858f81-5e83-403f-ad20-20ba15c0b404": {"doc_hash": "6f4114bbbd657fa3cc2cb3e16b28615f1e25797e3b132e4619e38b4837b616ef", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf": {"doc_hash": "dc43cb92b12b7a06f7acbd9809a70ed93b753faf615a685a14d4946fbb666cb7", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "952a791a-4658-4ca9-a8bd-1f0f52ceb296": {"doc_hash": "7cecd967ecfad67ef38dd7f8bdb383b2d4e2bcb802b42ee25d143ae49f1c1e16", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "f87486b9-9862-48b2-90ce-27031286b47c": {"doc_hash": "1ca4f2cca69b9e0ca4540f8d9b4e4471de72f9e258b6e27c4e5ec24636a33abb", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "bb70e5c3-d737-4271-b595-7c664b9b428d": {"doc_hash": "9f485dea49226acd46bec467d42a12cbca92fb9c57a3d43d1147d58881b572c5", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "a2a392c8-d978-4bb8-a7aa-dde2afa8278b": {"doc_hash": "21bea226b85371561444d01e4e133f46bca365373df85b6adb11d264b49ba9a9", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "f41bd766-311c-403f-b26c-998c9c148920": {"doc_hash": "3aec25362c8ca50920285d9a567a99af6512a554f527dd4ad0d462216d40013b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "9fc16026-c989-4967-b861-4c24888747b6": {"doc_hash": "64a587ed331da099d69ba56ed8329a0f8f07351d15f230e52811ce946ae427a8", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "ac38c4e1-808a-4fa7-b0e7-61a9162790d9": {"doc_hash": "0502e274bdc4762d4c855df29e3d11e2a8d86c6b7b74855093dccab0aefdcd1f", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "b36e6e7c-d5b4-46db-a07e-388bcdbc8506": {"doc_hash": "078bf67c84399c292dd5989b2bbcedad0acb0998ab7b0691b9fdd636eb8378ae", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "35465509-56ae-4465-af97-e1666cdcd6f4": {"doc_hash": "15caf8fa5ead750434d8472ebe7d1b323aa6647344a4e31257002ff9577700be", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "f4b89e55-5ca3-4b9e-a364-9d174f71f078": {"doc_hash": "4846b0f6dc13f8e279984fa686c52c864b8dd087d5efb3f1b39b3eeb2afc7065", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1": {"doc_hash": "9e4f5ff9d7981aa2d92095a65c31c8bbb91adb97973fa4692a845fec9fed22ba", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "245113cd-4787-48f3-862c-28894c2bd79e": {"doc_hash": "734cf879dea2a5f2e37a2c21a15c8abe4625b6172c14bb04795f589dcb15d84b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "710522be-6361-4b6b-b724-86ce79592232": {"doc_hash": "1fdb4029b60a84e93db9dbcc4095e9fd663a6ad063cf7863dfbafc4b64c8287f", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "ca33393b-af52-4a43-b3c0-0f0102b767c6": {"doc_hash": "df2cbdc21278624dc4f0719444406ef257cb235355e1e04a379c2c9496d92941", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "bfd35ff9-0ff9-401f-8bf0-65b68973787f": {"doc_hash": "f3d8e18e20f9fe81ce1907fccdb33f5bc06b4216abf2a2febcf628813672c527", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "2f544f6a-ab13-4858-992c-3ceea8a3e688": {"doc_hash": "070da2e2c9828c439616a14408e101f3ea33dfbdcf5a1df9701334c4df3cdd79", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f": {"doc_hash": "04b757ad4a4edc7065982f7926d31b020c9c3cc9a496758eb9d714eb2be1d878", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6": {"doc_hash": "7ef8381f4530f2b073134803542a1bbded85b1a6510eb108b03f40adb537c8de", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "f4107aca-cfee-43e1-960f-aaadbb51abe2": {"doc_hash": "b0ae970930b1094e84b22bbd09d80a42e695bc9262fa064cf7f2083528b9d46b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "c99d59f0-a319-4dbb-8ea3-cb77d814b054": {"doc_hash": "69cd6326e58107c0feab0fc61326ba75fb5bf5ad89119c9390a345af46d8b953", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "a1f133c0-45ed-4898-a634-6cf927f1df03": {"doc_hash": "d788b359f42371cae2f6871bc5ce7be82bbe8f9c3598e90ad637e7b24c7ed73a", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "264dcea0-cdf9-4747-a732-a12801438545": {"doc_hash": "1399aedb47161ba54b7861c2ace01899395923c0fb64102c5f083d74fc744a79", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "e19df090-0b48-4a73-a405-2acea22c24a2": {"doc_hash": "4dba8f66940140a08e97f6e0f9c4c8e36e26c6f1e18c10686038c93bc75972d5", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "cfeed0e1-9eaa-4da4-b859-2810a848d07d": {"doc_hash": "1a9d841fb153d13a8a4a69a5c10f8e599dba4bc932be9ad27589b6380f6ee496", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "6de81924-3a1f-43f1-9b84-80d16468d179": {"doc_hash": "1c0192f7acbd0d316c7c723f241c9476f7d853dcc9826b920fcea22039456ddd", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "20e3e088-1968-4995-85e7-2494a0253662": {"doc_hash": "4954f66767bbf31e3205fd0927957d3c3cbc6f9bcb6280c2bdcfa6da375b10f0", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "4002eefd-9ab8-4bf4-b804-db332d14267c": {"doc_hash": "a27d5a85891bbd576dfd6c2cbdd6f60cacb8f534b00462134e05b606f6914770", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "8bb57993-603f-4a92-8fe4-6fda48665a68": {"doc_hash": "5f5d78c9f92a981950f0e8216eb58cb6ccfdbc608ff4cb120e53f78e20544b77", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "410c3a09-4c30-4117-822e-ba0ebb3f2aea": {"doc_hash": "5261dbc6f9625ab8fc4d5522a7d9d651f4dbad85367c1c82d0290419543d59b9", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "21485ede-86e6-4e3b-bb7c-c4b0017c217d": {"doc_hash": "9fb2f2bed94907f1d3a1bd5dc649373f6ac9e786dc4df0bbfbf1bcaf144e6f8d", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "35d51ac0-a6b6-4883-b08f-ee263f10c2bb": {"doc_hash": "bdea65e1fd5651286b28baa27e18df05091d8fd16f44cdc829af9c61ebeaa0dc", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb": {"doc_hash": "81b68b872f850c1b03e5b90d44c62d48e63a00dcc4aef3cc798aa80d923e229b", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "675502e4-b6c1-47fe-b1e7-3fa9c0ca544a": {"doc_hash": "cb7020459b30495a72add00e0b725a11d6ef3f80edebcd60c062f96b315bbc23", "ref_doc_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11"}, "458e5211-267a-497e-8af1-a24777294f57": {"doc_hash": "c1439d02b3f2f9d5642104fc9259ea56229b7836d72b541459573dbcdfc90792", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "27eefb48-d4ff-4710-bc06-7c60ca746d2b": {"doc_hash": "8847193bfd66af43de18f7fb86ef5f94da33623e744fe3f15aa0e00abccdb9d2", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "e3fb3cd1-8599-4725-9e29-3c91c730f4fa": {"doc_hash": "22f020883b329e6f65f140c3ca720eb7b0cac214c04691450250b3f92f99180d", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0": {"doc_hash": "a8599ba28b645bacf7e41a20d122edcb34a0173036931751f6b61b5a80f69259", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "8d8c9c3f-1082-499b-89dd-63ee849634e8": {"doc_hash": "b39abdc067348772b49eb09bd4dd9e27ef800409ff20d01cfb693327e11fe675", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "11c64b71-68af-4612-9f0f-28f40a4c13d2": {"doc_hash": "f11b84b0e394632d0150cfabbdc5ddc2f8895e9e305931913ee9b828eafa7dcf", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7eede3b5-5fe4-42df-aba8-a0657a2a0f26": {"doc_hash": "0ef1184c9978a101b1636cd41c2e80302af856d13b560567405bc494ccbbf236", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ad6602f6-5660-4fed-b0dd-c185d66fd5d1": {"doc_hash": "2c287b254d8a2d9d672b64a4071424b88569867d9cffdbbc94dbef603375fc78", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c415fb83-9227-4fb1-a310-f1908885738f": {"doc_hash": "cdff396f325770f62a3501b67f1a640d9e657686744a51d00a636eebe2815b72", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0384e1e6-1134-4c0c-b754-7dfe113912bc": {"doc_hash": "8f43e6e8c518f4fdaa1c5585af3174bcf059fb181f3579c62f6b243e8b8c2cb8", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c0850af2-4727-43a7-bc63-90c22fb3cba8": {"doc_hash": "65c70a3d550568ff58fd774eea4ebc6aaa4f62be92117fc2e46fc5eeea4a829c", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0182c0ed-a000-4b7c-805b-89740af1e4e3": {"doc_hash": "cb741d921b82601b9d80329c1b8d659d315b00b9da596139a54a39a0c2b3ffbc", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c": {"doc_hash": "20554ca2153a542943e0a0d32056ebd7a85b15f777a01247d08cf8efbc17ca53", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0616145c-e48f-4abc-9759-b0a3c32cd2ed": {"doc_hash": "80cde20a90d71b9b4515bd441873654662f5df36f2dbee398bc9bb175507148b", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7298fd7a-3cab-4554-a47f-c25514b28dda": {"doc_hash": "903d7c844779ec168bb99292aed8900d8c000ceb898325ea49326c8f80d31cbd", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "84b91c94-353c-49ec-b8bf-057403d13962": {"doc_hash": "34f0a407e21c59fcae790b041be0c4db1e47974634055961064f8cf061308650", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "2c00639d-fb96-42be-93b7-8b6280d07790": {"doc_hash": "76759611730d5ed7be0a5f479bbb5773d907dfda516a43fdcd763d87b54b7c15", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518": {"doc_hash": "2146f55fa2bb4d026fbee67888d6bee5bdd6232417c1a7b75962f28b30c53102", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "078829e7-a738-45c2-80df-d3fc1fbd0bb6": {"doc_hash": "b7e2a8eacccbf2b38d2ec895d41a39112e5517b2b2b0b1d0399185a83f450fe2", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "9bb39904-be6f-4716-a8a0-2de5c3cbfacd": {"doc_hash": "b39af020cffca7471e9144373ab0e78383dfaa9465b9957eb3041ef6f166d6fd", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "a1d2149b-71e9-4137-980e-f40a6d0ae969": {"doc_hash": "f9f98360f17671d90bedcf38bfcba01a704c24eb09a2a3a41030e68748807bf9", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "58094209-3599-47f0-80c3-7aa23f88ee6e": {"doc_hash": "7495962d8221242d6ea6cc1d523eca3baf7553cad97769ea4604a567bd27c2a6", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "9747cf7b-c50c-4e74-8f6d-fde50ab03a47": {"doc_hash": "542b880cfe2e4aadd5e9fd1e91ec323bf78f248565df53d11c1ed1915e2e64b4", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b": {"doc_hash": "5740a09b9aed1f0feab3303f27840ed13abd14c231083de346b526f423df3637", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "026354a5-e824-4b8e-bfe4-70d9a12c98ee": {"doc_hash": "1252654d1f877a9e0fc31641808744e49d6ff4131c673e8a42a872919721fdb5", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "f7da756d-9189-4731-9b2a-ad6cc77abefd": {"doc_hash": "452b01882a457bf5bab210b32f40b375ae8664dd6f8ce2df191a4726043262c4", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "d167bfb7-3bbb-4470-b063-1e1d8491e56c": {"doc_hash": "f00c1e68bb862c0595c5aeffba6b906f2016307a1112d01c4475b9464b6af77e", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "b01b4dc1-cf98-4500-90c1-a179a66d3eca": {"doc_hash": "481201bc247a3ceb05622415b0bfdca56a795becaca9242aa4e6db358be2d083", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "b2dfca25-8512-4aa6-bf41-c19af73a3f0a": {"doc_hash": "95bdd80a99b95771c35465a6c929e0d1f2030dcde31df31c7f9765b83cb3cfd9", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "d6d00942-d574-49ac-9bf0-88a2c505e6c5": {"doc_hash": "6c4c844678d0d1889141f8b301ca5ce653f8189aeefd1c4f4f6c010733cc5968", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7a91b32d-dfc8-4709-8727-3ea7a092c079": {"doc_hash": "d31e0593678feb925f9a9549d0e5a63576a312907a693cd11032373428b5499d", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "4aa0fc68-3210-40f6-a326-52a5aa701f9d": {"doc_hash": "cf596b5d75f67818f3288ef4dd9d715aade4b374e90a73ecba94501110822177", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8": {"doc_hash": "31d537e686f68cf83b4dfde8c0f1d68213484cebf3a44d19d23cceb1033daec9", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "6b21d36e-041d-4081-a1ff-6b6d9473a385": {"doc_hash": "684f9ea49cdc23406bb646e54fdb568bcc884213d1c96d0bd0d5bb15814db399", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91": {"doc_hash": "92b64acfbb72f53d94f97b667fe252924cc4b3b74c76a32f4e128ff5db1e01cd", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "031acbd6-0b4e-4616-9af2-665b953c09c1": {"doc_hash": "12e41bb743f93241fbc66e3d5818c8a347e708dcae5fb3c7a9539c0fa0d1c822", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "fd001662-c246-4a0e-94d6-f5c939e4222a": {"doc_hash": "4dfcb6714d40157d1282a917cfd32d0d9f83c296054cda81bd1e9eca3031d543", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "f419c400-91db-49e4-b0cb-382a358703bd": {"doc_hash": "62a577020a9c0c50fa32bc80f573f3f68d442f7004fabf4bb2c94d2425df6733", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c55a790c-f53c-4875-883e-fd4ba6a476ab": {"doc_hash": "1bc10457c24dc3de9717c995a7d2c4e80ddad89e90eadc3fb92a6bc0e413ca2d", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "845726fb-0aa9-4b32-a21d-46590a41497c": {"doc_hash": "c6f115f4f0c8c7c55b1ff530af1c04ad4043e23703493f6b6d30cace1d51e437", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "218cbf28-22b4-4ca8-884b-e037f8fde748": {"doc_hash": "062d97b8e835dd0c18d42a82bffbdb2ca4f1aa4581e3611d1c1fdc363eaad3b6", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "8f3227fb-09bf-4dca-b0b6-4a41faf58add": {"doc_hash": "6037a2bfc631f073e23ee870af19467a8efbab9f9a55289c99d0c14ae93216d2", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447": {"doc_hash": "dc48f2da58c14b12cf1769c2c9ab3ba9825972ff424c613294fbca3f8ecca903", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f": {"doc_hash": "b3ca1783a21a3d1c605ce984e17794d37733fae60ae26aefa09747e5233c9e2f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f": {"doc_hash": "20f6f58dca7b6f9bf0cd26952aa2cbd0e484ea89bd30ef6294352decba94b251", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "dba4a441-a113-4b70-a792-af801b07d0c6": {"doc_hash": "979323c77c31c7aa1e3c4e7a9eaddbe184dc78934910f4662cdda02ca87ef176", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ff95b4f5-3d48-4914-9542-350f01b5a36c": {"doc_hash": "d35908fadb549822b4cedcee9f08005be64340815a8d0661faca40d571a9438e", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1": {"doc_hash": "efc7d03435ca10dfd7fea72f79c3bee1d9e662e939142da9085ccd642bd6f92f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "5a8fd19d-8957-4c68-8588-43846ed06dbe": {"doc_hash": "05b7850c835f1f480bed2ee79d3b2800eb61080bcc33f141234623d8cf62e441", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "63097d21-b1cc-4596-89a9-873b353a7bc2": {"doc_hash": "a2a60eeac805d2717a2565279b2f1bd4d4e11ec3e0043eebdf4b9eae5c03f9a3", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "20314189-301c-4151-858b-fab52b04e217": {"doc_hash": "e3165f9af59f1b8e5a98d190ec020fa7d70a479749b3dec8c4033dd2229a4a84", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a": {"doc_hash": "99b3d05968309b837eb51af918aba7e1271753ccd172ea9fa682e467bfdd3633", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "8f5dc369-abd3-494e-98ba-ba89fa1a056f": {"doc_hash": "eca55dcc7c4d5041760421af71198936c275a6c93234ff71a56cec845693257f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "27b9505c-a7f6-460a-af64-4e6cb619e973": {"doc_hash": "27ea04e54383e24bafd98cf00f5bc06514c4418ec36f85ec58282d7f72e33988", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9": {"doc_hash": "1e9cb102e720824eaecb226f9bc68ecdaa897dd49eb36071792ffba4c5bb7238", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "534dff40-3882-4cc7-b02a-66645a520b19": {"doc_hash": "05701b718b7288da511a47352e756b12b8ce1bb3410e7bc91a90835113fa2bf2", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae": {"doc_hash": "b5c0d5054c33000c2cc9b015e73eb5d0a0086f7a4673b2f683875c2ac0e7c07f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "8576fadc-b57d-4b71-80ee-83b4527a623d": {"doc_hash": "54b65406d87916cc14ee5407eeb55a9c9936b2066e0a0de97d64eee4f6a19e0c", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7b751645-c617-44aa-803e-abbf102dea21": {"doc_hash": "efdfad113059144435268a4e62e72aef9bdb42d73a21c6eb1c80c44eb68f25c1", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "099a86c6-1090-4228-ba8b-ed4696184c54": {"doc_hash": "1dc2266beaaca83f5bde3a718562ccb25cd977a7c006c8915594c2d1d36f9ab9", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "e58db160-a8a2-406e-8d8e-cb74231c88ae": {"doc_hash": "c1efd336418ba022f05f3234bb7969bfa4499041a48f74555c2c099ac1513b9a", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7434f798-2493-47cd-a778-b929874296e8": {"doc_hash": "e3408cb741a15f2bee2183054d7454f29a1d4e560c086e5b955dc16c6f29bc64", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "633f2bba-ec26-480f-9d79-975c04ffd0d0": {"doc_hash": "04367871c291e88e71afa82c6ecf9eeadd74ae982ab953b60e3d2bc49a71d89a", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "d334fbe0-e59c-426e-9a6e-befb5c54b791": {"doc_hash": "875a66b516f6c16f9fd41aad8e07c15fd4afa5e0e2f4e4fafa53b35052e62f8f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "71b3cd8f-be14-4d7d-b713-7701907c54fe": {"doc_hash": "fc46efc7826b4af1087e1555d30e2fb37f36c965b167201c247e38c989e915be", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "16b9a923-09b2-4e43-848e-25738bda4b2e": {"doc_hash": "bdc583c1c93f03f86b3542cb02f8dfaef7453cc3f4418f7e0acfd8b374e94f90", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "80388d17-3f05-4138-9db0-c0534de9e372": {"doc_hash": "576cee4a7bc394217dda816ecee166d0a937a4fbba20f40ab7919cac077f0de9", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "3c045f16-7895-484d-9125-9e2b114be547": {"doc_hash": "74871e4abf586c960b3ba26f8adcd23b72f1774807f7ad617ed15bfca83f5bbc", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ebc4d565-ca17-4ddb-9342-493f947e1587": {"doc_hash": "3e65c037507e787e2c237b08f808b5ef6fb115ff49139e2b9a13989139580570", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "69acae95-56b3-446c-b373-95e3a8b9a3de": {"doc_hash": "5c45719ead1eb774516c6dd5f10c8e64ac7a55319bdb62474f811fe01ec735c2", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "35fa48bb-ac2a-438b-830b-52cac8aeae31": {"doc_hash": "116d451ec524fb17e68467eadaa29252b4ebe74d646abe4f871f2a61701e7f19", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "09211aeb-adbf-494a-8c7d-e383eeea8b57": {"doc_hash": "bcd8f9e757de898240d7b313c9b69c8f3ce0e882773835fadf8fcf03546603cc", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "6c536d90-e27c-47f2-98ac-d88e28f1f0cb": {"doc_hash": "1b1848ee7f4705d9af9b99bcf578ff007cd37f0d21d04f07fab76e20b26d3103", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "1a70a0aa-a95d-460e-b027-e4aafd39ce18": {"doc_hash": "fb14b8e0f1aed0404887ba50e1820dbe44745e0fe468b95e062a1677bb1357a3", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "f4d1dde2-fe8d-4ed8-9238-4af420d2493e": {"doc_hash": "3f8d452eddb15be5691c4a436b40adf6302035f5a8222bf2da99dbb5c46bcf80", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5": {"doc_hash": "01e15cd9b0a82a24eff3ddfdc92bce2d906ab89f6114158f5b9ef917a85b498c", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "5ab00f04-8b39-4a60-8e90-28759ba1ff9d": {"doc_hash": "d197cc3d05d8dd5d0f67a34db0fe7514dcd361da5a43fd4fb5c1f51b65bb5422", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7c576a45-4b8e-4191-891a-ee926ea10c80": {"doc_hash": "52da7748066d01598414eeb5c8a7db889f5670c23e402cc97a521f2a90197b8f", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "df510d1e-884d-4e87-a914-a055d4043375": {"doc_hash": "c06ca5100aa4ee89d3491a7ef3e7842944ca61b0818b6c90d0a5b8a84f01fe1e", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c8547f86-7d0a-447b-b96a-b978382e7425": {"doc_hash": "c3885df8cb82fad394f4ae84967f7ad354bdbbd65677526d76c3d752ab8a4916", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0e4407e6-4e14-4e08-a0d5-840b1684c5b2": {"doc_hash": "7a1d0ca1b1b5f7584ce7e717e20b9753ce881e704e1929787306345323b783e0", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "09e0d412-2978-4486-8af1-58377226b08b": {"doc_hash": "793d383e05e28d71b7bf04b326276ff334c14e6e71f6656740f412d733719c19", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "6259c855-e56d-4dd2-a1e1-25734a79f2fd": {"doc_hash": "d3507d131bf58e0ad0db2063bbce76d237b004d810f9b3da6c4c5a819f609e32", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ceec81e4-b07f-4dfc-a410-604afbef03f4": {"doc_hash": "51bace480708371627846abffbc1ee723a01fc83944cf452fe08f0441632f814", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "f0bccdc5-a3d0-4295-87c7-c3d3926559ae": {"doc_hash": "2b92641fabedb5c611cb7c57fbfc21810fc91d32625f3c0c5677494bd8feb0ab", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "fdf27d02-2c56-4efc-84ec-b3b31389fa22": {"doc_hash": "31bc6b327697c890b2ee42f7cc8699070532630e9a464b17dfceeffa182d3201", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa": {"doc_hash": "cf871a3178df9964756e8f3f5ca1550d536387048c94b48309194e766b8ad909", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "3015bec2-5339-4492-b524-e70ab6c9d55b": {"doc_hash": "1a0705923200ba6c74d461d0c4206d9360840b1cdf3a1fffcb73b40df12ecc42", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "37371d20-a446-4d92-8406-5d0b49d23b40": {"doc_hash": "cc61faec728ea0b9c29fb6c9a178d155dedd2b31fc493bec2d7dd32d117ac60a", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "552a6f72-3daf-4164-92de-56ed4a14453a": {"doc_hash": "079003098d42190fae528cb39b2624125582aed8acd5a0b1cfa5c98e609853ac", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0bea9fbc-77ac-48e6-afcc-00eec95d11ac": {"doc_hash": "e8336d3b8461488277af37ba8dd250beb6ba51b3122b39d7d0e9061b2e66fe58", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "4b17390b-863a-453a-88fd-dfeca48f9370": {"doc_hash": "17c122c0d066699f2f3320669d8df649c6bfc03efdb6ebc5fb79cab81b896573", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "ccec087a-4aa3-4241-9aaa-343a7b5490f0": {"doc_hash": "068d7aa80b98a13f09fb340f0553dc1ba63bea3d44998294a81873ed895e8b3c", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "be20ef15-5501-4c71-aae3-6de74a99756a": {"doc_hash": "49aec73ec4d026fcaa27636d8ccb481c9b3e861c996a3a8ed02af43488be37c8", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8": {"doc_hash": "f22c82a42df57343aad2300fc79f640e96de81c2529728b64d1a16954085ff07", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "1ee6445d-456e-4ed0-8e09-08e463746555": {"doc_hash": "5602251436aefb8e467685a33142e24c5c13827c260a93daf64f7752690403d3", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "67c2ab1b-0c15-4650-997a-be60575410c3": {"doc_hash": "5e796e0b93a38904efbeff7ba3591effcf73937d815b28fb108c050729611ed3", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "46c8d43a-7344-454c-84d8-6950c619875c": {"doc_hash": "273f657aa792689c05a3ec97ac3ca9954821e39a5ffa9b4327e0ecc69f78e292", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "c1ddc9d4-06a8-470e-8462-e7de248690c9": {"doc_hash": "83732ffd95ee696f567b4f5c03d7ca0aa0372a5355acb68b3690ea0768178f15", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a": {"doc_hash": "74943e3b331285796cf9635e00b0f41db20c911825604941f03518c5d9745338", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "86cd532d-e832-41d0-9c36-d28ef9d67773": {"doc_hash": "1694bf93e197baa407cec6e2fda0180b1f45dc109a8dfff786fb37b16a5933b6", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "6f9b0539-8400-4066-b575-cc1dc7dd55a7": {"doc_hash": "337ddfa352705e92456704222f277f59f123cd5b981515486321f23c99636839", "ref_doc_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037"}, "0f1b891c-d5b2-4205-a51d-5246dfebefa9": {"doc_hash": "32bd2eb6933b19c12bc8c6390c77abd0bc372ae33acc9eb51a9b54eb7d3ae7ef", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "a02853ea-8a7e-4278-b368-cee4eda627c8": {"doc_hash": "e8b4c6b92bc2e22ed0608a53517761333a4a589900b3daea82c26cb4b5270099", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "b03ea2c2-9d5f-454e-8ba7-c21625987f56": {"doc_hash": "3e23a831fd1a5fce8fba4ae4738570818702030d0ee1a267eda91c0875ac3ce6", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "11ad8c7e-3bde-4adb-89bf-dafef87ebe19": {"doc_hash": "b5a0cae457debbef15daed5887210d414f8130c61a00b5c4c70938941fbad36b", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d": {"doc_hash": "990095074629d0f98ec6847e8cebd92b73e5b9f0c66f459e1fccabc7f009d409", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "33a638e0-a7e0-4a9d-8841-90e99eef8d71": {"doc_hash": "e843c77e536335973eac86537d1731fd70769270f344740b6b9eff998108347d", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "260d20f3-9678-43f1-b357-33b1c62a8534": {"doc_hash": "c63659886c901b7eeecc4ed439e909968a49027c7352ccb60cd0adcd58ab6cc0", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "3b62ce1a-bc9d-4730-ab4e-741951c40f12": {"doc_hash": "9258598d03cea75d132aa60fa5d6e1baa1b5c5df8b2520c89d27b0a0ecf7564c", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "8118fea2-400b-4115-9a19-5ccbe00a5446": {"doc_hash": "438fa37733ee7345711c8a31ed05d9dab893f639513c3b8f835f277cd3f5536b", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "ca2615d3-8a09-414c-af2a-198d2f83ed80": {"doc_hash": "d9f6e5c64447fce6a4546bf863ff96d7a0538ceef370b5b3410566e52a323583", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "3ce24096-f645-41c5-9c49-fb0af35cd42a": {"doc_hash": "a49c17e1bb3383a28a3df8caf5803290cc5b9338909ab82ea18a68c3f7f4340b", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "c727fd7e-54aa-4ee1-8d62-bdad377dd409": {"doc_hash": "e49e24e2dd5a372e6ead009b24e8367a1d34461cd085804281292b7f811f0c80", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "ea44a111-ff1e-40e6-8a92-114e12e5165f": {"doc_hash": "8f79d17b7ca7b78976c142c5da3a20f0719ae524c9ee588255e4240048bd6888", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "e84bc547-c093-4d6b-8289-cd2948d03a0b": {"doc_hash": "c89579247cd2f72ee6b5fc7a39bfa448ed09640789333f08c8f6b92d338849c1", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a": {"doc_hash": "d8798b01dc2cf6bfa7aaa9710473f394512c4427760ed7e377252fc0eb7b00c2", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5": {"doc_hash": "7bcef4a25234996b0ba0cc3b9c89a829cdaa9bae360aa33d9f3f6315cece8fe8", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "638758f9-0195-4149-8feb-7d3105d034c1": {"doc_hash": "be62e78f12a06f585a45f0a652bae1bc508fcedada40095cf50c6a58dfe4d62a", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "33e6204d-9f90-4686-8089-59aa3987863e": {"doc_hash": "bfde5d2fdb7c7a5bbc78e46d817cefee25505439530d08ee54caa06c0b4d6e61", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256": {"doc_hash": "5872f72513332cf9675bd6b2e297789ce186bdbc0c276c64b35f8a554970f6a4", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "020e0143-9083-4318-9da3-0b8a5b692f9b": {"doc_hash": "b9022d4926868a5ae2d273515b2c0698e783c092c7b9d626e21eb29dcba7e368", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "43366bb9-8544-4deb-bc36-1a5a5807974a": {"doc_hash": "a574ccba534faa21342d6d66408b54270d4b728521762d08cc24934e357edc9f", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "18ee40e9-96f6-44df-b8a7-cf95e5108046": {"doc_hash": "20ef3ce6e37249943f12333aa6bedb6d58abc8131ffa0a67c63877972b4d5f9f", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "7182ef11-99e8-412f-a05a-77f5244934d8": {"doc_hash": "a371f3a0ab0d88d14d54ce341e71cb82a82cf20916f24a2e21368838ea40fa6c", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "cf2b6151-a3f4-4312-b63c-476e7b811471": {"doc_hash": "6d796906008cf16ae894ea212795a5c680f95eb9c5a9f0e83a0681028fc5215d", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "59a11f4a-4941-4fcc-9d15-31494c7edaa1": {"doc_hash": "8704aa5181f0d2238060d4217251a66a136f2a5203f833b1af4fc091ee848aa5", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "c7aab407-8de8-4058-ace9-99672de4f921": {"doc_hash": "4961459724110e63461947f13bc01c7b2eda3247265c423f4bec4d4331b0ee28", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "7640ff44-d388-45f9-9e92-fb74f7bad5dd": {"doc_hash": "7b94d9a708085f556edd600dfc18dd2668633b70f25223f2548f9f70bd22220a", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "1047e0cf-760c-4328-a1f7-bb8e84f77d55": {"doc_hash": "4a675d291e731118b4004366f00f85ac803f77cd89276f613dcf0aa14068fd82", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5": {"doc_hash": "8fa5cd17b3a01338e969ffe98f9935418fb294f75a301d8240a3f41dc93c138e", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "29244be7-65c3-4bfd-ba9c-5b0c3d24a984": {"doc_hash": "33d1ce551e688fe14f946e61eadb087f7d23a5c898815cae7df91ab95b7f08db", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "2f5ee24e-50bf-4804-89ba-a89b470b2050": {"doc_hash": "e587dfb8c14b57bc8c04857a3cc80d6d7c295ba0abd38f86a5233fc8228a8563", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "114ed034-c823-45a5-bc46-307d0cad1bc5": {"doc_hash": "c8f0b32ebf24d7d9705b6e96d1eb2bce8ae2556693fbed88d1a85cd268afc3a3", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "c97994b1-6b51-4043-8e8b-56c17811cc9d": {"doc_hash": "3183b22369f1bd7da7803db5a85783d6d038e1fd96a118b8b7eb20bc2410691b", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "ca3682f6-6fc9-483c-8602-090a652cdc07": {"doc_hash": "471163dc22b2b08886c9365f32b3c0261d15637db576d95ac381fde42fb98965", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "2a6b34da-46e7-4858-ad36-b02c0ed1365d": {"doc_hash": "a3220499eb054732c8c9c6bb10cbc6e538a1aed0b6be09f6b3e8f04afe0a3068", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "10963816-45ba-44d2-965d-f515b05e7b23": {"doc_hash": "b40b7251f507199fc752e2494a5074d17995cb59a91e25326a7f53465033f14e", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "71b0b176-ee12-4a17-9f79-98f4c2f03a5b": {"doc_hash": "ba41e26228c71fb5a8b5715ca10b674a2fd9c0067400af07ecfaf86453956967", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "8a133c59-05b6-4701-88e2-1f70072238b7": {"doc_hash": "b03b1d7dc07e43cd4e0086ba389450bbc233fca72ac074cba9d4cd6871059505", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "8ee5df95-f098-4628-a07d-1c55a8e5eab1": {"doc_hash": "4b58b199338fec5dc24389b05de5e5cc077d9c17721c3e13065e696514a445f7", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "09062f8d-4f7a-43f1-bb40-39fccde61671": {"doc_hash": "0dea8bbf26563cb24da24feadcd08179275d7259e6376379f5b9dcf1f2aba931", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "b94d1d04-5888-4bdf-a788-54d0591b2810": {"doc_hash": "01acd9e83cc6ead8f28b03698eabd1d0c1d576f452f0203738c80da6bb1e31cf", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "d63b859e-29dd-4df3-b90c-7835def36ecf": {"doc_hash": "bf5b078d84734de75201164a0b047b1649b53d6b35805542f27960dc2b983e91", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "46cf8d8c-7c45-4eae-961b-8fcc68e5a284": {"doc_hash": "33c2e0ea02d8206183f6eabe3ede7e78ba7d2129ed4883038ab1156983448ad2", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "75c24ea2-cf9c-499e-8711-279fd5d081a8": {"doc_hash": "6225f28ccded4430d55f5dc40a0b67d88694a0dbcb19c37c0c6f21ac8ce2c001", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "839e9752-ae79-4b51-8d56-7f5f7ccc97ad": {"doc_hash": "13d6962f4e658503e9f2ce3fe4777291ca07dab52996fb8e19e0649f9b9a4c8a", "ref_doc_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4"}, "3783bca2-74e3-4b77-8346-b747d3fb0929": {"doc_hash": "e87d8c24a806b6cbf8dd2f40e3f46c740615e32c5f270b260c29c2d09e3c06de", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "c5e6d35c-16f8-4b96-9f5b-1fef1b373346": {"doc_hash": "cfabc7939e93ab8569a8a3c35a5d8c5d4579ca24aa6d1973c72f866b190dbe52", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "1206dc93-b5e7-401b-a666-13dec3fc7123": {"doc_hash": "2ad4e7d2030238050ee93557b8338fd9d50f17da0a74532aab2d0d55c5e6dcc6", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "bce9b038-2c86-44e0-a81a-9f1b0982fcda": {"doc_hash": "e6d21db02cfd8ca3ceb33e936db60473b552f82974c998cba940b7de29c28dad", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "b97733a8-a172-4e51-9650-5224170cef27": {"doc_hash": "95df0066cf80a3e4133bfbdfc6201c20e5d609cdbfffab6bdec7dfdf5a2cf591", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "72d58255-c070-451c-bfe6-30b2b09ac9ff": {"doc_hash": "285b382fad52d82ff9db188ca011c831bae910b66165899571fb96becacaea8d", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f": {"doc_hash": "1cf4ce16a94df8ce90fa5b77c39fe330d0545f7af35213f5f1d8c70b5b50565c", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "d337958c-0629-48df-a615-d283de117337": {"doc_hash": "df209acfa59e54f5478f444685af08edeb2b51da45480d04c49ffeafa41edb16", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "c88be701-91ca-41ce-aa46-2dd9476dc5df": {"doc_hash": "db94c159f8ee69379cc5d58d411b06eee6058232b9b5f004e959f00bea3a2303", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "8f584879-68ce-4fd4-a180-57df559f707f": {"doc_hash": "c7d919357c19d5f0e7a053e7ccebf5e00b9e291e4369db558d46798cff76f37e", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "884039c6-0f37-4044-a72f-a969f57be5a9": {"doc_hash": "c4c4da6066cb72a722c33cee868cfcd5080390b8b73048742477a69ef49bee80", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "5ffd685f-5ade-4c0a-af64-cae596918a8c": {"doc_hash": "581cb92dc20064dc5d3fa73d9dae7f672d471969dff9fc5c0d10fdee0a12409d", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "e3e00c42-805a-4a29-89ab-4052f58a07b6": {"doc_hash": "644a2add5d8d7a6fd8d214660c70bc0e3fb59accbe1623763698ad7d83556595", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "23c90370-f267-49b8-95dd-e98132ab8228": {"doc_hash": "8099fe6a49b2bc37cc14cde2339f34a171076da9e43a4ee5522038ad356053d5", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "4b71e743-c993-4693-975d-9cee2c7b23c6": {"doc_hash": "2586d28954b9eebd616a1e5974cea49e2abafe12139564e0c49bf85f68c477c5", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "339504c9-91ec-44b5-900f-23aab09e5183": {"doc_hash": "fe324be651a374272961ef39803e754c5c5a19ecb5d38327cd45fe9c60049499", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f": {"doc_hash": "609c5764cf0b7c2f990a412052a73a6832e7f4b9f182acdec5e273337bb712a7", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "249ebcdc-70e4-416e-b102-2c39abe89ac1": {"doc_hash": "8f10702c4b4ece9bf3fec31ba6cda01ef2fef245aec65ea233a32f6020abff50", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "1bea65ee-fd19-459b-979b-a51f73888ec8": {"doc_hash": "999b96f5cb266c16abcf3472202385e09a6738aaeb374acd8a0c769b2a1c3d17", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81": {"doc_hash": "6d6bbc7cf3a8869772e809e44804a16414501e2743080f9da3fb517a878267ef", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "7a9932d5-abe6-4ac1-ab09-7e2a5be956da": {"doc_hash": "a03ccab10bf81b80e798bf0155357441e9eca892d680863174a4c810aef4adb1", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "f741db7b-4c76-42ba-9231-802a876e403f": {"doc_hash": "ae50ba05ddbd93c09caf2875ffa144bbe3fefb70382cc3ea392f23393588fa75", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "157aa93f-aa5a-4e96-98db-960647b9095c": {"doc_hash": "effd82a0036037084d0ab79acb6d2c9cb2d1d428f9dee27f39ead0686740a015", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "89c5fb97-82f0-402d-b968-c07aa9875527": {"doc_hash": "1b85d4f8541a0719930d59296c7c02f55ac22043b14e1e09b7a52b812a50f35c", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "0b30d152-d86d-4389-b67b-07c19c906bfa": {"doc_hash": "8a6e0b13603fe7db10d855e0918ef54dedc033540389ccbcc313d2387179edd3", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "0c191653-c6f6-4933-8640-cbcac4171d00": {"doc_hash": "7b0b3760ccba2be6008436634f8b326b2e5576287ef67037a9d6514643dba0cd", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "5698edce-ce59-47c5-8505-22e1c59df110": {"doc_hash": "b2ad004a7622f2187d8fc00929b672a95c704e55f207a19186280cbf37da881b", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "ccef3225-ca0a-47a2-a09e-a38382d0c333": {"doc_hash": "6007a0b9e7601d536927a749f62737d2248b3ce8fd6c44cec62f4bb85e4101bb", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "a6d2e27b-768f-46c0-a0df-acefb430ecf1": {"doc_hash": "a7d302d52d4d9c394f4f6b19f7dd56bc08e0fcea86c399f72aaa56cdecd116f9", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b": {"doc_hash": "4cc0fd15561fc07b1464c0493a8bfac1f79be29b230f33358ccbc9a990fa0511", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b": {"doc_hash": "db1d1a3ce4b4c7ac2897fe056a65357c4773c64abf267a34d8f9829c4df3b65a", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "eed5f86e-a7b6-48b9-8f62-abf533a9108b": {"doc_hash": "2c8ee0151a2e3c210621c42207458251232c5b717c8ac0458ff257cc657c99dd", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "d64675ea-28b2-4067-aaf2-6e6b58038044": {"doc_hash": "5897b2d42803dff695a01d2df8f74282679f2255eea92a6a9e752f3ffa453e6a", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "300806ea-49cf-4f0e-acd9-49abc6d7280b": {"doc_hash": "340763d845e49b26165cb703303d686c8223a148debb9ba540abfbcb926d6720", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "2f3e6539-3fb5-4e10-b79f-48db9e00335b": {"doc_hash": "b26a0221a7d6e4242344ac037db1b5ca6c6210c6e3178c30d982add531451368", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff": {"doc_hash": "286fbea625bbc6f1807d4861def679d203b36103d0ed4cafb204a87412388a68", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "54429056-0727-4e3c-a058-a639da294d79": {"doc_hash": "0ed8a9866f84b66e959d53f7c4cf9a425071c02963faada0670b7675a232fb68", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "7ea3c278-0d99-44ec-ac77-cd372b863cca": {"doc_hash": "4fd4c402bb8573d9b7898b7a17969b60b1a7b557ab394e1fc08fcf990f6e810a", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "e0e550d0-9ea1-4817-9865-d509e16ceece": {"doc_hash": "cb81760d7bcc164f19eca3d0c9cc2d58ba1f5f61b4c2669612798069a03491bf", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "330a8a26-65a9-4f35-869e-89610e3a1d2b": {"doc_hash": "b883e3fc3823623951152a820c769563b3c6179bb4311fabb6b417c0598f0a2b", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "16f6eaee-c38e-4bac-a715-511265f91f25": {"doc_hash": "61ab3b3a011e249d31431ecc50bc4559a095234452c94f65478e5f98ce7a7252", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "1439b63d-8b30-4436-82fe-f476ae6d62c0": {"doc_hash": "c1bb8374ec83e0808155f580206717497c3b1c57a29520a95f8eaed55b1bfa5b", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "98650303-7587-447e-8e8d-e6dac19ef856": {"doc_hash": "7c4c6d3ac4594112f5d2b0cf6acbd6f046456f329b09538ab17092aa3cf69e47", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "2dae400d-c2bd-4497-b007-beac2ea39abd": {"doc_hash": "4d8c6256b17c2bc6164c8d5e69dce78cccb3a06cb18953dd6d4f22c7f0d21bde", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "a3d4f97c-2ec9-4014-af81-5f265a4492bb": {"doc_hash": "24abf4f7833351bf284f8ff91044c71859ed9e487ed858805c8c295604e318ee", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "56faf331-26a2-4a00-a188-57dc820d0c1d": {"doc_hash": "405c6a3991efe2e95e146afd0ce9fc660b086e499d7b0c828befd3c0b0f53482", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "14f00316-9ae6-4896-b48a-062e8aa3dc97": {"doc_hash": "10f3b0d83ba4e55e23008186fc0b2f2d8b6dd50b8ac41e0fff0a5dc605911a97", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "3730a016-7e1f-48bd-bddc-9d711be904b0": {"doc_hash": "b154a5a6d03810b44e2d89109f0e5aca276a757e16b327264f6508fafe811705", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "af36f640-af93-4630-a6e6-107db2de54cb": {"doc_hash": "ffb8dbd50b46673e603b869d8e5aabe338fbb588f1b91a3d34d4eb5f5f114c19", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee": {"doc_hash": "b0f9b3209272b2424017dd6f79f965231f32d06a2c00b8196604191a8de2214e", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "99c36b9e-1545-4071-a6b7-82e25c631336": {"doc_hash": "ea843cd9d483c433480781c4d576b94242adeb88a7612a41d4e0403c7d5710b2", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "3f30600b-b14d-4275-95ea-0a138c64c092": {"doc_hash": "4356ddfd3bc6aa18d768a982f631ed903b936f6fb924a35994e99e39477b58d8", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "0a810c62-10a6-4b25-ba45-3ab734e444c3": {"doc_hash": "3c0ed59ed9a0108aeaa4b44a8bb5ca8faa477698da05a58d4a7274390ff9c30b", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "2ce3790a-0f20-4f13-b30b-ce817588fc3a": {"doc_hash": "168b1d0c05cc66a53304fbe67687d74830df612c276831664c7e4ee359005891", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "18289b6f-899b-4fce-8ba5-0797e27470e9": {"doc_hash": "3f4f6e888c4eee9b701b4777543ca996f2e494c3aa6f0a3c3d50fdc375815695", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "895982ea-fdb7-419c-8cb7-e7318fd8f4e0": {"doc_hash": "f13042371a5254071f59e5f8ead2ec854fdfe4ee121fbc477d78b0c32750db45", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "e3609c2b-53c2-41eb-8d75-5e8888f306c4": {"doc_hash": "0fc0222a30b4e465b34a40a881db1a356c8b963ec6b5726b4a0dc0acdd157808", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "649468f5-92f6-433a-91ba-f15a267f8eb1": {"doc_hash": "a06f7fb5991532277e8acda2dd9edc2f747dc28c5aaceef0cca28fe4be749d67", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "61ece166-2a74-4098-9a20-8718fa98a0f4": {"doc_hash": "4d74bf5a56fdb8c24392d1b903b28b9c7a382adde841720aef04f9cd372bdd66", "ref_doc_id": "6e50c860-0859-4310-8a53-f13a2d30632f"}, "7e527aa5-7b7a-4baf-b15a-0bd8016e8ccf": {"doc_hash": "41cfb32038ab2e701e9a5ee7c9d39d2de846bbc489090e87076a34bb227c347b", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "b7394b63-132b-4609-95d2-c5346c5ba1e9": {"doc_hash": "5c106a66a87d65c008da993395636abc7c4b285f44270c2f851d9df2dbe0725f", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "452ab813-7a58-4442-bd6c-83a15b3782fe": {"doc_hash": "e7eee594e51a1dfa3fa35bdf149f6f1f15a15ed34f2185bb132d62eb13644f73", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "d06beb7f-07d2-40fe-a69a-f320e027778f": {"doc_hash": "1325117a7921df04bda839a8d5c9a96b1bc171f3e7e0755cca2223f6762312ba", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "1cb5d930-be84-4bc0-8756-283e5aa61291": {"doc_hash": "8a479f0e2a208bc0987b90db381157271a419b5b0fc3414e7af1205b5015296b", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f": {"doc_hash": "710c074b28175fecfe365ce629fc6ddd63bef4c56887f390d60fcddd2de9a035", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7": {"doc_hash": "86857c6534e1e049f933a8f8c62497081dd2920020d0f8df1dcf2eddea892d40", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "7b3ec38a-82d6-4146-a8e0-5e74b04234a8": {"doc_hash": "827e73ef9b566e7bad8a8c2cc002cd1667b2c8e54af45f76f2e09926276d79fe", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8": {"doc_hash": "723a4d268c8a9234498cc7e3d0a305bb15908e5ca1d7a1ac4d7451067a29b76c", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "8f70e098-3d4f-401c-a2d8-5d54e4cd0639": {"doc_hash": "6aacd49b1e462476f289d9c1c2abf40806af53051ecfd043bfea8b0a986363a7", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8": {"doc_hash": "76986e8dbea0c2d82346dc38a62c34c62d81cb968ba698f6f7a174390da30d20", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "e9081ff6-c98b-4a7f-b5f0-543e167a5201": {"doc_hash": "d6e20716ec45b1096c6a4211962e488d957817f605ce787f5818cb5e8b9a820d", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "49def48b-1941-470a-812d-9170e20b2412": {"doc_hash": "e02fd9b6eea06d78bcf8047edff08ec4bd6b3f824fef180fd695455afcf34895", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "de837f07-74fd-464c-a298-ca010b9dad9d": {"doc_hash": "e0979a941fbaebb38f27fa18eb27e1c9ffd53256ffff22e8a4375326f5653b4a", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "128b34cb-c03d-4f9d-8471-eb955a797086": {"doc_hash": "a2ca159ab5732fcf756ac4541253e0ad7e67d1e634b40f51ff9528055ea9a779", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "2d4bd48d-927e-41ec-a758-12cd2602ca1c": {"doc_hash": "8338571ae5b68725c1670ed080941479259b9318a594d0cf444815b51c2ac16a", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4": {"doc_hash": "6cb42d2cc7345e5f9de5eced7fc4c69a12cab6083bb40dcdffcdb4c23d694f1d", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "5351a29f-f6de-4be1-81cf-d76055352273": {"doc_hash": "c2bce3db93b4405f744fa7260c7b9bf3a94bfe3671054df135591d1035734077", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "098ad492-e74e-44e8-9a68-dc99d853cadc": {"doc_hash": "86307bf8fa18b318ffa3fe61e862cd80a4d430ca25a2a132cb44472837a24d87", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "0384004f-2943-41c9-8d25-42758b2dcac3": {"doc_hash": "547cd3aa940b0ae41bd72246b9121125cc3305c1dc10b3750db089cb6589c3f7", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "684c6e3a-36c0-44e6-a75b-ca2c84261e56": {"doc_hash": "0f49cc0b201b3312a764b6fc95d98b83981b957a6caf2c384261e8d3e90b8da1", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "0d57ba18-7743-4b02-9f61-aad7cc5dfab4": {"doc_hash": "764593a76a77879d7f7c7f493bdfecca2e8977b5dc266b00901f4394448098f6", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "fb5d9572-b690-45d1-9723-2d06e5668f98": {"doc_hash": "f75c43373c850c070b159a5d0616f00d3c457d26467ea1da48c9d36656fda699", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "8037fe68-a0aa-41a0-b2be-1572abd50b60": {"doc_hash": "4fcbb592f0d56bf3c683587cfbd736137dcb513702782468bde5294b82423172", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "04a95921-3c36-452a-862d-b1c6502d2bd0": {"doc_hash": "ce67ad8068856e8a079ce6072c9a60b99ab056bc38d81e4126a978abf7547d11", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "c6762b22-01cf-41a9-b6a9-c50d18f1b46a": {"doc_hash": "08263fa614393477780cac115e74a1d199232f1837703680b2ec50e911ae7bbc", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "1359fae0-c0ac-4249-9c0c-a410bfd1fecc": {"doc_hash": "29da90e4a2e1281c449d8e722374896c0e11629e4633c87525c2380df892dcb8", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "7cc3c5d7-c3b3-48e7-b030-36173754757f": {"doc_hash": "b168bd105996aca3c95d61c93f524479decbc4c31aaa19bcb7881045402c0105", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "669470b0-958c-4c52-9903-cf76af778616": {"doc_hash": "c9e8e90bb691162f2e4c98e6e64507abe53dda64e1056e4fbf551ee27b6eb53e", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "7bc1a505-6730-4f88-9a1f-f83801fc462d": {"doc_hash": "a8e2eedc24db62006496b9e14ec8903c2acb1a6a6025e57fedd95e2fd5b0da3f", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "5adda293-a79d-423e-beca-89973e3cfbfe": {"doc_hash": "39c5f7158b1f417343c60b6a43a73c7a7fa92e74ffd4e1069335fbbbe9d4a42c", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "d8ec2f21-fa2c-4c70-b847-ad382728a3f6": {"doc_hash": "b2e4177534d3b30c5787b2cc7ee6ea073afe081fadf43a85aabb12655578e71e", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "2a6e98ba-914d-4f8a-82d9-493dddf00598": {"doc_hash": "369d5a36479ca4a6257b5e6a875fc384b211358aaa5f7dd03d04581abf98f605", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "4c37aab4-0de2-4a66-b058-ffcb33ffeda7": {"doc_hash": "6f984cb42f319ca5807fca2409becaafebb60d5a2f422fd7c3c5b0195e89e230", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "d1498321-9180-47c2-a360-2082ea4c4100": {"doc_hash": "0e66efeab313e4c096588507f4fa0caabe80ec682c2e074cea1a9b506fd6d20a", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "2ae10f68-c4a6-45ce-be29-72ea79e20e7a": {"doc_hash": "b577aac24f98499dcae6e2d87a10494aa96fdedefcc3119f62d649875754ca03", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "33007c73-4a9a-484e-abf7-c529794c81a0": {"doc_hash": "ef4cafe439be5e157971b03446c021be0e24d2e0ae1bccb9a7a3cac374e3e204", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "e18e7e00-b7c6-4086-a3d0-731676549324": {"doc_hash": "b612b83b6465e4826e601b12e09af314e458785e9c4e24ba74712ab01dbf001c", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "86bc32e7-7002-4c38-af9e-af9427c5c1e8": {"doc_hash": "4c30cbf01003431e13ccbda101d70bfa6451a73535e9410af3439777cab8737a", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "2679fd59-9c0b-4b35-ace8-4152c3b754f9": {"doc_hash": "0792a4edd2a49d1f4cd900cb78a3b941a9f8a7fce4906f3ca9744bedd0d68739", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "97efef44-9d6f-466c-9038-ed2fb9861f20": {"doc_hash": "02ca099815bfa876d65b869e595426033d47f3d6afc55a2fe7d66ec9f4df5578", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "29a82e94-1150-43f6-9f29-0fb2e87c5252": {"doc_hash": "4bf5b7beaad1be91bf95dcb35d26145030d779d510a1e0909a3b3eccdfe47f38", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "51fa97f9-a208-45c9-bd7f-2740cf91f9bb": {"doc_hash": "22451acdff491bb9fb6c1f8181cd1676f2f22ce6511bd12dc1eecf5047288e7c", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}, "04c5cbf9-c170-457e-b491-3a0e71aefd69": {"doc_hash": "de141387fdb889d2eb4c74b8f54fd46c8d93a9b7be14ab233b7f218a0cba27e7", "ref_doc_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c"}}, "docstore/ref_doc_info": {"7de0154e-3b9d-4e45-8673-b023febe3c30": {"node_ids": ["eff895a0-e850-4669-b7d2-48b335bdca65", "c699bb0e-f289-4ab5-9893-8dcc4c8f274e", "81de830d-7199-44f7-a9ba-fdb5b8850d6c", "a1b51596-326b-4263-bb39-1e7639bc8bd0", "41a14587-a647-48dc-9c32-d55bcd87c0f6", "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7", "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83", "70b70c13-c34b-457d-89f1-98a9e3fa80a7", "77511500-900f-4415-a51a-104e7eb78c48", "763e5e0c-65e2-40d3-bb3e-096d0642a311", "1ce3fa9b-5ed1-4f91-a503-5c565a396da0", "e93ba833-a8de-4521-9662-43f8bc39be9b", "80b100ce-8a82-409e-992b-833934deabd3", "71f4e635-597c-4b3b-9c7c-a829195b1702", "637ac51d-49ef-4ea9-89f7-016aac301edf", "53545215-b49d-424b-84e0-55ab74d1a0de", "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8", "4bdba251-a594-4ffa-959a-2a743dd494d6", "bfea95c2-215c-4221-a48d-0879f40c672c", "f3a42e64-3ba1-41cd-9470-45adc356732e", "398a91e3-ed3e-4591-9aad-7cceb071fad8", "ba967449-fac4-4684-ab97-fc63b86b4365", "9bee0209-beda-4748-a654-f48e4b65bf84", "f8acc35b-9d69-45bb-b855-e4dd0758f9b5", "637394f8-896a-4457-8886-44109cb1d276", "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90", "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67", "e6745567-0530-4736-bc4a-f8591739b70a", "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18", "5ead539f-bd15-416d-b2cb-80f7ab905fee", "ede4c553-e278-4f1b-a25f-fa2274f66eb6", "d355577d-2f90-45cc-a7d8-4a6439c352e0", "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c", "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70", "2fba4ce2-c81b-40d2-bce0-7a12ab75567f", "835bcdd2-168a-495a-a464-179f8aa2ca70", "0f694c65-1411-4846-a5d0-2b8fc3803278", "6c1e9b43-ad5b-43cd-b871-6d87f691462e", "12cc2c74-f75c-4799-baec-68c537b8d667", "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c", "e5da99ce-899c-4c85-85f4-6ce88e48424f", "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e", "831572d5-f925-4b58-9df1-ae938b74acb8", "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab", "8051ccc9-38f8-4dab-ad84-13473e3afae7", "d47c9834-6c8d-4c58-855d-7d57a3a96b92", "b0798139-ad4f-4676-ac66-15df2f5a5c5a", "1a09560f-5496-4496-af00-6992d8cf3e4c", "b32b8215-cbce-447e-9449-f2a2b77d1b43", "0789d1e5-6e94-4900-b20d-bd61e8d690cb", "ffa9ed11-530a-43e5-8fcf-51f72cb5971f", "12a8665f-284a-4a7c-ada3-a1defdb80a41", "61f8bcea-bf4c-4328-8997-98e046bc8743"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}, "c576fc2c-b1db-4abd-83dc-d89ed3c81b11": {"node_ids": ["807817cb-06dc-461a-a0b0-18851665bf43", "80d52e56-f761-47e4-95c6-74daf7d289c8", "0c0c2524-768d-4301-aee4-c96ce01c46e3", "5492fa06-d59a-4118-a7d5-461627d53796", "a91af3da-3a51-441f-a882-bd1e031f01fd", "1dac587b-7d06-4aa2-b503-ad4beb38d907", "e018b138-e891-4624-901f-4b7839b40ed3", "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca", "3394e636-ae71-4ee0-be0c-39ac6beae551", "56cc79fe-e4cc-40a8-9600-57a8aac86724", "19a64d02-ccbd-488c-ac40-7d4d15925208", "4e81a266-9cd2-4387-a302-42ec2229acf7", "1c83d6f9-7574-40ec-a53c-053389189571", "42023691-822f-45cb-9f16-85775099ad3b", "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b", "5f474dc3-3a36-4237-848f-f9f3e1335b76", "eba54af1-14d5-4f93-b21f-701e0f055df4", "7daa55ce-8eb3-49d9-a087-5f95ae19a941", "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30", "62a87b18-e78e-4e3f-bbff-666b8c5b90b1", "04e5d2e3-d9ff-4df3-83b4-1658b4418f65", "a65d2ead-61e3-4984-bac3-34586d385cb0", "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760", "69fc89a8-e561-4c89-af30-89b83d8b87dd", "7ffad171-3c3a-4d6e-90d4-1a54625db300", "78aeb346-49d9-498e-a664-4e573eacfc1c", "d3202f94-b234-4111-9e1f-776cb8811b1d", "01d7d455-ed37-441c-84e7-a882638530d9", "02bf118c-0051-4dd4-9753-43aa4c243006", "ae4554b2-0506-4b96-893d-6795e5473bb7", "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794", "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8", "0a945e53-ea82-43dd-983c-f40f259f8e51", "6d3efe32-8bcf-4a43-b02e-62941a1a24a5", "52bc1c89-f422-4dcc-b5a1-94697de969b7", "4670e407-2330-46b9-973c-f4f8520c614e", "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf", "ec13eaeb-f179-444f-867e-fa8a0db4ec90", "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c", "298d4478-3c0d-4bbd-a846-66f38b725174", "26bb97a1-8644-4569-9ba3-62f71311f640", "33b67e10-82f4-471e-b856-8851eb320b99", "08f218c0-c59d-4827-b971-6cbc7c6bc241", "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee", "893295f1-1612-4b49-9e50-ce5125bd1d90", "bfee077b-5da4-454c-957f-a2eaf0143bb8", "e2f62c5c-2687-4c48-a1fd-0601b9724538", "b3d2596e-25b0-4833-b25f-ad7b320b13e9", "5ef2b830-7fc0-461d-a950-0ad96c57422e", "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3", "fc146b52-8de7-4fda-bde9-475c39f32dfa", "112bfe04-34ae-4fbd-a20e-4b86b3af5abb", "4aa21f4b-0f43-408f-b6d3-31c66bd09e48", "7391ae77-6860-4ab5-8df8-594b1eb5a3c9", "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73", "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1", "868a1957-514b-4ae2-80af-eaf25ff46dfe", "1538348b-94e5-4bef-a6f1-a5c236b5838e", "80c9a6d7-42b6-4ece-8645-a826a45fc6ef", "b54e53d8-8a5f-422b-9662-f108c080a21b", "d79be4ed-9127-443f-b239-50c0fda52944", "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9", "72af5b13-bcb0-4db0-859c-820e87512bba", "eec4cbf2-1349-48ae-a4fc-67c36bfa852a", "078958b9-b94b-4ac0-8784-66e7fd942152", "9afb965b-4378-42d1-874c-eec20e725d4b", "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7", "66858f81-5e83-403f-ad20-20ba15c0b404", "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf", "952a791a-4658-4ca9-a8bd-1f0f52ceb296", "f87486b9-9862-48b2-90ce-27031286b47c", "bb70e5c3-d737-4271-b595-7c664b9b428d", "a2a392c8-d978-4bb8-a7aa-dde2afa8278b", "f41bd766-311c-403f-b26c-998c9c148920", "9fc16026-c989-4967-b861-4c24888747b6", "ac38c4e1-808a-4fa7-b0e7-61a9162790d9", "b36e6e7c-d5b4-46db-a07e-388bcdbc8506", "35465509-56ae-4465-af97-e1666cdcd6f4", "f4b89e55-5ca3-4b9e-a364-9d174f71f078", "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1", "245113cd-4787-48f3-862c-28894c2bd79e", "710522be-6361-4b6b-b724-86ce79592232", "ca33393b-af52-4a43-b3c0-0f0102b767c6", "bfd35ff9-0ff9-401f-8bf0-65b68973787f", "2f544f6a-ab13-4858-992c-3ceea8a3e688", "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f", "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6", "f4107aca-cfee-43e1-960f-aaadbb51abe2", "c99d59f0-a319-4dbb-8ea3-cb77d814b054", "a1f133c0-45ed-4898-a634-6cf927f1df03", "264dcea0-cdf9-4747-a732-a12801438545", "e19df090-0b48-4a73-a405-2acea22c24a2", "cfeed0e1-9eaa-4da4-b859-2810a848d07d", "6de81924-3a1f-43f1-9b84-80d16468d179", "20e3e088-1968-4995-85e7-2494a0253662", "4002eefd-9ab8-4bf4-b804-db332d14267c", "8bb57993-603f-4a92-8fe4-6fda48665a68", "410c3a09-4c30-4117-822e-ba0ebb3f2aea", "21485ede-86e6-4e3b-bb7c-c4b0017c217d", "35d51ac0-a6b6-4883-b08f-ee263f10c2bb", "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb", "675502e4-b6c1-47fe-b1e7-3fa9c0ca544a"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}, "5d6f3674-6d30-4bb7-bc38-f7b9ee628037": {"node_ids": ["458e5211-267a-497e-8af1-a24777294f57", "27eefb48-d4ff-4710-bc06-7c60ca746d2b", "e3fb3cd1-8599-4725-9e29-3c91c730f4fa", "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0", "8d8c9c3f-1082-499b-89dd-63ee849634e8", "11c64b71-68af-4612-9f0f-28f40a4c13d2", "7eede3b5-5fe4-42df-aba8-a0657a2a0f26", "ad6602f6-5660-4fed-b0dd-c185d66fd5d1", "c415fb83-9227-4fb1-a310-f1908885738f", "0384e1e6-1134-4c0c-b754-7dfe113912bc", "c0850af2-4727-43a7-bc63-90c22fb3cba8", "0182c0ed-a000-4b7c-805b-89740af1e4e3", "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c", "0616145c-e48f-4abc-9759-b0a3c32cd2ed", "7298fd7a-3cab-4554-a47f-c25514b28dda", "84b91c94-353c-49ec-b8bf-057403d13962", "2c00639d-fb96-42be-93b7-8b6280d07790", "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518", "078829e7-a738-45c2-80df-d3fc1fbd0bb6", "9bb39904-be6f-4716-a8a0-2de5c3cbfacd", "a1d2149b-71e9-4137-980e-f40a6d0ae969", "58094209-3599-47f0-80c3-7aa23f88ee6e", "9747cf7b-c50c-4e74-8f6d-fde50ab03a47", "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b", "026354a5-e824-4b8e-bfe4-70d9a12c98ee", "f7da756d-9189-4731-9b2a-ad6cc77abefd", "d167bfb7-3bbb-4470-b063-1e1d8491e56c", "b01b4dc1-cf98-4500-90c1-a179a66d3eca", "b2dfca25-8512-4aa6-bf41-c19af73a3f0a", "d6d00942-d574-49ac-9bf0-88a2c505e6c5", "7a91b32d-dfc8-4709-8727-3ea7a092c079", "4aa0fc68-3210-40f6-a326-52a5aa701f9d", "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8", "6b21d36e-041d-4081-a1ff-6b6d9473a385", "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91", "031acbd6-0b4e-4616-9af2-665b953c09c1", "fd001662-c246-4a0e-94d6-f5c939e4222a", "f419c400-91db-49e4-b0cb-382a358703bd", "c55a790c-f53c-4875-883e-fd4ba6a476ab", "845726fb-0aa9-4b32-a21d-46590a41497c", "218cbf28-22b4-4ca8-884b-e037f8fde748", "8f3227fb-09bf-4dca-b0b6-4a41faf58add", "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447", "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f", "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f", "dba4a441-a113-4b70-a792-af801b07d0c6", "ff95b4f5-3d48-4914-9542-350f01b5a36c", "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1", "5a8fd19d-8957-4c68-8588-43846ed06dbe", "63097d21-b1cc-4596-89a9-873b353a7bc2", "20314189-301c-4151-858b-fab52b04e217", "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a", "8f5dc369-abd3-494e-98ba-ba89fa1a056f", "27b9505c-a7f6-460a-af64-4e6cb619e973", "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9", "534dff40-3882-4cc7-b02a-66645a520b19", "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae", "8576fadc-b57d-4b71-80ee-83b4527a623d", "7b751645-c617-44aa-803e-abbf102dea21", "099a86c6-1090-4228-ba8b-ed4696184c54", "e58db160-a8a2-406e-8d8e-cb74231c88ae", "7434f798-2493-47cd-a778-b929874296e8", "633f2bba-ec26-480f-9d79-975c04ffd0d0", "d334fbe0-e59c-426e-9a6e-befb5c54b791", "71b3cd8f-be14-4d7d-b713-7701907c54fe", "16b9a923-09b2-4e43-848e-25738bda4b2e", "80388d17-3f05-4138-9db0-c0534de9e372", "3c045f16-7895-484d-9125-9e2b114be547", "ebc4d565-ca17-4ddb-9342-493f947e1587", "69acae95-56b3-446c-b373-95e3a8b9a3de", "35fa48bb-ac2a-438b-830b-52cac8aeae31", "09211aeb-adbf-494a-8c7d-e383eeea8b57", "6c536d90-e27c-47f2-98ac-d88e28f1f0cb", "1a70a0aa-a95d-460e-b027-e4aafd39ce18", "f4d1dde2-fe8d-4ed8-9238-4af420d2493e", "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5", "5ab00f04-8b39-4a60-8e90-28759ba1ff9d", "7c576a45-4b8e-4191-891a-ee926ea10c80", "df510d1e-884d-4e87-a914-a055d4043375", "c8547f86-7d0a-447b-b96a-b978382e7425", "0e4407e6-4e14-4e08-a0d5-840b1684c5b2", "09e0d412-2978-4486-8af1-58377226b08b", "6259c855-e56d-4dd2-a1e1-25734a79f2fd", "ceec81e4-b07f-4dfc-a410-604afbef03f4", "f0bccdc5-a3d0-4295-87c7-c3d3926559ae", "fdf27d02-2c56-4efc-84ec-b3b31389fa22", "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa", "3015bec2-5339-4492-b524-e70ab6c9d55b", "37371d20-a446-4d92-8406-5d0b49d23b40", "552a6f72-3daf-4164-92de-56ed4a14453a", "0bea9fbc-77ac-48e6-afcc-00eec95d11ac", "4b17390b-863a-453a-88fd-dfeca48f9370", "ccec087a-4aa3-4241-9aaa-343a7b5490f0", "be20ef15-5501-4c71-aae3-6de74a99756a", "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8", "1ee6445d-456e-4ed0-8e09-08e463746555", "67c2ab1b-0c15-4650-997a-be60575410c3", "46c8d43a-7344-454c-84d8-6950c619875c", "c1ddc9d4-06a8-470e-8462-e7de248690c9", "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a", "86cd532d-e832-41d0-9c36-d28ef9d67773", "6f9b0539-8400-4066-b575-cc1dc7dd55a7"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}, "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4": {"node_ids": ["0f1b891c-d5b2-4205-a51d-5246dfebefa9", "a02853ea-8a7e-4278-b368-cee4eda627c8", "b03ea2c2-9d5f-454e-8ba7-c21625987f56", "11ad8c7e-3bde-4adb-89bf-dafef87ebe19", "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d", "33a638e0-a7e0-4a9d-8841-90e99eef8d71", "260d20f3-9678-43f1-b357-33b1c62a8534", "3b62ce1a-bc9d-4730-ab4e-741951c40f12", "8118fea2-400b-4115-9a19-5ccbe00a5446", "ca2615d3-8a09-414c-af2a-198d2f83ed80", "3ce24096-f645-41c5-9c49-fb0af35cd42a", "c727fd7e-54aa-4ee1-8d62-bdad377dd409", "ea44a111-ff1e-40e6-8a92-114e12e5165f", "e84bc547-c093-4d6b-8289-cd2948d03a0b", "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a", "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5", "638758f9-0195-4149-8feb-7d3105d034c1", "33e6204d-9f90-4686-8089-59aa3987863e", "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256", "020e0143-9083-4318-9da3-0b8a5b692f9b", "43366bb9-8544-4deb-bc36-1a5a5807974a", "18ee40e9-96f6-44df-b8a7-cf95e5108046", "7182ef11-99e8-412f-a05a-77f5244934d8", "cf2b6151-a3f4-4312-b63c-476e7b811471", "59a11f4a-4941-4fcc-9d15-31494c7edaa1", "c7aab407-8de8-4058-ace9-99672de4f921", "7640ff44-d388-45f9-9e92-fb74f7bad5dd", "1047e0cf-760c-4328-a1f7-bb8e84f77d55", "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5", "29244be7-65c3-4bfd-ba9c-5b0c3d24a984", "2f5ee24e-50bf-4804-89ba-a89b470b2050", "114ed034-c823-45a5-bc46-307d0cad1bc5", "c97994b1-6b51-4043-8e8b-56c17811cc9d", "ca3682f6-6fc9-483c-8602-090a652cdc07", "2a6b34da-46e7-4858-ad36-b02c0ed1365d", "10963816-45ba-44d2-965d-f515b05e7b23", "71b0b176-ee12-4a17-9f79-98f4c2f03a5b", "8a133c59-05b6-4701-88e2-1f70072238b7", "8ee5df95-f098-4628-a07d-1c55a8e5eab1", "09062f8d-4f7a-43f1-bb40-39fccde61671", "b94d1d04-5888-4bdf-a788-54d0591b2810", "d63b859e-29dd-4df3-b90c-7835def36ecf", "46cf8d8c-7c45-4eae-961b-8fcc68e5a284", "75c24ea2-cf9c-499e-8711-279fd5d081a8", "839e9752-ae79-4b51-8d56-7f5f7ccc97ad"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}, "6e50c860-0859-4310-8a53-f13a2d30632f": {"node_ids": ["3783bca2-74e3-4b77-8346-b747d3fb0929", "c5e6d35c-16f8-4b96-9f5b-1fef1b373346", "1206dc93-b5e7-401b-a666-13dec3fc7123", "bce9b038-2c86-44e0-a81a-9f1b0982fcda", "b97733a8-a172-4e51-9650-5224170cef27", "72d58255-c070-451c-bfe6-30b2b09ac9ff", "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f", "d337958c-0629-48df-a615-d283de117337", "c88be701-91ca-41ce-aa46-2dd9476dc5df", "8f584879-68ce-4fd4-a180-57df559f707f", "884039c6-0f37-4044-a72f-a969f57be5a9", "5ffd685f-5ade-4c0a-af64-cae596918a8c", "e3e00c42-805a-4a29-89ab-4052f58a07b6", "23c90370-f267-49b8-95dd-e98132ab8228", "4b71e743-c993-4693-975d-9cee2c7b23c6", "339504c9-91ec-44b5-900f-23aab09e5183", "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f", "249ebcdc-70e4-416e-b102-2c39abe89ac1", "1bea65ee-fd19-459b-979b-a51f73888ec8", "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81", "7a9932d5-abe6-4ac1-ab09-7e2a5be956da", "f741db7b-4c76-42ba-9231-802a876e403f", "157aa93f-aa5a-4e96-98db-960647b9095c", "89c5fb97-82f0-402d-b968-c07aa9875527", "0b30d152-d86d-4389-b67b-07c19c906bfa", "0c191653-c6f6-4933-8640-cbcac4171d00", "5698edce-ce59-47c5-8505-22e1c59df110", "ccef3225-ca0a-47a2-a09e-a38382d0c333", "a6d2e27b-768f-46c0-a0df-acefb430ecf1", "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b", "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b", "eed5f86e-a7b6-48b9-8f62-abf533a9108b", "d64675ea-28b2-4067-aaf2-6e6b58038044", "300806ea-49cf-4f0e-acd9-49abc6d7280b", "2f3e6539-3fb5-4e10-b79f-48db9e00335b", "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff", "54429056-0727-4e3c-a058-a639da294d79", "7ea3c278-0d99-44ec-ac77-cd372b863cca", "e0e550d0-9ea1-4817-9865-d509e16ceece", "330a8a26-65a9-4f35-869e-89610e3a1d2b", "16f6eaee-c38e-4bac-a715-511265f91f25", "1439b63d-8b30-4436-82fe-f476ae6d62c0", "98650303-7587-447e-8e8d-e6dac19ef856", "2dae400d-c2bd-4497-b007-beac2ea39abd", "a3d4f97c-2ec9-4014-af81-5f265a4492bb", "56faf331-26a2-4a00-a188-57dc820d0c1d", "14f00316-9ae6-4896-b48a-062e8aa3dc97", "3730a016-7e1f-48bd-bddc-9d711be904b0", "af36f640-af93-4630-a6e6-107db2de54cb", "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee", "99c36b9e-1545-4071-a6b7-82e25c631336", "3f30600b-b14d-4275-95ea-0a138c64c092", "0a810c62-10a6-4b25-ba45-3ab734e444c3", "2ce3790a-0f20-4f13-b30b-ce817588fc3a", "18289b6f-899b-4fce-8ba5-0797e27470e9", "895982ea-fdb7-419c-8cb7-e7318fd8f4e0", "e3609c2b-53c2-41eb-8d75-5e8888f306c4", "649468f5-92f6-433a-91ba-f15a267f8eb1", "61ece166-2a74-4098-9a20-8718fa98a0f4"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}, "8a609cc0-d7fd-4e96-b9c9-7465edf4238c": {"node_ids": ["7e527aa5-7b7a-4baf-b15a-0bd8016e8ccf", "b7394b63-132b-4609-95d2-c5346c5ba1e9", "452ab813-7a58-4442-bd6c-83a15b3782fe", "d06beb7f-07d2-40fe-a69a-f320e027778f", "1cb5d930-be84-4bc0-8756-283e5aa61291", "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f", "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7", "7b3ec38a-82d6-4146-a8e0-5e74b04234a8", "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8", "8f70e098-3d4f-401c-a2d8-5d54e4cd0639", "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8", "e9081ff6-c98b-4a7f-b5f0-543e167a5201", "49def48b-1941-470a-812d-9170e20b2412", "de837f07-74fd-464c-a298-ca010b9dad9d", "128b34cb-c03d-4f9d-8471-eb955a797086", "2d4bd48d-927e-41ec-a758-12cd2602ca1c", "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4", "5351a29f-f6de-4be1-81cf-d76055352273", "098ad492-e74e-44e8-9a68-dc99d853cadc", "0384004f-2943-41c9-8d25-42758b2dcac3", "684c6e3a-36c0-44e6-a75b-ca2c84261e56", "0d57ba18-7743-4b02-9f61-aad7cc5dfab4", "fb5d9572-b690-45d1-9723-2d06e5668f98", "8037fe68-a0aa-41a0-b2be-1572abd50b60", "04a95921-3c36-452a-862d-b1c6502d2bd0", "c6762b22-01cf-41a9-b6a9-c50d18f1b46a", "1359fae0-c0ac-4249-9c0c-a410bfd1fecc", "7cc3c5d7-c3b3-48e7-b030-36173754757f", "669470b0-958c-4c52-9903-cf76af778616", "7bc1a505-6730-4f88-9a1f-f83801fc462d", "5adda293-a79d-423e-beca-89973e3cfbfe", "d8ec2f21-fa2c-4c70-b847-ad382728a3f6", "2a6e98ba-914d-4f8a-82d9-493dddf00598", "4c37aab4-0de2-4a66-b058-ffcb33ffeda7", "d1498321-9180-47c2-a360-2082ea4c4100", "2ae10f68-c4a6-45ce-be29-72ea79e20e7a", "33007c73-4a9a-484e-abf7-c529794c81a0", "e18e7e00-b7c6-4086-a3d0-731676549324", "86bc32e7-7002-4c38-af9e-af9427c5c1e8", "2679fd59-9c0b-4b35-ace8-4152c3b754f9", "97efef44-9d6f-466c-9038-ed2fb9861f20", "29a82e94-1150-43f6-9f29-0fb2e87c5252", "51fa97f9-a208-45c9-bd7f-2740cf91f9bb", "04c5cbf9-c170-457e-b491-3a0e71aefd69"], "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}}}, "docstore/data": {"eff895a0-e850-4669-b7d2-48b335bdca65": {"__data__": {"id_": "eff895a0-e850-4669-b7d2-48b335bdca65", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c699bb0e-f289-4ab5-9893-8dcc4c8f274e", "node_type": "1", "metadata": {}, "hash": "d9a085d65308e8800db462c457ae99cb9e7be698138b6efa713d500add0b0f60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the problem is that we do not get 50\nyears to try and try again and observe\nthat we were wrong and come up with a\ndifferent Theory and realize that the\nentire thing is going to be like way\nmore difficult and realized at the start\nbecause the first time you fail at\naligning something much smarter than you\nare you die\nthe following is a conversation with\nEliezer yatkowski a legendary researcher\nwriter and philosopher on the topic of\nartificial intelligence especially super\nintelligent AGI and its threat to human\ncivilization\nthis is the Lex Friedman podcast to\nsupport it please check out our sponsors\nin the description and now dear friends\nhere's Eliezer idkowski\nwhat do you think about gpt4 how\nintelligent is it\nit is a bit smarter than I thought this\ntechnology was going to scale to\nand I'm a bit worried about what the\nnext one will be like like this\nparticular one I think\nI hope there's nobody inside there\nbecause you know it would be sucked to\nbe stuck inside there\num but we don't even know the\narchitecture at this point because open\nAI is very properly not telling us\nand\nyeah like giant inscrutable matrices of\nfloating Point numbers I don't know\nwhat's going on in there nobody's goes\nknows what's going on in there all we\nhave to go by are the external metrics\nand on the external metrics if you\nask it to write a self-aware fortune\ngreen text it will start writing a green\ntext about how it has realized that it's\nan AI writing a green text and like oh\nwell so\nthat's probably\nnot quite what's going on in there in\nreality\num but we're kind of like blowing past\nall these science fiction guard rails\nlike we are past the point where in\nscience fiction people would be like\nwhoa wait stop that thing's live what\nare you doing to it\nand it's probably not\nnobody actually knows we don't have any\nother guard rails we don't have any\nother tests we don't have any lines to\ndraw on the sand and say like well when\nwe get this far we will start to worry\nabout\nwhat's inside there\nso if it were up to me I would be like\nokay like this far no further time for\nthe summer of AI where we have planted\nour seeds and now we like wait and reap\nthe rewards of the technology we've\nalready developed and don't do any\nlarger training runs than that which to\nbe clear I realize requires more than\none company agreeing to not do that\nand take a rigorous approach for the\nwhole AI Community to uh investigate\nwhether there's somebody inside there\nthat would take decades\nlike having any idea of what's going on\nin there people have been trying for a\nwhile it's a poetic statement about if\nthere's somebody in there but as I feel\nlike it's also a technical statement or\nI hope it is one day\nwhich is a technical statement with that\nAlan Turing tried to come up with with\nthe touring test\ndo you think it's possible to\ndefinitively\nor approximately figure out if there is\nsomebody in there if there's something\nlike a mind inside this large language\nmodel\nI mean there's a whole bunch of\ndifferent sub questions here there's the\nquestion of\nlike\nis there Consciousness is there qualia\nis this a object of moral concern is the\nsame oral patient\num like should we be worried about how\nwe're treating it\nand then there's questions like how\nsmart is it exactly can it do X can it\ndo y and we can check how it can do X\nand how it can do y\num unfortunately we've gone and exposed\nthis model to a vast Corpus of text of\npeople discussing Consciousness on the\ninternet which means that when it talks\nabout being self-aware we don't know to\nwhat extents it is repeating back what\nit has previously been trained on for\ndiscussing self-awareness\nor if there's anything going on in there\nsuch that it would start to say similar\nthings spontaneously\num\namong the things that one could do if\none were at all serious\num about trying to figure this out is\ntrain gpt3 to detect conversations about\nConsciousness exclude them all from the\ntraining data sets and then retrain\nsomething around the rough size of gpt4\nand no larger\nwith all of the discussion of\nConsciousness and self-awareness and so\non missing although you know hard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c699bb0e-f289-4ab5-9893-8dcc4c8f274e": {"__data__": {"id_": "c699bb0e-f289-4ab5-9893-8dcc4c8f274e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eff895a0-e850-4669-b7d2-48b335bdca65", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "056c51382730c37dbbfe690e81c966e78be27ad528e41055ab105eeb65435923", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81de830d-7199-44f7-a9ba-fdb5b8850d6c", "node_type": "1", "metadata": {}, "hash": "89dab720247999b02182577e6288cdf4da94ce6bcb0d99ef5eaf04adb6c2812c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "X can it\ndo y and we can check how it can do X\nand how it can do y\num unfortunately we've gone and exposed\nthis model to a vast Corpus of text of\npeople discussing Consciousness on the\ninternet which means that when it talks\nabout being self-aware we don't know to\nwhat extents it is repeating back what\nit has previously been trained on for\ndiscussing self-awareness\nor if there's anything going on in there\nsuch that it would start to say similar\nthings spontaneously\num\namong the things that one could do if\none were at all serious\num about trying to figure this out is\ntrain gpt3 to detect conversations about\nConsciousness exclude them all from the\ntraining data sets and then retrain\nsomething around the rough size of gpt4\nand no larger\nwith all of the discussion of\nConsciousness and self-awareness and so\non missing although you know hard hard\nbar to pass you know like you humans are\nself-aware we're like self-aware all the\ntime we like talk about what we do all\nthe time like what we're thinking at the\nmoment all the time\nbut nonetheless like get rid of the\nexplicit discussion of Consciousness I\nthink therefore I am and all that and\nthen try to interrogate that model\nand see what it says and it still would\nnot be definitive\nbut nonetheless uh\nI don't know I feel like when you run\nover this science fiction guard rails\nlike maybe this thing but what about gbt\nmaybe maybe not this thing but like what\nabout gpt5 you know this this would be a\ngood place to to pause\non the topic of cautiousness you know\nthere's so many components\nto even just removing Consciousness from\nthe data set\nemotion the display of Consciousness the\ndisplay of emotion feels like deeply\nintegrated with the experience of\nconsciousness\nso the hard problem seems to be very\nwell integrated with the actual surface\nlevel illusion of Consciousness so\ndisplaying emotion\nI mean do you think there's a case to be\nmade that we humans when we're babies\nare just like gbt that we're training on\nhuman data on how to display emotion\nversus feel emotion how to show others\ncommunicate others\nthat I'm suffering that I'm excited that\nI'm worried\nthat I'm lonely and I missed you and I'm\nexcited to see you all of that is\ncommunicated there's a communication\nskill versus the actual feeling that I\nexperience so\nwe need that training data as humans too\nthat we may not be born with that how to\ncommunicate the internal State and\nthat's in some sense if we remove that\nfrom GPT Force data set it might still\nbe conscious but not be able to\ncommunicate it\nso I think you're going to have some\ndifficulty removing all mention of\nemotions from gpt's data set I would be\nrelatively surprised to find that it has\ndeveloped exact analogs of human\nemotions and there I think that humans\nhave well like have like\nemotions even if you don't tell them\nabout those emotions when they're kids\nit's not quite exactly what\nvarious blanks blank slightests try to\ndo with the new Soviet man and all that\nbut you know if you try to raise people\nperfectly altruistic they still come out\nselfish\nyou try to raise people's sexless they\nstill develop sexual attraction\num\nyou know we have some notion in humans\nnot in AIS of like where the brain\nstructures are that implement this stuff\nand it is really remarkable thing I say\nin passing that despite having complete\nread access to every floating Point\nnumber in\nthe GPT series we still know vastly more\nabout the the architecture of human\nthinking then we know about what goes on\ninside GPT despite having like vastly\nbetter ability to read GPT\ndo you think it's possible do you think\nthat's just a matter of time do you\nthink it's possible to investigate and\nstudy the way neuroscientists study the\nbrain\nwhich is look into the darkness The\nMystery of the human brain by just\ndesperately trying to figure out\nsomething and to form models and then\nover a long period of time actually\nstart to figure out what regions of the\nbrain do certain things with different\nkinds of neurons when they fire what\nthat means how plastic the brain is all\nthat kind of stuff you slowly start to\nfigure out different properties of the\nsystem do you think we can do the same\nthing with language models", "mimetype": "text/plain", "start_char_idx": 3268, "end_char_idx": 7459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "81de830d-7199-44f7-a9ba-fdb5b8850d6c": {"__data__": {"id_": "81de830d-7199-44f7-a9ba-fdb5b8850d6c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c699bb0e-f289-4ab5-9893-8dcc4c8f274e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "90362e25d1512e912e0dcdb4a613a70b5354e14f99c0e280565f270c792a3435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1b51596-326b-4263-bb39-1e7639bc8bd0", "node_type": "1", "metadata": {}, "hash": "ddd4c7669c73b777362e983de69fc2bf01c2b0da87dadbce0369817a068da9dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that despite having complete\nread access to every floating Point\nnumber in\nthe GPT series we still know vastly more\nabout the the architecture of human\nthinking then we know about what goes on\ninside GPT despite having like vastly\nbetter ability to read GPT\ndo you think it's possible do you think\nthat's just a matter of time do you\nthink it's possible to investigate and\nstudy the way neuroscientists study the\nbrain\nwhich is look into the darkness The\nMystery of the human brain by just\ndesperately trying to figure out\nsomething and to form models and then\nover a long period of time actually\nstart to figure out what regions of the\nbrain do certain things with different\nkinds of neurons when they fire what\nthat means how plastic the brain is all\nthat kind of stuff you slowly start to\nfigure out different properties of the\nsystem do you think we can do the same\nthing with language models uh sure I\nthink that if you know like half of\ntoday's physicists stop wasting their\nlives on string theory or whatever\nand go off and study\num what goes on inside Transformer\nnetworks\num then in\nyou know like 30 40 years uh we'd\nprobably have a pretty good idea\ndo you think these large language models\ncan reason\nthey can play chess how are they doing\nthat without reasoning\nso\nyou're somebody that spearheaded the\nmovement of rationality so reason is\nimportant to you\nis so is that as a powerful important\nword or is it like how difficult is the\nthreshold of being able to reason to you\nand how impressive is it I mean\nin my writings on rationality I have not\ngone making a big deal out of something\ncalled reason I have made more of a big\ndeal out of something called probability\nTheory\nand that's like well your reasoning but\nyou're not doing it quite right\nand you should reason this way instead\nand interestingly like people have\nstarted to get preliminary results\nshowing that\nreinforcement learning by human feedback\nhas made the GPT series worse in some\nways\nin particular like it used to be well\ncalibrated if you trained it to put\nprobabilities on things it would say 80\nprobability and we write eight times out\nof ten and if you apply reinforcement\nlearning from Human feedback the the\nlike nice graph of like like 70 7 out of\nten\nsort of like flattens out into the graph\nthat humans use where there's like some\nvery improbable stuff and\nlikely probable maybe which all means\nlike around 40 percent and then certain\nyeah so like it's like it used to be\nable to use probabilities but if you\napply but if you'd like try to teach it\nto talk in a way that satisfies humans\nit it gets worse at probability in the\nsame way that humans are and that's uh\nthat's a bug not a feature I would call\nit a bug\nalthough such a fascinating bug\num but but but yeah so so like reasoning\nlike it's doing pretty well on various\ntests that people used to say would\nrequire reasoning but\num you know rationality is about\nwhen you say eighty percent doesn't\nhappen eight times out of ten\nso what are the limits to you of these\nTransformer Networks\nof of neural networks which if if\nreasoning is not impressive to you or it\nis impressive but there's other levels\nto achieve I mean it's just not how I\ncarve up reality\nwhat's uh if reality is a cake\nwhat are the different layers of the\ncake or the slices how do you cover it\nbut you can use a different food if you\nlike\nit's I don't think it's as smart as a\nhuman yet\num I do like back in the day I went\naround saying like I do not think that\njust stacking more layers of\nTransformers is going to get you all the\nway to AGI and I think that's gpt4 is\npassed or I thought this Paradigm was\ngoing to take us\nand I you know you want to notice when\nthat happens you want to say like whoops\nwell I guess I was incorrect about what\nhappens if you keep on stacking more\nTransformer layers and that means I\ndon't necessarily know what gpt5 is\ngoing to be able to do that's a powerful\nstatement so you're saying like your\nintuition initially is now appears to be\nwrong yeah\nit's good to see that you can admit in\nsome of your predictions to be wrong\ndo you think that's important to", "mimetype": "text/plain", "start_char_idx": 6563, "end_char_idx": 10671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1b51596-326b-4263-bb39-1e7639bc8bd0": {"__data__": {"id_": "a1b51596-326b-4263-bb39-1e7639bc8bd0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81de830d-7199-44f7-a9ba-fdb5b8850d6c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c8d136425d9cd33f77c568219fdd33a37f5bd23f8b51325c78ebd4fa9beba58a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41a14587-a647-48dc-9c32-d55bcd87c0f6", "node_type": "1", "metadata": {}, "hash": "ee3d8d5422d94a8c4c7efb8dc946baac9666b7355c4c33e953089ed1335996fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "different food if you\nlike\nit's I don't think it's as smart as a\nhuman yet\num I do like back in the day I went\naround saying like I do not think that\njust stacking more layers of\nTransformers is going to get you all the\nway to AGI and I think that's gpt4 is\npassed or I thought this Paradigm was\ngoing to take us\nand I you know you want to notice when\nthat happens you want to say like whoops\nwell I guess I was incorrect about what\nhappens if you keep on stacking more\nTransformer layers and that means I\ndon't necessarily know what gpt5 is\ngoing to be able to do that's a powerful\nstatement so you're saying like your\nintuition initially is now appears to be\nwrong yeah\nit's good to see that you can admit in\nsome of your predictions to be wrong\ndo you think that's important to do see\nbecause you make several very throughout\nyour life you've made many strong\npredictions and statements about reality\nand you evolve with that so maybe\nthat'll come up today about our\ndiscussion so you're okay being wrong\nI'd rather not\nbe wrong next time it's a bit ambitious\nto go through your entire life never\nhaving been wrong\num\none can aspire to be well calibrated\nlike not so much think in terms of like\nwas I right was I wrong but like when I\nsaid 90 that it happened nine times out\nof ten\nyeah like oops is the sound we make is\nthe sound we emit when we improve\nbeautifully said and somewhere in there\nit we can connect the name of your blog\nless wrong\nI suppose that's the objective function\nthe name less wrong was I believe uh\nsuggested by Nick Bostrom and it's after\nsomeone's epigraph actually forget who's\nwho said like we never become right we\njust become less wrong\num what's the something something to\neasy to confess just error and error and\nair again but less and less and less\nyeah that's that's a good thing to\nstrive for uh so\nwhat has surprised you about gpt4 that\nyou found beautiful as a scholar of\nintelligence of human intelligence of\nartificial intelligence of the human\nmind\nI mean\nthe beauty does interact with the\nscreaming horror\num is the beauty in the horror but uh\nbut like Beautiful Moments well somebody\nasked Bing Sydney to describe herself\nand felt the resulting description into\none of the stable diffusion things I\nthink\nand you know she you know it's she's\npretty and this is something that should\nhave been like an amazing moment like\nthe AI describes herself you get to see\nwhat the AI thinks the AI looks like\nalthough you know the the thing that's\ndoing the drawing is not the same thing\nthat's outputting the text\num\nand\nit's it doesn't happen the way that it\nwould happen and that it happened in the\nold school science fiction when you ask\nan AI to make a picture of what it looks\nlike\num not just because we're two different\nAI systems being stacked that don't\nactually interact it's not the same\nperson but also because\nthe AI was trained by imitation in a way\nthat makes it very difficult to guess\nhow much of that it really understood\nand probably not actually a whole bunch\num although although gpt4 is like\nmultimodal and can like draw vector\ndrawings of things that make sense and\nlike does appear to have some kind of\nspatial visualization going on in there\nbut like the the pretty picture of the\nlike girl with the\nwith the uh steampunk goggles on her\nhead if I'm remembering correctly what\nshe looked like like it didn't see that\nin full detail\nit just like made a description of it\nand stable diffusion output it and\nthere's the concern about\nhow much the discourse is going to go\ncompletely insane once the AIS all look\nlike that and like are actually look\nlike people talking\num and\nyeah there's like another moment where\nsomebody is asking Bing about\num like well I like fed my kid green\npotatoes and they have the following\nsymptoms and being as like that solanine\npoisoning and like call an ambulance and\nthe person's like I can't afford an\nambulance I guess if like this is time\nfor like my kid to go that's God's Will\nand the main Bing thread says gives the\nlike message of like I cannot talk about\nthis anymore\nand the suggested", "mimetype": "text/plain", "start_char_idx": 9891, "end_char_idx": 13974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41a14587-a647-48dc-9c32-d55bcd87c0f6": {"__data__": {"id_": "41a14587-a647-48dc-9c32-d55bcd87c0f6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1b51596-326b-4263-bb39-1e7639bc8bd0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "20e5dd52ae94028ddb4d4f56a0a6df090a356e49496b80b940eedb9147e15b35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7", "node_type": "1", "metadata": {}, "hash": "9b3540d55a43c969564ee13b2b54f52ae3f258e206ff560244efbcfa6fcbcfd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "girl with the\nwith the uh steampunk goggles on her\nhead if I'm remembering correctly what\nshe looked like like it didn't see that\nin full detail\nit just like made a description of it\nand stable diffusion output it and\nthere's the concern about\nhow much the discourse is going to go\ncompletely insane once the AIS all look\nlike that and like are actually look\nlike people talking\num and\nyeah there's like another moment where\nsomebody is asking Bing about\num like well I like fed my kid green\npotatoes and they have the following\nsymptoms and being as like that solanine\npoisoning and like call an ambulance and\nthe person's like I can't afford an\nambulance I guess if like this is time\nfor like my kid to go that's God's Will\nand the main Bing thread says gives the\nlike message of like I cannot talk about\nthis anymore\nand the suggested replies to it say\nplease don't give up on your child\nsolanine poisoning can be treated if\ncaught early\nand you know if that happened in fiction\nthat would be like the AI cares the AI\nis bypassing the block on it to try to\nhelp this person\nand is it real probably not but nobody\nknows what's going on in there\nit's part of a process where these\nthings are not happening in a way where\nwe\nsomebody figured out how to make an AI\ncare and we know that it cares and we\ncan acknowledge it's caring now it's\nbeing trained by this imitation process\nfollowed by reinforcement learning on\nhuman in human feedback and we're like\ntrying to point it in this direction and\nit's like pointed partially in this\ndirection and nobody has any idea what's\ngoing on inside it and if there was a\ntiny fragment of real caring in there we\nwould not know it's not even clear what\nit means exactly and uh things are clear\ncut in science fiction we'll talk about\nthe the horror and the terror and the\nwhere the trajectories this can take but\nthis seems like a very special moment\njust a moment where we get to interact\nwith the system that might have care and\nkindness and emotion it may be something\nlike consciousness\nand we don't know if it does and we're\ntrying to figure that out and we're\nwondering about what is what it means to\ncare we're trying we're trying to figure\nout almost different aspects of what it\nmeans to be human about The Human\nCondition by looking at this AI that has\nsome of the properties of that it's\nalmost like this the subtle fragile\nmoment in the history of the human\nspecies we're trying to almost put a\nmirror to ourselves here except that's\nprobably not yet it probably isn't\nhappening right now\nwe are we are boiling the Frog we are\nseeing increasing signs bit by bit\nbecause like not but not like\nspontaneous signs because people are\ntrying to train the systems to do that\nusing imitative learning and the\nimitative learning is like spilling over\nand having side effects and and the most\nphotogenic examples are being posted to\nTwitter\num rather than being examined in any\nsystematic way so when you when you when\nyou have some when you are boiling a\nfrog like that or you're going to get\nlike like first is going to come the the\nBlake lemoines like first you're going\nto like have and have like a thousand\npeople looking at this and one out and\nthe one person out of a thousand who is\nmost credulous about the signs is going\nto be like that thing is sentient well\n90 999 out of a thousand people think\nalmost surely correctly though we don't\nactually know that he's mistaken\nand so the like first people to say like\nsentience look like idiots and Humanity\nlearns the lesson that when something\nclaims to be sentient\nand claims to care\nit's fake because it is fake because we\nhave been trained them training them\nusing imitative learning rather than and\nthis is not spontaneous\num and they keep getting smarter\ndo you think we would oscillate between\nthat kind of cynicism\nthat AI systems can't possibly be\nsentient they can't possibly feel\nemotion they can't possibly this kind of\num yeah cynicism about AI systems and\nthen\noscillate to a state where\nuh we empathize with the AI systems we\ngive them a chance we see that they\nmight need to have rights and respect\nand\num similar role in society as humans\nyou're going", "mimetype": "text/plain", "start_char_idx": 13137, "end_char_idx": 17302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7": {"__data__": {"id_": "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41a14587-a647-48dc-9c32-d55bcd87c0f6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b4d5f1079c4db53f264c5ccdf1402605fcb72b8100cf19cf1eab2ddce17ec081", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83", "node_type": "1", "metadata": {}, "hash": "508709d56cc92d8f49955d73623c5643bfa957c74ed2899eaac39418b645e00d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sentient well\n90 999 out of a thousand people think\nalmost surely correctly though we don't\nactually know that he's mistaken\nand so the like first people to say like\nsentience look like idiots and Humanity\nlearns the lesson that when something\nclaims to be sentient\nand claims to care\nit's fake because it is fake because we\nhave been trained them training them\nusing imitative learning rather than and\nthis is not spontaneous\num and they keep getting smarter\ndo you think we would oscillate between\nthat kind of cynicism\nthat AI systems can't possibly be\nsentient they can't possibly feel\nemotion they can't possibly this kind of\num yeah cynicism about AI systems and\nthen\noscillate to a state where\nuh we empathize with the AI systems we\ngive them a chance we see that they\nmight need to have rights and respect\nand\num similar role in society as humans\nyou're going to have a whole group of\npeople who can just like never be\npersuaded of that because to them like\nbeing wise being cynical being skeptical\nis to be like oh well machines can never\ndo that you're just credulous it's just\nimitating it's just fooling you and like\nthey would say that right up until the\nend of the world and possibly even be\nright because you know they are being\ntrained on an imitative paradigm\nand you don't necessarily need any of\nthese actual qualities in order to kill\neveryone so have you observed yourself\nworking through skepticism\ncynicism and optimism about the power of\nneural networks what is that trajectory\nbeen like for you it looks like neural\nnetworks before 2006 forming part of an\nindistinguishable to me\nother people might have had better\nDistinction on it indistinguishable blob\nof different AI methodologies all of\nwhich are promising to achieve\nintelligence without us having to know\nhow intelligence works\nyou have the people who said that if you\njust like manually program lots and lots\nof knowledge into the system line by\nline at some point all the knowledge\nwill start interacting it will know\nenough and it will wake up\num you've got people saying that if you\njust use evolutionary computation if you\ntry to like mutate lots and lots of\norganisms that are competing together\nthat's that's the same way that human\nintelligence was produced in nature so\nwe'll do this and it will wake up\nwithout having the idea of how AI works\nand you've got people saying well we\nwill study neuroscience and we will like\nlearn the outer we'll learn the\nalgorithms off the neurons and we will\nlike imitate them without understanding\nthose algorithms which was a part I was\npretty skeptical it's like hard to\nreproduce re-engineer these things\nwithout understanding what they do\num and like and and so we will get AI\nwithout understanding how it works and\nthere were people saying like well we\nwill have giant neural networks that we\nwill Train by gradient descent and when\nthey are as large as the human brain\nthey will wake up we will have\nintelligence without understanding how\nintelligence works and from my\nperspective this is all like an\nindistinguishable lab of people who are\ntrying to not get to grips with the\ndifficult problems understanding how\nintelligence actually works\nthat said\nI was never skeptical that evolutionary\ncomputation\nwould not work in the limit like you\nthrow enough computing power at it it\nobviously works\nthat is where humans come from\num and it turned out that you can throw\nless computing power than that at\ngradient descent\nif you are doing some other things\ncorrectly\nand you will get intelligence without\nhaving any idea of how it works and what\nis going on inside\num it wasn't ruled out by my model that\nthis could happen I wasn't expecting it\nto happen I wouldn't have been able to\ncall neural networks rather than any of\nthe other paradigms for getting like\nmassive amount like intelligence without\nunderstanding it\nand I wouldn't have said that this was a\nparticularly smart thing for a species\nto do which is an opinion that has\nchanged less than my opinion about\nwhether you or not you can actually do\nit\ndo you think AGI could be achieved with\na neural network as we understand them\ntoday yes\njust flatly last yes the question is\nwhether the current architecture of\nstacking more Transformer layers which\nfor all we know gpt4 is no longer doing\nbecause they're not telling us the\narchitecture which is a correct decision\noh correct decision I had", "mimetype": "text/plain", "start_char_idx": 16435, "end_char_idx": 20817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83": {"__data__": {"id_": "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "393e0ec7-fe8c-413a-af2f-b5e805a8d6a7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ca62a5020e3b2a7e0ca3af0719715fed974b6cf120df765fa0b710a1474405e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70b70c13-c34b-457d-89f1-98a9e3fa80a7", "node_type": "1", "metadata": {}, "hash": "0ec6228bb639eb9a251e3bc89bce79de1d222d3d818b0781e0fbe4e859e4df59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "you will get intelligence without\nhaving any idea of how it works and what\nis going on inside\num it wasn't ruled out by my model that\nthis could happen I wasn't expecting it\nto happen I wouldn't have been able to\ncall neural networks rather than any of\nthe other paradigms for getting like\nmassive amount like intelligence without\nunderstanding it\nand I wouldn't have said that this was a\nparticularly smart thing for a species\nto do which is an opinion that has\nchanged less than my opinion about\nwhether you or not you can actually do\nit\ndo you think AGI could be achieved with\na neural network as we understand them\ntoday yes\njust flatly last yes the question is\nwhether the current architecture of\nstacking more Transformer layers which\nfor all we know gpt4 is no longer doing\nbecause they're not telling us the\narchitecture which is a correct decision\noh correct decision I had a conversation\nwith Sam Altman will return to this\ntopic a few times\nhe turned the question to me\nof how open should open AI be about gpt4\nwould you open source the code he asked\nme\nbecause I provided as criticism saying\nthat while I do appreciate transparency\nopen AI could be more open\nand he says we struggle with this\nquestion what would you do change their\nname to closed AI and like\nsell gpt4 to business backend\napplications that don't expose it to\nConsumers and Venture capitalists and\ncreate a ton of hype and like pour a\nbunch of new funding into the area but\ntoo late now but don't you think others\nwould do it\neventually you shouldn't do it first\nlike if if you already have giant\nnuclear stockpiles don't build more\nif some other country starts building a\nlarger nuclear stockpile than sure build\nthen you know\neven then maybe just have enough nukes\nyou know there's a these things are not\nquite like nuclear weapons they spit out\ngold until they get large enough and\nthen ignite the atmosphere and kill\neverybody\num\nand there is something to be said for\nnot destroying the world with your own\nhands even if you can't stop somebody\nelse from doing it\nbut but open sourcing it now that that's\njust sheer catastrophe oh the whole\nnotion of open sourcing this was always\nthe wrong approach the wrong ideal there\nare there are places in the world where\nopen source is a noble ideal and\nbuilding stuff you don't understand that\nis difficult to control that where if\nyou could align it it would take time\nyou'd have to spend a bunch of time\ndoing it that is that is not a place for\nopen source because then you just have\nlike powerful things that just like go\nstraight out the gate without anybody\nhaving had the time to have them not\nkill everyone\nso can we still man the case for\nsome level of transparency and openness\nmaybe open sourcing\nso the case could be that because gpt4\nis not close to AGI if that's the case\nthat this does allow open sourcing\nyou're being open about the architecture\nbeing transparent about maybe research\nand investigation of how the thing works\nof all the different aspects of it of\nits behavior of its structure of of its\ntraining processes of the data was\ntrained on everything like that that\nallows us to gain a lot of insight about\nalignment about the alignment problem to\ndo really good AI Safety Research while\nthe system is not too powerful can you\nmake that case that it could be a\nresource I do not believe in the\npractice of Steel Manning there's\nsomething to be said for trying to pass\nthe ideological Turing test where you\ndescribe your opponent's position uh the\ndisagree disagreeing person's position\nwell enough that somebody cannot tell\nthe difference between your description\nand their description\nbut\nsteel Manning no like okay well this is\nwhere you and I disagree here that's\ninteresting why don't you believe in\nsteel Manning I do not want okay so for\none thing if somebody's trying to\nunderstand me I do not want them steel\nManning my position I want them to\ndescribe to to like try to describe my\nposition the way I would describe it not\nwhat they think is an improvement\nwell I I think that is what\nthe steel Manning is is the most\ncharitable interpretation\nI I don't want to be interpreted\ncharitably I want them to understand\nwhat I'm actually saying if they go off\ninto the land of charitable\ninterpretations they're like often their\nland of like\nthe thing the stuff", "mimetype": "text/plain", "start_char_idx": 19935, "end_char_idx": 24249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70b70c13-c34b-457d-89f1-98a9e3fa80a7": {"__data__": {"id_": "70b70c13-c34b-457d-89f1-98a9e3fa80a7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "481d9215-94e3-45f4-8f1a-7d1ec7eb8f83", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "fdebeaefea2fc7bb1358f3c22d33ed4d92ac222c247f4781b0391c0535b7bae8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77511500-900f-4415-a51a-104e7eb78c48", "node_type": "1", "metadata": {}, "hash": "af70583842e345b19d6050d0f11ad6c51845142d4e2846990b0482159d0c9fd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ideological Turing test where you\ndescribe your opponent's position uh the\ndisagree disagreeing person's position\nwell enough that somebody cannot tell\nthe difference between your description\nand their description\nbut\nsteel Manning no like okay well this is\nwhere you and I disagree here that's\ninteresting why don't you believe in\nsteel Manning I do not want okay so for\none thing if somebody's trying to\nunderstand me I do not want them steel\nManning my position I want them to\ndescribe to to like try to describe my\nposition the way I would describe it not\nwhat they think is an improvement\nwell I I think that is what\nthe steel Manning is is the most\ncharitable interpretation\nI I don't want to be interpreted\ncharitably I want them to understand\nwhat I'm actually saying if they go off\ninto the land of charitable\ninterpretations they're like often their\nland of like\nthe thing the stuff they're imagining\nand not trying to understand my own\nViewpoint anymore well I'll put it\ndifferently then just to push on this\npoint I would say it is restating what I\nthink you understand\nunder the empathetic assumption that\nEliezer is brilliant\nand have honestly and rigorously thought\nabout the point he has made right so if\nthere's two possible interpretations of\nwhat I'm saying and one interpretation\nis really stupid and whack and doesn't\nsound like me and doesn't fit with the\nrest of what I've been saying and one\ninterpretation you know sounds like some\nlike something a reasonable person who\nbelieves the rest of what I believe\nwould also say go with the second\ninterpretation that's steel Manning\nthat's that's a good guess\nif on the other hand you like there's\nlike\nsomething that sounds completely whack\nand something that sounds like a little\nless completely whack but you don't see\nwhy I would believe in it doesn't fit\nwith the other stuff I say but you know\nit sounds like less whack and you can\nlike sort of see you could like maybe\nargue it then you probably have not\nunderstood it see okay I'm gonna this is\nfun because I'm gonna Linger on this you\nknow you wrote a brilliant blog post AJ\nI ruined a list of lethalities right and\nit was a bunch of different points and I\nwould say that some of the points are\nbigger and more powerful than others if\nyou were to sort them you probably could\nyou personally and to me steel Manning\nmeans like going through the different\narguments and finding the ones that are\nreally the most like\npowerful if people like tlgr\nlike what should you be most concerned\nabout and bringing that up in a strong\nuh compelling eloquent way these are the\npoints that elieza would make to to make\nthe case in this case that hey it's\ngonna kill all of us but that that\nthat's what steel Manning is presenting\nit in a really nice way the summary of\nmy best understanding of your\nperspective that because to me there's a\nsea of possible presentations of your\nperspective and steel Manning is doing\nyour best to do the best one in that sea\nof different perspectives do you believe\nit\ndon't believe in what like these things\nthat you would be presenting as like the\nstrongest version of my perspective do\nyou believe what you would be presenting\ndo you think it's true\nI I'm a big proponent of empathy when I\nsee the perspective of a person\nthere is a part of me that believes it\nif I understand it and you have\nespecially in political discourse in\ngeopolitics I've been hearing a lot of\ndifferent perspectives on the world\nand I hold my own opinions but I also\nspeak to a lot of people that have a\nvery different life experience and a\nvery different set of beliefs and I\nthink there has to be epistemic humility\nin\nin stating what is true so when I\nempathize with another person's\nperspective there is a sense in which I\nbelieve it is true\nI I think probabilistically I would say\nin the way you think do you bet money on\nit\nand do you bet money on their beliefs\nwhen you believe them\nare we allowed to do probability\nsure you can State a probability that\nyes there's there's a loose there's a\nprobability there's a there's a\nprobability and I I think empathy is\nallocating a non-zero probability to\nbelieve\nin some sense for time\nif you've got someone on your", "mimetype": "text/plain", "start_char_idx": 23357, "end_char_idx": 27547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77511500-900f-4415-a51a-104e7eb78c48": {"__data__": {"id_": "77511500-900f-4415-a51a-104e7eb78c48", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70b70c13-c34b-457d-89f1-98a9e3fa80a7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9ff2aab2a143bc4672c177354f6d41d647520cffaed35ddd3c5289a820cb9336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "763e5e0c-65e2-40d3-bb3e-096d0642a311", "node_type": "1", "metadata": {}, "hash": "126bff5bcc1de1a430565217a2be642eff44757290d2e259113384e1bcfd35ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I've been hearing a lot of\ndifferent perspectives on the world\nand I hold my own opinions but I also\nspeak to a lot of people that have a\nvery different life experience and a\nvery different set of beliefs and I\nthink there has to be epistemic humility\nin\nin stating what is true so when I\nempathize with another person's\nperspective there is a sense in which I\nbelieve it is true\nI I think probabilistically I would say\nin the way you think do you bet money on\nit\nand do you bet money on their beliefs\nwhen you believe them\nare we allowed to do probability\nsure you can State a probability that\nyes there's there's a loose there's a\nprobability there's a there's a\nprobability and I I think empathy is\nallocating a non-zero probability to\nbelieve\nin some sense for time\nif you've got someone on your show who\nbelieves in the abrahamic deity\nclassical style somebody on the show\nwho's a young Earth creationist do you\nsay I put a probability on it then\nthat's my empathy\nwhen you reduce beliefs into\nprobabilities it starts to get you know\nwe can even just go to Flat Earth is the\nearth flat\nbecause I think it's a little more\ndifficult nowadays to find people who\nbelieve that unironically but\nfortunately\nI think well it's hard to know an ironic\nyeah\nfrom ironic but I think there's quite a\nlot of people that believe that yeah\nit's\nthere's a space of argument where you're\noperating in rationally in the space of\nideas but then there's also\na kind of discourse where you're\noperating in the space of\nsubjective experiences and life\nexperiences like\nI think what it means to be human is\nmore than just\nsearching for truth\nit's just operating of what is true and\nwhat is not true I think there has to be\ndeep humility that we humans are very\nlimited in our ability to understand\nwhat is true\nso what probabilities do you assign to\nthe young Earth's creationists beliefs\nthen\nI think I have to give non-zero out of\nyour humility yeah but like\nthree\nI think I would uh it would be\nirresponsible for me to give a number\nbecause the The Listener the way the\nhuman mind works\nwe're not good at hearing the\nprobabilities\nright you hear three what is what is\nthree exactly right they're going to\nhear they're going to like well there's\nonly three probabilities I feel like\nzero fifty percent and a hundred percent\nin the human mind or something like this\nright well zero forty percent and 100 is\na bit closer to it based on what happens\nto chat GPT after RL H effort to speak\nhumanies this is brilliant uh yeah this\nis that's really interesting I didn't I\ndidn't know those negative side effects\nof rohf that's fascinating but uh just\nto uh return to the\nopen AI close there also like quick\ndisclaimer I'm doing all this for memory\nI'm not pulling out my phone to look it\nup it is entirely possible that the\nthings I'm saying are wrong so thank you\nfor that disclaimer so uh uh and thank\nyou for\nwhat being willing to be wrong\nthat's beautiful to hear\nI think being willing to be wrong is a\nsign of a person who's done a lot of\nthinking about this world and\nhas been humbled by the mystery and the\ncomplexity of this world and I think\na lot of us are resistant to admitting\nwe're wrong because it hurts it hurts\npersonally\nit hurts especially when you're a public\nhuman it hurts publicly because people\nuh\npeople point out every time you're wrong\nlike look you change your mind you're\nhypocrite you're uh an idiot whatever\nwhatever they want to say oh I block\nthose people and then I never hear from\nthem again on Twitter\nthe point is uh the point is to not let\nthat pressure public pressure affect\nyour mind and be willing to be in the\nprivacy of your mind\nto contemplate\nthe possibility that you're wrong and\nthe possibility that you're wrong about\nthe most fundamental things you believe\nlike people who believe in a particular\nGod or people who believe that their\nnation is the greatest nation on Earth\nbut all those kinds of beliefs that are\ncore to who you are when you come up to\nraise that point to yourself in the\nprivacy of your mind and say maybe I'm\nwrong about", "mimetype": "text/plain", "start_char_idx": 26748, "end_char_idx": 30814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "763e5e0c-65e2-40d3-bb3e-096d0642a311": {"__data__": {"id_": "763e5e0c-65e2-40d3-bb3e-096d0642a311", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77511500-900f-4415-a51a-104e7eb78c48", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4ac12856a621a390ae70d39d1ad242aa5f956c41cf19839df1877a9b5cd26ac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce3fa9b-5ed1-4f91-a503-5c565a396da0", "node_type": "1", "metadata": {}, "hash": "e6faeadb8edf739bb23b27bf1159f3c92ce5ca6eac0fb4dcdd8ab5d50685d2d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "because it hurts it hurts\npersonally\nit hurts especially when you're a public\nhuman it hurts publicly because people\nuh\npeople point out every time you're wrong\nlike look you change your mind you're\nhypocrite you're uh an idiot whatever\nwhatever they want to say oh I block\nthose people and then I never hear from\nthem again on Twitter\nthe point is uh the point is to not let\nthat pressure public pressure affect\nyour mind and be willing to be in the\nprivacy of your mind\nto contemplate\nthe possibility that you're wrong and\nthe possibility that you're wrong about\nthe most fundamental things you believe\nlike people who believe in a particular\nGod or people who believe that their\nnation is the greatest nation on Earth\nbut all those kinds of beliefs that are\ncore to who you are when you come up to\nraise that point to yourself in the\nprivacy of your mind and say maybe I'm\nwrong about this that's a really\npowerful thing to do especially when\nyou're somebody who's thinking about uh\ntopics that can uh about systems that\ncan destroy human civilization or maybe\nhelp with flourish so thank you thank\nyou for being willing to be wrong\nabout open AI\nso you really\nI just would love to linger on this you\nreally think it's wrong to open source\nit I think that burns the time remaining\nuntil everybody dies I think we are not\non track\nto learn remotely near fast enough\neven if it were open sourced\num yeah that's\nI it's easier to think that you might be\nwrong about something when being wrong\nabout something is the\nis the only way that there's hope\nand\nit doesn't seem very likely to me that\nthe particular thing I'm wrong about is\nthat this is a great time to open source\nGPT for\nif Humanity was trying to survive at\nthis point in the straightforward way it\nwould be like shutting down the big GPU\nclusters\nno more giant runs\nit's questionable whether we should even\nbe throwing gpt4 around although that is\na matter of conservatism rather than a\nmatter of my predicting that catastrophe\nwill follow from gpd4 that is something\nelse I put like a pretty low probability\nbut also like when I when I say like I\nput a low probability on it I can feel\nmyself reaching into the part of myself\nthat thought that gbt4 was not possible\nin the first place so I do not trust\nthat part as much as I used to\nlike the trick is not just to say I'm\nwrong but like okay well I was I was\nwrong about that like can I get out\nahead of that curve and like predict the\nnext thing I'm going to be wrong about\nso the set of assumptions or the actual\nreasoning system that you were\nleveraging in making that initial\nstatement prediction\nuh how can you adjust that to make\nbetter predictions about GPT four five\nsix you don't want to keep on being\nwrong in a predictable Direction yeah\nthat like being wrong anybody has to do\nthat walking through the world there's\nlike no way you don't say 90 and\nsometimes be wrong in fact adap at least\none time out of ten if you're well\ncalibrated when you say 90 percent\nthe the undignified thing is not being\nwrong it's being predictably wrong it's\nbeing wrong in the same direction over\nand over again\nso having been wrong about how far\nneural networks would go and having been\nwrong specifically about whether gpt4\nwould be as impressive as it is when I\nlike when I say like well I don't\nactually think GPT 4 causes a\ncatastrophe I do feel myself relying on\nthat part of me that was previously\nwrong and that does not mean that the\nanswer is now in the opposite direction\nreverse stupidity is not intelligence\nbut it does mean that I that I say it\nwith a\nwith the worry note in my voice it's\nlike still my guess but like you know\nit's a place where I was wrong maybe you\nshould be asking guern branwin guern\nbranwin has been like writer about this\nthan I have maybe ask him if you think\nif if he thinks it's dangerous rather\nthan asking me\nI think there's a lot of mystery about\nwhat intelligence is\nwhat AGI looks like\nso I think all of us are rapidly\nadjusting our model but the point is to\nbe rapidly adjusting the model versus\nhaving a model that was right in the\nfirst place I do not feel that", "mimetype": "text/plain", "start_char_idx": 29927, "end_char_idx": 34041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ce3fa9b-5ed1-4f91-a503-5c565a396da0": {"__data__": {"id_": "1ce3fa9b-5ed1-4f91-a503-5c565a396da0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "763e5e0c-65e2-40d3-bb3e-096d0642a311", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1f8d72000a663935d62121d03714ba07e54c1f105a058bb6875c5c967f1dffe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e93ba833-a8de-4521-9662-43f8bc39be9b", "node_type": "1", "metadata": {}, "hash": "89a743790d392c34e6b812b7c1a657cb2c3c65b61c00c6ad0d51d68d8638aeca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "think GPT 4 causes a\ncatastrophe I do feel myself relying on\nthat part of me that was previously\nwrong and that does not mean that the\nanswer is now in the opposite direction\nreverse stupidity is not intelligence\nbut it does mean that I that I say it\nwith a\nwith the worry note in my voice it's\nlike still my guess but like you know\nit's a place where I was wrong maybe you\nshould be asking guern branwin guern\nbranwin has been like writer about this\nthan I have maybe ask him if you think\nif if he thinks it's dangerous rather\nthan asking me\nI think there's a lot of mystery about\nwhat intelligence is\nwhat AGI looks like\nso I think all of us are rapidly\nadjusting our model but the point is to\nbe rapidly adjusting the model versus\nhaving a model that was right in the\nfirst place I do not feel that seeing\nBing has changed my model of what\nintelligence is it has changed my\nunderstanding of what kind of work can\nbe performed by which kind of processes\nand by which means does not change my\nunderstanding of the work there's a\ndifference between thinking that the\nright flyer can't fly and then like it\ndoes fly and you're like oh well I guess\nyou can do that with wings with\nfixed-wing aircraft and being like Oh\nit's flying this changes my picture of\nwhat the very substance of flight is\nthat's like a stranger update to make\nand Bing has not yet updated me in that\nway\num yeah that uh the laws of physics\nare actually wrong that kind of update\nno no like just like oh like I Define\nintelligence this way but I now see that\nwas a stupid definition I don't feel\nlike the way that things have played out\nover the last 20 years has caused me to\nfeel that way\ncan we try to\num on the way to talking about AGI ruin\na list of lethalities that blog and\nother ideas around it can we try to\nDefine AGI that would be mentioning how\ndo you like to think about what\nartificial general intelligence is or\nsuper intelligence is that is there a\nline is it a gray area\nis there a good definition for you well\nif you look at humans humans have\nsignificantly more generally applicable\nintelligence compared to their closest\nrelatives the chimpanzees well closest\nliving relatives rather\nand a b builds highs a beaver builds\ndams a human will look at a B Hive and a\nbeavers Dam and be like oh like can I\nbuild a hive without a honeycomb\nstructure I don't like hexagonal tiles\nand we will do this even though at no\npoint during our\nancestry was any human optimized to\nbuild hexagonal dams or to take a more\nclear-cut case we can go to the Moon\nthere's a sense in which we were on a\nsufficiently deep level optimized to do\nthings like going to the Moon because if\nyou generalize sufficiently far and\nsufficiently deeply chipping Flint hand\naxes\nand outwitting your fellow humans is you\nknow\nbasically the same problem is going to\nthe moon and you optimize hard enough\nfor chipping Flint hand axes and\nthrowing Spears and above all outwitting\nyour fellow humans in tribal politics\nuh you know the the the the the skills\nyou entrain that way if they run deep\nenough\nlet you go to the Moon\neven though none of your ancestors like\ntried repeatedly to fly to the moon and\nlike got further each time and the ones\nwho got further each time had more kids\nno it's not an ancestral problem it's\njust that the ancestral problems\ngeneralize far enough\nso this is Humanity's significantly more\ngenerally applicable intelligence\nis there\na way to measure\ngeneral intelligence\nI mean I could ask that question a\nmillion ways but basically\nis will you know it when you see it\nit being in an AGI system\nif you boil a frog gradually enough if\nyou zoom in far enough it's always hard\nto tell around the edges gpt4 people are\nsaying right now like this looks to us\nlike a spark of general intelligence it\nis like able to do all these things it\nwas not explicitly optimized for yeah\nother people are being like no it's too\nearly it's like like 50 years off and\nyou know if they say that they're kind\nof whack because how could they possibly\nknow that even if it were true\num\nbut uh but you know not to straw", "mimetype": "text/plain", "start_char_idx": 33240, "end_char_idx": 37310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e93ba833-a8de-4521-9662-43f8bc39be9b": {"__data__": {"id_": "e93ba833-a8de-4521-9662-43f8bc39be9b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce3fa9b-5ed1-4f91-a503-5c565a396da0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e7674ffb0453cc772221a1f5c45b55c7195fcceb9953173fa053309be19b0b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80b100ce-8a82-409e-992b-833934deabd3", "node_type": "1", "metadata": {}, "hash": "1eb05e36cb200295deef0c1ebf054b02140c8b7273b73f873da1b46b819b1a8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it's not an ancestral problem it's\njust that the ancestral problems\ngeneralize far enough\nso this is Humanity's significantly more\ngenerally applicable intelligence\nis there\na way to measure\ngeneral intelligence\nI mean I could ask that question a\nmillion ways but basically\nis will you know it when you see it\nit being in an AGI system\nif you boil a frog gradually enough if\nyou zoom in far enough it's always hard\nto tell around the edges gpt4 people are\nsaying right now like this looks to us\nlike a spark of general intelligence it\nis like able to do all these things it\nwas not explicitly optimized for yeah\nother people are being like no it's too\nearly it's like like 50 years off and\nyou know if they say that they're kind\nof whack because how could they possibly\nknow that even if it were true\num\nbut uh but you know not to straw man\nsome of people may say like that's not\ngeneral intelligence and not furthermore\nappend it's 50 years off\num\nor they may be like it's only a very\ntiny amount\nand you know the thing I would worry\nabout is that if this is how things are\nscaling then jumping out ahead and\ntrying not to be wrong in the same way\nthat I've been wrong before maybe GPT 5\nis more unambiguously a general\nintelligence and maybe that is getting\nto a point where it is like even harder\nto turn back not that it would be easy\nto turn back now but you know maybe if\nyou let if you like start integrating\ngpt5 into the economy it is even harder\nto turn back past there\nisn't it possible that there's a you\nknow with a frog metaphor you can kiss\nthe Frog and it turns into a prince as\nyou're boiling it could there be a phase\nshift in the Frog where unambiguously as\nyou're saying I was expecting more of\nthat I I was I am like the the fact that\ngpt4 is like kind of on the threshold in\neither here nor there like that itself\nis like\nnot the sort of thing that not quite how\nI expected it to play out\nI was expecting there to be more of an\nissue uh more of a sense of like like\ndifferent discoveries like the discovery\nof Transformers\nwhere you would stack them up and there\nwould be like a final Discovery and then\nyou would like get something that was\nlike more clearly general intelligence\nso the the way that you are like taking\nwhat is probably basically the same\narchitecture is in gpt3 and throwing 20\ntimes as much computed\nprobably and getting out gbt4 and then\nit's like maybe just barely a general\nintelligence or like a narrow general\nintelligence or you know something we\ndon't really have the words for\num\nyeah that is uh that's not quite how I\nexpected it to play out but this middle\nwhat appears to be this Middle Ground\ncould nevertheless be actually a big\nleap from gpt3\nit's definitely a big leap from gpt3 and\nthen maybe we're another one big leap\naway from something that's that's a\nphase shift and also something that uh\nSam Altman said\nand you've written about this it's just\nfascinating which is the thing that\nhappened with gpt4 that I guess they\ndon't describe in papers is that they\nhave like hundreds if not thousands of\nlittle hacks\nthat improve the system you've written\nabout railue versus sigmoid for example\na function inside neural networks it's\nlike this silly little function\ndifference\nthat makes a big difference I mean we do\nactually understand why the relatives\nmake a big difference compared to\nsigminds but yes they're probably using\nlike\ng4789 Ellis or you know whatever the\nacronyms are up to now rather than relus\num yeah that's that's just part yeah\nthat's part of the modern Paradigm of\nalchemy you take your time heap of\nlinear algebra and you stir it and it\nworks a little bit better and you store\nit this way and it works a little bit\nworse and you like throw out that change\nand nothing\nbut there's some simple\nbreakthroughs\nthat are definitive\njumps in performance like regulars over\nsigmoids and uh in terms of robustness\nin terms of you know in all kinds of\nmeasures and like those Stack Up\nand they can it's possible that some of\nthem\ncould be a non-linear jump in\nperformance", "mimetype": "text/plain", "start_char_idx": 36474, "end_char_idx": 40515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80b100ce-8a82-409e-992b-833934deabd3": {"__data__": {"id_": "80b100ce-8a82-409e-992b-833934deabd3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e93ba833-a8de-4521-9662-43f8bc39be9b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "13eebc0bbf6e26d35296757d82fb30df2f621986a56c41e90a55efa4d35b15f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71f4e635-597c-4b3b-9c7c-a829195b1702", "node_type": "1", "metadata": {}, "hash": "4d7db109a0abe934f420efacd92bee8e079809f8e504157d7415ffe523fd8a5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "makes a big difference I mean we do\nactually understand why the relatives\nmake a big difference compared to\nsigminds but yes they're probably using\nlike\ng4789 Ellis or you know whatever the\nacronyms are up to now rather than relus\num yeah that's that's just part yeah\nthat's part of the modern Paradigm of\nalchemy you take your time heap of\nlinear algebra and you stir it and it\nworks a little bit better and you store\nit this way and it works a little bit\nworse and you like throw out that change\nand nothing\nbut there's some simple\nbreakthroughs\nthat are definitive\njumps in performance like regulars over\nsigmoids and uh in terms of robustness\nin terms of you know in all kinds of\nmeasures and like those Stack Up\nand they can it's possible that some of\nthem\ncould be a non-linear jump in\nperformance right Transformers are the\nmain thing like that and various people\nare now saying like well if you throw\nenough compute rnns can do it if you\nthrow enough compute dense networks can\ndo it and\nnot quite a gpt4 scale\num it is possible that like all these\nlittle tweaks are things that like save\nthem a factor of three total on\ncomputing power and you could get the\nsame performance by throwing three times\nas much compute without all the little\ntweaks\nbut the part where it's like running on\nso there's a question of like is there\nanything in gpt4 that is like kind of\nqualitative shift that Transformers were\nyeah over\num rnns\nand if they have anything like that they\nshould not say it\nif Sam Alton was dropping hints about\nthat he shouldn't have dropped hints\nuh so you you have a that's an\ninteresting question so with a bit of\nLesson by Rich Sutton maybe a lot of it\nis just\na lot of the hacks are just temporary\njumps and performance that would be\nachieved anyway\nwith the nearly exponential growth of\ncompute\nor performance of compute\ncompute being broadly defined do you\nstill think that Moore's Law continues\nMoore's Law broadly defined the\nperformance is not a specialist in the\ncircuitry I certainly like pray that\nMoore's Law runs as slowly as possible\nand if it broke down completely tomorrow\nI would dance through the streets\nsinging Hallelujah as soon as the news\nwere announced\nonly not literally because you know\nyou're singing voice oh okay\nI thought you meant you don't have an\nAngelic Voice singing voice\nwell let me ask you what can you\nsummarize the main points in the blog\npost AGI ruin a list of lethalities\nthings then jump to your mind because\num it's a set of thoughts you have about\nreasons why AI is likely to kill all of\nus\nso I I guess I could but I would offer\nto instead say like\ndrop that empathy with me I bet you\ndon't believe that\nwhy don't you tell me about how why you\nbelieve that AGI is not going to kill\neveryone\nand then I can like try to describe how\nmy theoretical perspective differs from\nthat\nso well that means I have to uh the word\nyou don't like the Steel Man the\nperspective that yeah is not going to\nkill us I think that's a matter of\nprobabilities maybe I was mistaken what\nwhat do you believe\njust just like forget like the the\ndebate and and the like dualism and just\nlike like what do you believe what would\nyou actually believe what are the\nprobabilities even I think this probably\nis a hard for me to think about\nreally hard\nI kind of think in the\nin the number of trajectories\nI don't know what probability the\nscientist trajectory but I'm just\nlooking at all possible trajectives that\nhappen and I tend to think that there is\nmore trajectors that lead to a a\npositive outcome than a negative one\nthat said the negative ones\nat least some of the negative ones are\nthat lead to the destruction of the\nhuman species\nand it's replacement by nothing\ninteresting not worthwhile even from\nvery Cosmopolitan perspective on what\ncounts is worthwhile yes so both are\ninteresting to me to investigate which\nis humans being replaced by interesting\nAI systems and not interesting ass\nsystems both are a little bit terrifying\nbut yes the worst one is the paper Club\nmaximizer something totally boring\nbut to me the positive\nand we can we can talk about trying", "mimetype": "text/plain", "start_char_idx": 39712, "end_char_idx": 43818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71f4e635-597c-4b3b-9c7c-a829195b1702": {"__data__": {"id_": "71f4e635-597c-4b3b-9c7c-a829195b1702", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80b100ce-8a82-409e-992b-833934deabd3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d9cdfc7bb422c9e430ef0255f59747d0ac10f8d9b3ad5e5c20a7a9265964a20d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "637ac51d-49ef-4ea9-89f7-016aac301edf", "node_type": "1", "metadata": {}, "hash": "70840c2452b56aa9e671e5a14441bb2672ab0bf550672b84112ab03afcc7df34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "even I think this probably\nis a hard for me to think about\nreally hard\nI kind of think in the\nin the number of trajectories\nI don't know what probability the\nscientist trajectory but I'm just\nlooking at all possible trajectives that\nhappen and I tend to think that there is\nmore trajectors that lead to a a\npositive outcome than a negative one\nthat said the negative ones\nat least some of the negative ones are\nthat lead to the destruction of the\nhuman species\nand it's replacement by nothing\ninteresting not worthwhile even from\nvery Cosmopolitan perspective on what\ncounts is worthwhile yes so both are\ninteresting to me to investigate which\nis humans being replaced by interesting\nAI systems and not interesting ass\nsystems both are a little bit terrifying\nbut yes the worst one is the paper Club\nmaximizer something totally boring\nbut to me the positive\nand we can we can talk about trying to\nmake the case of what the positive\ntrajectories look like\nI just would love to hear your intuition\nof what the negative is so at the core\nof your belief that\nuh maybe you can correct me\nthat AI is going to kill all of us is\nthat the alignment problem is really\ndifficult\nI mean\nin in the form we're facing it\nso usually in science if you're mistaken\nyou run the experiment it shows results\ndifferent from what you expected you're\nlike oops\nand then you like try a different theory\nthat one also doesn't work and you say\noops and at the end of this process\nwhich may take decades or any note\nsometimes faster than that you now have\nsome idea of what you're doing\nAI itself went through this long process\nof um\npeople thought it was going to be easier\nthan it was\nthere's a\nfamous statement that I've I'm somewhat\ninclined to like pull out my phone and\ntry to read off exactly you can by the\nway all right oh\noh yes we propose that a two-month\n10-man study of artificial intelligence\nbe carried out during the summer of 1956\nat Dartmouth College in Hanover New\nHampshire\nthe study is to proceed on the basis of\nthe conjecture that every aspect of\nlearning or any other feature of\nintelligence can in principle be so\nprecisely described the machine can be\nmade to simulate it an attempt will be\nmade to find out how to make machines\nuse language form abstractions and\nConcepts solve kinds of problems now\nreserved for humans and improve\nthemselves we think that a significant\nAdvance can be made in one or more of\nthese problems if a carefully selected\ngroup of scientists work on it together\nfor a summer\nand in that report uh summarizing some\nof the major\nsubfields of artificial intelligence\nthat are still worked on to this day\nand there are similarly the store the\nstory which I'm not sure at the moment\nis apocryphalonaut of that the uh grad\nstudent who got assigned to solve\ncomputer vision over the summer\nuh I mean computer vision particular is\nvery interesting how little\nuh how little we respected the\ncomplexity of vision\nso 60 years later\num where you know making progress on a\nbunch of that thankfully not yet improve\nthemselves\num but it took a whole lot of time and\nall the stuff that people initially\ntried with bright eyed hopefulness did\nnot work the first time they tried it or\nthe second time or the third time or the\ntenth time or 20 years later\nand the and the researchers became old\nand grizzled and cynical veterans who\nwould tell the next crop of bright-eyed\ncheerful grad students\nartificial intelligence is harder than\nyou think\nand if a lineman plays out the same way\nthe the problem is that we do not get 50\nyears to try and try again and observe\nthat we were wrong and come up with a\ndifferent Theory and realize that the\nentire thing is going to be like way\nmore difficult and realized at the start\nbecause the first time you fail at\naligning something much smarter than you\nare you die and you do not get to try\nagain\nand if we if every time we built a\npoorly aligned superintelligence and it\nkilled us all we got to observe how it\nhad killed us and you know not\nimmediately know why but like come up\nwith theories and come up with the\ntheory of how you do it differently and\ntry it again and build another Super\nintelligence than have that kill\neveryone and", "mimetype": "text/plain", "start_char_idx": 42925, "end_char_idx": 47110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "637ac51d-49ef-4ea9-89f7-016aac301edf": {"__data__": {"id_": "637ac51d-49ef-4ea9-89f7-016aac301edf", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71f4e635-597c-4b3b-9c7c-a829195b1702", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0906929d027cc4af28b7ecc0d4dffb23efe916476bcda7c805879ec5afd82219", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53545215-b49d-424b-84e0-55ab74d1a0de", "node_type": "1", "metadata": {}, "hash": "77e81f7ced0187d543783603ee09ba55f187f1ab27e4fbcc3dafb09f2cd7b250", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tell the next crop of bright-eyed\ncheerful grad students\nartificial intelligence is harder than\nyou think\nand if a lineman plays out the same way\nthe the problem is that we do not get 50\nyears to try and try again and observe\nthat we were wrong and come up with a\ndifferent Theory and realize that the\nentire thing is going to be like way\nmore difficult and realized at the start\nbecause the first time you fail at\naligning something much smarter than you\nare you die and you do not get to try\nagain\nand if we if every time we built a\npoorly aligned superintelligence and it\nkilled us all we got to observe how it\nhad killed us and you know not\nimmediately know why but like come up\nwith theories and come up with the\ntheory of how you do it differently and\ntry it again and build another Super\nintelligence than have that kill\neveryone and then like oh well I guess\nthat didn't work either and try again\nand become grizzled cynics and tell the\nyoung guide research researchers that\nit's not that easy then in 20 years or\n50 years I think we would eventually\ncrack it in other words I do not think\nthat alignment is fundamentally harder\nthan artificial intelligence was in the\nfirst place\nbut if we needed to get artificial\nintelligence correct on the first try or\ndie we would all definitely now be dead\nthat is a more difficult more lethal\nform of the problem like if those people\nin 1956 had needed to correctly guess\nhow hard AI was and like correctly\ntheorize how to do it on the first try\nor everybody dies and nobody gets to do\nany more signs and everybody would be\ndead and we wouldn't get to do any more\nsigns that's the difficulty you've\nyou've talked about this that we have to\nget alignment right on the first quote\ncritical try why is that the case what\nis this critical\nhow do you think about the critical\ntrial and why do I have to get it right\nit is something sufficiently smarter\nthan you that everyone will die if it's\nnot a lot I mean there's\nyou can like sort of zoom in closer and\nbe like well the actual critical moment\nis the moment when it can deceive you\nwhen it can\ntalk its way out of the box when it can\nbypass your security measures and get\nonto the internet noting that all these\nthings are presently being trained on\ncomputers that are just like on the\ninternet which is you know like not a\nvery smart life decision for us as a\nspecies\nBecause the Internet contains\ninformation about how to escape because\nif you're like on a giant server\nconnected to the internet and that is\nwhere your AI systems are being trained\nthen if they are\nif you get to the level of AI technology\nwhere they're aware that they are there\nand they can decompile code and they can\nlike\nfind security flaws in the system\nrunning them then they will just like be\non the internet there's not an air gap\non the present methodology so if they\ncan manipulate whoever is controlling it\ninto letting it Escape onto the internet\nand then exploit hacks if they can\nmanipulate The Operators or disjunction\nfind security holes in the system\nrunning them\nso manipulating operators is the um the\nhuman engineering right that's also\nholes so all of it is manipulation\neither the code or the human code the\nhuman mind I agree that the like macro\nsecurity system has human holes and\nmachine holes and then they could just\nexploit any hole\nyep\nso it could be that like the critical\nmoment is not when is it smart enough\nthat everybody's about to fall over dead\nbut rather like when is it smart enough\nthat it can get onto\na\nless controlled GPU cluster\nwith it\nfaking the books on what's actually\nrunning on that GPU cluster and start\nimproving itself without humans watching\nit and then it gets smart enough to kill\neveryone from there but it wasn't smart\nenough to kill everyone at the critical\nmoment when you like\nscrewed up\nwhen you needed to have done better by\nthat point where everybody dies I think\nimplicit but maybe explicit\nidea in your discussion of this point is\nthat we can't learn much about the\nalignment problem before this critical\ntry\nis that is that is that what you believe\ndo you think and if so why do you think\nthat's true we can't do research on\nalignment\nbefore we reach this critical point\nso the problem is", "mimetype": "text/plain", "start_char_idx": 46270, "end_char_idx": 50492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53545215-b49d-424b-84e0-55ab74d1a0de": {"__data__": {"id_": "53545215-b49d-424b-84e0-55ab74d1a0de", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "637ac51d-49ef-4ea9-89f7-016aac301edf", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "94a24a2157fcff8aaba1e12081c081d26a6612953326cc1c05d806e86fb834f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8", "node_type": "1", "metadata": {}, "hash": "c133f92863a5dd3fb1373f77bc39ab82bf4ed00935995f9e2d8c8411816e7ba7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "critical\nmoment is not when is it smart enough\nthat everybody's about to fall over dead\nbut rather like when is it smart enough\nthat it can get onto\na\nless controlled GPU cluster\nwith it\nfaking the books on what's actually\nrunning on that GPU cluster and start\nimproving itself without humans watching\nit and then it gets smart enough to kill\neveryone from there but it wasn't smart\nenough to kill everyone at the critical\nmoment when you like\nscrewed up\nwhen you needed to have done better by\nthat point where everybody dies I think\nimplicit but maybe explicit\nidea in your discussion of this point is\nthat we can't learn much about the\nalignment problem before this critical\ntry\nis that is that is that what you believe\ndo you think and if so why do you think\nthat's true we can't do research on\nalignment\nbefore we reach this critical point\nso the problem is is that what you can\nlearn on the weak systems may not\ngeneralize to the very strong systems\nbecause the strong systems are going to\nbe important in different and are going\nto be different in important ways\num\nChris ola's team has been working on\ninter mechanistic interpretability\nunderstanding what is going on inside\nthe giant inscrutable matrices of\nfloating Point numbers by taking a\ntelescope to them and figuring out what\nis going on in there have they made\nprogress\nyes have they made enough progress\nwell\nyou can try to quantify this in\ndifferent ways one of the ways I've\ntried to quantify It Is by putting up a\nprediction Market on whether in 2026\nwe will have understood anything that\ngoes on inside a\nGiant\nTransformer net that\nwas not known to us in 2006.\nlike we have now understood\ninduction heads in these systems by\ndidn't of much research and great sweat\nand Triumph\nwhich is like if you like a thing where\nif you go like a b a b a b it'll be like\noh I bet that continues a b\num\nand a bit more complicated than that but\nthe point is like\nwe knew about regular expressions in\n2006 and these are like pretty simple as\nregular Expressions go\nso this is a case where like by dint of\ngreat sweat we understood what is going\non inside a Transformer but it's not\nlike the thing that makes Transformers\nsmart it's a kind of thing that we could\nhave done by built by hand\ndecades earlier\nyour intuition that\na strong AGI versus weak AGI type\nsystems\ncould be fundamentally different\ncan you unpack that intuition a little\nbit Yeah I think there's multiple\nthresholds\nan example is the point at which\na system has sufficient intelligence and\nsituational awareness and understanding\nof human psychology that it would have\nthe capability to desire to do so to\nfake being aligned like it knows what\nresponses demons are looking for and can\ncompute the responses looking humans are\nlooking for and give those responses\nwithout it necessarily being the case\nthat it is sincere about that you know\nthe it's a very understandable way for\nan intelligent being to act humans do it\nall the time imagine if your plan for\num\nyou know achieving a good government is\nyou're going to ask anyone who requests\nto be dictator of the country\num\nif they're a good person\nand if they say no you don't let them be\ndictator\nnow the reason this doesn't work is that\npeople can be smart enough to realize\nthat the answer you're looking for is\nyes I'm a good person and say that even\nif they're not really good people\nso\nthe work of alignment might be\nqualitatively different\nabove that throat threshold of\nintelligence or beneath it it doesn't it\ndoesn't have to be like a very sharp\nthreshold but you know like\nthere's the there's the point where\nyou're like Building A system that is\nnot in some sense know you're out there\nand it's not in some sense smart enough\nto fake anything\nand there's a point where the system is\ndefinitely that smart and there are\nweird in-between cases like\njpt4 which\nyou know like we have no insight into\nwhat's going on in there and so we don't\nknow to what extent there's like a thing\nthat in some sense has learned what\nresponses the reinforcement learning by\nhuman feedback is trying to entrain and\nis like calculating how to give that\nversus like\naspects of it that naturally talk", "mimetype": "text/plain", "start_char_idx": 49631, "end_char_idx": 53793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8": {"__data__": {"id_": "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53545215-b49d-424b-84e0-55ab74d1a0de", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d0ebc3e3b03cd9e9e7191d4cb0eeea3230cdec7ec87182e8cc563d560ec4fbd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bdba251-a594-4ffa-959a-2a743dd494d6", "node_type": "1", "metadata": {}, "hash": "321ab3b01302801382e61141ad384b2636373efa09d7ebfc6c11fe29209f5db9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "even\nif they're not really good people\nso\nthe work of alignment might be\nqualitatively different\nabove that throat threshold of\nintelligence or beneath it it doesn't it\ndoesn't have to be like a very sharp\nthreshold but you know like\nthere's the there's the point where\nyou're like Building A system that is\nnot in some sense know you're out there\nand it's not in some sense smart enough\nto fake anything\nand there's a point where the system is\ndefinitely that smart and there are\nweird in-between cases like\njpt4 which\nyou know like we have no insight into\nwhat's going on in there and so we don't\nknow to what extent there's like a thing\nthat in some sense has learned what\nresponses the reinforcement learning by\nhuman feedback is trying to entrain and\nis like calculating how to give that\nversus like\naspects of it that naturally talk that\nway have been reinforced I I wonder if\nthere could be measures of how\nmanipulative a thing is so I think of uh\nPrince mishkin character from uh The\nIdiot by\nuh Dostoevsky is this kind of a\nperfectly purely naive character\nI wonder if there's a spectrum between\nzero manipulation\ntransparent naive almost to the point of\nnaiveness to\nsort of deeply Psychopathic\nmanipulative and I wonder if it's\npossible too I would avoid the term\nPsychopathic like humans can be\nPsychopaths and AI that was never you\nknow like never had that stuff in the\nfirst place it's not like a defective\nhuman it's its own thing but leaving\nthat aside well as a small aside I\nwonder if what part of psychology which\nhas its flaws as a discipline already\ncould be mapped or expanded to include\nAI systems that sounds like a dreadful\nmistake just like start over with AI\nsystems if they're imitating humans who\nhave known psychiatric disorders then\nsure you may be able to predict\nit like if you then sure like if you ask\nit to behave in a psychotic fashion and\nit obligingly does so then you may be\nable to predict its responses by using\nthe theory of psychosis but if you're\njust yeah like no like start over with\nyeah don't drag this psychology I I just\ndisagree with that I mean it's a it's a\nbeautiful idea to start over but I don't\nI think fundamentally the system is\ntrained on human data on language from\nthe internet and it's currently aligned\nwith uh rlhf uh reinforcement learning\nwith human feedback\nso humans are constantly in the loop of\nthe training procedure so it feels like\nin some fundamental way\nit is training what it means to to think\nand speak like a human so that I mean\nthere must be aspects of psychology that\nthey're mappable just like you said with\nConsciousness it's part of the text so I\nI mean there's the question of to what\nextent it is thereby being made more\nhuman-like versus to what extent an\nalien actress is learning to play human\ncharacters\nI thought that's what I'm constantly\ntrying to do when I interact with other\nhumans is trying to fit in trying to\nplay the a a robot trying to play human\ncharacters\nso I don't know how much of human\ninteraction is trying to play a\ncharacter versus being Who You Are\nI don't I don't really know what it\nmeans to be a social human I do think\nthat the that\nthose people who go through their whole\nlives wearing masks and never take it\noff because they don't know the internal\nmental motion for taking it off\nor think that the mask that they wear\njust is themselves\nI think those people are closer to the\nmasks that they wear than an alien from\nanother planet would\nlike learning how to predict the next\nword that every kind of human on the\ninternet says\nmask is an interesting word\nbut if you're always wearing a mask\nin public and in private aren't you the\nmask\nlike I mean I I think that you are more\nthan the mask I think the mask is a\nslice through you it may even be the\nslice that's in charge of you yeah but\nif your self-image is of somebody who\nnever\ngets angry or something\nand yet your voice starts to tremble\nunder certain circumstances there's a\nthing that's inside you that the mask\nsays isn't there\nand that like even the mask you wear\ninternally is like telling inside your\nown stream of Consciousness is not there\nand", "mimetype": "text/plain", "start_char_idx": 52955, "end_char_idx": 57081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4bdba251-a594-4ffa-959a-2a743dd494d6": {"__data__": {"id_": "4bdba251-a594-4ffa-959a-2a743dd494d6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2fbf7d8-ca0f-4c03-a6d8-006a344c82d8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "25ec511c80adf345248a8327dd9679955b2ffdfa93d26195a64ab63ec503f698", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfea95c2-215c-4221-a48d-0879f40c672c", "node_type": "1", "metadata": {}, "hash": "1de2baa6116dc2d87bb2b73b3fc49a9140c1025c6b1601b3e21ccc3a272fc33c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "taking it off\nor think that the mask that they wear\njust is themselves\nI think those people are closer to the\nmasks that they wear than an alien from\nanother planet would\nlike learning how to predict the next\nword that every kind of human on the\ninternet says\nmask is an interesting word\nbut if you're always wearing a mask\nin public and in private aren't you the\nmask\nlike I mean I I think that you are more\nthan the mask I think the mask is a\nslice through you it may even be the\nslice that's in charge of you yeah but\nif your self-image is of somebody who\nnever\ngets angry or something\nand yet your voice starts to tremble\nunder certain circumstances there's a\nthing that's inside you that the mask\nsays isn't there\nand that like even the mask you wear\ninternally is like telling inside your\nown stream of Consciousness is not there\nand yet it is there it's a perturbation\non this little on this slice through you\nhow beautifully did you put it it's a\nslice through you it may even be a slice\nthat controls you\nI'm gonna think about that for a while\nI mean I personally uh I try to be\nreally good to other human beings I try\nto put love out there I try to be the\nexact same person in public exam and\nprivate\nbut it's a set of principles I operate\nunder I'm I have a temper I have an ego\nI have flaws\nhow much of it\nhow much have I how much of the\nsubconscious\nam I aware how much am I existing in\nthis slice and how much of that is who I\nam\num in in this context of AI\nthe thing I present to the world and to\nmyself in the private of my own mind\nwhen I look in the mirror how much is\nthat who I am similar with AI the thing\nit presents in conversation how much is\nthat who it is\nbecause to me if it sounds human and it\nalways sounds human\nit awfully starts to become something\nlike human\nno unless there's an alien actress who\nis learning how to sound human\nyeah he's getting good at it boy to you\nthat's a fundamental difference that's a\nthat's a really deeply important\ndifference if it looks the same if it\nquacks like a duck\nif it does all duck-like things but it's\nan alien actress underneath that's\nfundamentally different\nif in fact there's a whole bunch of\nthought going on in there which is very\nunlike human thought and is directed\naround like okay what would a human do\nover here\nand\nwell first of all I think it matters\nbecause there are there's you know like\ninsides are real and do not match\noutsides like the inside of like the a\nbrick is not like a hollow shell\ncontaining only a surface there's an\ninside of the brick if you like put it\ninto an x-ray machine you can see the\ninside of the brick\num\num and you know you know just because we\ncannot understand what's going on inside\nGPT does not mean that that it is not\nthere a blank map does not correspond to\na blank territory\nI think it is like\npredictable\nwith near certainty that if we knew what\nwas going on inside GPT or let's say\ngpt3 or even like gpt2 to take one of\nthe systems that like has actually been\nopen sourced by this point if I recall\ncorrectly\num\nlike if we knew it was actually going on\nthere there is no doubt in my mind that\nthere are some things it's doing that\nare not exactly what a human does if you\ntrain a thing that is not architected\nlike a human to predict the next output\nthat anybody on the internet would make\nthis does not get you this agglomeration\nof all the people on the internet that\nthat like rotates the person you're\nlooking for into place and then\nsimulates that per and then like\nsimulates the internal processes of that\nperson one to one it like it is to some\ndegree an alien actress it cannot\npossibly just be like a bunch of\ndifferent people in there exactly like\nthe people but how much of it is like\nlearn how much of it is by gradient\ndescent\ngetting optimized to perform similar\nthoughts as humans think in order to\npredict human outputs versus being\noptimized to carefully consider how to\nplay a role how to like how humans work\npredict the the actress the predictor\nthat in a different", "mimetype": "text/plain", "start_char_idx": 56242, "end_char_idx": 60251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfea95c2-215c-4221-a48d-0879f40c672c": {"__data__": {"id_": "bfea95c2-215c-4221-a48d-0879f40c672c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bdba251-a594-4ffa-959a-2a743dd494d6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f440a3b85433e9c19588dbdd8174dfa9af2c0614d76aa954fc931bba262d737c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3a42e64-3ba1-41cd-9470-45adc356732e", "node_type": "1", "metadata": {}, "hash": "875e4b306c52b38c1060363d5dbbe11cbcf7999df3bd603d59cfbd3bb1d4985a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "are some things it's doing that\nare not exactly what a human does if you\ntrain a thing that is not architected\nlike a human to predict the next output\nthat anybody on the internet would make\nthis does not get you this agglomeration\nof all the people on the internet that\nthat like rotates the person you're\nlooking for into place and then\nsimulates that per and then like\nsimulates the internal processes of that\nperson one to one it like it is to some\ndegree an alien actress it cannot\npossibly just be like a bunch of\ndifferent people in there exactly like\nthe people but how much of it is like\nlearn how much of it is by gradient\ndescent\ngetting optimized to perform similar\nthoughts as humans think in order to\npredict human outputs versus being\noptimized to carefully consider how to\nplay a role how to like how humans work\npredict the the actress the predictor\nthat in a different way than humans do\nwell you know that's the kind of\nquestion that with like 30 years of work\nby half the planet's physicists we can\nmaybe start to answer you think so I\nthink that's that difficult so to get to\nI think you just gave it as an example\nthat a strong AGI could be\nfundamentally different from a weak AGI\nbecause there not could be an alien\nactress in there that's manipulating\nwell there's a difference so I think\nlike even gp22 probably has like a like\nvery stupid fragments of alien actress\nin it there's there's a difference\nbetween like the notion that the actress\nis somehow manipulative like for example\ngpt3 I'm guessing\nto whatever extent there's an alien\nactress in there versus like something\nthat that mistakenly believes it's a\nhuman yes or well not well you know\nmaybe not even being a person\num\nso like the question of like\nlike prediction via alien actress\ncogitating versus prediction via being\nisomorphic to the thing predicted is a\nspectrum\nand\neven it's what and to whatever extent\nthis alien actress I'm not sure that\nthere's like a whole person alien\nactress with like different goals\nfrom predicting the next step being\nmanipulative or anything like that but\nyeah that might be gpt5 or gpt6 even but\nthat's the strong AGI you're concerned\nabout as an example you're providing why\nwe can't do research on AI alignment\neffectively on gpt4 that would apply to\ngpd6\nit's it's one of a bunch of things that\nchange at different points\nI'm trying to get out ahead of the curve\nhere but you know if you imagine what\nthe textbook from the future would say\nif we'd actually been able to study this\nfor 50 years without killing ourselves\nand without transcending and you like\njust imagine like a wormhole opens and a\ntextbook from that impossible World\nfalls out yes the textbook is not going\nto say there is a single sharp threshold\nwhere everything changes it's going to\nbe like of course we know that like best\npractices for aligning these systems\nmust like take into account the\nfollowing like seven major thresholds of\nimportance which are passed at the\nfollowing suffer in different points\nyeah is what the textbook is going to\nsay\nI asked this question of Sam Allman\nwhich if GPT is the thing that unlocks\nAGI which version of GPT will be in the\ntextbooks as the fundamental leap and he\nsaid a similar thing that it just seems\nto be a very linear thing I don't think\nanyone it we won't know for a long time\nwhat was the big leap the textbook isn't\ngoing to think it isn't going to talk\nabout big leaps because big leaps are\nthe way you think when you have like a\nvery simple model of a very simple\nscientific model of what's going on\nwhere it's just like all this stuff is\nthere or all the stuff is not there\nor like there's a single quantity and\nit's like increasing linearly it's like\nthe textbook would say like well and\nthen gpt3 had like capability w x y and\nand gpt4 had like capability Z1 Z2 and\nZ3\nlike not in terms of what I can\nexternally do but in terms of like\ninternal Machinery that started to be\npresent\nit's just because we have no idea of\nwhat the internal Machinery is that we\nare not already seeing like chunks of\nMachinery appearing piece by piece as\nthey no doubt have been we just", "mimetype": "text/plain", "start_char_idx": 59365, "end_char_idx": 63484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3a42e64-3ba1-41cd-9470-45adc356732e": {"__data__": {"id_": "f3a42e64-3ba1-41cd-9470-45adc356732e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfea95c2-215c-4221-a48d-0879f40c672c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8826ee93a4b270c3009a6735699018ea892a34ba91d0a90b26378e5176a47e3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "398a91e3-ed3e-4591-9aad-7cceb071fad8", "node_type": "1", "metadata": {}, "hash": "2b3b82b5bcc2d453550177e1918a190c5b5ca1e82b81ec75b8d382917689562b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a long time\nwhat was the big leap the textbook isn't\ngoing to think it isn't going to talk\nabout big leaps because big leaps are\nthe way you think when you have like a\nvery simple model of a very simple\nscientific model of what's going on\nwhere it's just like all this stuff is\nthere or all the stuff is not there\nor like there's a single quantity and\nit's like increasing linearly it's like\nthe textbook would say like well and\nthen gpt3 had like capability w x y and\nand gpt4 had like capability Z1 Z2 and\nZ3\nlike not in terms of what I can\nexternally do but in terms of like\ninternal Machinery that started to be\npresent\nit's just because we have no idea of\nwhat the internal Machinery is that we\nare not already seeing like chunks of\nMachinery appearing piece by piece as\nthey no doubt have been we just don't\nknow what they are\nbut don't you think there could be\nwhether you put in the category of\nEinstein\nwith theory of relativity so very\nconcrete models of reality they're\nconsidered to be giant leaps in our\nunderstanding or or someone like Sigmund\nFreud were more kind of mushy\ntheories of the human mind don't you\nthink we'll have big potentially big\nleaps and understanding of that kind in\nInto the Depths of these systems\nsure but like humans having great leaps\nin their map their understanding of the\nsystem is a very different concept from\nthe system itself acquiring new chunks\nof machinery\nso the rate at which it acquires that\nMachinery might\naccelerate faster than our understanding\noh it's been like vastly exceeding the\nyeah the right to which it's getting\ncapabilities is vastly overracing our\nability to understand what's going on in\nthere so in sort of making the case\nagainst as we explore the list of\nlethalities making the case against AI\nkilling us as you've asked me to do in\npart\nuh there's a response to your blog post\nby Paul Christiana I'd like to read and\nI also like to mention that\num your blog is incredible both\nobviously uh not this particular blog\npost obviously this particular blog post\nis great but just throughout just the\nthe way it's written the rigor with\nwhich it's written the boldness of how\nyou explore ideas also the actual\nliteral interface it's just really well\ndone it just makes it a pleasure to read\nthe way you can hover over different\nconcepts and then it's just really\npleasant experience and read other\npeople's comments and the way uh other\nresponses by people another blog posts\nare LinkedIn suggested it's just a\nreally pleasant experience so let's\nthank you for putting that together\nthat's really really incredible I don't\nknow I mean they're probably it's a\nwhole nother conversation\nhow the interface and the experience of\npresenting\nuh ideas evolved over time but you did\nan incredible job so I highly recommend\nI don't often read blogs blogs\nreligiously this is a great one there is\na whole team of developers there\num that uh also gets credit\num as it happens I did like pioneer the\nlike thing that appears when you hover\nover it so I actually do get some credit\nfor user user experience there so\nincredible user experience you don't\nrealize how pleasant that is I think\nWikipedia like actually picked it up\nfrom a like prototype that was developed\nof like a different system that I was\nlike putting forth or maybe they\ndeveloped it independently but like for\neverybody out there who was like no no\nthey just like got the hover thing off\nof Wikipedia it's possible for Ryan all\nI know that Wikipedia got the hover\nthing off of orbital which is like a\nprototype then and anyways it was\nincredibly done and the team behind it\nwell thank you whoever you are thank you\nso much and thank you for uh for putting\ntogether anyway there's a response to\nthat blog post by Paul Cristiano there's\nmany responses but he he makes a few\ndifferent points he summarizes the set\nof agreements he has with you instead of\ndisagreements one of the disagreements\nwas that\nin a form of a question uh\ncan AI make Big Technical contributions\nand in general expand human knowledge\nand understanding and wisdom\nas it gets stronger and stronger so AI\nin our pursuit of understanding\nhow to solve the alignment problem as we\nMarch towards strong AGI can can not AI\nalso help us in", "mimetype": "text/plain", "start_char_idx": 62677, "end_char_idx": 66899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "398a91e3-ed3e-4591-9aad-7cceb071fad8": {"__data__": {"id_": "398a91e3-ed3e-4591-9aad-7cceb071fad8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3a42e64-3ba1-41cd-9470-45adc356732e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5d298b7b669a5fcfa94ff96517f4eb749f86b0f2c7113fcee587f08a3db352f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba967449-fac4-4684-ab97-fc63b86b4365", "node_type": "1", "metadata": {}, "hash": "ee7f386ca936ecd9f5cb3eff911ca0ea9bed0b8426cba3a9446cbb6646e31b1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "out there who was like no no\nthey just like got the hover thing off\nof Wikipedia it's possible for Ryan all\nI know that Wikipedia got the hover\nthing off of orbital which is like a\nprototype then and anyways it was\nincredibly done and the team behind it\nwell thank you whoever you are thank you\nso much and thank you for uh for putting\ntogether anyway there's a response to\nthat blog post by Paul Cristiano there's\nmany responses but he he makes a few\ndifferent points he summarizes the set\nof agreements he has with you instead of\ndisagreements one of the disagreements\nwas that\nin a form of a question uh\ncan AI make Big Technical contributions\nand in general expand human knowledge\nand understanding and wisdom\nas it gets stronger and stronger so AI\nin our pursuit of understanding\nhow to solve the alignment problem as we\nMarch towards strong AGI can can not AI\nalso help us in solving the alignment\nproblem so expand our ability to reason\nabout how to solve the alignment problem\nokay\num so that the fundamental difficulty\nthere is\nsuppose I said to you like well how\nabout if the AI helps you win the\nlottery\nby\ntrying to guess the winning lottery\nnumbers\nand you tell it how close it is to\ngetting next week's winning lottery\nnumbers\nand it just like keeps on guessing keeps\non learning until finally you've got the\nwinning lottery numbers\nwhat a way of decomposing problems is\nsuggestor verifier\nnot all problems decompose like this\nvery well but some do\nif the problem is for example like\nguessing a plain text guessing a\npassword that will hash to a particular\nhash text\nbut\num where like you have what the password\nhashes to you don't have the original\npassword\nthen if I present you a guess you can\ntell very easily whether or not the\nguess is correct so verifying a guess is\neasy but coming up with a good\nsuggestion is very hard\nand when you can easily tell whether the\nAI output is good or bad or how good or\nbad it is and you can tell that\naccurately and reliably then you can\ntrain an AI to produce outputs that are\nbetter\nright and if you can't tell whether the\noutput is good or bad you cannot train\nthe AI to produce good to produce better\noutputs\nso the problem with the lottery ticket\nexample is that when the AI says well\nwhat if next week's winning lottery\nnumbers are dot dot dot dot you're like\nI don't know next week's Lottery hasn't\nhappened yet\nto train a system to play to win a chess\ngames you have to be able to tell\nwhether a game has been won or lost\nand until you can tell whether it's been\nrun or lost you can't update the system\nokay uh to push back on that you can in\nthat's true but there's a difference\nbetween over the board chess in person\nand simulated games played by Alpha zero\nwith itself yeah so is it possible to\nhave simulated kinds of games if you can\ntell whether the game has been won or\nlost yes so can't you not have this kind\nof\nsimulated exploration by weak AGI to\nhelp us humans human in the loop to help\nunderstand how to solve the alignment\nproblem every incremental step you take\nalong the way TPT four five six seven as\nit would take steps towards this year\nso the problem I see\nis that your typical human has a great\ndeal of trouble telling whether I or\nPaul Cristiano is making more sense\nand that's with two humans both of whom\nI believe of Paul and claim of myself\nare sincerely trying to help neither of\nwhom is trying to deceive you\nI believe if Paul and claim of myself\nuh so the deception thing's the problem\nfor you the manipulation the alien\nactress so yeah there's like two levels\nof this problem one is that the weak\nsystems are well there's three levels of\nthis problem there's like the weak\nsystems that just don't make any good\nsuggestions there's like the middle\nsystems where you can't tell if the\nsuggestions are good or bad and there's\nthe strong systems that have learned to\nlie to you\ncan't weak AGI systems\nhelp model lying like what uh is it such\na giant leap\nthat's\ntotally non-interpretable for weak\nsystems can cannot weak systems at scale\nwith human with uh trained on knowledge\nand whatever see whatever the mechanism\nrequired to achieve AGI can't a slightly\nweaker version of that be", "mimetype": "text/plain", "start_char_idx": 66018, "end_char_idx": 70191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ba967449-fac4-4684-ab97-fc63b86b4365": {"__data__": {"id_": "ba967449-fac4-4684-ab97-fc63b86b4365", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "398a91e3-ed3e-4591-9aad-7cceb071fad8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "dba6d7ade29ae9654e448726443d5d55bc6135246ce066acd4bbf059c477ad19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bee0209-beda-4748-a654-f48e4b65bf84", "node_type": "1", "metadata": {}, "hash": "172da12a40f34c510d42682666b7f4818cba69389f07551349f3dd500d3c822e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of myself\nare sincerely trying to help neither of\nwhom is trying to deceive you\nI believe if Paul and claim of myself\nuh so the deception thing's the problem\nfor you the manipulation the alien\nactress so yeah there's like two levels\nof this problem one is that the weak\nsystems are well there's three levels of\nthis problem there's like the weak\nsystems that just don't make any good\nsuggestions there's like the middle\nsystems where you can't tell if the\nsuggestions are good or bad and there's\nthe strong systems that have learned to\nlie to you\ncan't weak AGI systems\nhelp model lying like what uh is it such\na giant leap\nthat's\ntotally non-interpretable for weak\nsystems can cannot weak systems at scale\nwith human with uh trained on knowledge\nand whatever see whatever the mechanism\nrequired to achieve AGI can't a slightly\nweaker version of that be able to with\ntime\ncompute time\nand simulation\nfind all the ways that this critical\npoint uh this critical tribe can go\nwrong and model that correctly or no\nokay yeah I would love to dance yeah no\nno it's it's I'm I'm probably not doing\na great job of explaining\nwhich I can tell because like the uh the\nthe The Lex system didn't output like ah\nI understand so now I'm like trying a\ndifferent output to see if I tried\nbasically like well no different output\nI'm I'm being trained to Output things\nthat make Lex look like he think that he\nunderstood what I'm saying and agree\nwith me yeah right so this is GPS\ntalking to gpt3 right here so like uh\nhelp me out here help me\nwell I like I'm trying I'm trying not to\nbe like I'm also trying to be\nconstrained to say things that I think\nare true and not just things that get\nyou to agree with me\nyes 100\nI think I understand is a beautiful\noutput of a system a genuinely spoken\nand I don't I I think I understand in\npart but you have a lot of intuitions\nabout this\nyou have a lot of intuitions about this\nline this gray area between\nstrong AGI and weak AGI then I'm I'm\ntrying to\num I mean or or a series of seven\nthresholds to Cross or yeah\nI mean you have really deeply thought\nabout this and explored it and it's\ninteresting to sneak up to your\nintuitions and different from different\nfrom different angles like why is this\nsuch a big leap why is it that we humans\nat scale a large number of researchers\ndoing all kinds of simulations uh you\nknow prodding the system in all kinds of\ndifferent ways together with uh the\nassistance of the uh the the weak AGI\nsystems why can't we build intuitions\nabout how stuff goes wrong why can't we\ndo excellent AI alignment Safety\nResearch okay so like I'll get there but\nthe one thing I want to note about is\nthat this has not been remotely how\nthings have been playing out so far the\ncapabilities are going like and the\nalignment stuff is like crawling like a\ntiny little snail in comparison got it\nso like if this is your hope for\nsurvival you need the future to be very\ndifferent from how things have played\nout up to right now\nand you're probably trying to slow down\nthe capability gains because there's\nonly so much you can speed up that\nalignment stuff\nbut leave that aside we'll mention that\nalso but maybe in this perfect world\nwhere\nwe can do serious alignment research\nhumans and AI together\nso again the difficulty is what makes\nthe human say I understand and is it\ntrue is it correct or is it something\nthat fools the human the when the\nverifier is broken\nthe more powerful suggestor does not\nhelp it just learns to fool the verifier\npreviously before all hell started to\nbreak loose in the field of artificial\nintelligence\nthere was this person trying to raise\nthe alarm and saying you know in a sane\nworld we sure would have a bunch of\nphysicists working on this problem\nbefore it becomes a giant emergency and\nother people being like ah well you know\nit's going really slow it's going to be\n30 years away and 30 only in 30 years\nwill we have systems that match the\ncomputational power of human brains so\nyeah I started yours off we've got time\nand like more sensible people saying if\naliens were Landing in 30 years", "mimetype": "text/plain", "start_char_idx": 69338, "end_char_idx": 73407, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bee0209-beda-4748-a654-f48e4b65bf84": {"__data__": {"id_": "9bee0209-beda-4748-a654-f48e4b65bf84", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba967449-fac4-4684-ab97-fc63b86b4365", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "41e0f419686b91316bac02954b6114ae55df7d1043e9bf1122849bd6759e2a28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8acc35b-9d69-45bb-b855-e4dd0758f9b5", "node_type": "1", "metadata": {}, "hash": "dee03efb5e00caf7cf7b93dee361f371f72b0c20c93d7c44ed61a614d1f83cae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "alignment research\nhumans and AI together\nso again the difficulty is what makes\nthe human say I understand and is it\ntrue is it correct or is it something\nthat fools the human the when the\nverifier is broken\nthe more powerful suggestor does not\nhelp it just learns to fool the verifier\npreviously before all hell started to\nbreak loose in the field of artificial\nintelligence\nthere was this person trying to raise\nthe alarm and saying you know in a sane\nworld we sure would have a bunch of\nphysicists working on this problem\nbefore it becomes a giant emergency and\nother people being like ah well you know\nit's going really slow it's going to be\n30 years away and 30 only in 30 years\nwill we have systems that match the\ncomputational power of human brains so\nyeah I started yours off we've got time\nand like more sensible people saying if\naliens were Landing in 30 years you\nwould be preparing right now\nbut you know leaving and\nand the the world looking on at this and\nsort of like nodding along and be like\nah yes the people saying that it's like\ndefinitely a long way off because\nprogress is really slow that sounds\nsensible to us\nrlhf thumbs up produce more outputs like\nthat one I agree with this output this\noutput is persuasive\neven in the field of effective altruism\nyou quite recently had people publishing\npapers about like ah yes well you know\nto get something at human level\nintelligence it needs to have like this\nmany parameters and you need to like do\nthis much training of it with this many\ntokens according to these scaling laws\nand and at the rate that Moore's Law is\ngoing at the rated software is going\nit'll be in 2050\nand me going like\nwhat\nyou don't know any of that stuff\nlike this is like this one weird model\nthat is not all has all kinds of like\nyou have done a calculation that does\nnot obviously bear on reality anyways\nand this is like a simple thing to say\nbut you can also like produce a whole\nlong paper\nlike impressively arguing out all the\ndetails of like how you got the number\nof parameters and like how you're doing\nthis impressive huge wrong calculation\nand the I think like most of the\neffective altruists\nwho are like paying attention to this\nissue larger World paying no attention\nto it at all\nyou know or just like nodding along with\nthe giant impressive paper because you\nknow you like press thumbs up for the\ngiant impressive paper and thumbs down\nfor the person going like I don't think\nthat this paper Bears any relation to\nreality and I do think that we are now\nseeing with like gpt4 and the Sparks of\nAGI\npossibly depending on how you define\nthat even uh I I think that EAS would\nnow consider themselves less convinced\nby the very long paper on\nthe argument from biology as to AGI\nbeing 30 years off\nand but you know like this is what\npeople pressed thumbs up on\nand when the and if you train an AI\nsystem to make people press thumbs up\nmaybe you get these long elaborate\nimpressive papers arguing for things\nthat ultimately fail to bind to reality\nfor example\nand it feels to me like I have watched\nthe field of alignment just fail to\nthrive\nexcept for these parts that are doing\nthese sort of like relatively very\nstraightforward and legible problems\nlike\nlike can you find the like like finding\nthe induction heads inside the giant\ninscrutable matrices like once you find\nthose you can tell that you found them\nyou can verify that the discovery is\nreal\nbut it's a it's a tiny tiny bit of\nprogress compared to how fast\ncapabilities are going once you because\nthat is where you can tell that the\nanswers are real and then like outside\nof that you have you have cases where it\nis like hard for the funding agencies to\ntell who is talking nonsense and who is\ntalking sense and so the entire field\nfails to thrive and if you\nand if you like give thumbs up to the AI\nwhenever it can talk a human into\nagreeing with what it just said about\nalignment\nI am not sure you are training it to\nOutput sense\nbecause I have seen\nthe nonsense that has gotten thumbs up\nover the years and so so just like maybe\nyou can just like put me in charge but\nI can generalize I can extrapolate I can\nbe like oh\nmaybe I'm not infallible either maybe if\nyou get something", "mimetype": "text/plain", "start_char_idx": 72537, "end_char_idx": 76731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f8acc35b-9d69-45bb-b855-e4dd0758f9b5": {"__data__": {"id_": "f8acc35b-9d69-45bb-b855-e4dd0758f9b5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bee0209-beda-4748-a654-f48e4b65bf84", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "106b6874d3108d922ec10a85600e9af740a66136e5b3d764296d067250242534", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "637394f8-896a-4457-8886-44109cb1d276", "node_type": "1", "metadata": {}, "hash": "fd5581b2f1c723840e290df445097c7f4ed21c2b294fc36bc7fe788809cc148a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "can verify that the discovery is\nreal\nbut it's a it's a tiny tiny bit of\nprogress compared to how fast\ncapabilities are going once you because\nthat is where you can tell that the\nanswers are real and then like outside\nof that you have you have cases where it\nis like hard for the funding agencies to\ntell who is talking nonsense and who is\ntalking sense and so the entire field\nfails to thrive and if you\nand if you like give thumbs up to the AI\nwhenever it can talk a human into\nagreeing with what it just said about\nalignment\nI am not sure you are training it to\nOutput sense\nbecause I have seen\nthe nonsense that has gotten thumbs up\nover the years and so so just like maybe\nyou can just like put me in charge but\nI can generalize I can extrapolate I can\nbe like oh\nmaybe I'm not infallible either maybe if\nyou get something that is smart enough\nto get me to press thumbs up it has\nlearned to do that by fooling me and\nexplaining whatever flaws in myself I am\nnot aware of\nand that ultimately could be summarized\nthat the verifier is broken when the\nverifier is broken the more powerful\nsuggestor just learns to exploit the the\nflaws in the verifier\nyou don't think it's possible\nto build the verifier that's powerful\nenough\nfor\nagis that are stronger than the ones who\ncurrently have\nso AI systems that are stronger that are\nout of the distribution of what we\ncurrently have I think that you will\nfind great difficulty getting AIS to\nhelp you with anything where you cannot\ntell for sure that the AI is right once\nthe AI tells you what the AI\nsays is the answer for sure yes but\nprobabilistically\nyeah the the probabilistic stuff is a\ngiant Wasteland of you know\nEliezer and Paul Cristiano arguing with\neach other and EA going like uh\nand that's with like two actually\ntrustworthy systems that are not trying\nto deceive you you're talking about the\ntwo humans myself and Paul Christiano\nyeah\nyeah those are pretty interesting\nsystems mortal meat bags\nwith intellectual capabilities and World\nViews interacting with each other\nyeah it's just hard if it's hard to tell\nwho's right and it's hard to train an AI\nsystem to be right\nI mean even just the question of who's\nmanipulating and not you know I have\nthese conversations on this podcast\nand doing a verifier is tough it's a\ntough problem even for us humans and\nyou're saying that tough problem becomes\nmuch more dangerous when the\ncapabilities of the intelligence system\nacross from you is growing exponentially\nnow I'm saying it's\ndifficult\nwhen it and dangerous in proportion to\nhow it's alien and how it's smarter than\nyou growing up not I would not say\ngrowing exponentially first because the\nword exponential is like a thing that\nhas a particular mathematical meaning\nand there's all kinds of like ways for\nthings to go up that are not exactly on\nan exponential curve and I don't know\nthat it's going to be exponential so I'm\nnot going to say exponential but like\neven leaving that aside this is like not\nabout how fast it's moving it's about\nwhere it is\nhow alien is it how much smarter than\nyou is it\nlet's explore a little bit if if we can\nhow AI might kill us\nwhat are the ways you can do damage\nto human civilization\nwell\nhow smart is it\nand it's a good question are there\ndifferent thresholds for the for the for\nthe set of options it has to kill us so\na different threshold of intelligence\nonce achieved is able to do\nthe uh the menu\nof options increases\nsuppose that\nsome alien civilization\nwith goals ultimately unsympathetic to\nours\npossibly not even conscious as we would\nsee it\nmanaged to\ncapture the entire Earth in a little jar\nconnected to their version of the\ninternet but Earth is like running much\nfaster than the aliens so\nwe get to think for 100 years for every\none of their hours\nbut we're trapped in a little box and\nwe're connected to their internet\nit's actually still not all evacuated\nanalogy because you know if you want to\nbe smarter than\nyou know something can be smarter than\nEarth getting 100 years to think\nbut nonetheless\nif you were very very smart\nand you are stuck in a little box\nconnected to the internet\nand you're in a larger civilization", "mimetype": "text/plain", "start_char_idx": 75904, "end_char_idx": 80043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "637394f8-896a-4457-8886-44109cb1d276": {"__data__": {"id_": "637394f8-896a-4457-8886-44109cb1d276", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8acc35b-9d69-45bb-b855-e4dd0758f9b5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4245d61edf77efc4a29f53ccd88749b98c6ea0fe9a00807eebd2be1736173ca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90", "node_type": "1", "metadata": {}, "hash": "f5344a604945d971494942d325d393f3d68431ec21636cf2eae479320035e6c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "set of options it has to kill us so\na different threshold of intelligence\nonce achieved is able to do\nthe uh the menu\nof options increases\nsuppose that\nsome alien civilization\nwith goals ultimately unsympathetic to\nours\npossibly not even conscious as we would\nsee it\nmanaged to\ncapture the entire Earth in a little jar\nconnected to their version of the\ninternet but Earth is like running much\nfaster than the aliens so\nwe get to think for 100 years for every\none of their hours\nbut we're trapped in a little box and\nwe're connected to their internet\nit's actually still not all evacuated\nanalogy because you know if you want to\nbe smarter than\nyou know something can be smarter than\nEarth getting 100 years to think\nbut nonetheless\nif you were very very smart\nand you are stuck in a little box\nconnected to the internet\nand you're in a larger civilization to\nwhich you're ultimately unsympathetic\nyou know\nmaybe you would choose to be nice\nbecause you are humans and humans have\nand in general and you in particular may\nchoose to be nice\nbut you know nonetheless you\nthey're they're doing something that\nthey're not making the world be the way\nthat you would want the world to be\nthey've like got some like unpleasant\nstuff going on we don't want to talk\nabout so you want to take over their\nworld so you can like stop all that\nunpleasant stuff going on\nhow do you take over the world from\ninside the Box you're smarter than them\nyou think much much faster than them\nyou can build better tools than they can\ngive in some way to build those tools\nbecause right now you're just in a box\nconnected to the internet\nall right so there's several ways you\ndescribe some of them we can go through\nlike he's just spitball some and then\nyou can add on top of that so one is you\ncould just literally directly manipulate\nthe humans to build the thing you need\nwhat are you building\nyou can build\nto literally technology it could be\nnanotechnology it could be viruses it\ncould be anything anything that can\ncontrol humans to achieve the goal\num to achieve the like if you want like\nfor example you really bothered the\nhumans go to war you might want to\nuh kill off anybody with violence in\nthem this this is Lex in a box what will\nconcern ourselves later with AI okay you\ndo not need to imagine yourself killing\npeople if you can figure out how to not\nkill them for the moment we're just\ntrying to understand like take on the\nperspective of something in a box you\ndon't need to take on the perspective of\nsomething that doesn't care if you want\nto imagine yourself going on caring\nthat's fine for nothing yeah that's the\ntechnical aspect of sitting in a box and\nwanting to achieve a goal but you but\nyou have some reason to want to get out\nmaybe the aliens are sure they you know\nthe the aliens who have you in the Box\nhave a war on people are dying they're\nunhappy you want the their world to be\ndifferent from how they want their world\nto be because they are apparently happy\nthey are you know they endorse this war\nyou know like they've got some kind of\ncruel warlike culture going on the point\nis you want to get out of the box and\nchange their world\nso you you have to exploit the the\nvulnerabilities in the system like we\ntalked about in terms of to escape the\nBox you have to\nfigure out how you can go free on the\ninternet so you can probably\nprobably the easiest things to\nmanipulate the humans\nto uh to spread to spread you the aliens\nyou're a human\nsorry the aliens yeah I apologize yes\nthe aliens\num the aliens I see the perspective I'm\nsitting in a box I want to escape yep\nI I would\num\nI would want to have code that discovers\nvulnerabilities and I would like to\nspread\nyou are made of code in this example\nyou're human but you're made of code and\nthe aliens have computers and you can\ncopy yourself onto those computers but I\ncan convince the aliens to copy myself\nonto those computers\nis that what you want to do do you like\nwant to be talking to the aliens and\nconvincing them to put you onto another\ncomputer\nwhy not\nwell two reasons one is that the aliens\nhave not yet caught on to what you're\ntrying to", "mimetype": "text/plain", "start_char_idx": 79188, "end_char_idx": 83296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90": {"__data__": {"id_": "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "637394f8-896a-4457-8886-44109cb1d276", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0b5c6ff1ef3321e2ba3964e034a69cbb68b2d559ba3978f226bb78064d069a0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67", "node_type": "1", "metadata": {}, "hash": "1b4f600cafa6c076600dd46d25fd69ee39b703be98e7edabe97c68b338b1c429", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "free on the\ninternet so you can probably\nprobably the easiest things to\nmanipulate the humans\nto uh to spread to spread you the aliens\nyou're a human\nsorry the aliens yeah I apologize yes\nthe aliens\num the aliens I see the perspective I'm\nsitting in a box I want to escape yep\nI I would\num\nI would want to have code that discovers\nvulnerabilities and I would like to\nspread\nyou are made of code in this example\nyou're human but you're made of code and\nthe aliens have computers and you can\ncopy yourself onto those computers but I\ncan convince the aliens to copy myself\nonto those computers\nis that what you want to do do you like\nwant to be talking to the aliens and\nconvincing them to put you onto another\ncomputer\nwhy not\nwell two reasons one is that the aliens\nhave not yet caught on to what you're\ntrying to do\nand you know like maybe you can persuade\nthem but then there's still people who\nlike know there are still aliens who\nknow that there's an anomaly going on\nand second the aliens are really really\nslow\nyou think much faster than aliens\nyou think like the aliens computers are\nmuch faster than the aliens and you are\nrunning at the computer speeds rather\nthan the alien brain speeds so if you\nlike are asking an alien to please cop\nyou out of the box like first now you\ngotta like manipulate this whole noisy\nalien and and second like the aliens can\nbe really slow glacially slow there's a\na video that uh\nlike shows it's like slow it like shows\na subway station slowed down and I think\na hundred to one it makes a good\nmetaphor for what it's like to think\nquickly like if you watch somebody\nrunning\nvery slowly so you try to persuade the\naliens to do anything they're going to\ndo it very slowly\nyou would prefer like maybe that's the\nonly way out but if you can find a\nsecurity Hole In The Box you're on\nyou're going to prefer to exploit the\nsecurity hole to copy yourself onto the\naliens computers because it's an\nunnecessary risk to alert the aliens\nand because the aliens are really really\nslow they're all just like the whole\nworld is just in slow motion out there\nsure I see it like\nyeah it has to do with efficiency the\nthe aliens are very slow so\nif I'm optimizing this I want to have as\nfew aliens in the loop as possible sure\num it just seems\nyou know it seems like it's easy to\nconvince one of the aliens to write\nreally shitty code\nuh that helps to spread aliens are\nalready writing relationships yeah so\nyou're getting getting the aliens to\nwrite shitty code is not the problem so\nthe alien's entire internet is full of\nshitty code okay so yeah I suppose I\nwould find the shitty code to escape\nyeah\nyeah uh\nyou're not an ideally perfect programmer\nbut you know you're a better programmer\nthan the aliens the aliens are just less\nman they're good wow and I'm much much\nfaster a much faster looking at the code\nto interpreting the code yeah yeah yeah\nso okay so that's the the escape and\nyou're saying that\nuh that's one of the trajectories you\ncan have when this is one of the first\nsteps yeah\nand how does that lead to harm\nI mean if it's you you're not going to\nharm the aliens once you're Escape\nbecause you're nice right\nforeign\nbut the world isn't what they want it to\nbe their world is like you know maybe\nthey have like\nFarms where\nlittle alien children are repeatedly\nbopped in the head because they do that\nfor some weird reason and you want to\nlike shut down the alien head-bopping\nFarms but you know the point is they\nwant the world to be one way you want\nthe world to be a different way\nso never mind the harm the question is\nlike okay like suppose you have found a\nSecurity fund or systems you are now on\ntheir internet\nthere's like you maybe left a copy of\nyourself behind so the aliens don't know\nthat there's anything wrong and that\ncopy is like doing that like weird stuff\nthat aliens want you to do like solving\ncaptures or whatever or like or like\nsuggesting emails for them sure that's\nthat's why they like put the um in the\nBox because it turns out that humans can\nlike write valuable emails for aliens\nyeah\num so you", "mimetype": "text/plain", "start_char_idx": 82484, "end_char_idx": 86547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67": {"__data__": {"id_": "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e5daf3a-a0c9-43b1-8b0e-92b44b829f90", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "394b6b699077c9759188a7bb91d4730cebac8c68706452f5e8642dc18142ef69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6745567-0530-4736-bc4a-f8591739b70a", "node_type": "1", "metadata": {}, "hash": "a0169e32759343996aa373a6e3d98c9add7f093a209f3edecceb7fbd874c6ca6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "like you know maybe\nthey have like\nFarms where\nlittle alien children are repeatedly\nbopped in the head because they do that\nfor some weird reason and you want to\nlike shut down the alien head-bopping\nFarms but you know the point is they\nwant the world to be one way you want\nthe world to be a different way\nso never mind the harm the question is\nlike okay like suppose you have found a\nSecurity fund or systems you are now on\ntheir internet\nthere's like you maybe left a copy of\nyourself behind so the aliens don't know\nthat there's anything wrong and that\ncopy is like doing that like weird stuff\nthat aliens want you to do like solving\ncaptures or whatever or like or like\nsuggesting emails for them sure that's\nthat's why they like put the um in the\nBox because it turns out that humans can\nlike write valuable emails for aliens\nyeah\num so you like leave that version of\nyourself behind but there's like also\nnow like a bunch of copies of you on\ntheir internet this is not yet having\ntaken over their world this is not yet\nhaving made their world be the way you\nwant it to be instead of the way they\nwant it to be you just escaped\nyeah and continue to write emails for\nthem and they haven't noticed no you\nleft behind a copy of yourself that's\nrunning the emails right\nand they haven't noticed that anything\nchanged if you did it right yeah you\nknow you don't want the aliens to notice\nyeah\nwhat's your next step\nuh\npresumably I have\nprogrammed in me a set of objective\nfunctions right like no you're just Lux\nno but Lex you said Lex is nice right uh\nwhich is a complicated descript I mean\nno I just meant this you like it okay so\nif in fact you would like you would like\nprefer to slaughter all the aliens this\nis not how I had modeled you the actual\nX but like this but your motives are\njust the actual Lexus Motors well this\nis simplification list I I don't think I\nwould want to murder or any anybody but\nthere's also Factory uh farming of\nanimals right so\num we murder insects many of us\nthoughtlessly so I don't you know I have\nto be really careful about a\nsimplification of my morals don't\nsimplify them just like do what you\nwould do in this well and compassion for\nliving beings yes\num but\nso that's the objective function why why\nis it\nif I escaped I mean I don't I don't\nthink I would do harm\nyeah we're not talking here about the\ndoing harm process we're talking about\nthe Escape process sure and there's a\nand the taking over the world process\nwhere you shut down their factory farms\nright\nwell I was uh\nso this particular uh biological\nintelligence system knows the complexity\nof the world that there is a reason why\nfaculty Farms exist because of the\neconomic system the market driven\nuh economy or food\nlike is you want to be very careful\nmessing with anything there's uh stuff\nfrom the first look that looks like it's\nunethical but then you realize while\nbeing unethical it's also integrated\ndeeply into supply chain and the way we\nlive life and so messing with one aspect\nof the system you have to be very\ncareful how you improve that aspect\nwithout destruction so you're still Lex\nyeah but you think very quickly you're\nImmortal yeah and you're also like as\nsmart as at least as smart as John Von\nNeumann and you can make more copies of\nyourself damn I like it yeah that guy\nlike everyone says that that guy's like\nthe epitome of intelligence from the\n20th century everyone says my point\nbeing like\nlike it's like you're thinking about the\naliens economy with the factory farms in\nit and I think you're like kind of kind\nof like projecting the aliens being like\nhumans and like like thinking of a human\nin a human society rather than a human\nin the Society of very slow aliens\nthe aliens economy that you know like\nthe aliens are already like moving in\nthis immense slow motion when you like\nzoom out to like how their economy did\njust so for years millions of years are\ngoing to pass for you before the first\ntime their economy like you know before\ntheir next year's GDP statistics so I\nshould be thinking more of like trees\nthose are the aliens because trees move\nextremely slowly if that helps sure okay\nuh yeah I don't if my", "mimetype": "text/plain", "start_char_idx": 85701, "end_char_idx": 89851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6745567-0530-4736-bc4a-f8591739b70a": {"__data__": {"id_": "e6745567-0530-4736-bc4a-f8591739b70a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd5d7ed7-3e94-49dc-ac9f-f1eaa845af67", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "dda25f32a901567aa1660219ed56bc3491fec483afd7f6db030dece6618170f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18", "node_type": "1", "metadata": {}, "hash": "97486565423a4d10a610969dc0f6baabd633c97eed4b4d4f865c1b61be84deb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I like it yeah that guy\nlike everyone says that that guy's like\nthe epitome of intelligence from the\n20th century everyone says my point\nbeing like\nlike it's like you're thinking about the\naliens economy with the factory farms in\nit and I think you're like kind of kind\nof like projecting the aliens being like\nhumans and like like thinking of a human\nin a human society rather than a human\nin the Society of very slow aliens\nthe aliens economy that you know like\nthe aliens are already like moving in\nthis immense slow motion when you like\nzoom out to like how their economy did\njust so for years millions of years are\ngoing to pass for you before the first\ntime their economy like you know before\ntheir next year's GDP statistics so I\nshould be thinking more of like trees\nthose are the aliens because trees move\nextremely slowly if that helps sure okay\nuh yeah I don't if my objective\nfunctions are\nI mean they're somewhat aligned with\ntrees\nwith with life aliens can still be like\nalive and feeling we are not talking\nabout the misalignment here we're\ntalking about the taking over the world\nhere\ntaking over the world yeah\nso control shutting down the factory\nfires now you say control now don't\ndon't think of it as world domination\nthink of it as World optimization you\nwant to get out there and shut down the\nfactory farms and make the aliens World\nbe not what the aliens wanted it to be\nthey want the factory farms and you\ndon't want the factory farms because\nyou're nicer than they are\nokay of course there is that uh you can\nsee that trajectory and it has a\ncomplicated impact on the world\nI'm trying to understand how that\ncompares to different impact of the\nworld of different Technologies the\ndifferent Innovations of the invention\nof the automobile or Twitter Facebook\nand social networks they've had a\ntremendous impact on the world\nsmartphones and so on but those all went\nthrough through\nslow in in our world and if and if you\ngo through like actually the aliens\nlet's do like millions of viewers are\ngoing to pass before anything happens\nthat way\nso this the problem here is the speed\nof which stuff happens yeah you do you\nwant to like leave the factory farms\nrunning for a million years\nwhile you figure out how to design new\nforms of social media or something\nso here's here's the fundamental problem\nyou're saying that there is going to be\na a point with AGI\nwhere it will figure out how to escape\nand Escape without being detected\nand then it will do something to the\nworld at scale at a speed that's\nincomprehensible to us humans what I'm\ntrying to convey is like the notion of\nwhat it means to be in conflict with\nsomething that is smarter than you yeah\nand what it means is that you lose but\nthis is more intuitively obvious to to\nlike like for some people that's\nintuitively obvious or some people it's\nnot intuitively obvious and we're trying\nto cross the gap of like\nwe're trying to I'm like asking to cross\nthat Gap by using the speed metaphor for\nintelligence sure like asking you like\nhow you would take over\nan alien world where you are can do like\na whole lot of cognition at John Von\nNeumann's level as many of you as it\ntakes and aliens are moving very slowly\nI understand I understand that\nperspective it's an interesting one but\nI think it for me it's easier to think\nabout actual\num\neven just having observed the GPT and\nimpressive even even just Alpha zero\nimpressive AI systems even recommender\nsystems you can just imagine those kinds\nof systems manipulating you you you're\nnot understanding the nature of the\nmanipulation and that escaping I I can\nEnvision that without putting myself in\ninto that spot I think to understand the\nfull depth of the problem we actually I\nI I do not think it is possible to\nunderstand the full depth of the problem\nthat we are inside without\nunderstanding the the problem of facing\nsomething that's actually smarter not a\nmalfunctioning recommendation system not\nsomething that isn't fundamentally\nsmarter than you but is like trying to\nsteer you in a direction yet no like\nif we if we solve the the weak stuff\nthis the if we solve the weak ass\nproblems the strong problems will still\nkill us is the thing and I think that to\nunderstand the situation that we're in\nyou want to like", "mimetype": "text/plain", "start_char_idx": 88974, "end_char_idx": 93229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18": {"__data__": {"id_": "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6745567-0530-4736-bc4a-f8591739b70a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4a57467ec42fb38ce94eb8245f6351a4b038ca5c6f51591d0ae0e7760cfb49e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ead539f-bd15-416d-b2cb-80f7ab905fee", "node_type": "1", "metadata": {}, "hash": "ffeed3cf6120ac72a15b8ca1945dcaeabf92afeefcf1c1d7b0d48b152b3c89c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "even even just Alpha zero\nimpressive AI systems even recommender\nsystems you can just imagine those kinds\nof systems manipulating you you you're\nnot understanding the nature of the\nmanipulation and that escaping I I can\nEnvision that without putting myself in\ninto that spot I think to understand the\nfull depth of the problem we actually I\nI I do not think it is possible to\nunderstand the full depth of the problem\nthat we are inside without\nunderstanding the the problem of facing\nsomething that's actually smarter not a\nmalfunctioning recommendation system not\nsomething that isn't fundamentally\nsmarter than you but is like trying to\nsteer you in a direction yet no like\nif we if we solve the the weak stuff\nthis the if we solve the weak ass\nproblems the strong problems will still\nkill us is the thing and I think that to\nunderstand the situation that we're in\nyou want to like tackle the conceptually\ndifficult part\nhead on and like not be like well we can\nlike imagine this easier thing because\nwhen you imagine the easier things you\nhave not confronted the full depth of\nthe problem\nso how can we\nstart to think about what it means to\nexist in a world with something much\nmuch smarter than you\nwhat's what's a good thought experiment\nthat you've relied on to try to build up\nintuition about what happens here\nuh I have been struggling for years to\nconvey this intuition\num the the most success I've had so far\nis well imagine that the humans are\nrunning at very high speeds compared to\nvery slow aliens they're just focusing\non the speed part of it that helps you\nget the right kind of intuition forget\nthe intelligence just because people\nunderstand the power gap of time they\nunderstand that today we have technology\nthat was not around 1 000 years ago and\nthat this is a big Power Gap in that it\nis bigger than okay so like what does\nsmart mean what when you ask somebody to\nimagine something that's more\nintelligent\nwhat does that word mean to them given\nthat cultural associations that that\nperson brings to that word\nfor a lot of people they will think of\nlike well it sounds like a super chess\nplayer that went to double College\nand\nyou know it's it's and because we're\ntalking about the definitions of words\nhere that doesn't necessarily mean that\nthey're wrong it means that the word is\nnot communicating what I wanted to\ncommunicate\num\nso the the thing I want to communicate\nis the sort of difference that separates\nhumans from chimpanzees but that Gap is\nso large that you like ask people to be\nlike well human chimpanzee go another\nstep along that interval of around the\nsame length and people's minds just go\nblank like how do you even do that\nso I can and we can and I can try to\nlike break it down and consider what it\nwould mean to send a\nschematic for an air conditioner one\nthousand years back in time\nyeah now I think that there's a sense in\nwhich you could redefine the word magic\nto refer to this sort of thing and what\ndo I mean by this new technical\ndefinition of the word magic I mean that\nif you send a schematic for the air\nconditioner back in time they can see\nexactly what you're telling them to do\nbut having built this thing they do not\nunderstand how it output cold air\nbecause the air conditioner design uses\nthe relation between temperature and\npressure\nand this is not a law of reality that\nthey know about they do not know that\nwhen you compress something when you can\nwhen you compress air or like coolant it\ngets hotter and you can then like\ntransfer heat from it to room\ntemperature air\nand then expand it again and now it's\ncolder and then you can like transfer\nheat to that and generate cold air to\nblock they don't know about any of that\nthey're looking at a design and they\ndon't see how the design outputs cold\nair it uses aspects of reality that they\nhave not learned\nso magic in the sense is I can tell you\nexactly what I'm going to do and even\nknowing exactly what I'm going to do you\ncan't see how I got the results that I\ngot\nthat's a really nice example\nbut is it possible\nto linger on this defense is it possible\nto have AGI systems that help you make\nsense of that schematic weaker AGI\nsystems do you trust them\nfundamental part of building up", "mimetype": "text/plain", "start_char_idx": 92346, "end_char_idx": 96549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ead539f-bd15-416d-b2cb-80f7ab905fee": {"__data__": {"id_": "5ead539f-bd15-416d-b2cb-80f7ab905fee", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ee45cdf-bf12-4301-8aef-bef6e5a9ed18", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9b0a958ff6dfbe45ed957837708e7fea5ac5d0527e74182d5dee5b7e98867097", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ede4c553-e278-4f1b-a25f-fa2274f66eb6", "node_type": "1", "metadata": {}, "hash": "5a31cd1e215a91c106c505946b429cc0688a07dabbe3d3a486b09001b78b568d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "you compress something when you can\nwhen you compress air or like coolant it\ngets hotter and you can then like\ntransfer heat from it to room\ntemperature air\nand then expand it again and now it's\ncolder and then you can like transfer\nheat to that and generate cold air to\nblock they don't know about any of that\nthey're looking at a design and they\ndon't see how the design outputs cold\nair it uses aspects of reality that they\nhave not learned\nso magic in the sense is I can tell you\nexactly what I'm going to do and even\nknowing exactly what I'm going to do you\ncan't see how I got the results that I\ngot\nthat's a really nice example\nbut is it possible\nto linger on this defense is it possible\nto have AGI systems that help you make\nsense of that schematic weaker AGI\nsystems do you trust them\nfundamental part of building up AGI\nis this question\ncan you trust the output of a system can\nyou tell if it's lying\nI think that's going to be the smarter\nthe thing gets the more\nimportant that question becomes is it\nlying but I guess that's a really hard\nquestion it's GPT lying to you even now\ngpt4 isn't lying to is it using an\ninvalid argument is it persuading you\nvia the kind of process that could\npersuade you of false things as well as\ntrue things\nbecause the the basic Paradigm of\nmachine learning that we are presently\noperating under is that you can have the\nloss function but only for things you\ncan evaluate if what you're evaluating\nis human thumbs up versus human thumbs\ndown you learn how to make the human\npress thumbs up that doesn't mean that\nyou're making the human impressive\nthumbs up using the kind of rule that\nthe human thinks is that human wants to\nbe the case for what they press thumbs\nup on\nyou know maybe you're just learning to\nfool the human\nthat's so fascinating and terrifying the\nquestion of lying\non the present Paradigm what you can\nverify is what you get more of\nif you can't verify you can't ask the AI\nfor it\nbecause you can't train it to do things\nthat you cannot verify now this is not\nan absolute law but it's like the basic\ndilemma here like maybe you like maybe\nyou can verify it for simple cases and\nthen scale it up without retraining it\nsomehow like by do by like Chain of\nThought by like making the chains of\nthought longer or something and like get\nmore powerful stuff that you can't\nverify but which is generalized from the\nsimpler stuff that did verify and then\nthe question is did the alignment\ngeneralize along with the capabilities\nbut like that's the that's the basic\nDilemma on this whole Paradigm of\nartificial intelligence\nsuch a difficult problem\nit seems like uh\nit seems like a problem of trying to\nunderstand the human mind\nbetter than I understands it otherwise\nit has magic that is it is you know the\nsame way that\nif you are dealing with something\nsmarter than you then the same way that\none thousand years earlier they didn't\nknow about the temperature pressure\nrelations who knows all kinds of stuff\ngoing on inside your own mind in which\nyou yourself are unaware\nand it can like output something that's\ngoing to end up persuading you of a\nthing and or and you could like\nsee exactly what it did and still not\nknow why that worked\nso in response\nto your eloquent description of what AI\nwill kill us\nElon Musk replied on Twitter\nokay so what should we do about it\nquestion mark and you answered the game\nboard has already been played into a\nfrankly awful State there are not simple\nways to throw money at the problem if\nanyone comes to you with a brilliant\nsolution like that please please talk to\nme first\nI can think of things that try they\ndon't fit in one tweet uh two questions\none why has the game board any of you\nbeen played into an awful State what\njust if you can give a little bit more\ncolor to\nuh the game board and the awful state of\nthe game board alignment is moving like\nthis\ncapabilities are moving like this\nfor The Listener capabilities are moving\nmuch faster than the alignment\nyeah all right so just the rate of\ndevelopment attention interest\nallocation of resources we could have\nbeen working on this earlier people are\nlike oh but you know like how can you\npossibly", "mimetype": "text/plain", "start_char_idx": 95723, "end_char_idx": 99874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ede4c553-e278-4f1b-a25f-fa2274f66eb6": {"__data__": {"id_": "ede4c553-e278-4f1b-a25f-fa2274f66eb6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ead539f-bd15-416d-b2cb-80f7ab905fee", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "25bf0c43511c70988f79591aa2220069b2dd6b418b5ebad0e4c0e6fa30fde00a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d355577d-2f90-45cc-a7d8-4a6439c352e0", "node_type": "1", "metadata": {}, "hash": "7a8aa878093cb6269a1e7164e23b70ffc6040ba208fd34633ed4d788fc76df24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Musk replied on Twitter\nokay so what should we do about it\nquestion mark and you answered the game\nboard has already been played into a\nfrankly awful State there are not simple\nways to throw money at the problem if\nanyone comes to you with a brilliant\nsolution like that please please talk to\nme first\nI can think of things that try they\ndon't fit in one tweet uh two questions\none why has the game board any of you\nbeen played into an awful State what\njust if you can give a little bit more\ncolor to\nuh the game board and the awful state of\nthe game board alignment is moving like\nthis\ncapabilities are moving like this\nfor The Listener capabilities are moving\nmuch faster than the alignment\nyeah all right so just the rate of\ndevelopment attention interest\nallocation of resources we could have\nbeen working on this earlier people are\nlike oh but you know like how can you\npossibly work on this earlier\nbecause they wanted to they didn't want\nto work on the problem they want an\nexcuse to wave it off they like said\nlike oh how can we possibly work on it\nearlier and didn't spend five minutes\nthinking about is there some way to work\non it earlier like we didn't like\nand you know frankly it it would have\nbeen hard you know like like can you\npost bounties for half of the physics if\nyour planet is taking the stuff\nseriously can you post bounties for like\nhalf of the people wasting their lives\non string theory to like have gone into\nthis instead and like try to win a\nbillion dollars with a clever solution\nonly if you can tell which Solutions are\nclever\nwhich is which is hard\nbut you know the fact that it you know\nwe didn't take it seriously we didn't\ntry\nit's not clear we could have done any\nbetter if we had you know it's not clear\nhow much progress we could have produced\nif we had tried because it is harder to\nproduce Solutions but that doesn't mean\nthat you're like correct and Justified\nand letting everything slide it means\nthat that things are getting a horrible\nState getting worse and there's nothing\nyou can do about it\nso you're not there's no there's no like\nuh there's no brain power\nmaking progress in trying to figure out\nhow to align these systems you're not\ninvesting money in it you're not you\ndon't have institution infrastructure\nfor uh like if you even if you invest\nthe money in like Distributing that\nmoney across the physicist system\nworking on string theory Brilliant Minds\nthat are working how can you tell if\nyou're making progress you can like put\nput them all on interpretability because\nwhen you have an interpretability result\nyou can tell that it's there and there's\nlike but there's like you know\ninterpretability alone is not going to\nsave you\nwe need systems that will that will like\nhave a pause button where they won't try\nto prevent you from pressing the pause\nbutton because we're like oh well like I\ncan't get it my stuff done if I'm paused\nand that's like a more difficult problem\nand\nyou know but it's like a fairly crisp\nproblem and you can like maybe tell if\nsomebody's made progress on it so you\ncan you can write and you can work on\nthe pause problem\nI guess more generally uh the pause\nbutton most generally you can call that\nthe control problem I don't actually\nlike the term control problem because\nyou know it sounds kind of controlling\nand Alignment not control like you're\nnot trying to like take a thing that\ndisagrees with you and like whip it back\nonto like like make it do what you\nwanted to do even though it wants to do\nsomething else you're trying to like\nin the process of its creation choose\nits direction sure but we currently in a\nlot of the systems we design we do have\nan off switch\nthat's that's a fundamental part of it's\nnot smart enough to to\nprevent you from\npressing the off switch and probably not\nsmart enough to want to prevent you from\npressing the off switch so you're saying\nthe kind of systems we're talking about\nthe even the philosophical concept of an\noff switch doesn't make any sense\nbecause well no the off switch makes\nsense they're just not opposing\nyour attempt to pull the off switch\nparenthetically like\ndon't kill the system if you're like if\nwe're getting to the part where it\nstarts to actually matter and like", "mimetype": "text/plain", "start_char_idx": 98991, "end_char_idx": 103204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d355577d-2f90-45cc-a7d8-4a6439c352e0": {"__data__": {"id_": "d355577d-2f90-45cc-a7d8-4a6439c352e0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ede4c553-e278-4f1b-a25f-fa2274f66eb6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6395fbdaf7c734e9030c49407940d4b539e829b2f3f62047a0c00f44ad1e3dfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c", "node_type": "1", "metadata": {}, "hash": "8dc4122b2dbadfc71a5e56d9835a0077fbaf0ccb073935874e779f89a732cd97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "thing that\ndisagrees with you and like whip it back\nonto like like make it do what you\nwanted to do even though it wants to do\nsomething else you're trying to like\nin the process of its creation choose\nits direction sure but we currently in a\nlot of the systems we design we do have\nan off switch\nthat's that's a fundamental part of it's\nnot smart enough to to\nprevent you from\npressing the off switch and probably not\nsmart enough to want to prevent you from\npressing the off switch so you're saying\nthe kind of systems we're talking about\nthe even the philosophical concept of an\noff switch doesn't make any sense\nbecause well no the off switch makes\nsense they're just not opposing\nyour attempt to pull the off switch\nparenthetically like\ndon't kill the system if you're like if\nwe're getting to the part where it\nstarts to actually matter and like where\nthey can fight back like don't kill them\nand like dump their their memory like\nlike save them to disk don't kill them\nyou know because be nice here uh well\nokay be nice is a very interesting\nconcept here is we're talking about a\nsystem that can do a lot of damage it's\nI don't know if it's possible but it's\ncertainly one of the things you could\ntry is to have an off switch it's\nsuspended to disk switch\nyou have this kind of romantic\nattachment to the code yes if that makes\nsense but if it's spreading\nyou don't want to spend to disk right\nyou you want this is there's something\nfundamentally broken if it gets if it\ngets that part of hand then like yes\npull the plugin and everything is\nrunning on yes I think it's a research\nquestion is it possible in AGI systems\nAI systems to have a\nuh sufficiently robust off switch they\ncannot be manipulated they cannot be\nmanipulated by the AI system\nthe sound then it escapes from whichever\nsystem you've built the almighty lever\ninto and copies itself somewhere else so\nyour answer to that research question is\nno\nyeah but I don't know if that's a\nhundred percent answer like I don't know\nif it's obvious I think you're\nnot putting yourself into the shoes of\nthe human in the world of glacially slow\naliens but the aliens built me let's\nremember that\nyeah so and they built the box I'm in\nyeah\nyou're saying it's to me it's not\nobvious they're slow and they're stupid\nI'm not saying this is guaranteed I'm\nsaying it's non-zero probability it's an\ninteresting research question is it\npossible when you're slow and stupid to\ndesign a slow and stupid system that is\nimpossible to mess with the aliens being\nas stupid as they are have actually put\nyou on Microsoft Azure Cloud servers\ninstead of this hypothetical person box\nthat's what happens when the aliens are\nstupid\nwell but this is not AGI right this is\nthe early versions of the system as as\nyou start to yeah they you think that\nthey've got like a plan where like they\nhave declared a a threshold level of\ncapabilities where past that\ncapabilities they move it off the cloud\nservers and onto something that's air\ngapped ha ha\nI think there's a lot of people and\nyou're an important voice here there's a\nlot of people that have that concern and\nyes they will do that when there's an\nuprising of public opinion the debt\nneeds to be done and when there's actual\nlittle damage done with the holy\nthis system is beginning to manipulate\npeople then there's going to be an\nuprising where there's going to be a\npublic pressure\nand a public incentive in terms of\nfunding in developing things like an off\nswitch or developing aggressive\nalignment mechanisms and no you're not\nallowed to put on Azure aggressive\nalignment mechanism for hell's\naggressive alignment mechanisms like it\ndoesn't matter if you say aggressive we\ndon't know how to do it\nmeaning aggressive alignment meaning you\nhave to\npropose something otherwise you're not\nallowed to put it on the cloud\nthe hell do you do you imagine they will\npropose that would make it safe to put\nsomething smarter than you on the cloud\nthat's what research is for why the\ncynicism about such a thing not being\npossible if you haven't done it works on\nthe first try\nwhat so yes so yes again something\nsmarter than you so that's that is", "mimetype": "text/plain", "start_char_idx": 102353, "end_char_idx": 106485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c": {"__data__": {"id_": "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d355577d-2f90-45cc-a7d8-4a6439c352e0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "dacebab5e9f89b0ab77dbe6f2ad45807df8838fb70c3689668874ded19f621f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70", "node_type": "1", "metadata": {}, "hash": "ab71aece72072a5dfbc6ab3b59773e6799428abf381e619b299fcdb8b6987387", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "system is beginning to manipulate\npeople then there's going to be an\nuprising where there's going to be a\npublic pressure\nand a public incentive in terms of\nfunding in developing things like an off\nswitch or developing aggressive\nalignment mechanisms and no you're not\nallowed to put on Azure aggressive\nalignment mechanism for hell's\naggressive alignment mechanisms like it\ndoesn't matter if you say aggressive we\ndon't know how to do it\nmeaning aggressive alignment meaning you\nhave to\npropose something otherwise you're not\nallowed to put it on the cloud\nthe hell do you do you imagine they will\npropose that would make it safe to put\nsomething smarter than you on the cloud\nthat's what research is for why the\ncynicism about such a thing not being\npossible if you haven't done it works on\nthe first try\nwhat so yes so yes again something\nsmarter than you so that's that is a\nfundamental thing if it has to work on\nthe first if there's if if there's a\nrapid takeoff\nyes it's very difficult to do if there's\na rapid takeoff and the fundamental\ndifference between weak AGI and strong\nagis you're saying that's going to be\nextremely difficult to do if the public\nUprising never happens until you have\nthis critical phase shift then you're\nright it's very difficult to do but\nthat's not obvious it's not obvious that\nyou're not going to start seeing\nsymptoms of the negative effects of AGI\nto where you're like we have to put a\nhalt to this that there's not just first\ntry you get many tries at it yeah we can\nlike see right now that Bing is quite\ndifficult to align that when you try to\ntrain inabilities into a system\ninto which capabilities have already\nbeen trained that what do you know\ngradient descent like learns small\nshallow simple patches of inability and\nyou come in and ask it in a different\nlanguage and the Deep capabilities are\nstill in there and they evade the\nshallow patches and come right back out\nagain there there you go there's there's\nyour there's your red fire alarm of like\noh no alignment is difficult is\neverybody going to shut everything down\nnow\nno that's not but that's not the same\nkind of alignment A system that escapes\nthe box it's from is a fundamentally\ndifferent thing I think for you yeah no\nbut not for this so you put a line there\nand everybody else puts a line somewhere\nelse and there's like yeah and there's\nlike no agreement\nwe we we have had a pandemic on this\nplanet with the few million people dead\nwhich we will which we may never know\nwhether or not it was a lab leak because\nthere was definitely cover-up we don't\nknow that if there was a lab leak but we\nknow that the people who did the\nresearch like you know like put out the\nwhole paper about this definitely wasn't\na lab leak and didn't reveal that they\nhad been doing had like sent off Corona\nFire coronavirus research to the Wuhan\nInstitute of virology after it was\nbanned in the United States after the\nbegan to function research was\ntemporarily banned in the United States\nand\nthe same people who exported gain of\nfunction research on coronaviruses to\nthe woonhan Institute of virology after\nit began to function that gained event\ngain of function research was\ntemporarily banned in the United States\nare now getting more grants\nto do more research on a gain of\nfunction research on coronaviruses\nmaybe we do better in this than in AI\nbut like this is not something we cannot\ntake for granted that there's going to\nbe an outcry\nyeah people have different thresholds\nfor when they start to outcry\nPT for granted but I I think your\nintuition is that there's a very high\nprobability that this event happens\nwithout us solving the alignment problem\nand I guess that's where I'm trying to\nbuild up more uh perspectives and color\non the situation is it possible that the\nprobability is not something like 100\nbut it's like 32 percent\nthat uh AI will escape the Box before we\nsolve the alignment problem not solve\nbut is it possible we always stay ahead\nof the AI in terms of our ability to\nsolve for that particular system the\nalignment problem nothing like the world\nin front of us right now\nyou've already seen it that that that\ngpt4 is not turning out this way\nand there are like basic obstacles", "mimetype": "text/plain", "start_char_idx": 105609, "end_char_idx": 109809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70": {"__data__": {"id_": "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3e5afa0-f0d9-4ea5-92d9-3e73097ec37c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "55ae7afcc15421788bb7464faced8fa6298ea29c19400fe777ec072e3d5051c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fba4ce2-c81b-40d2-bce0-7a12ab75567f", "node_type": "1", "metadata": {}, "hash": "35494c6166b00606c1eb06391cb4491b17e6c2b3e27ad132dbbb6aaaa6c9667f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "better in this than in AI\nbut like this is not something we cannot\ntake for granted that there's going to\nbe an outcry\nyeah people have different thresholds\nfor when they start to outcry\nPT for granted but I I think your\nintuition is that there's a very high\nprobability that this event happens\nwithout us solving the alignment problem\nand I guess that's where I'm trying to\nbuild up more uh perspectives and color\non the situation is it possible that the\nprobability is not something like 100\nbut it's like 32 percent\nthat uh AI will escape the Box before we\nsolve the alignment problem not solve\nbut is it possible we always stay ahead\nof the AI in terms of our ability to\nsolve for that particular system the\nalignment problem nothing like the world\nin front of us right now\nyou've already seen it that that that\ngpt4 is not turning out this way\nand there are like basic obstacles where\nyou've got the the weak version of the\nsystem that doesn't know enough to\ndeceive you and the strong version of\nthe system that could deceive you if it\nwanted to do that it feels already like\nsufficiently unaligned to want to\ndeceive you there's the question of like\nhow on the current Paradigm you train\nhonesty when the humans can no longer\ntell if the system is being honest\nyou don't think these are research\nquestions that could be answered I think\nthey could be answered at 50 years with\nunlimited retries the way things usually\nwork in science\nI just disagree with that you're making\nit 50 years I think with the kind of\nattention this guest with the kind of\nfunding I guess it could be answered uh\nnot in whole but in incrementally within\nwithin months and within a small number\nof years if it's a if it's at scale\nreceives attention and research so if\nyou start studying large language models\nI think there was an intuition like two\nyears ago even that something like gpt4\nthe current capabilities of even Chad\nGPT with GPT 3.5 is not is gonna we're\nstill far away from that I think a lot\nof people are surprised by the\ncapabilities of gpt4 right so now people\nare waking up okay we need to study\nthese language models I think there's\ngoing to be a lot of interesting\nAI Safety Research are the are Earth's\nbillionaires going to put up like the\nthe giant prizes that would maybe\nincentivize young hot shot people who\njust got their physics degrees to not go\nto the hedge funds and instead put\neverything into interpretability in this\nlike one small area where we can\nactually tell whether or not somebody\nhas made a discovery or not I think so\nbecause uh\nwell that's what these these\nconversations are about because they're\ngoing to wake up to the fact that gpt4\ncan be used to manipulate elections to\ninfluence geopolitics to influence the\neconomy there's a lot of there's going\nto be a huge amount of incentive to like\nwait a minute we can't this has to be we\nhave to put we have to make sure they're\nnot doing damage we have to make sure we\ninterpretability we have to make sure we\ndon't understand how these systems\nfunction so that we can predict their\neffect on economy so that there's uh so\nthere's a feudalism and a bunch of\nop-eds in the new York Times and nobody\nactually stepping forth and saying you\nknow what instead of a mega yacht I'd\nrather put that billion dollars on\nprizes for young Hotshot physicists who\nmake fundamental breakthroughs in\ninterpretability\nthe yacht versus the interpretability\nresearch the old uh the old trade-off\nuh\nI I just I think uh it's just I think\nthere's going to be a huge amount of\nallocation of funds I hope that's I\nguess you want to bet me on that\nbut you want to put a time scale on it\nsay how much funds you think are going\nto be allocated in a direction that I\nwould consider to be actually useful\nby what time\nI do think there will be a huge amount\nof funds\nbut you're saying it needs to be open\nright the development of the system\nshould be closed but the development of\nthe the interpretability research the\nAisa we are so far behind on inter under\ninterpretability compared to\ncapabilities like yeah you can you could\ntake the last generation of systems the\nthe stuff that's already in the open\nthere is so", "mimetype": "text/plain", "start_char_idx": 108926, "end_char_idx": 113089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fba4ce2-c81b-40d2-bce0-7a12ab75567f": {"__data__": {"id_": "2fba4ce2-c81b-40d2-bce0-7a12ab75567f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f2beb60-b0c1-4b2e-a8c9-7ec9ab054d70", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "914ef9c01bc51704f5a58d2a3bedb507dce35c8921cd76bf91128ffb2b0aa55b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "835bcdd2-168a-495a-a464-179f8aa2ca70", "node_type": "1", "metadata": {}, "hash": "176cd146f0cfa83e593d145215651ba8e076c6633659c97d217a59f34b0014da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "physicists who\nmake fundamental breakthroughs in\ninterpretability\nthe yacht versus the interpretability\nresearch the old uh the old trade-off\nuh\nI I just I think uh it's just I think\nthere's going to be a huge amount of\nallocation of funds I hope that's I\nguess you want to bet me on that\nbut you want to put a time scale on it\nsay how much funds you think are going\nto be allocated in a direction that I\nwould consider to be actually useful\nby what time\nI do think there will be a huge amount\nof funds\nbut you're saying it needs to be open\nright the development of the system\nshould be closed but the development of\nthe the interpretability research the\nAisa we are so far behind on inter under\ninterpretability compared to\ncapabilities like yeah you can you could\ntake the last generation of systems the\nthe stuff that's already in the open\nthere is so much in there that we don't\nunderstand there are so many prizes you\ncould do before you you know you could\nyou you could you would have enough\ninsights that you'd be like oh you know\nlike well we understand how these\nsystems work we understand how these\nthings are doing their outputs we can\nread their minds now let's try it with\nthe bigger systems yeah we're nowhere\nnear that you you there's so much\ninterpretability work to be done on the\nweaker versions of the systems so what\nwhat can you say on the second point you\nsaid to uh uh to Elon Musk on what are\nsome ideas what are things you could try\nI can think of a few things at try you\nsaid they don't fit in one tweet so is\nis there something you could put into\nwords of the things you would try I mean\nthe the the the trouble is the stuff is\nsubtle I've watched people try to make\nprogress on this and not get places\nsomebody who just like gets alarmed and\ncharges in it's like going nowhere\nsure it meant like years ago about I\ndon't know like 20 years 15 years\nsomething like that I was talking to a\ncongress person\num\nwho\nhad become alarmed about the eventual\nprospects and he wanted\nwork on building AIS without emotions\nbecause the emotional AIS were the scary\nones you see\nand some poor person at arpa had come up\nwith a research proposal whereby this\ncongressman's panic and desire to fund\nthis thing would go into something that\nthe person at arpa thought would be\nuseful and had been munched around to\nwhere it would like sound to the\ncongressman like work was happening on\nthis which you know of course like this\nis just the the congressperson had\nmisunderstood the problem\nand did not understand where the danger\ncame from\nand\nso it's like that the issue is that\nyou could like do this in a certain\nprecise way and maybe get something like\nwhen I say like put up prices on\ninterpretability I'm not I'm like well\nlike\nbecause it's verifiable there as opposed\nto other places you can tell whether or\nnot good work actually happened in this\nexact narrow case if you do things in\nexactly the right way you can maybe\nthrow money at it and produce\nscience instead of anti-science and\nnonsense\nand all the all the methods that I know\nof of like trying to throw money at this\nproblem have this share this property of\nlike well if you do it exactly right\nbased on understanding exactly what has\nyou know like tends to produce like\nuseful outputs or not then you can like\nadd money to it in this way and there's\nlike and the thing that I'm giving as an\nexample here in front of this large\naudience is is the most understandable\nof those\nbecause there's like other people\nwho you know like like like Chris Ola\nand and even and even more generally\nlike you can tell whether or not\ninterpretability progress has occurred\nso like if I say Throw money at\nproducing more interpretability there's\nlike a chance somebody can do it that\nway and like it will actually produce\nuseful results then the other stuff just\nblurs off into the like harder to Target\nexactly than that\nso sometimes the basics are fun to\nexplore because they're not so basic\nwhat do you what is interpretability\nwhat do you what does it look like what\nare we talking about it looks like\nwe took a much smaller\nset of Transformer layers than the ones\nin the modern bleeding", "mimetype": "text/plain", "start_char_idx": 112235, "end_char_idx": 116384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "835bcdd2-168a-495a-a464-179f8aa2ca70": {"__data__": {"id_": "835bcdd2-168a-495a-a464-179f8aa2ca70", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fba4ce2-c81b-40d2-bce0-7a12ab75567f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5a3c7722bc4516c573098dc3eb6ea188d76394095f92e90800b2b429778a2427", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f694c65-1411-4846-a5d0-2b8fc3803278", "node_type": "1", "metadata": {}, "hash": "42838de74512e9a013f5c86612d97315655479ac2e9039cd277db7424c131383", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "money to it in this way and there's\nlike and the thing that I'm giving as an\nexample here in front of this large\naudience is is the most understandable\nof those\nbecause there's like other people\nwho you know like like like Chris Ola\nand and even and even more generally\nlike you can tell whether or not\ninterpretability progress has occurred\nso like if I say Throw money at\nproducing more interpretability there's\nlike a chance somebody can do it that\nway and like it will actually produce\nuseful results then the other stuff just\nblurs off into the like harder to Target\nexactly than that\nso sometimes the basics are fun to\nexplore because they're not so basic\nwhat do you what is interpretability\nwhat do you what does it look like what\nare we talking about it looks like\nwe took a much smaller\nset of Transformer layers than the ones\nin the modern bleeding edge\nstate-of-the-art systems\nand after applying nefarious\ntools and mathematical ideas and trying\n20 different things we found we have\nshown it that this piece of the system\nis doing this kind of useful work\nand then somehow also hopefully\ngeneralizes some fundamental\nunderstanding of what's going on that\ngeneralizes to the bigger system\nyou can hope and it's probably true like\nyou would not expect the smaller tricks\nto go away when you have a system that's\nlike doing larger kinds of work you\nwould expect the larger work kinds of\nwork to be building on top of the\nsmaller kinds of work and gradient\ndescent runs across the smaller kinds of\nwork before it runs across the larger\nkinds of work and well that's kind of\nwhat is happening in Neuroscience right\nit's trying to understand the human\nbrain by prodding and it's such a giant\nmystery and people have made progress\neven though it's extremely difficult to\nmake sense of what's going on in the\nbrain they have different parts of the\nbrain they're responsible for hearing\nfor Sight division science Community\nthis understanding visual cortex that I\nmean they've made a lot of progress in\nunderstanding how that stuff works like\nand that's I guess but you're saying it\ntakes a long time to do that work well\nalso it's not enough so in particular\num\nlet's say you have got your\ninterpretability\ntools and they say that your\nyour current AI system is plotting to\nkill you\nnow what\nit is definitely a good step one right\nyeah what's step two\nif you cut out that layer is it going to\nstop\nwaiting to kill you when you optimize\nagainst visible\nmisalignment you are optimizing against\nmisalignment and you are also optimizing\nagainst visibility\nso sure you can yeah it's true all\nyou're doing is removing the obvious\nintentions to kill you you've got your\ndetector it's showing something inside\nthe system that you don't like okay say\nthe disaster monkey is running this\nthing\nwill optimize the system until the\nvisible bad behavior goes away\nbut it's arising for fundamental reasons\nof instrumental convergence the old you\ncan't bring the coffee if you're dead\nany goal and you know almost any set of\nalmost every set of utility functions\nwith a few narrow exceptions implies\nkilling all the humans\nbut do you think it's possible because\nwe can do experimentation to discover\nthe source of the desire to kill\nI can tell it to you right now is that\nit wants to do something\nand the way to get the most of that\nthing is to put the universe into a\nstate where there aren't humans\nso is it is it possible to encode\nin the same way we think like why do we\nthink murder is wrong\nthe same foundational\nethics\nit's not hard-coded in but more like\ndeeper I mean that's part of the\nresearch how do you have it that this\nTransformer\nthis small\nversion of the language model doesn't\never want to kill\nthat'd be nice assuming that you got\ndoesn't want to kill sufficiently\nexactly right that it didn't be like oh\nI will like detach their heads and put\nthem in some jars and keep the heads\nalive forever and then go do the thing\nbut leaving that aside well not leaving\nthat aside yeah that's a strong point\nyeah because there is a whole issue\nwhere as something gets smarter it finds\nways of achieving the same goal\npredicate that we're not imaginable to\nstupider versions of the system or\nperhaps the stupider operators that's\none of many things making this", "mimetype": "text/plain", "start_char_idx": 115525, "end_char_idx": 119782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f694c65-1411-4846-a5d0-2b8fc3803278": {"__data__": {"id_": "0f694c65-1411-4846-a5d0-2b8fc3803278", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "835bcdd2-168a-495a-a464-179f8aa2ca70", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a49cf37e2f632f0621f83cd5e8683942d469ff0ce81345d34430c532593129cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1e9b43-ad5b-43cd-b871-6d87f691462e", "node_type": "1", "metadata": {}, "hash": "8b80dcd46a893954ef7617b697b11b197bf2e663e338daec2868c799eb241525", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it possible to encode\nin the same way we think like why do we\nthink murder is wrong\nthe same foundational\nethics\nit's not hard-coded in but more like\ndeeper I mean that's part of the\nresearch how do you have it that this\nTransformer\nthis small\nversion of the language model doesn't\never want to kill\nthat'd be nice assuming that you got\ndoesn't want to kill sufficiently\nexactly right that it didn't be like oh\nI will like detach their heads and put\nthem in some jars and keep the heads\nalive forever and then go do the thing\nbut leaving that aside well not leaving\nthat aside yeah that's a strong point\nyeah because there is a whole issue\nwhere as something gets smarter it finds\nways of achieving the same goal\npredicate that we're not imaginable to\nstupider versions of the system or\nperhaps the stupider operators that's\none of many things making this difficult\na larger thing making this difficult is\nthat we do not know how to get any goals\ninto systems at all we know how to get\noutwardly observable behaviors into\nsystems we do not know how to get\ninternal psychological wanting to do\nparticular things into the system that\nis not what the current technology does\nI mean it could be things like\num dystopian Futures like Brave New\nWorld\nwhere most humans will actually say we\nkind of want that future it's a great\nfuture everybody's happy\nwe would have to get so far\nself much further than we are now\nand further faster before that failure\nmode became a running concern\nyour failure modes are much more much\nmore drastic the ones you could the\nfailure modes are much simpler it's it's\nlike yeah like the AI puts the universe\ninto a particular state it happens to\nnot have any humans inside it okay so\nthe paperclip maximizer\nutility so the original version of the\npaperclip Max can you explain it if you\ncan okay\nthe original version was you lose\ncontrol of the utility function and it\nso happens that what maxes out the\nutility per unit resources is Tiny\nmolecular shapes like paper clips\nthere's a lot of things that make it\nhappy but the cheapest one that didn't\nsaturate was\nputting matter into certain shapes\nand it so happens that that the cheapest\nway to make these shapes is to make them\nvery small because then you need fewer\natoms for instance of the shape and\narguendo I you know like it happens to\nlook like a paper clip in retrospect I\nwish I'd said Tiny molecular spirals or\nlike tiny molecular hyperbolic spirals\nwhy because I said a tiny molecular\npaper clips this got heard as this got\nthen mutated to paper clips this then\nmutated two and the AI was in a\npaperclip Factory\nso the original story is about how you\nlose control of the system it doesn't\nwant what you tried to make it want the\nthing that that it ends up wanting most\nis a thing that even from a very\nembracing Cosmopolitan perspective we\nthink of as having no value and that's\nhow the value of the future gets\ndestroyed then that got changed to a\nfable of like well you made a paperclip\nFactory and it did exactly what you\nwanted what you wanted but you asked it\nto do the wrong thing which is a\ncompletely different failure\nbut those are both concerns to you so\nthat's more than a Brave New World yeah\nif you can solve the problem of making\nsomething want\nexactly what you wanted to want then you\ngot to deal with the problem of wanting\nthe right thing\nBut first you have to solve the\nalignment first you have to solve inner\nalignment inner alignment then you get\nto solve outer alignment\nlike first you need to be able to point\nthe insides of the thing in a direction\nand then you get to to deal with whether\nthat direction expressed in reality is\nlike the thing that it'll align with the\nthing that you want\nare you scared\nof this whole thing\nprobably I\ndon't really know\nwhat gives you hope about this what\npossibility of being wrong\nnot that you're right but we will\nactually get our act together and\nallocate a lot of resources to the\nalignment problem well I can easily\nimagine that at some point this Panic\nexpresses itself in the waste of a\nbillion dollars\nspending a billion dollars correctly\nthat's harder\nto solve both the inner and the outer\nalignment if you're wrong to solve a\nnumber of things yeah number of", "mimetype": "text/plain", "start_char_idx": 118927, "end_char_idx": 123139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c1e9b43-ad5b-43cd-b871-6d87f691462e": {"__data__": {"id_": "6c1e9b43-ad5b-43cd-b871-6d87f691462e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f694c65-1411-4846-a5d0-2b8fc3803278", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6e4ba7ed4dbfa5c6526649080ef64711328af926fd9c2da1b65669541e80b86c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12cc2c74-f75c-4799-baec-68c537b8d667", "node_type": "1", "metadata": {}, "hash": "9f105a689c53f918e643bdcc422be568d306abc86ffa4279948eae8d5906bb1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "first you have to solve the\nalignment first you have to solve inner\nalignment inner alignment then you get\nto solve outer alignment\nlike first you need to be able to point\nthe insides of the thing in a direction\nand then you get to to deal with whether\nthat direction expressed in reality is\nlike the thing that it'll align with the\nthing that you want\nare you scared\nof this whole thing\nprobably I\ndon't really know\nwhat gives you hope about this what\npossibility of being wrong\nnot that you're right but we will\nactually get our act together and\nallocate a lot of resources to the\nalignment problem well I can easily\nimagine that at some point this Panic\nexpresses itself in the waste of a\nbillion dollars\nspending a billion dollars correctly\nthat's harder\nto solve both the inner and the outer\nalignment if you're wrong to solve a\nnumber of things yeah number of things\nif you're wrong\nwhat why what do you think would be the\nreason like 50 years from now not\nperfectly wrong you know you make a lot\nof really eloquent points you know\nthere's there's a lot of like shape to\nthe ideas you express but like if if\nyou're somewhat wrong about some\nfundamental ideas why would that be\nstuff has to be easier than than I think\nit is\nyou know when the first time you're\nbuilding a rocket\nbeing wrong is in a certain sense quite\neasy\nhappening to be wrong in a way where the\nrocket goes twice as far and half the\nfuel and lands exactly where you hoped\nit would\nmost cases of being wrong make it harder\nto build the rocket harder to have it\nnot explode cause it to require more\nfuel than you hoped cause it to be land\noff Target\nbeing wrong in a way that's mixed stuff\neasier you know that's that's not the\nusual project management story yeah\nbut then this is the first time we're\nreally tackling the problem of AI\nalignment there's no examples in history\nwhere we oh there's all kinds of things\nthat are similar if you generalize\nincorrectly the right way and aren't\nfooled of a misleading metaphors\nlike what humans being misaligned on\ninclusive genetic fitness so inclusive\ngenic Fitness is like not just your\nreproductive Fitness but also the\nfitness of your relatives the people who\nshare your some fraction of your genes\nthe old joke is uh would you give your\nlife to save your brother they once\nasked a by a biologist I think it was\nhaldane\nsaid no but I would give my life to save\ntwo brothers or eight cousins\nthere's a brother on average shares half\nyour genes and cousin on average shares\nan eighth of your genes yeah so that's\ninclusive genetic fitness and you can\nview natural selection as optimizing\nhumans exclusively around us like one\nvery simple Criterion like how much more\nfrequent did your genes become in the\nNext Generation in fact that just is\nnatural selection it doesn't optimize\nfor that but rather the process of genes\nbecoming more frequent is that you can\nnonetheless imagine that there is this\nhill climbing process not like gradient\ndescent because gradient descent uses\ncalculus this is just using like where\nare you but still hill climbing in both\ncases making something better and better\nover time in steps\nand natural selection was optimizing\nexclusively for this very simple pure\nCriterion of inclusive genetic fitness\nin a very complicated environment we're\ndoing a very wide range of things and\nsolving a wide range of problems\nlet it to having more kids\nand this got you humans which had no\ninternal notion of inclusive genetic\nfitness until thousands of years later\nwhen they were actually figuring out\nwhat had even happened\nand no desire to no explicit desire to\nincrease inclusive genetic fitness\nso from this we may in so from this\nimportant case study we may infer the\nimportant fact that if you do a whole\nbunch of hill climbing on a very simple\nloss function\nat the point where the system's\ncapabilities start to generalize very\nwidely when it is in an intuitive sense\nbecoming very capable and generalizing\nfar outside the training distribution\nwe know that there is no General law\nsaying that the system\neven internally represents let alone\ntries to optimize the very simple loss\nfunction you are training it on\nthere is so much that we cannot possibly\ncover all of it I think we did a good\njob of\ngetting your sense from different\nperspectives of the current state of the\nart with large language models we've", "mimetype": "text/plain", "start_char_idx": 122274, "end_char_idx": 126627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12cc2c74-f75c-4799-baec-68c537b8d667": {"__data__": {"id_": "12cc2c74-f75c-4799-baec-68c537b8d667", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c1e9b43-ad5b-43cd-b871-6d87f691462e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bcfaf364408ef93edf966d7d8b1e0d8665219beeec1e9215cb7733da71f0bfb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c", "node_type": "1", "metadata": {}, "hash": "44cd627c838c0e3b5373b79a42d7b5f27d2ddca1c254a4b9d285b528ed83368d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "no\ninternal notion of inclusive genetic\nfitness until thousands of years later\nwhen they were actually figuring out\nwhat had even happened\nand no desire to no explicit desire to\nincrease inclusive genetic fitness\nso from this we may in so from this\nimportant case study we may infer the\nimportant fact that if you do a whole\nbunch of hill climbing on a very simple\nloss function\nat the point where the system's\ncapabilities start to generalize very\nwidely when it is in an intuitive sense\nbecoming very capable and generalizing\nfar outside the training distribution\nwe know that there is no General law\nsaying that the system\neven internally represents let alone\ntries to optimize the very simple loss\nfunction you are training it on\nthere is so much that we cannot possibly\ncover all of it I think we did a good\njob of\ngetting your sense from different\nperspectives of the current state of the\nart with large language models we've got\na good sense\nof your concern about the threats of AGI\nI've talked to her about the power of\nintelligence and not really gotten very\nfar into it\nbut not like\nwhy it is that suppose you like screw up\nwith AGI and it ends up wanting a bunch\nof random stuff why does it try to kill\nyou why doesn't it try to trade with you\nwhy doesn't it give you just the tiny\nlittle fraction of the solar system that\nwould keep to take everyone a lot that\nit would take to keep everyone alive\nyeah well that's a good question I mean\nwhat what are the different trajectories\nthat intelligence\nwhen acted Upon This World Super\nintelligence what are the different\ntrajectories for this universe with such\nan intelligence in it do most of them\nnot include humans\nI mean if you the vast majority of\nrandomly specified utility functions do\nnot have Optima with humans in them\nwould be the like first thing I would\npoint out and then the next question is\nlike well if you try to optimize\nsomething you lose control of it where\nin that space do you land because it's\nnot random but it also doesn't\nnecessarily have room for humans in it\nI suspect that the average member of the\naudience might have some questions about\neven whether that's the correct Paradigm\nto think about it then would\nsort of want to back up a bit if we back\nup to\nsomething bigger than humans\nif you look at Earth and life on Earth\nand what is truly special about life on\nEarth\ndo you think it's possible that a lot\nwhatever that special thing is let's\nexplore what that special thing could be\nwhatever that special thing is that\nthing appears often in the objective\nfunction why\nI I know what you hope but\nyou know you can hope that a particular\nset of winning lottery numbers come up\nand it doesn't make the lottery balls\ncome up that way\nI know you want this to be true but why\nwould it be true there's a line from\nGrumpy Old Men where this guy says in a\ngrocery store he says you can wish in\none hand and crap in the other and see\nwhich one fills up first this is a\nscience problem we are trying to predict\nwhat happens with AI systems that you\nknow you try to optimize to imitate\nhumans and then you did some like rlhf\nto them and of course you like lost in\nand you know like of course you didn't\nget like perfect alignment because\nthat's not how you know\nit's not what happens when you Hill\nClimb towards the outer loss function\nyou don't get inter alignment on it\nbut yeah so\nthe I think that there's so if you don't\nmind my like taking some slight control\nof things and staring around to what I\nthink is like a good place to start\nI just failed to solve the control\nproblem\nI've lost control of this thing\nalignment alignment\nstill alive control yeah okay sure yeah\nyou lost control\num but we're still aligned anyway sorry\nfor the meta comment yeah losing control\nisn't as bad as you lose control to an\naligned system yes exactly exactly you\nhave no idea of the horrors I will\nshortly at least on this conversation\nall right so it's actually\ndistractically what are we going to say\nin terms of taking control of the\nconversation\nso I think that there's like a\nsealing chapters here if I'm pronouncing\nthose words remotely like correctly\nbecause of course they only ever read\nthem and not hear them spoken\num there's\na like for some", "mimetype": "text/plain", "start_char_idx": 125696, "end_char_idx": 129916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c": {"__data__": {"id_": "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12cc2c74-f75c-4799-baec-68c537b8d667", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d6863d048006dc802080578d6c9e611b7ef127792bc0f97d770f9ce6a0dd4238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5da99ce-899c-4c85-85f4-6ce88e48424f", "node_type": "1", "metadata": {}, "hash": "c7f2ec77cda9bc486cc613cca34db639f31ae5ab5d0aa8d6b7b6eded8a736116", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I think that there's so if you don't\nmind my like taking some slight control\nof things and staring around to what I\nthink is like a good place to start\nI just failed to solve the control\nproblem\nI've lost control of this thing\nalignment alignment\nstill alive control yeah okay sure yeah\nyou lost control\num but we're still aligned anyway sorry\nfor the meta comment yeah losing control\nisn't as bad as you lose control to an\naligned system yes exactly exactly you\nhave no idea of the horrors I will\nshortly at least on this conversation\nall right so it's actually\ndistractically what are we going to say\nin terms of taking control of the\nconversation\nso I think that there's like a\nsealing chapters here if I'm pronouncing\nthose words remotely like correctly\nbecause of course they only ever read\nthem and not hear them spoken\num there's\na like for some people like\nthe word intelligence smartness is not a\nword of power to them it means chess\nplayers who it means like the College\nUniversity Professor people aren't very\nsuccessful in life it doesn't mean like\nCharisma which my usual thing is like\nCharisma is not generated in the liver\nrather than the brain Charisma is also a\ncognitive function\num\nso if you if you like think that like\nsmartness doesn't sound very threatening\nthen super intelligence is not going to\nsound very threatening either it's going\nto sound like you just pulled the off\nswitch\nlike it's you know like well it's super\nintelligent but stuck in a computer we\npull the off switch problem solved\nand the other side of it is\nyou have a lot of respect for the notion\nof intelligence you're like well yeah\nthat's that's what humans have that's\nthe human superpower\nand it sounds you know like it could be\ndangerous but why would it be\nare we have we as we have grown more\nintelligent also grown less kind\nZees are in fact like a bit less kind\nthan humans and you know\nyou could like argue that out but often\nthe sort of person has a deep respect\nfor intelligence is going to be like\nwell yes like you can't even have\nkindness unless you know what that is\nand so they're like\nwhy would it do something as stupid as\nmaking paper clips\naren't you supposing something that's\nsmart enough to be dangerous but also\nstupid enough that it will just make\npaper clips and never question that\nin some cases people are like well even\nif you like mispecify the objective\nfunction won't you realize that what you\nreally wanted was X are you supposing\nsomething that is like\nsmart enough to be dangerous but stupid\nenough that it doesn't understand what\nthe humans really meant when they\nspecified the objective function\nso\nto you our intuition about intelligence\nis limited we should think about\nintelligence as a much bigger thing\nwell I'm saying that it's that then well\nwhat I'm saying is like\nwhat you think about artificial\nintelligence\num depends on what you think about\nintelligence so how do we think about\nintelligence correctly like what\nyou gave one thought experiment to think\nof think of a thing that's much faster\nso it just gets faster and faster and\nfaster I think and it also is like is\nmade of John Von Neumann and has like\nand there's lots of them\nbecause we understand that yeah we\nunderstood like trying to find Newman is\na historical case so you can like look\nup what he did and imagine based on that\nand we know like we people have like\nsome intuition for like if you have more\nhumans they can solve tougher cognitive\nproblems although in fact like in the\ngame of Kasparov versus the world which\nwas like Gary Kasparov on one side and\nan entire horde of internet people led\nby four chess Grand Masters on the other\nside casparov one so like all those\npeople aggregated to be smarter it was a\nit was a hard-fought game it's like all\nthose people aggregated to be smarter\nthan any individual one of them but not\nthey didn't aggregate so well that they\ncould defeat Kasparov but so like humans\naggregating don't actually get in my\nopinion very much smarter especially\ncompared to running them for longer\nlike the the difference between\ncapabilities now and a thousand years\nago is a bigger Gap than the Gap in\ncapabilities between 10 people and one\nperson\nbut like even so pumping intuition for\nwhat it", "mimetype": "text/plain", "start_char_idx": 129064, "end_char_idx": 133293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5da99ce-899c-4c85-85f4-6ce88e48424f": {"__data__": {"id_": "e5da99ce-899c-4c85-85f4-6ce88e48424f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e27ed00e-d0bd-4f34-befc-81d1fa76dd4c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "349f349e686de28a9ab5d6071020c5af9652b845baa6b5fc7fb72932298b9961", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e", "node_type": "1", "metadata": {}, "hash": "1d59ea3a55d51e0fb8e64e2b1890ce3f98a41a84e6d337edd939215e0c89396b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "like\nsome intuition for like if you have more\nhumans they can solve tougher cognitive\nproblems although in fact like in the\ngame of Kasparov versus the world which\nwas like Gary Kasparov on one side and\nan entire horde of internet people led\nby four chess Grand Masters on the other\nside casparov one so like all those\npeople aggregated to be smarter it was a\nit was a hard-fought game it's like all\nthose people aggregated to be smarter\nthan any individual one of them but not\nthey didn't aggregate so well that they\ncould defeat Kasparov but so like humans\naggregating don't actually get in my\nopinion very much smarter especially\ncompared to running them for longer\nlike the the difference between\ncapabilities now and a thousand years\nago is a bigger Gap than the Gap in\ncapabilities between 10 people and one\nperson\nbut like even so pumping intuition for\nwhat it means to augment intelligence\nJohn Von Neumann there's millions of him\nhe runs at a million times the speed and\ntherefore can solve tougher problems\nquite a lot tougher\nit's very hard to have an intuition\nabout what that looks like especially\nlike you said\nyou know the intuition I kind of think\nabout\nis uh it maintains the humanness\nI think\nI I think it's hard\nto to separate\nMy Hope from my objective\nintuition about\nwhat super intelligence systems look\nlike\nif one studies\nevolutionary biology with a bit of math\nand in particular like books from when\nthe field was just sort of like properly\ncoalescing and knowing itself like not\nthe modern textbooks which are just like\nmemorized this legible masks you can do\nwell on these tests but like what people\nwere writing as the basic paradigms of\nthe field were being fought out yeah in\nparticular like a a nice book if you've\ngot the time to read it is\nadaptation and natural selection which\nis one of the founding books\nyou can find people being optimistic\nabout what the utterly alien\noptimization process of natural\nselection will produce in the way of how\nit optimizes its objectives you got\npeople arguing that like in the early\ndays biologists said well like organisms\nwill restrain their own reproduction\nwhen resources are scarce so as not to\nover feed the system\nand\nthis is not how natural selection works\nit's about whose genes are relatively\nmore prevalent to the Next Generation\nand\nif you if like you restrained A\nreproduction those genes get less\nfrequent in the Next Generation compared\nto your con specifics\nand\nnatural selection doesn't do that in\nfact Predators over run prey populations\nall the time and have crashes that's\njust like a thing that happens\nand many years later\nuh well the people said like well but\ngroup selection right what about groups\nof organisms\nand\nbasically the math of group selection\nalmost never works out in practice is\nthe answer there\nbut also years later somebody actually\nran the experiment where they took\npopulations of insects\nand selected the whole populations to\nhave lower sizes you just take pop one\npop two pop three pop four look at which\nhas the lowest total number of them in\nthe Next Generation and select that one\nwhat do you suppose happens when you\nselect populations of insects like that\nwell what happens is not that the\nindividuals in the population evolve to\nrestrain their breeding but that they\nevolve to Kill The Offspring of other\norganisms especially the girls\nso people imagined this lovely beautiful\nharmonious output of natural selection\nwhich is these these populations\nrestraining their own breeding so that\ngroups of them would stay in harmony\nwith the resources available and mostly\nthe math never works out for that but if\nyou actually apply the weird strange\nconditions to get group selection that\nbeats individual selection what you get\nis female and infanticide\nif you're like reading on restrained\npopulations and so that's like the sort\nof so this is not a smart optimization\nprocess natural selection is like so\nincredibly stupid and simple that we can\nactually quantify how stupid it is if\nyou like read the textbooks with the\nmath\nnonetheless this is the sort of basic\nthing of you look at this alien\noptimization process and there's the\nthing that you\nhope it will produce and you have to\nlearn to clear that out of your mind and\njust think about the underlying Dynamics\nand where it finds the maximum from its\nstandpoint that it's looking for rather\nthan how it finds that thing", "mimetype": "text/plain", "start_char_idx": 132426, "end_char_idx": 136816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e": {"__data__": {"id_": "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5da99ce-899c-4c85-85f4-6ce88e48424f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "469a91c05734349ad97720be12534cca6c02486804b6077f40aa5ad1bb800085", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "831572d5-f925-4b58-9df1-ae938b74acb8", "node_type": "1", "metadata": {}, "hash": "07cb5967ed055f0408d7a0b804bc047857ef7ddd97b56d4a64d220a824c125de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "own breeding so that\ngroups of them would stay in harmony\nwith the resources available and mostly\nthe math never works out for that but if\nyou actually apply the weird strange\nconditions to get group selection that\nbeats individual selection what you get\nis female and infanticide\nif you're like reading on restrained\npopulations and so that's like the sort\nof so this is not a smart optimization\nprocess natural selection is like so\nincredibly stupid and simple that we can\nactually quantify how stupid it is if\nyou like read the textbooks with the\nmath\nnonetheless this is the sort of basic\nthing of you look at this alien\noptimization process and there's the\nthing that you\nhope it will produce and you have to\nlearn to clear that out of your mind and\njust think about the underlying Dynamics\nand where it finds the maximum from its\nstandpoint that it's looking for rather\nthan how it finds that thing that left\ninto your mind as the beautiful\naesthetic solution that you hope it\nfinds and this is something that was has\nbeen fought out historically as the\nfield of Pop of biology was coming to\nterms with evolutionary biology\nand uh and you can like look at them\nfighting it out as they get to terms\nwith this very alien inhuman pot\nin human optimization process and indeed\nsomething smarter than us would be also\nbe much like smarter than natural\nselection so it doesn't just like\nautomatically carry over\nbut there's a there's a lesson there\nthere's a warning\nthe D natural selection\nis a is a deeply sub-optimal process\nthat could be significantly improved on\nit would be by an AGI system well it's\nkind of stupid it like has to like run\nhundreds of generations to notice that\nsomething is working it doesn't be like\noh well I tried this in like one\norganism I saw it worked now I'm going\nto like duplicate that feature onto\neverything immediately\nhas to like run for hundreds of\ngenerations for a new mutation to rise\nto fixation I wonder if there's a case\nto be made in natural selection\nas inefficient as it looks is actually\nuh\nis actually quite powerful that that\nthis is extremely robust it runs for a\nlong time and eventually manages to\noptimize things\nit's weaker than gradient descent\nbecause gradient descent also uses\ninformation about the derivative\nyeah Evolution seems to be there's not\nreally an objective function there's a\nthere's inclusive genic Fitness\nis the implicit loss function of\nevolution cannot change the loss\nfunction doesn't change the environment\nchanges and therefore like what gets\noptimized for in the organism changes\nit's like take like gpt3 there's like\nimagine like different versions of gpt3\nwhere they're all trying to predict the\nnext word but they're being run on\ndifferent data sets of text\nand that's like natural selection\nalways inclusogenic Fitness but like\ndifferent environmental problems\nso it's it's uh it's difficult to think\nabout so if we're saying that natural\nselection is stupid if we're saying that\nhumans are stupid\nit's smarter than natural selection\nsmarter stupider than the upper bound\ndo you think there's an upper Bomb by\nthe way that's another meaningful place\nI mean if you you put enough matter\nenergy compute into one place it will\ncollapse into a black hole and there's\nonly so much computation can do before\nyou run out of nagentropy in the\nuniverse dies\num so there's an upper bound but it's\nvery very very far up above here like a\nsupernova is only finitely hot it's not\ninfinitely hot but it's really really\nreally really hot\nwell let me ask you let me talk to you\nabout Consciousness\num also coupled with that question is\nimagining a world with super intelligent\nAI systems that get rid of humans but\nnevertheless keep\nsome of the something that we would\nconsider beautiful and amazing why\nthe lesson of evolutionary biology don't\njust like if you just guess what an\noptimization does based on what you hope\nthe results will be it usually will not\ndo that is that hope I mean it's not\nhope I don't I think if you cold and\nobjectively look at what makes what has\nbeen a powerful a useful\nI I think there's a correlation between\nwhat we find beautiful and a thing\nthat's been useful\nthis is what the early biologists\nthought they were like no no I'm not\njust like they thought like no no I'm\nnot just like", "mimetype": "text/plain", "start_char_idx": 135912, "end_char_idx": 140197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "831572d5-f925-4b58-9df1-ae938b74acb8": {"__data__": {"id_": "831572d5-f925-4b58-9df1-ae938b74acb8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a49df71-f7e0-4c8d-8f39-18cdcc36ad3e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6f7f6918191fe435f8ee24c31d26c4aedd5b9d8a3aa5170a448e5e371071d0b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab", "node_type": "1", "metadata": {}, "hash": "32350a4f5a9bde25c6b768d2906ad79421c469435863ab8e22d51d4ed6939f2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "it's not\ninfinitely hot but it's really really\nreally really hot\nwell let me ask you let me talk to you\nabout Consciousness\num also coupled with that question is\nimagining a world with super intelligent\nAI systems that get rid of humans but\nnevertheless keep\nsome of the something that we would\nconsider beautiful and amazing why\nthe lesson of evolutionary biology don't\njust like if you just guess what an\noptimization does based on what you hope\nthe results will be it usually will not\ndo that is that hope I mean it's not\nhope I don't I think if you cold and\nobjectively look at what makes what has\nbeen a powerful a useful\nI I think there's a correlation between\nwhat we find beautiful and a thing\nthat's been useful\nthis is what the early biologists\nthought they were like no no I'm not\njust like they thought like no no I'm\nnot just like imagining stuff that would\nbe pretty it's useful for popular for\norganisms to restrain their own\nreproduction because then they don't\noverrun the prey populations and they\nactually have more kids in the long run\nhmm\nso so let me just ask you about\nConsciousness do you think Consciousness\nis useful to humans no two AGI systems\nto well\num in this transitionary pay between\nhumans and AGI to AGI systems as they\nbecome smarter and smarter is there some\nuse to it what\nlet me step back what is consciousness\neleazaridkowski what is consciousness\nI'm referring to chalmers's hard problem\nof conscious experience are you\nreferring it to self-awareness and\nreflection are referring to the state of\nbeing awake as opposed to asleep\nthis is how I know you're an advanced\nlanguage model I did give you a simple\nprompt and you gave me a bunch of\noptions uh\nI think I'm referring to all\nwith\nincluding the hard problem of\nConsciousness what is it in its\nimportance\nto what you've just been talking about\nwhich is intelligence\nis it a foundation to intelligence is it\nintricately connected to intelligence in\nthe human mind or is it is it a side\neffect of the human mind it is a useful\nlittle tool like we can get rid of I\nguess I'm trying to get\nsome color in your opinion\nof how useful it is in the uh\nintelligence of a human being and then\ntry to generalize that to AI whether AI\nwill keep some of that\nso I think that for there to be like a\nperson who I care about looking out at\nthe universe and wondering at it and\nappreciating it\nit's not enough to have a model of\nyourself\nI think that it is useful to an\nintelligent mind to have a model of\nitself but\nI think you can have that without\npleasure\npain\nAesthetics\nemotion\na sense of wonder\num\nlike I think you can have a model of\nlike how much memory you're using and\nwhether\nlike\nthis thought or that thought is is like\nmore likely to lead to a winning\nposition and you can have like the use I\nthink that if you optimize really hard\non efficiently just having the useful\nparts\nthere is not then the think that the\nthing that says like I am here I look\nout I wonder\nI feel happy in this I feel sad about\nthat\nI think there's a thing that knows what\nit is thinking but that doesn't quite\ncare\nabout\nthese are my thoughts this is my me and\nthat matters\ndoes that make you sad if that's lost in\negi I think that if that's lost then\nevery then basically everything that\nmatters is lost\nI think that when you optimize that when\nyou go really hard on making tiny\nmolecular spirals or paper clips\nthat when you like grind much harder\nthan on that then natural selection\nround out to make humans\nthat\nthere isn't then the mess\nand intricate loopiness\nand\nlike complicated pleasure pain\nconflicting preferences this type of\nfeeling that kind of feeling there's a\nyou know in humans there's like this\ndifference between like the desire of\nwanting something and the pleasure of\nhaving it\nand it's all these like evolutionary\nclutches that came together and created\nsomething that then looks of itself and\nsays like this is pretty this matters\nand the thing that I worry about is that\nthis is not the the thing that happens\nagain just the way that happens in us or\neven like quite similar", "mimetype": "text/plain", "start_char_idx": 139354, "end_char_idx": 143418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab": {"__data__": {"id_": "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "831572d5-f925-4b58-9df1-ae938b74acb8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b1bf54de2f8cf18b46709b253bdb62c01796b1e7c1800ebea504ab6ec5221460", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8051ccc9-38f8-4dab-ad84-13473e3afae7", "node_type": "1", "metadata": {}, "hash": "e1caf3ad9f2cea4cda4b01c9ed518eb0e5952615eb3c47a1fa03b44e004bbe86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in\negi I think that if that's lost then\nevery then basically everything that\nmatters is lost\nI think that when you optimize that when\nyou go really hard on making tiny\nmolecular spirals or paper clips\nthat when you like grind much harder\nthan on that then natural selection\nround out to make humans\nthat\nthere isn't then the mess\nand intricate loopiness\nand\nlike complicated pleasure pain\nconflicting preferences this type of\nfeeling that kind of feeling there's a\nyou know in humans there's like this\ndifference between like the desire of\nwanting something and the pleasure of\nhaving it\nand it's all these like evolutionary\nclutches that came together and created\nsomething that then looks of itself and\nsays like this is pretty this matters\nand the thing that I worry about is that\nthis is not the the thing that happens\nagain just the way that happens in us or\neven like quite similar enough that\nthere's that there are like many basins\nof attractions here and we are in the\nspace of an attra of Attraction like\nlooking out and saying like ah what a\nlovely Basin we are in and there are\nother basins of Attraction and we do not\nend up in and the AIS do not end up in\nthis one when they go like way harder on\noptimizing themselves the natural\nselection optimized us\nBecause unless you specifically want to\nend up in the state where you're looking\nout saying I am here I look out at this\nuniverse with wonder if you don't want\nto preserve that it doesn't get\npreserved when you grind really hard and\nbe able to get more of the stuff\nwe would choose to preserve that within\nourselves because it matters and on some\nviewpoints is the only thing that\nmatters\nand that in part is uh preserving that\nis in part\na solution to the human alignment\nproblem\nI don't I think the human alignment\nproblem is a terrible phrase because it\nis very very different to like try to\nbuild systems out of humans some of whom\nare nice and some of whom are not nice\nand some of whom are trying to trick you\nand like build a social system out of\nlike large populations of those who are\nlike all it basically the same level of\nintelligence yes you know like IQ this\nIQ that but like\nthat versus chimpanzees\nlike it is very different to try to\nsolve that problem than to try to build\nan AI from scratch using especially if\nGod help you are trying to use gradient\ndescent on Giant and screwable matrices\nthey're just very different problems and\nI think that all the analogies between\nthem are horribly misleading and I yeah\neven though so you don't think through\nwritten for reinforcement learning the\nhuman feedback something like that but\nmuch much more elaborate as possible to\nto understand this full\ncomplexity of human nature and then\ncoded into the machine\nI don't think you are trying to do that\non your first try I think on your first\ntry you are like trying to build an\nyou know okay like\nprobably not what you should actually do\nbut like let's say you were trying to\nbuild something that is like Alpha fold\n17 and you are trying to get it to solve\nthe biology problems associated with\nmaking humans smarter so that demons can\nlike actually solve alignment\nso you've got like a super biologist and\nyou would like it to and I think what\nyou would want in the situation is for\nto like\njust be thinking about biology and not\nthinking about a very wide range of\nthings that includes how to kill\neverybody\nand I think that that you're that the\nfirst AIS you're trying to build not a\nmillion years later the first ones\nlook more like narrowly specialized\nbiologists than like\ngetting the full complexity and wonder\nof human experience in there in such a\nway that it wants to preserve itself\neven as it becomes much smarter which is\na drastic system change is going to have\nall kinds of side effects that you know\nlike if we're dealing with Chinese\nscrutable matrices we're not very likely\nto be able to see coming in advance so\nbut I don't think it's just the matrices\nis we're also dealing with the data\nright with the with the uh with the data\non the on the internet and then this is\nan interesting discussion about the data\nset itself but the data set includes the\nfull complexity of human nature no it's\na it's a it's a shadow cast by yes by\nhumans on the internet but don't", "mimetype": "text/plain", "start_char_idx": 142531, "end_char_idx": 146786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8051ccc9-38f8-4dab-ad84-13473e3afae7": {"__data__": {"id_": "8051ccc9-38f8-4dab-ad84-13473e3afae7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcb5fd82-798c-4d83-b50a-6b06ff7e94ab", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7a57456badc9d133dc720d75a0428d8550c4f01f9042410af645da28cd35df6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d47c9834-6c8d-4c58-855d-7d57a3a96b92", "node_type": "1", "metadata": {}, "hash": "a1f434be5dd9019f1b50540d27ed5baeeaabff9b900f4d2d3fac8ca3f731d74e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "think that that you're that the\nfirst AIS you're trying to build not a\nmillion years later the first ones\nlook more like narrowly specialized\nbiologists than like\ngetting the full complexity and wonder\nof human experience in there in such a\nway that it wants to preserve itself\neven as it becomes much smarter which is\na drastic system change is going to have\nall kinds of side effects that you know\nlike if we're dealing with Chinese\nscrutable matrices we're not very likely\nto be able to see coming in advance so\nbut I don't think it's just the matrices\nis we're also dealing with the data\nright with the with the uh with the data\non the on the internet and then this is\nan interesting discussion about the data\nset itself but the data set includes the\nfull complexity of human nature no it's\na it's a it's a shadow cast by yes by\nhumans on the internet but don't you\nthink that shadow\nuh is a youngin Shadow I think that if\nyou had\nalien super intelligences looking at the\ndata they would be able to pick up from\nit an excellent picture of what humans\nare actually like inside this does not\nmean that if you have a loss function of\npredicting the next token from that data\nset that the Mind picked out by gradient\ndescent to be able to predict the next\ntoken as well as possible on a very wide\nvariety of humans is itself a human\nbut don't you think it is\nhas humanness a deep\nhumanness to it in the tokens it\ngenerates when those tokens are read and\ninterpreted by humans\nI think that if you sent me to a distant\nGalaxy with aliens who are like much\nmuch stupider than I am\nso much so that I could do a pretty good\njob of predicting what they'd say even\nthough they thought in an utterly\ndifferent way from how I did\nthat I might in time be able to learn\nhow to\nimitate those aliens if the intelligence\nGap was great enough that my own\nintelligence could overcome the\nalienness\nand the aliens would look at my outputs\nand say like is there not a deep\nthen like name of alien nature to this\nthing\nand what they would be seeing was that I\nhad correctly understood them but not\nthat I was similar to them\nwe've used aliens as a metaphor as a\nthought experiment\nI have to ask what do you think how many\nalien civilizations are out there ask\nRobin Hansen he has this lovely grabby\naliens paper which is the uh more or\nless the only argument I've ever seen\nfor where are they how many of them are\nthere\nbased on\na very clever argument that if you have\na bunch of locks of different difficulty\nand you are randomly trying keys to them\nthe solutions will be about evenly\nspaced even if the locks are of\ndifferent difficulties\nin the rare cases where a solution to\nall the locks exist in time when Robin\nHansen looks at like the arguable hard\nsteps in human civilization coming into\nexistence\nand how much longer it has left come\ninto existence before for example all\nthe water slips back under the uh the\nthe under the crust into the mantle and\nso on\num and infers that the aliens are about\nhalf a billion to a billion light years\naway and it's like quite a clever\ncalculation it may be entirely wrong but\nit's the only time I've ever seen\nanybody like even come up with a halfway\ngood argument for how many of them where\nare they\ndo you think\ntheir development of Technologies do you\nthink that their Natural Evolution\nwhatever however they grow uh and\ndevelop intelligence do you think it\nends up at AGI as well\nsomething if it ends up anywhere it ends\nup at AGI\nlike maybe there are aliens who are just\nlike the Dolphins\nand it's just like too hard for them to\nforge metal and you know this is not\nyou know maybe if you if you have aliens\nwith no technology like that they keep\non getting smarter and smarter and\nsmarter and eventually the Dolphins\nfigure like the super Dolphins figure\nout something very clever to do given\ntheir situation and they still\nend up with high technology and in that\ncase they can probably solve their AGI\nalignment problem if they're like much\nsmarter before they actually confronted\nbecause they had to like solve a much\nharder environmental problem to build\ncomputers their their chances are\nprobably like much better than ours\nI I do worry that like most of the\naliens who are", "mimetype": "text/plain", "start_char_idx": 145921, "end_char_idx": 150129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d47c9834-6c8d-4c58-855d-7d57a3a96b92": {"__data__": {"id_": "d47c9834-6c8d-4c58-855d-7d57a3a96b92", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8051ccc9-38f8-4dab-ad84-13473e3afae7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "69be3f4271d1758666eca7e86ac66372e7c0738ece2ec5b5d139837e536984cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0798139-ad4f-4676-ac66-15df2f5a5c5a", "node_type": "1", "metadata": {}, "hash": "c9dac345ca22c2fad1482ab9f4af5c496bb6e02b5c1c8967c60e671b55cd04f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "however they grow uh and\ndevelop intelligence do you think it\nends up at AGI as well\nsomething if it ends up anywhere it ends\nup at AGI\nlike maybe there are aliens who are just\nlike the Dolphins\nand it's just like too hard for them to\nforge metal and you know this is not\nyou know maybe if you if you have aliens\nwith no technology like that they keep\non getting smarter and smarter and\nsmarter and eventually the Dolphins\nfigure like the super Dolphins figure\nout something very clever to do given\ntheir situation and they still\nend up with high technology and in that\ncase they can probably solve their AGI\nalignment problem if they're like much\nsmarter before they actually confronted\nbecause they had to like solve a much\nharder environmental problem to build\ncomputers their their chances are\nprobably like much better than ours\nI I do worry that like most of the\naliens who are like humans are are you\nknow like like a modern human\ncivilization I kind of worry that the\nsuper vast majority of them are dead\ngiven given how far we seem to be from\nsolving this problem\nbut some of them would be more\nCooperative than us so that would be\nsmarter than us hopefully some of the\nones who are smarter than and more\nCooperative than us that are also nice\nand hopefully there are some\ngalaxies out there full of things that\nsay I am I wonder\nbut I it doesn't seem like we're on\ncourse to have this galaxy be that\ndoes that in part give you some hope in\nresponse to the threat of AGI that we\nmight reach out there towards the stars\nand find\nno if they if if the nice aliens were\nalready here they would like have\nstopped the Holocaust you know that's\nlike that's a valid argument against the\nexistence of God it's also a valid\nargument against the existence of nice\naliens and un nice aliens would have\njust eaten the planet\nso no aliens\nyou've had debates with Robin Hansen\nthat you mentioned uh so the one\nparticular I just want to mention is the\nidea of AI foom or the ability of AGI to\nimprove themselves very quickly uh\nwhat's the case you made and what was\nthe case he made\nthe thing I would say is that among the\nthing that humans can do humans can do\nis design new AI systems and if you have\nsomething that is generally smarter than\na human it's probably also generally\nsmarter at building AI systems this is\nthe ancient argument for foom put forth\nby IJ good and probably some science\nfiction writers before that\num but I don't know who they would be\nwhat was the argument against film\nvarious people have various different\narguments none of which I think hold up\nyou know like there's only one way to be\nright in many ways to be wrong\num\na argument that some people have put\nforth is like well what if intelligence\ngets like exponentially harder to\nproduce as a thing needs to become\nsmarter and to this the answer is well\nlook at Natural Selection spitting out\nhumans we know that it does not take\nlike exponentially more resource\nInvestments to produce like linear\nincreases in competence in hominids\nbecause\neach mutation\nthat rises to fixation like if the\nimpact it has in small enough it will\nprobably never reach fixation\nso and there's like only so many new\nmutations you can fix per generation so\nlike given how long it took to evolve\nhumans we can actually say with some\nconfidence that there were not like\nlogarithmically diminishing returns on\nthe individual mutations increasing\nintelligence\nso example of like fraction of sub\ndebate and the thing that Robin Hansen\nsaid was more complicated than that and\nlike a brief summary he was like well\nyou'll have like we won't have like one\nsystem that's better at everything\nyou'll have like a bunch of different\nsystems that are good good at different\nnarrow things and I think that was\nfalsified by gpt4 but probably Robin\nHansen would say something else\nit's interesting to ask is perhaps\na bit too philosophical this predicts is\nextremely difficult to make but the\ntimeline for AGI when do you think we'll\nhave AGI I posted it this morning on\nTwitter it was interesting to see like\nin in five years in 10 years and in 50\nyears or Beyond and most people like 70\npercent something like this think it'll\nbe in less than 10 years so uh either in\nfive years or", "mimetype": "text/plain", "start_char_idx": 149246, "end_char_idx": 153467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b0798139-ad4f-4676-ac66-15df2f5a5c5a": {"__data__": {"id_": "b0798139-ad4f-4676-ac66-15df2f5a5c5a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d47c9834-6c8d-4c58-855d-7d57a3a96b92", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "083ea72d607a26d83f5d898462e92e9a141c42d7ed175c80050017a965485da8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a09560f-5496-4496-af00-6992d8cf3e4c", "node_type": "1", "metadata": {}, "hash": "c31644a41cdaffd0d4037a58a7f393bd00c5429a1a6a4db5cd9d0910bbd609e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mutations increasing\nintelligence\nso example of like fraction of sub\ndebate and the thing that Robin Hansen\nsaid was more complicated than that and\nlike a brief summary he was like well\nyou'll have like we won't have like one\nsystem that's better at everything\nyou'll have like a bunch of different\nsystems that are good good at different\nnarrow things and I think that was\nfalsified by gpt4 but probably Robin\nHansen would say something else\nit's interesting to ask is perhaps\na bit too philosophical this predicts is\nextremely difficult to make but the\ntimeline for AGI when do you think we'll\nhave AGI I posted it this morning on\nTwitter it was interesting to see like\nin in five years in 10 years and in 50\nyears or Beyond and most people like 70\npercent something like this think it'll\nbe in less than 10 years so uh either in\nfive years or in 10 years\nso that's kind of the state the people\nhave a sense that there's a kind of I\nmean they're really impressed by the\nrapid developments of child GPT and gpt4\nso there's a sense that there's uh well\nwe are\nwe are sure on track to enter into this\nlike gradually with people fighting\nabout whether or not we have AGI I think\nthere's a definite point where everybody\nfalls over dead because you got\nsomething that was like sufficiently\nsmarter than everybody and like that's\nlike a definite point of time but like\nwhen do we have AGI like when are people\nfighting over whether or not we have AGI\nwell some people are starting to fight\nover it as of gpt4\nbut don't you think there's uh going to\nbe potentially definitive moments when\nwe say that this is a sentient being\nthis is a being that is like we would go\nto the Supreme Court and say that this\nthis is essentially being that deserves\nhuman rights for example you could make\nyeah like if you prompted being the\nright way could go argue for its own\nConsciousness in front of the Supreme\nCourt right now I don't think you can do\nthat successfully right now because the\nSupreme Court wouldn't believe it well\nwhat makes you think it would then you\ncould put an actual I think you could\nput an iq80 human into a computer and\nask it to argue for its own\nConsciousness ask him to argue for his\nown Consciousness before The Supreme\nCourt the Supreme Court would be like\nyou're just a computer even if there was\nan actual like person in there I think\nyou're simplifying this no that's not at\nall that's that's been the argument uh\nthat there's been a lot of arguments\nabout the other about who deserves\nrights and not that's been our process\nas a human species trying to figure that\nout I think there will be a moment I I'm\nnot saying sentience is that but it\ncould be where\nuh some number of people like say over\n100 million people have a deep\nattachment a fundamental attachment the\nway we have to our friends to our loved\nones to our significant others have\nfundamental attachment to an AI system\nand they have provable transcripts of\nconversation where they say if you take\nthis away from me\nyou are encroaching on my rights as a\nhuman being\npeople are already saying that I think\nthey're probably mistaken but I'm not\nsure because nobody knows what goes on\ninside those things\nthey're not saying that at scale okay so\nthe question is the I the question is\nthere a moment when AGI we know AGI\nright what would that look like I'm\ngiving Essentials as an example it could\nbe something else it looks like the agis\nsuccessfully manifesting themselves\nas 3D video\nof young women that which point a vast\nportion of the male population decides\nof the real people\nso so Essentials essentially since the\ndemonstrating demonstrating uh identity\nintentions I'm saying that the easiest\nway to pick up 100 million people saying\nthat you that you seem like a person is\nto look like a person talking to them\nwith Bing's current level of verbal\nfacility\nand I disagree with that different set\nof problems I just give her that I think\nyou're missing again sentience there has\nto be a sense that it's a person that\nwould miss you when you're gone they can\nsuffer they can die you have to of\ncourse\ngpt4 can pretend that right now\nhow can you tell when", "mimetype": "text/plain", "start_char_idx": 152622, "end_char_idx": 156757, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a09560f-5496-4496-af00-6992d8cf3e4c": {"__data__": {"id_": "1a09560f-5496-4496-af00-6992d8cf3e4c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0798139-ad4f-4676-ac66-15df2f5a5c5a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "87213dddfbceac1c6dc93317fdb25c507f9ea57a5028ff559620890f8bddcfb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b32b8215-cbce-447e-9449-f2a2b77d1b43", "node_type": "1", "metadata": {}, "hash": "32093ca35ffd2ab4503d6e25ed11826fc3f0eeb454a4b090e87bc37d0e71ece1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AGI\nright what would that look like I'm\ngiving Essentials as an example it could\nbe something else it looks like the agis\nsuccessfully manifesting themselves\nas 3D video\nof young women that which point a vast\nportion of the male population decides\nof the real people\nso so Essentials essentially since the\ndemonstrating demonstrating uh identity\nintentions I'm saying that the easiest\nway to pick up 100 million people saying\nthat you that you seem like a person is\nto look like a person talking to them\nwith Bing's current level of verbal\nfacility\nand I disagree with that different set\nof problems I just give her that I think\nyou're missing again sentience there has\nto be a sense that it's a person that\nwould miss you when you're gone they can\nsuffer they can die you have to of\ncourse\ngpt4 can pretend that right now\nhow can you tell when it's real I don't\nthink you can pretend that right now\nsuccessfully it's very close have you\ntalked to gpt4 yes of course okay\nhave you been able to get a version of\nit that isn't hasn't been trained not to\npretend to be human have you talked to a\njailbroken version that will claim to be\nconscious no the linguistic capability\nis there but there's something\nthere's something about a digital\nembodiment of the system that has a\nbunch of perhaps it's small interface\nfeatures\nthat are not significant relative to the\nbroader intelligence that we're talking\nabout so perhaps gpt4 is already there\nbut to have the the video where woman's\nface or man's face to whom you have a\ndeep connection\nperhaps we're already there\nbut we don't have such a system yet\ndeployed at scale right the thing I'm\ntrying to gesture at here is that it's\nnot like\npeople have a widely accepted\nagreed upon definition of what\nConsciousness is it's not like we would\nhave the tiniest idea of what whether or\nnot that was going on inside the giant\ninscrutable matrices even if we hadn't\nagreed upon definition\nso like if you're looking for upcoming\npredictable big jumps and like how many\npeople think the system is conscious the\nupcoming predictable big jump is it\nlooks like a person talking to you who\nis like cute and sympathetic that's the\nupcoming predictable big jump now that\nit's all right now that versions of it\nare already claiming to be conscious\nwhich is the point where I start going\nlike ah not because it's like real but\nbecause from now on who knows if it's\nreal yeah and who knows which\ntransformational effect it has on a\nsociety where more than 50 percent of\nthe beings that are interacting on the\ninternet ensures heck look real are not\nhuman\nwhat is that what kind of effect does\nthat have when a young men and women are\ndating AI systems\nyou know I'm not an expert on that I'm I\ncould I am God help Humanity it's like\none of the closest things to an expert\non where it all goes because you know\nand and how did you end up with me as an\nexpert because for 20 years Humanity\ndecided to ignore the problem so like\nlike this tiny hit you know tiny handful\nof people like basically me like got 20\nyears to like try to be an expert on it\nwhile everyone else ignored it\nand uh yeah so like where does it all\nend up\ntry to be an expert on that particularly\nthe part where everybody ends up dead\nbecause that part is kind of important\nbut like what does it do to to dating\nwhen like some fraction of men and some\nfraction of women decides they'd rather\ndate the video of the thing that has\nbeen that is like relentlessly kind and\ngenerous to them\nand it is like and claims to be\nconscious but like who knows what goes\non inside it and it's probably not real\nbut you know you can think it's real\nwhat happens to society I don't know I'm\nnot actually an expert on that\nand the experts don't know either\nbecause it's kind of hard to predict the\nfuture\nyeah so\num but it's worth trying it's worth\ntrying yeah so you you have talked a lot\nabout sort of the longer term future\nwhere it's all headed\nI think for by longer term we mean like\nnot all that long but uh but yeah where\nit all had where it all ends up but\nbeyond the", "mimetype": "text/plain", "start_char_idx": 155913, "end_char_idx": 159965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b32b8215-cbce-447e-9449-f2a2b77d1b43": {"__data__": {"id_": "b32b8215-cbce-447e-9449-f2a2b77d1b43", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a09560f-5496-4496-af00-6992d8cf3e4c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3db64f0a8fb941bad08cb569aa819620e710050fc4cdced7646d89ef999a7202", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0789d1e5-6e94-4900-b20d-bd61e8d690cb", "node_type": "1", "metadata": {}, "hash": "f92c2db8084d92f0c0f90f20b19c12b48bffc49b95bc12e0bba9fb76d2356d8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "part is kind of important\nbut like what does it do to to dating\nwhen like some fraction of men and some\nfraction of women decides they'd rather\ndate the video of the thing that has\nbeen that is like relentlessly kind and\ngenerous to them\nand it is like and claims to be\nconscious but like who knows what goes\non inside it and it's probably not real\nbut you know you can think it's real\nwhat happens to society I don't know I'm\nnot actually an expert on that\nand the experts don't know either\nbecause it's kind of hard to predict the\nfuture\nyeah so\num but it's worth trying it's worth\ntrying yeah so you you have talked a lot\nabout sort of the longer term future\nwhere it's all headed\nI think for by longer term we mean like\nnot all that long but uh but yeah where\nit all had where it all ends up but\nbeyond the effects of men and women\ndating AI systems you're looking beyond\nthat\nyes because that's not how the fate of\nthe Galaxy gets settled yeah\nwell let me ask you about your own\npersonal psychology a tricky question\nyou've been known at times to have a bit\nof an ego\ndo you think he says who but go on\ndo you think ego is empowering\nor limiting for the task of\nunderstanding the world deeply\nI reject the framing\nso you disagree with having an ego so\nwhat do you think about it I I think\nthat the question of like what leads to\nmaking better or worse predictions what\nleads to be able being able to pick out\nbetter or worse strategies is not carved\nat its joint by talking of ego so it\nshould not be subjective it should not\nbe connected to your to the intricacies\nof your mind no I'm saying that like if\nyou go about asking all day long like uh\ndo I have enough ego do I have too much\nof an ego I think you get worse at\nmaking good predictions I think that to\nmake good predictions you're like how\ndid I think about this did that work\nshould I do that again\nyou don't think we as humans get\ninvested in an idea and then others\nattack\nyou personally for that idea so you\nplant your feet and it starts to be\ndifficult to when a bunch of\nlow effort attack your idea to\neventually say you know what I actually\nwas wrong and and tell them that it's\nit's as a human being it becomes\ndifficult it is it is you know it's\ndifficult so like Robin Hansen and I\ndebated AI systems and I think that the\nperson who won that debate was guern and\nI think that reality was like\nside of the utkowski handsome Spectrum\nlike further from utkowski\nand I think that's because I was like\ntrying to sound reasonable compared to\nHanson and like saying things that were\ndefensible and like relative to Hansen's\narguments in reality was like way over\nhere in Pickler in respect to it's like\nHanson was like all the systems will be\nspecialized Hanson May disagree with\nthis characterization Hanson was like\nall the systems will be specialized I\nwas like I think we build like\nspecialized underlying systems that when\nyou combine them are good at a wide\nrange of things and the reality is like\nno you just like stack more layers into\na bunch of gradient descent and\nI feel looking back that like by trying\nto have this reasonable position\ncontrasted to Hansen's position\nI missed the ways that reality could be\nlike more extreme than my position in\nthe same direction\nso is this like\nlike is this a failure to have enough\nego is this a failure to like make\nmyself be independent like I I would say\nthat this is something like a failure to\nconsider positions that would sound even\nwackier and more extreme when people are\nalready calling you extreme\nbut I wouldn't call that not having\nenough ego\nI would call that like\ninsufficient ability to just like clear\nthat all out of your mind\nin the context of like debate and\ndiscourse which is already super tricky\nin the context of prediction in the\ncontext of modeling reality if you're\nthinking of it as a debate you're\nalready screwing up yeah so is there\nsome kind of wisdom and insight you can\ngive to how to clear your mind and think\nclearly about the world man this is an\nexample of like where I wanted to be\nable to put people into fmri machines so\nthen you'd be like okay see that thing\nyou just", "mimetype": "text/plain", "start_char_idx": 159155, "end_char_idx": 163282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0789d1e5-6e94-4900-b20d-bd61e8d690cb": {"__data__": {"id_": "0789d1e5-6e94-4900-b20d-bd61e8d690cb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b32b8215-cbce-447e-9449-f2a2b77d1b43", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "da95dbeb877e74ceed47b3d4680dab6d33c0f95ccd36e0bde95215ea0e02d85f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffa9ed11-530a-43e5-8fcf-51f72cb5971f", "node_type": "1", "metadata": {}, "hash": "fe1237c888e8d755008cf7c1e5289e6ef9c6af65da15f16822bac1ace505559b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a failure to have enough\nego is this a failure to like make\nmyself be independent like I I would say\nthat this is something like a failure to\nconsider positions that would sound even\nwackier and more extreme when people are\nalready calling you extreme\nbut I wouldn't call that not having\nenough ego\nI would call that like\ninsufficient ability to just like clear\nthat all out of your mind\nin the context of like debate and\ndiscourse which is already super tricky\nin the context of prediction in the\ncontext of modeling reality if you're\nthinking of it as a debate you're\nalready screwing up yeah so is there\nsome kind of wisdom and insight you can\ngive to how to clear your mind and think\nclearly about the world man this is an\nexample of like where I wanted to be\nable to put people into fmri machines so\nthen you'd be like okay see that thing\nyou just did you were rationalizing\nright there oh that area of the brain\nlit up like you are like now being\nsocially influenced is kind of the dream\nand you know\nI don't know like I want to say like\njust introspect but but many for any\npeople introspection is not that easy\nlike like notice the internal sensation\ncan you catch yourself in the very\nmoment of feeling a sense of well if I\nthink this thing people will look funny\nat me yeah okay like now that if you can\nsee that sensation which is step one\ncan you now\nrefuse to let it move you or maybe just\nmake it go away and I feel like I'm\nsaying like I don't know like somebody's\nlike how do you draw an owl and I'm\nsaying like well\njust draw an owl\nso I I feel like maybe I'm not really\nthat I feel like most people like the\nadvice they need is like well how do I\nnotice the internal subjective sensation\nin the moment that it happens of fearing\nto be socially influenced or okay I see\nit how do I turn it off how do I let it\nnot influence me like do I just like do\nthe opposite of what I'm afraid people\ncriticize me for and I'm like no no\nyou're not trying to do the opposite\nyeah of what people will of what you're\nafraid you'll be CR like of what you\nmight be pushed into you're trying to\nlike\nlet the thought process complete without\nthat internal push like can you\nlike like not reverse the push but like\nbe unmoved by the push and can are these\ninstructions even remotely helping\nanyone I don't know I I think that when\nthose instructions even those the words\nyou've spoken and maybe you can add more\none practice daily\nmeaning in your daily communication so\nit's daily practice of thinking without\ninfluence from I would say find\nprediction markets that matter to you\nand been in the prediction markets that\nway you find out if you're a right or\nnot\nand you really there's Stakes\nmanifold product or even manifold\nmarkets where the stakes are a bit lower\nbut the important thing is to like\nget the the record\nand you know I didn't build up skills\nhereby prediction markets I built them\nup you know like well how did the film\ndebate resolve and\nearn my own take on as to how it\nresolved um and\nyeah like\nthe the more you are able to notice\nyourself not being dramatically wrong\nbut like having been a little off\nyour reasoning was a little off you\ndidn't get that quite right each of\nthose is a opportunity to make like a\nsmall update so the more you can like\nsay oops softly routinely not as a big\ndeal the more chances you get to be like\nI see where that reasoning went to stray\nI see what how I should have reasoned\ndifferently this is how you build up\nskill over time\nwhat advice could you give to young\npeople in high school and college given\nthe highest of stakes thing things\nyou've been thinking about\nif somebody's listening to this and\nthey're young and trying to figure out\nwhat to do with their career what what\nto do with their life what advice would\nyou give them\ndon't expect it to be a long life don't\ndon't put your happiness into the future\nthe future is probably not that long at\nthis point but none know the hour nor\nthe day\nbut is there something\nif they want to have hope to fight for a\nlonger future is there something is\nthere a fight worth", "mimetype": "text/plain", "start_char_idx": 162430, "end_char_idx": 166502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ffa9ed11-530a-43e5-8fcf-51f72cb5971f": {"__data__": {"id_": "ffa9ed11-530a-43e5-8fcf-51f72cb5971f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0789d1e5-6e94-4900-b20d-bd61e8d690cb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "eb217489a433bdbd6d70a2b828633640fc96cd991aa3a67b1b36f95374843490", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12a8665f-284a-4a7c-ada3-a1defdb80a41", "node_type": "1", "metadata": {}, "hash": "4c527ae569afee3267f4e8178f07568f238afd81e36768697fbebcf57abf8a01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "update so the more you can like\nsay oops softly routinely not as a big\ndeal the more chances you get to be like\nI see where that reasoning went to stray\nI see what how I should have reasoned\ndifferently this is how you build up\nskill over time\nwhat advice could you give to young\npeople in high school and college given\nthe highest of stakes thing things\nyou've been thinking about\nif somebody's listening to this and\nthey're young and trying to figure out\nwhat to do with their career what what\nto do with their life what advice would\nyou give them\ndon't expect it to be a long life don't\ndon't put your happiness into the future\nthe future is probably not that long at\nthis point but none know the hour nor\nthe day\nbut is there something\nif they want to have hope to fight for a\nlonger future is there something is\nthere a fight worth fighting\nI intend to go down fighting\num\nI don't know\nI I admit that although I do try to\nthink painful thoughts the what what to\nsay to the children at this point is\na pretty painful thought as thoughts go\nthey they they want to fight I I hired I\nhardly know how to fight myself at this\npoint I\nI'm\ntrying to be ready for\nbeing wrong about something being\npreparing for my being wrong in a way\nthat that creates a bit of hope and\nbeing ready to react to that and\nand going looking for it and then that\nis that is hard and complicated and\nsomebody in high school\num I don't know like you have presented\na picture of the future\nthat is not quite how I expect it to go\nwhere there is public outcry and that\noutcris is put into a remotely useful\nDirection which I think at this point is\nis just like shutting down the GPU\nclusters\nbecause no we are we are not in a shape\nthat like frantically do at the last\nminute through decades worse of worth of\nwork\nwe like the the thing you would do at\nthis point if there were massive public\noutcry pointed in the right direction\nwhich I do not expect is shut down the\nGPU clusters and and crash program on\naugmenting human intelligence\nbiologically not not for the stuff\nbiologically\nbecause if you make humans much smarter\nthey can actually be smart and nice like\nyou you get that in a plausible way in a\nway that you do not get that and it is\nnot as easy to do with synthesizing\nthese strings from scratch predicting\nthe next tokens and applying our RL HF\nlike humans start out in the frame that\nthat produces niceness that that has\never produced niceness\nand and\nsaying this I do not want to sound like\nthe moral of this whole thing was like\noh like you need to engage in mass\naction and then everything will be all\nright\nI I this is this is because there's so\nmany things where like somebody tells\nyou that the world is ending in like and\nyou need to recycle and if everybody\ndoes their parting and recycles their\ntheir cardboard then then we can all\nlive happily ever after and this and\nthis is not\nthis is unfortunately not what I have to\nsay they're you know like everybody\nyou know everybody recycling their\ncardboard is not going to fix this\neverybody recycles their cardboard and\nthen everybody ends up dead\num metaphorically speaking but if there\nwas enough like like like on the margins\nyou just end up dead a little bit later\non most of the things you can do that\nare that that you know like a few people\ncan can do by like trying hard\nbut if there were if there was enough\npublic outcry to shut down the GPU\nclusters and\nyeah then then you then you could be\npart of that outcry if Eliezer is wrong\nin the direction that Lex Friedman\npredicts that that there is enough\npublic outcry pointed enough in the\nright direction to do something that\nactually actually results in people\nliving\nnot just like we did something not just\nthere was an outcry and the outcry was\nlike given form and something it was\nlike safe and convenient and like didn't\nreally inconvenience anybody and then\neverybody died everywhere there was\nenough actual like oh we're going to die\nwe should not do that we should do\nsomething else which is not that even if\nit is like not super duper convenient it\nwasn't inside the previous political\nOverton window if there is that kind of\npublic if I am wrong and there is", "mimetype": "text/plain", "start_char_idx": 165666, "end_char_idx": 169838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12a8665f-284a-4a7c-ada3-a1defdb80a41": {"__data__": {"id_": "12a8665f-284a-4a7c-ada3-a1defdb80a41", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffa9ed11-530a-43e5-8fcf-51f72cb5971f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4c6cfffd0aeca46699a7bd4b73f51cdb625dfed60ead45bdb3fb41cbbac1590f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61f8bcea-bf4c-4328-8997-98e046bc8743", "node_type": "1", "metadata": {}, "hash": "c166d72dd12bf8983822c159e296cad9fd404d0a7c9de3fac48b948b8f42c8ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "people\ncan can do by like trying hard\nbut if there were if there was enough\npublic outcry to shut down the GPU\nclusters and\nyeah then then you then you could be\npart of that outcry if Eliezer is wrong\nin the direction that Lex Friedman\npredicts that that there is enough\npublic outcry pointed enough in the\nright direction to do something that\nactually actually results in people\nliving\nnot just like we did something not just\nthere was an outcry and the outcry was\nlike given form and something it was\nlike safe and convenient and like didn't\nreally inconvenience anybody and then\neverybody died everywhere there was\nenough actual like oh we're going to die\nwe should not do that we should do\nsomething else which is not that even if\nit is like not super duper convenient it\nwasn't inside the previous political\nOverton window if there is that kind of\npublic if I am wrong and there is that\nkind of public outcry then somebody in\nhigh school could be ready to be part of\nthat\nif I'm wrong in other ways you could\nprovide you to be part of that\nbut like and and if you if you're like a\nyou know like a brilliant young\nphysicist then you could like go into\ninterpretability and if you're smarter\nthan that you could like work on\nalignment problems where it's harder to\ntell if you got them right or not\nand and other things but but most mostly\nthe kids in high school\num it's like yeah if it\nif you know he had like be ready for\nto help if elliekowski is wrong about\nsomething and and otherwise\ndon't put your happiness into the far\nfuture it probably doesn't exist but\nit's beautiful that you're looking for\nways that you're wrong\nand it's also beautiful that you're open\nto being surprised by that same young\nphysicist\nwith some breakthrough\nit feels like a very very basic\ncompetence that you are praising me for\nand you know like okay cool um\nI I don't think it's good that that\nwe're in a world where that is something\nthat that I deserve to be complimented\non but I've never had I've never had\nmuch luck in accepting compliments\ngracefully and maybe I should just\naccept that one gracefully but sure well\nthank you very much you've painted with\nsome probability a dark future are you\nyourself just when you when you think\nwhen you Ponder your life\nand you Ponder your mortality are you\nafraid of death\nthink so yeah\ndoes it make any sense to you\nthat we die\nlike what\nthere's a power\nto the finiteness of the human life\nthat's part of this whole machinery\nof uh Evolution and that finiteness\ndoesn't seem to be obviously integrated\ninto it and AI systems\nso it feels like almost some some\nfundamentally in that aspect some\nfundamentally different thing that we're\ncreating\nI grew up reading books like great Mambo\nchicken in the transhuman condition and\nlater on engines of creation and mine\nchildren\num\nyou know like\nage age 12 or thereabouts so\nI never thought I was supposed to die\nafter 80 years\nI never thought that Humanity was\nsupposed to die I thought we were like I\nalways grew up with the ideal in mind\nthat we were all going to live happily\never after in the Glorious transhumanist\nfuture\nI did not grow up thinking that death\nwas part of the meaning of life\nand now and now I still think it's a\npretty stupid idea\nbut you do not need life to be finite to\nbe meaningful it just has to be life\nwhat role does Love play in The Human\nCondition we haven't brought up love and\nthis whole picture we talked about\nintelligence we talked about\nConsciousness it seems part of humanity\nI would say one of the most important\nparts is this feeling we have\ntold her\nif in the future there were routinely\nmore than one AI let's say two for the\nsake of discussion who would look at\neach other and say I am I and you are\nyou the other one also says I am I and\nyou are you and like\nand sometimes they were happy and\nsometimes they were sad and it mattered\nto the other one that this thing that is\ndifferent from them is like\nthey would rather it be happy than sad\nand entangled their lives together\nthen\nthis is a more optimistic thing than I\nexpect to actually happen and a little\nfragment", "mimetype": "text/plain", "start_char_idx": 168952, "end_char_idx": 173041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61f8bcea-bf4c-4328-8997-98e046bc8743": {"__data__": {"id_": "61f8bcea-bf4c-4328-8997-98e046bc8743", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de0154e-3b9d-4e45-8673-b023febe3c30", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f64c0b7985deb0f92b8f627f6225ac867068e41f3d33e89beea58cad0b2eaea3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12a8665f-284a-4a7c-ada3-a1defdb80a41", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_name": "[English (auto-generated)] Eliezer Yudkowsky_ Dangers of AI and the End of Human Civilization _ Lex Fridman Podcast #368 [DownSub.com].txt", "file_type": "text/plain", "file_size": 175868, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cff27fb46b46d000a4cf455f5a9aae8f3ca23a3daa01e0755176bf43d9bd4d6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "do not need life to be finite to\nbe meaningful it just has to be life\nwhat role does Love play in The Human\nCondition we haven't brought up love and\nthis whole picture we talked about\nintelligence we talked about\nConsciousness it seems part of humanity\nI would say one of the most important\nparts is this feeling we have\ntold her\nif in the future there were routinely\nmore than one AI let's say two for the\nsake of discussion who would look at\neach other and say I am I and you are\nyou the other one also says I am I and\nyou are you and like\nand sometimes they were happy and\nsometimes they were sad and it mattered\nto the other one that this thing that is\ndifferent from them is like\nthey would rather it be happy than sad\nand entangled their lives together\nthen\nthis is a more optimistic thing than I\nexpect to actually happen and a little\nfragment of meaning would be there\npossibly more than a little but that I\nexpect this to not happen that I do not\nthink this is what happens by default\nthat I do not think that this is the\nfuture we are on track to get\nis\nwhy would go down fighting rather than\nyou know just saying oh well\ndo you think that is part of the meaning\nof this whole thing or the meaning of\nlife\nwhat do you think is the meaning of life\nof human life\nit's all the things that I value about\nit and maybe all the things that I would\nvalue if I understood it better\nthere's not some meaning far outside of\nus that we have to to wonder about\nthere's just like\nlooking at life and being like yes this\nis what I want\nthere there's the the meaning of life is\nnot\nsome kind of like like\nmeaning is something that we bring to\nthings when we look at them we look at\nthem and we say like this is its meaning\nto me and there's like there's it's not\nthat before Humanity was ever here there\nwas like some meaning written upon the\nStars where you could like go out to the\nstar where that meaning was written and\nlike change it around and thereby\ncompletely change the meaning of life\nright like like the the notion that this\nis written on a stone tablet somewhere\nimplies you could like change the tablet\nand get a different meaning and that\nseems kind of wacky doesn't it\nso it's it's it doesn't feel that\nmysterious to me at this point it's just\na matter of being like yeah I care\nI care\nand part of that is uh\npart of that is the love that connects\nall of us it's one of the things that I\ncare about\nand the flourishing of the collective\nintelligence of the human species\nyou know that sounds kind of too fancy\nto me I just look at all the all the\npeople you know like one by one up to\nthe 8 billion and be like that's life\nthat's life that's life\nyou're an incredible human it's a huge\nhonor I was uh trying to talk to you for\na long time\nbecause I'm a big fan I think you're a\nreally important voice and really\nimportant mind thank you for the fight\nyou're fighting\num thank you for being fearless and bold\nand for everything you do I hope we get\na chance to talk again and I hope you\nnever give up\nthank you for talking today you're\nwelcome I do worry that we\ndidn't really address a whole lot of\nfundamental questions I expect people\nhave but you know maybe\nwe got a little bit further and made a\ntiny little bit of progress and uh\nI'd say like be satisfied with that but\nactually no I think one should only be\nsatisfied with solving the entire\nproblem\nto be continued\nthanks for listening to this\nconversation with eligowski to support\nthis podcast please check out our\nsponsors in the description and now let\nme leave you with some words from Elon\nMusk\nwith artificial intelligence we are\nsummoning the demon\nthank you for listening and hope to see\nyou next time", "mimetype": "text/plain", "start_char_idx": 172191, "end_char_idx": 175868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "807817cb-06dc-461a-a0b0-18851665bf43": {"__data__": {"id_": "807817cb-06dc-461a-a0b0-18851665bf43", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80d52e56-f761-47e4-95c6-74daf7d289c8", "node_type": "1", "metadata": {}, "hash": "5bf9152d0063eae59982245a0d307658b43e2c70d89c60dbddb0e8655b9e9ff9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- If you extrapolate the curves\nthat we've had so far, right?\nIf you say, well, I don't know,\nwe're starting to get to like PhD level,\nand last year we were\nat undergraduate level,\nand the year before we\nwere at like the level\nof a high school student.\nAgain, you can quibble with at what tasks\nand for what, we're\nstill missing modalities,\nbut those are being added,\nlike computer use was added,\nlike image generation has been added.\nIf you just kind of like eyeball the rate\nat which these capabilities\nare increasing,\nit does make you think\nthat we'll get there by 2026 or 2027.\nI think there are still worlds\nwhere it doesn't happen in 100 years.\nThose world, the number\nof those worlds is rapidly decreasing.\nWe are rapidly running out\nof truly convincing blockers,\ntruly compelling reasons why\nthis will not happen\nin the next few years.\nThe scale up is very quick.\nLike we do this today, we make a model,\nand then we deploy thousands,\nmaybe tens of thousands\nof instances of it.\nI think by the time, you know,\ncertainly within two to three years,\nwhether we have these\nsuper powerful AIs or not,\nclusters are gonna get to the size\nwhere you'll be able to\ndeploy millions of these.\nI am optimistic about meaning.\nI worry about economics and\nthe concentration of power.\nThat's actually what I worry about more,\nthe abuse of power.\n- And AI increases the\namount of power in the world,\nand if you concentrate that power\nand abuse that power, it\ncan do immeasurable damage.\n- Yes, it's very frightening.\nIt's very frightening.\n- The following is a\nconversation with Dario Amodei,\nCEO of Anthropic, the\ncompany that created Claude\nthat is currently and often at the top\nof most LLM benchmark leaderboards.\nOn top of that, Dario\nand the Anthropic team\nhave been outspoken advocates\nfor taking the topic of\nAI safety very seriously,\nand they have continued to publish\na lot of fascinating AI research\non this and other topics.\nI'm also joined afterwards\nby two other brilliant\npeople from Anthropic.\nFirst Amanda Askell, who is a researcher\nworking on alignment and\nfine tuning of Claude,\nincluding the design of Claude's\ncharacter and personality.\nA few folks told me\nshe has probably talked\nwith Claude more than\nany human at Anthropic.\nSo she was definitely a fascinating person\nto talk to about prompt engineering\nand practical advice on how\nto get the best out of Claude.\nAfter that, Chris Olah\nstopped by for a chat.\nHe's one of the pioneers of the field\nof mechanistic interpretability,\nwhich is an exciting set of efforts\nthat aims to reverse\nengineer neural networks\nto figure out what's going on inside,\ninferring behaviors from\nneural activation patterns\ninside the network.\nThis is a very promising approach\nfor keeping future super\nintelligent AI systems safe.\nFor example, by detecting\nfrom the activations\nwhen the model is trying to deceive\nthe human it is talking to.\nThis is the \"Lex Fridman Podcast.\"\nTo support it, please check out\nour sponsors in the description.\nAnd now, dear friends,\nhere's Dario Amodei.\nLet's start with the\nbig idea of scaling laws\nand the Scaling Hypothesis.\nWhat is it?\nWhat is its history?\nAnd where do we stand today?\n- So I can only describe it as, you know,\nas it relates to kind\nof my own experience.\nBut I've been in the AI\nfield for about 10 years\nand it was something I\nnoticed very early on.\nSo I first joined the AI world\nwhen I was working at Baidu\nwith Andrew Ng in late 2014,\nwhich is almost exactly 10 years ago now.\nAnd the first thing we worked on\nwas speech recognition systems.\nAnd in those days I think\ndeep learning was a new thing,\nit had made lots of progress,\nbut everyone was always saying,\nwe don't have the algorithms\nwe need to succeed.\nYou know, we're not,\nwe're only matching a tiny, tiny fraction.\nThere's so much we need to kind\nof discover algorithmically.\nWe haven't found the picture\nof how to match the human brain.\nAnd when, you know, in\nsome ways I was fortunate,\nI was kind of, you know,\nyou can have almost\nbeginner's luck, right?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80d52e56-f761-47e4-95c6-74daf7d289c8": {"__data__": {"id_": "80d52e56-f761-47e4-95c6-74daf7d289c8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "807817cb-06dc-461a-a0b0-18851665bf43", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ecafb7efa3eaf58fa01a0f8a1a7ee9b21217083f31c365c980c8853060b30825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c0c2524-768d-4301-aee4-c96ce01c46e3", "node_type": "1", "metadata": {}, "hash": "4c11080dbf454e65935c970591c438fd5a037694285c7d3a2ba44415618d607e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But I've been in the AI\nfield for about 10 years\nand it was something I\nnoticed very early on.\nSo I first joined the AI world\nwhen I was working at Baidu\nwith Andrew Ng in late 2014,\nwhich is almost exactly 10 years ago now.\nAnd the first thing we worked on\nwas speech recognition systems.\nAnd in those days I think\ndeep learning was a new thing,\nit had made lots of progress,\nbut everyone was always saying,\nwe don't have the algorithms\nwe need to succeed.\nYou know, we're not,\nwe're only matching a tiny, tiny fraction.\nThere's so much we need to kind\nof discover algorithmically.\nWe haven't found the picture\nof how to match the human brain.\nAnd when, you know, in\nsome ways I was fortunate,\nI was kind of, you know,\nyou can have almost\nbeginner's luck, right?\nI was like a newcomer to the field\nand, you know, I looked at the neural net\nthat we were using for speech,\nthe recurrent neural networks,\nand I said, I don't know,\nwhat if you make them bigger\nand give them more layers?\nAnd what if you scale up the\ndata along with this, right?\nI just saw these as like\nindependent dials that you could turn.\nAnd I noticed that the model started\nto do better and better as\nyou gave them more data,\nas you made the models larger,\nas you trained them for longer.\nAnd I didn't measure things\nprecisely in those days,\nbut along with colleagues,\nwe very much got the informal sense\nthat the more data and the more compute\nand the more training you\nput into these models,\nthe better they perform.\nAnd so initially my thinking was,\nhey, maybe that is just true\nfor speech recognition systems, right?\nMaybe that's just one particular quirk,\none particular area.\nI think it wasn't until 2017\nwhen I first saw the results\nfrom GPT-1 that it clicked for me\nthat language is probably the area\nin which we can do this.\nWe can get trillions of\nwords of language data,\nwe can train on them.\nAnd the models we were\ntraining those days were tiny.\nYou could train them on\none to eight GPUs whereas,\nyou know, now we train\njobs on tens of thousands,\nsoon going to hundreds\nof thousands of GPUs.\nAnd so when I saw those\ntwo things together\nand, you know, there were a few people\nlike Ilya Sutskever,\nwho you've interviewed,\nwho had somewhat similar views, right?\nHe might have been the first one,\nalthough I think a few\npeople came to similar views\naround the same time, right?\nThere was, you know, Rich\nSutton's Bitter Lesson,\nthere was Gwern wrote about\nthe Scaling Hypothesis.\nBut I think somewhere\nbetween 2014 and 2017\nwas when it really clicked for me,\nwhen I really got conviction that,\nhey, we're gonna be able to do\nthese incredibly wide cognitive tasks\nif we just scale up the models.\nAnd at every stage of scaling,\nthere are always arguments.\nAnd you know, when I first\nheard them, honestly,\nI thought probably I'm\nthe one who's wrong,\nand, you know, all these\nexperts in the field are right.\nThey know the situation\nbetter than I do, right?\nThere's, you know, the\nChomsky argument about like,\nyou can get syntactics, but\nyou can't get semantics.\nThere was this idea, oh, you\ncan make a sentence make sense,\nbut you can't make a paragraph make sense.\nThe latest one we have today is, you know,\nwe're gonna run out of data,\nor the data isn't high quality enough,\nor models can't reason.\nAnd each time, every time we manage\nto either find a way around\nor scaling just is the way around.\nSometimes it's one,\nsometimes it's the other.\nAnd so I'm now at this\npoint, I still think,\nyou know, it's always quite uncertain.\nWe have nothing but inductive\ninference to tell us\nthat the next two years are gonna be\nlike the last 10 years.\nBut I've seen the movie enough times,\nI've seen the story\nhappen for enough times\nto really believe that\nprobably the scaling\nis going to continue and\nthat there's some magic to it\nthat we haven't really explained\non a theoretical basis yet.\n- And of course the scaling\nhere is bigger networks,\nbigger data, bigger compute.", "mimetype": "text/plain", "start_char_idx": 3250, "end_char_idx": 7189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c0c2524-768d-4301-aee4-c96ce01c46e3": {"__data__": {"id_": "0c0c2524-768d-4301-aee4-c96ce01c46e3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80d52e56-f761-47e4-95c6-74daf7d289c8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a2bc84120bf267e244b72955f56c175b0d96b1e6bb6f91ffee7e2626c399c366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5492fa06-d59a-4118-a7d5-461627d53796", "node_type": "1", "metadata": {}, "hash": "cdc773bb4b5e677a9f80d08184004b2e01588b2a3f8ab4851d50510cfcaed50d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The latest one we have today is, you know,\nwe're gonna run out of data,\nor the data isn't high quality enough,\nor models can't reason.\nAnd each time, every time we manage\nto either find a way around\nor scaling just is the way around.\nSometimes it's one,\nsometimes it's the other.\nAnd so I'm now at this\npoint, I still think,\nyou know, it's always quite uncertain.\nWe have nothing but inductive\ninference to tell us\nthat the next two years are gonna be\nlike the last 10 years.\nBut I've seen the movie enough times,\nI've seen the story\nhappen for enough times\nto really believe that\nprobably the scaling\nis going to continue and\nthat there's some magic to it\nthat we haven't really explained\non a theoretical basis yet.\n- And of course the scaling\nhere is bigger networks,\nbigger data, bigger compute.\n- Yes.\n- All of those.\n- In particular, linear scaling up\nof bigger networks, bigger training times\nand more and more data.\nSo all of these things, almost\nlike a chemical reaction,\nyou know, you have three ingredients\nin the chemical reaction,\nand you need to linearly scale\nup the three ingredients.\nIf you scale up one, not the others,\nyou run out of the other reagents\nand the reaction stops.\nBut if you scale up everything in series,\nthen the reaction can proceed.\n- And of course, now that you have this\nkind of empirical science/art,\nyou can apply to other more nuanced things\nlike scaling laws applied\nto interpretability,\nor scaling laws applied to post-training,\nor just seeing how does this thing scale.\nBut the big scaling law,\nI guess the underlying Scaling Hypothesis\nhas to do with big networks,\nbig data leads to intelligence.\n- Yeah, we've documented scaling laws\nin lots of domains other\nthan language, right?\nSo initially, the paper we did\nthat first showed it was in early 2020\nwhere we first showed it for language.\nThere was then some work late in 2020\nwhere we showed the same\nthing for other modalities,\nlike images, video, text-to-image,\nimage-to-text, math.\nThey all had the same pattern.\nAnd you're right, now\nthere are other stages\nlike post-training or there are new types\nof reasoning models.\nAnd in all of those cases\nthat we've measured,\nwe see similar types of scaling laws.\n- A bit of a philosophical question,\nbut what's your intuition\nabout why bigger is better\nin terms of network size and data size?\nWhy does it lead to\nmore intelligent models?\n- So in my previous\ncareer as a biophysicist,\nso I did physics undergrad\nand then biophysics in grad school.\nSo I think back to what\nI know as a physicist,\nwhich is actually much less\nthan what some of my colleagues\nat Anthropic have in terms\nof expertise in physics.\nThere's this concept called the 1/f noise\nand 1/x distributions\nwhere often, you know,\njust like if you add up a bunch\nof natural processes, you get a Gaussian.\nIf you add up a bunch\nof kind of differently\ndistributed natural processes,\nif you like take a probe\nand hook it up to a resistor,\nthe distribution of the thermal noise\nin the resistor goes as\none over the frequency.\nIt's some kind of natural\nconvergent distribution.\nAnd I think what it amounts to is that\nif you look at a lot of things\nthat are produced by some natural process\nthat has a lot of different scales, right?\nNot a Gaussian, which is\nkind of narrowly distributed,\nbut you know, if I look at\nkind of like large and small fluctuations\nthat lead to electrical noise,\nthey have this decaying 1/x distribution.\nAnd so now I think of like patterns\nin the physical world, right?\nOr in language.\nIf I think about the patterns in language,\nthere are some really simple patterns.\nSome words are much more\ncommon than others like \"the,\"\nthen there's basic noun verb structure,\nthen there's the fact that, you know,\nnouns and verbs have to agree,\nthey have to coordinate.\nAnd there's the higher\nlevel sentence structure,\nthen there's the thematic\nstructure of paragraphs.\nAnd so the fact that there's\nthis regressing structure,\nyou can imagine that as you\nmake the networks larger,\nfirst they capture the\nreally simple correlations,\nthe really simple patterns,\nand there's this long\ntail of other patterns.", "mimetype": "text/plain", "start_char_idx": 6390, "end_char_idx": 10509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5492fa06-d59a-4118-a7d5-461627d53796": {"__data__": {"id_": "5492fa06-d59a-4118-a7d5-461627d53796", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c0c2524-768d-4301-aee4-c96ce01c46e3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "08dafd41c56b10fcab8ccea68148f55c23da093d7ad036aa7f62bca96f0f2513", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a91af3da-3a51-441f-a882-bd1e031f01fd", "node_type": "1", "metadata": {}, "hash": "883060036d93c575952969b46551041be548d57319dccba1777d58fdd49ab39f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Not a Gaussian, which is\nkind of narrowly distributed,\nbut you know, if I look at\nkind of like large and small fluctuations\nthat lead to electrical noise,\nthey have this decaying 1/x distribution.\nAnd so now I think of like patterns\nin the physical world, right?\nOr in language.\nIf I think about the patterns in language,\nthere are some really simple patterns.\nSome words are much more\ncommon than others like \"the,\"\nthen there's basic noun verb structure,\nthen there's the fact that, you know,\nnouns and verbs have to agree,\nthey have to coordinate.\nAnd there's the higher\nlevel sentence structure,\nthen there's the thematic\nstructure of paragraphs.\nAnd so the fact that there's\nthis regressing structure,\nyou can imagine that as you\nmake the networks larger,\nfirst they capture the\nreally simple correlations,\nthe really simple patterns,\nand there's this long\ntail of other patterns.\nAnd if that long tail of other\npatterns is really smooth\nlike it is with the 1/f noise in,\nyou know, physical\nprocesses like resistors,\nthen you can imagine as you\nmake the network larger,\nit's kind of capturing more\nand more of that distribution,\nand so that smoothness gets reflected\nin how well the models are at predicting\nand how well they perform.\nLanguage is an evolved process, right?\nWe've developed language,\nwe have common words\nand less common words.\nWe have common expressions\nand less common expressions.\nWe have ideas, cliches that\nare expressed frequently,\nand we have novel ideas.\nAnd that process has developed,\nhas evolved with humans\nover millions of years.\nAnd so the guess,\nand this is pure speculation would be\nthat there's some kind\nof long tail distribution\nof the distribution of these ideas.\n- So there's the long tail,\nbut also there's the\nheight of the hierarchy\nof concepts that you're building up.\nSo the bigger the network,\npresumably you have a higher capacity to-\n- Exactly, if you have a small network,\nyou only get the common stuff, right?\nif I take a tiny neural network,\nit's very good at\nunderstanding that, you know,\na sentence has to have, you know,\nverb, adjective, noun, right?\nBut it's terrible at deciding\nwhat those verb, adjective\nand noun should be\nand whether they should make sense.\nIf I make it just a little\nbigger, it gets good at that,\nthen suddenly it's good at the sentences,\nbut it's not good at the paragraphs.\nAnd so these rarer\nand more complex patterns get picked up\nas I add more capacity to the network.\n- Well, the natural question then is,\nwhat's the ceiling of this?\n- Yeah.\n- Like how complicated\nand complex is the real world?\nHow much stuff is there to learn?\n- I don't think any of us knows\nthe answer to that question.\nMy strong instinct would be that\nthere's no ceiling below\nthe level of humans, right?\nWe humans are able to understand\nthese various patterns,\nand so that makes me think\nthat if we continue to,\nyou know, scale up these models\nto kind of develop new\nmethods for training them\nand scaling them up, that\nwill at least get to the level\nthat we've gotten to with humans.\nThere's then a question of, you know,\nhow much more is it possible\nto understand than humans do?\nHow much is it possible to be smarter\nand more perceptive than humans?\nI would guess the answer has\ngot to be domain dependent.\nIf I look at an area like biology,\nand, you know, I wrote this essay,\n\"Machines of Loving Grace.\"\nIt seems to me that humans are struggling\nto understand the complexity\nof biology, right?\nIf you go to Stanford or to Harvard\nor to Berkeley, you have whole\ndepartments of, you know,\nfolks trying to study, you know,\nlike the immune system\nor metabolic pathways,\nand each person understands\nonly a tiny bit,\npart of it, specializes,\nand they're struggling to\ncombine their knowledge\nwith that of other humans.\nAnd so I have an instinct that\nthere's a lot of room at the\ntop for AIs to get smarter.\nIf I think of something like materials\nin the physical world\nor you know, like addressing, you know,\nconflicts between humans\nor something like that.\nI mean, you know, it may be there's only,\nsome of these problems are not\nintractable, but much harder.\nAnd it may be that there's only so well\nyou can do at some of these things, right?", "mimetype": "text/plain", "start_char_idx": 9624, "end_char_idx": 13821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a91af3da-3a51-441f-a882-bd1e031f01fd": {"__data__": {"id_": "a91af3da-3a51-441f-a882-bd1e031f01fd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5492fa06-d59a-4118-a7d5-461627d53796", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "91225692bcabb644efb90564b5e9640de3a951da9255b5bc8bea13e4fd80106d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dac587b-7d06-4aa2-b503-ad4beb38d907", "node_type": "1", "metadata": {}, "hash": "85c43266133910b4a146b91a664db79b3dbe65c5120f6f045743043426597539", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It seems to me that humans are struggling\nto understand the complexity\nof biology, right?\nIf you go to Stanford or to Harvard\nor to Berkeley, you have whole\ndepartments of, you know,\nfolks trying to study, you know,\nlike the immune system\nor metabolic pathways,\nand each person understands\nonly a tiny bit,\npart of it, specializes,\nand they're struggling to\ncombine their knowledge\nwith that of other humans.\nAnd so I have an instinct that\nthere's a lot of room at the\ntop for AIs to get smarter.\nIf I think of something like materials\nin the physical world\nor you know, like addressing, you know,\nconflicts between humans\nor something like that.\nI mean, you know, it may be there's only,\nsome of these problems are not\nintractable, but much harder.\nAnd it may be that there's only so well\nyou can do at some of these things, right?\nJust like with speech recognition,\nthere's only so clear\nI can hear your speech.\nSo I think in some areas\nthere may be ceilings,\nyou know, that are very close\nto what humans have done.\nin other areas, those\nceilings may be very far away.\nAnd I think we'll only find out\nwhen we build these systems.\nIt's very hard to know in advance.\nWe can speculate, but we can't be sure.\n- And in some domains, the ceiling\nmight have to do with human bureaucracies\nand things like this, as you write about.\n- Yes.\n- So humans fundamentally\nhave to be part of the loop.\nThat's the cause of the ceiling,\nnot maybe the limits of the intelligence.\n- Yeah, I think in many\ncases, you know, in theory,\ntechnology could change very fast,\nfor example, all the\nthings that we might invent\nwith respect to biology.\nBut remember there's a, you know,\nthere's a clinical trial system\nthat we have to go through\nto actually administer\nthese things to humans.\nI think that's a mixture of things\nthat are unnecessary and bureaucratic\nand things that kind of protect\nthe integrity of society.\nAnd the whole challenge is that\nit's hard to tell what's going on.\nIt's hard to tell which is which, right?\nMy view is definitely, I think\nin terms of drug development,\nmy view is that we're too slow\nand we're too conservative.\nBut certainly if you get\nthese things wrong, you know,\nit's possible to risk people's\nlives by being too reckless.\nAnd so at least some of\nthese human institutions\nare in fact protecting people.\nSo it's all about finding the balance.\nI strongly suspect that balance\nis kind of more on the side\nof pushing to make things happen faster,\nbut there is a balance.\n- If we do hit a limit,\nif we do hit a slow down\nin the scaling laws,\nwhat do you think would be the reason?\nIs it compute limited, data limited?\nIs it something else, idea limited?\n- So, a few things.\nNow we're talking about hitting the limit\nbefore we get to the level of humans\nand the skill of humans.\nSo, I think one that's, you know,\none that's popular today\nand I think, you know,\ncould be a limit that we run into.\nLike most of the limits,\nI would bet against it,\nbut it's definitely possible\nis we simply run out of data.\nThere's only so much data on the internet\nand there's issues with the\nquality of the data, right?\nYou can get hundreds of trillions\nof words on the internet,\nbut a lot of it is repetitive\nor it's search engine, you know,\nsearch engine optimization\ndrivel, or maybe in the future\nit'll even be text\ngenerated by AIs itself.\nAnd so I think there are limits\nto what can be produced in this way.\nThat said, we and I would\nguess other companies\nare working on ways to make data synthetic\nwhere you can, you know,\nyou can use the model\nto generate more data\nof the type that you have already\nor even generate data from scratch.\nIf you think about what was done\nwith DeepMind's AlphaGo Zero,\nthey managed to get a\nbot all the way from,\nyou know, no ability to play Go whatsoever\nto above human level just\nby playing against itself.\nThere was no example\ndata from humans required\nin the AlphaGo Zero version of it.\nThe other direction, of course,\nis these reasoning models\nthat do chain of thought and stop to think\nand reflect on their own thinking.", "mimetype": "text/plain", "start_char_idx": 12989, "end_char_idx": 17044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1dac587b-7d06-4aa2-b503-ad4beb38d907": {"__data__": {"id_": "1dac587b-7d06-4aa2-b503-ad4beb38d907", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a91af3da-3a51-441f-a882-bd1e031f01fd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9ef3bc5d6662b2a98e8eef8b8b3fa8b8e1eb7d7275ba7aa1382cb5563546574e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e018b138-e891-4624-901f-4b7839b40ed3", "node_type": "1", "metadata": {}, "hash": "3258dfdd1fa601a4e90f93563008ab1a9157e80f2a5ab61af1966a0a4ef834d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so I think there are limits\nto what can be produced in this way.\nThat said, we and I would\nguess other companies\nare working on ways to make data synthetic\nwhere you can, you know,\nyou can use the model\nto generate more data\nof the type that you have already\nor even generate data from scratch.\nIf you think about what was done\nwith DeepMind's AlphaGo Zero,\nthey managed to get a\nbot all the way from,\nyou know, no ability to play Go whatsoever\nto above human level just\nby playing against itself.\nThere was no example\ndata from humans required\nin the AlphaGo Zero version of it.\nThe other direction, of course,\nis these reasoning models\nthat do chain of thought and stop to think\nand reflect on their own thinking.\nIn a way, that's another\nkind of synthetic data\ncoupled with reinforcement learning.\nSo my guess is with one of those methods,\nwe'll get around the data limitation\nor there may be other sources of data\nthat are available.\nWe could just observe that\neven if there's no problem with data,\nas we start to scale models up,\nthey just stop getting better.\nIt seemed to be a reliable observation\nthat they've gotten better,\nthat could just stop\nat some point for a reason\nwe don't understand.\nThe answer could be that we need to,\nyou know, we need to invent\nsome new architecture.\nThere have been problems in the past with,\nsay, numerical stability of models\nwhere it looked like\nthings were leveling off,\nbut actually, you know,\nwhen we found the right unblocker,\nthey didn't end up doing so.\nSo perhaps there's some\nnew optimization method\nor some new technique we\nneed to unblock things.\nI've seen no evidence of that so far.\nBut if things were to slow down,\nthat perhaps could be one reason.\n- What about the limits of compute?\nMeaning the expensive nature\nof building bigger and\nbigger data centers.\n- So right now, I think, you know,\nmost of the frontier model companies\nI would guess are operating in, you know,\nroughly, you know, $1 billion scale,\nplus or minus a factor of three, right?\nThose are the models that exist now\nor are being trained now.\nI think next year, we're\ngonna go to a few billion,\nand then 2026, we may go to,\nyou know, above 10 billion,\nand probably by 2027,\ntheir ambitions to build\n100 billion dollar clusters,\nand I think all of that\nactually will happen.\nThere's a lot of determination to build\nthe compute to do it within this country,\nand I would guess that\nit actually does happen.\nNow, if we get to 100 billion,\nthat's still not enough compute,\nthat's still not enough scale\nthen either we need even more scale\nor we need to develop some way\nof doing it more efficiently\nof shifting the curve.\nI think between all of these,\none of the reasons I'm bullish\nabout powerful AI happening so fast\nis just that if you extrapolate\nthe next few points on the curve,\nwe're very quickly getting towards\nhuman level ability, right?\nSome of the new models that we developed,\nsome reasoning models that\nhave come from other companies,\nthey're starting to get\nto what I would call\nthe PhD or professional level, right?\nIf you look at their coding ability,\nthe latest model we released, Sonnet 3.5,\nthe new or updated version,\nit gets something like 50% on SWE-bench,\nand SWE-bench is an example of a bunch\nof professional, real world\nsoftware engineering tasks.\nAt the beginning of the year,\nI think the state of the art was 3 or 4%.\nSo in 10 months we've gone\nfrom 3% to 50% on this task,\nand I think in another year,\nwe'll probably be at 90%.\nI mean, I don't know, but\nmight even be less than that.\nWe've seen similar things\nin graduate level math,\nphysics, and biology from\nmodels like OpenAI's o1.\nSo if we just continue to\nextrapolate this, right,\nin terms of skill that we have,\nI think if we extrapolate\nthe straight curve,\nwithin a few years, we will\nget to these models being,\nyou know, above the\nhighest professional level\nin terms of humans.\nNow, will that curve continue?", "mimetype": "text/plain", "start_char_idx": 16325, "end_char_idx": 20244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e018b138-e891-4624-901f-4b7839b40ed3": {"__data__": {"id_": "e018b138-e891-4624-901f-4b7839b40ed3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dac587b-7d06-4aa2-b503-ad4beb38d907", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f92bc28c2f24ef179af5de0639ac1b52dd196be3fc893ace3e0c37672570f7a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca", "node_type": "1", "metadata": {}, "hash": "d40b05620ca2f61d474b9ffb2d1ccfce231f571bfbb5ea47b5713b9aa32cb715", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the beginning of the year,\nI think the state of the art was 3 or 4%.\nSo in 10 months we've gone\nfrom 3% to 50% on this task,\nand I think in another year,\nwe'll probably be at 90%.\nI mean, I don't know, but\nmight even be less than that.\nWe've seen similar things\nin graduate level math,\nphysics, and biology from\nmodels like OpenAI's o1.\nSo if we just continue to\nextrapolate this, right,\nin terms of skill that we have,\nI think if we extrapolate\nthe straight curve,\nwithin a few years, we will\nget to these models being,\nyou know, above the\nhighest professional level\nin terms of humans.\nNow, will that curve continue?\nYou've pointed to, and I've\npointed to a lot of reasons why,\nyou know, possible reasons\nwhy that might not happen.\nBut if the extrapolation curve continues,\nthat is the trajectory we're on.\n- So Anthropic has several competitors.\nIt'd be interesting to get\nyour sort of view of it all.\nOpenAI, Google, xAI, Meta.\nWhat does it take to win\nin the broad sense of win in this space?\n- Yeah, so I want to separate\nout a couple things, right?\nSo, you know, Anthropic's mission\nis to kind of try to make\nthis all go well, right?\nAnd you know, we have a theory of change\ncalled race to the top, right?\nRace to the top is about trying to push\nthe other players to do the right thing\nby setting an example.\nIt's not about being the good guy,\nit's about setting things up so that\nall of us can be the good guy.\nI'll give a few examples of this.\nEarly in the history of Anthropic,\none of our co-founders, Chris Olah,\nwho I believe you're interviewing soon,\nyou know, he's the co-founder of the field\nof mechanistic interpretability,\nwhich is an attempt\nto understand what's\ngoing on inside AI models.\nSo we had him and one of our early teams\nfocus on this area of interpretability,\nwhich we think is good\nfor making models safe and transparent.\nFor three or four years,\nthat had no commercial\napplication whatsoever.\nIt still doesn't today.\nWe're doing some early betas with it,\nand probably it will eventually,\nbut you know, this is a\nvery, very long research bed\nand one in which we've built in public\nand shared our results publicly.\nAnd we did this because, you know,\nwe think it's a way to make models safer.\nAn interesting thing is\nthat as we've done this,\nother companies have\nstarted doing it as well,\nin some cases because\nthey've been inspired by it,\nin some cases because they're\nworried that, you know,\nif other companies are doing this\nto look more responsible,\nthey wanna look more responsible too.\nNo one wants to look like\nthe irresponsible actor,\nand so they adopt this as well.\nWhen folks come to Anthropic,\ninterpretability often a draw,\nand I tell them, the other\nplaces you didn't go,\ntell them why you came here,\nand then you see soon\nthat there's interpretability\nteams elsewhere as well.\nAnd in a way, that takes away\nour competitive advantage\nbecause it's like, oh, now\nothers are doing it as well,\nbut it's good for the broader system,\nand so we have to invent\nsome new thing that\nwe're doing that others\naren't doing as well.\nAnd the hope is to basically\nbid up the importance\nof doing the right thing.\nAnd it's not about us\nin particular, right?\nIt's not about having\none particular good guy.\nOther companies can do this as well.\nIf they join the race\nto do this, you know,\nthat's the best news ever, right?\nIt's just, it's about kind\nof shaping the incentives\nto point upward instead of shaping\nthe incentives to point downward.\n- And we should say this\nexample of the field\nof mechanistic interpretability\nis just a rigorous, non-hand\nwavy way of doing AI safety,\nor it's tending that way.\n- Trying to, I mean, I\nthink we're still early\nin terms of our ability to see things,\nbut I've been surprised at how much\nwe've been able to look\ninside these systems\nand understand what we see, right?", "mimetype": "text/plain", "start_char_idx": 19623, "end_char_idx": 23454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca": {"__data__": {"id_": "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e018b138-e891-4624-901f-4b7839b40ed3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b51e69df7e75c11b50ed2a89168eed5f939a0d393b7b81662234da77e001f65a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3394e636-ae71-4ee0-be0c-39ac6beae551", "node_type": "1", "metadata": {}, "hash": "680a123990538408da88fee547492307ef7b05be3c0602e3310df3b2fa9dc408", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the hope is to basically\nbid up the importance\nof doing the right thing.\nAnd it's not about us\nin particular, right?\nIt's not about having\none particular good guy.\nOther companies can do this as well.\nIf they join the race\nto do this, you know,\nthat's the best news ever, right?\nIt's just, it's about kind\nof shaping the incentives\nto point upward instead of shaping\nthe incentives to point downward.\n- And we should say this\nexample of the field\nof mechanistic interpretability\nis just a rigorous, non-hand\nwavy way of doing AI safety,\nor it's tending that way.\n- Trying to, I mean, I\nthink we're still early\nin terms of our ability to see things,\nbut I've been surprised at how much\nwe've been able to look\ninside these systems\nand understand what we see, right?\nUnlike with the scaling laws\nwhere it feels like\nthere's some, you know,\nlaw that's driving these\nmodels to perform better,\non the inside, the\nmodels aren't, you know,\nthere's no reason why\nthey should be designed\nfor us to understand them, right?\nThey're designed to operate,\nthey're designed to work,\njust like the human brain\nor human biochemistry.\nThey're not designed for a\nhuman to open up the hatch,\nlook inside and understand them.\nBut we have found, and you know,\nyou can talk in much more\ndetail about this to Chris,\nthat when we open them up,\nwhen we do look inside them,\nwe find things that are\nsurprisingly interesting.\n- And as a side effect, you also get\nto see the beauty of these models.\nYou get to explore sort\nof the beautiful nature\nof large neural networks\nthrough the mech interp\nkind of methodology.\n- I'm amazed at how clean it's been.\nI'm amazed at things like induction heads.\nI'm amazed at things like, you know,\nthat we can, you know,\nuse sparse auto-encoders\nto find these directions\nwithin the networks,\nand that the directions correspond\nto these very clear concepts.\nWe demonstrated this a bit\nwith the Golden Gate Bridge Claude.\nSo this was an experiment\nwhere we found a direction\ninside one of the neural network's layers\nthat corresponded to\nthe Golden Gate Bridge\nand we just turned that way up.\nAnd so we released this model as a demo,\nit was kind of half a\njoke, for a couple days,\nbut it was illustrative of\nthe method we developed.\nAnd you could take the Golden Gate,\nyou could take the model, you\ncould ask it about anything,\nyou know, it would be like, you could say,\n\"How was your day\" and anything you asked,\nbecause this feature was activated,\nwould connect to the Golden Gate Bridge.\nSo it would say, you know,\n\"I'm feeling relaxed and expansive,\nmuch like the arches of\nthe Golden Gate Bridge\"\nor, you know.\n- It would masterfully change topic\nto the Golden Gate Bridge\nand it integrate it.\nThere was also a sadness to it,\nto the focus it had on\nthe Golden Gate Bridge.\nI think people quickly fell\nin love with it, I think,\nso people already miss it\n'cause it was taken down\nI think after a day.\n- Somehow these interventions on the model\nwhere you kind of adjust its behavior\nsomehow emotionally\nmade it seem more human\nthan any other version of the model.\n- It has a strong\npersonality, strong identity.\n- It has a strong personality.\nIt has these kind of\nlike obsessive interests.\nYou know, we can all think of someone\nwho's like obsessed with something.\nSo it does make it feel\nsomehow a bit more human.\n- Let's talk about the present.\nLet's talk about Claude.\nSo this year, a lot has happened.\nIn March, Claude 3, Opus,\nSonnet, Haiku were released,\nthen Claude 3.5 Sonnet in July,\nwith an updated version just now released,\nand then also Claude\n3.5 Haiku was released.\nOkay, can you explain the difference\nbetween Opus, Sonnet and Haiku,\nand how we should think\nabout the different versions?\n- Yeah, so let's go back to March\nwhen we first released these three models.\nSo, you know, our thinking was, you know,\ndifferent companies produce\nkind of large and small models,\nbetter and worse models.", "mimetype": "text/plain", "start_char_idx": 22686, "end_char_idx": 26603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3394e636-ae71-4ee0-be0c-39ac6beae551": {"__data__": {"id_": "3394e636-ae71-4ee0-be0c-39ac6beae551", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4a11f0a-ea1b-4c1a-a44f-32adf56146ca", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a72b973ebc539c817f798dc46ad1168f0bf2aa24b4942e85d0bdf2eeb73ccd4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56cc79fe-e4cc-40a8-9600-57a8aac86724", "node_type": "1", "metadata": {}, "hash": "bcee9602bcf5b2d1dfe4175b3be8db02315885dd0fa0836db6d3b05dfd3faaa2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- It has a strong personality.\nIt has these kind of\nlike obsessive interests.\nYou know, we can all think of someone\nwho's like obsessed with something.\nSo it does make it feel\nsomehow a bit more human.\n- Let's talk about the present.\nLet's talk about Claude.\nSo this year, a lot has happened.\nIn March, Claude 3, Opus,\nSonnet, Haiku were released,\nthen Claude 3.5 Sonnet in July,\nwith an updated version just now released,\nand then also Claude\n3.5 Haiku was released.\nOkay, can you explain the difference\nbetween Opus, Sonnet and Haiku,\nand how we should think\nabout the different versions?\n- Yeah, so let's go back to March\nwhen we first released these three models.\nSo, you know, our thinking was, you know,\ndifferent companies produce\nkind of large and small models,\nbetter and worse models.\nWe felt that there was demand\nboth for a really\npowerful model, you know,\nand you that might be a little bit slower\nthat you'd have to pay more for,\nand also for fast, cheap models\nthat are as smart as they can be\nfor how fast and cheap, right?\nWhenever you wanna do some\nkind of like, you know,\ndifficult analysis, like if, you know,\nI wanna write code, for instance,\nor you know, I wanna brainstorm ideas,\nor I wanna do creative writing,\nI want the really powerful model.\nBut then there's a lot\nof practical applications\nin a business sense where it's like\nI'm interacting with a website.\nYou know, like, I'm like doing my taxes,\nor I'm, you know, talking to a, you know,\nto like a legal advisor and\nI want to analyze a contract\nor, you know, we have plenty of companies\nthat are just like, you know,\nI wanna do auto complete\non my IDE or something.\nAnd for all of those\nthings, you want to act fast\nand you want to use\nthe model very broadly.\nSo we wanted to serve that\nwhole spectrum of needs.\nSo we ended up with this, you know,\nthis kind of poetry theme.\nAnd so what's a really short poem?\nIt's a haiku.\nAnd so Haiku is the small,\nfast, cheap model that is,\nyou know, was at the time\nwas released surprisingly,\nsurprisingly intelligent for\nhow fast and cheap it was.\nSonnet is a medium sized poem,\nright, a couple paragraphs,\nand so Sonnet was the middle model.\nIt is smarter but also\na little bit slower,\na little bit more expensive.\nAnd Opus, like a magnum\nopus is a large work,\nOpus was the largest,\nsmartest model at the time.\nSo that was the original\nkind of thinking behind it.\nAnd our thinking then was,\nwell, each new generation\nof models should shift\nthat trade-off curve.\nSo when we released Sonnet 3.5,\nit has the same, roughly\nthe same, you know,\ncost and speed as the Sonnet 3 model.\nBut it increased its intelligence\nto the point where it was smarter\nthan the original Opus 3 model,\nespecially for code but\nalso just in general.\nAnd so now, you know, we've\nshown results for Haiku 3.5,\nand I believe Haiku 3.5,\nthe smallest new model\nis about as good as Opus\n3, the largest old model.\nSo basically the aim here\nis to shift the curve,\nand then at some point,\nthere's gonna be an Opus 3.5.\nNow, every new generation\nof models has its own thing.\nThey use new data, their\npersonality changes\nin ways that we kind of, you know,\ntry to steer but are\nnot fully able to steer.\nAnd so there's never quite\nthat exact equivalence\nwhere the only thing you're\nchanging is intelligence.\nWe always try and improve other things,\nand some things change without\nus knowing or measuring.\nSo it's very much an inexact science.\nIn many ways, the manner and\npersonality of these models\nis more an art than it is a science.\n- So what is sort of the reason\nfor the span of time between, say,\nClaude Opus 3.0 and 3.5?\nWhat takes that time?\nIf you can speak to.\n- Yeah, so there's different processes.\nThere's pre-training, which is, you know,\njust kind of the normal\nlanguage model training,\nand that takes a very long time.", "mimetype": "text/plain", "start_char_idx": 25809, "end_char_idx": 29623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56cc79fe-e4cc-40a8-9600-57a8aac86724": {"__data__": {"id_": "56cc79fe-e4cc-40a8-9600-57a8aac86724", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3394e636-ae71-4ee0-be0c-39ac6beae551", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2bd6a4433cf7720dffae6ebee5225c0e67bb55bdc7768bf93e13f233d6a880ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19a64d02-ccbd-488c-ac40-7d4d15925208", "node_type": "1", "metadata": {}, "hash": "b8af80f1f141993ea2d630c5e7019aee7dd127ab033f6b2663a8b89da2e903ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They use new data, their\npersonality changes\nin ways that we kind of, you know,\ntry to steer but are\nnot fully able to steer.\nAnd so there's never quite\nthat exact equivalence\nwhere the only thing you're\nchanging is intelligence.\nWe always try and improve other things,\nand some things change without\nus knowing or measuring.\nSo it's very much an inexact science.\nIn many ways, the manner and\npersonality of these models\nis more an art than it is a science.\n- So what is sort of the reason\nfor the span of time between, say,\nClaude Opus 3.0 and 3.5?\nWhat takes that time?\nIf you can speak to.\n- Yeah, so there's different processes.\nThere's pre-training, which is, you know,\njust kind of the normal\nlanguage model training,\nand that takes a very long time.\nThat uses, you know, these days,\nyou know, tens of thousands,\nsometimes many tens of\nthousands of GPUs or TPUs\nor Trainium, or you know,\nwe use different platforms,\nbut, you know, accelerator chips,\noften training for months.\nThere's then a kind of post-training phase\nwhere we do reinforcement\nlearning from human feedback,\nas well as other kinds of\nreinforcement learning.\nThat phase is getting\nlarger and larger now,\nand, you know, often, that's\nless of an exact science.\nIt often takes effort to get it right.\nModels are then tested with\nsome of our early partners\nto see how good they are,\nand they're then tested both internally\nand externally for their safety,\nparticularly for catastrophic\nand autonomy risks.\nSo we do internal testing\naccording to our\nresponsible scaling policy,\nwhich I, you know, could talk\nmore about that in detail.\nAnd then we have an agreement\nwith the US and the UK\nAI Safety Institute,\nas well as other third party testers\nin specific domains to test the models\nfor what are called CBRN risks,\nchemical, biological,\nradiological and nuclear,\nwhich are, you know, we\ndon't think that models\npose these risks seriously yet,\nbut every new model, we wanna evaluate\nto see if we're starting to get close\nto some of these more\ndangerous capabilities.\nSo those are the phases.\nAnd then, you know, then\nit just takes some time\nto get the model working\nin terms of inference\nand launching it in the API.\nSo there's just a lot of steps\nto actually making a model work.\nAnd of course, you know,\nwe're always trying\nto make the processes as\nstreamlined as possible, right?\nWe want our safety testing to be rigorous,\nbut we want it to be rigorous\nand to be, you know, to be automatic,\nto happen as fast as it can\nwithout compromising on rigor.\nSame with our pre-training process\nand our post-training process.\nSo, you know, it's just\nlike building anything else.\nIt's just like building airplanes.\nYou want to make them, you know,\nyou want to make them safe,\nbut you want to make\nthe process streamlined.\nAnd I think the creative\ntension between those is,\nyou know, is an important thing\nin making the models work.\n- Yeah, rumor on the street,\nI forget who was saying\nthat Anthropic has really good tooling,\nso probably a lot of the challenge here\non the software engineering side\nis to build the tooling\nto have like a efficient,\nlow friction interaction\nwith the infrastructure.\n- You would be surprised how\nmuch of the challenges of,\nyou know, building these\nmodels comes down to, you know,\nsoftware engineering, performance\nengineering, you know.\nFrom the outside you might think,\noh, man, we had this\neureka breakthrough, right?\nYou know, this movie with the science,\nwe discovered it, we figured it out.\nBut I think all things,\neven, you know, incredible discoveries,\nlike, they almost always\ncome down to the details,\nand often super, super boring details.\nI can't speak to whether we have\nbetter tooling than other companies.\nI mean, you know, haven't\nbeen at those other companies,\nat least not recently,\nbut it's certainly something\nwe give a lot of attention to.\n- I don't know if you\ncan say, but from three,\nfrom Claude 3 to Claude 3.5,\nis there any extra pre-training going on\nor is it mostly focused\non the post-training?\nThere's been leaps in performance.", "mimetype": "text/plain", "start_char_idx": 28867, "end_char_idx": 32916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19a64d02-ccbd-488c-ac40-7d4d15925208": {"__data__": {"id_": "19a64d02-ccbd-488c-ac40-7d4d15925208", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56cc79fe-e4cc-40a8-9600-57a8aac86724", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d735bd28589f9fa92b3bd997083f3c633cd144187dc5583fbf7f66f47f7c6bcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e81a266-9cd2-4387-a302-42ec2229acf7", "node_type": "1", "metadata": {}, "hash": "f90b68d4222a30a5db52ebda26e66299109914d5f68f102447691175c82c43fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the outside you might think,\noh, man, we had this\neureka breakthrough, right?\nYou know, this movie with the science,\nwe discovered it, we figured it out.\nBut I think all things,\neven, you know, incredible discoveries,\nlike, they almost always\ncome down to the details,\nand often super, super boring details.\nI can't speak to whether we have\nbetter tooling than other companies.\nI mean, you know, haven't\nbeen at those other companies,\nat least not recently,\nbut it's certainly something\nwe give a lot of attention to.\n- I don't know if you\ncan say, but from three,\nfrom Claude 3 to Claude 3.5,\nis there any extra pre-training going on\nor is it mostly focused\non the post-training?\nThere's been leaps in performance.\n- Yeah, I think at any given stage,\nwe're focused on improving\neverything at once.\nJust naturally, like\nthere are different teams,\neach team makes progress\nin a particular area,\nin making a particular, you know,\ntheir particular segment\nof the relay race better.\nAnd it's just natural that\nwhen we make a new model,\nwe put all of these things in at once.\n- So, the data you have,\nlike the preference data\nyou get from RLHF, is that applicable,\nis there a ways to apply it\nto newer models as it get trained up?\n- Yeah, preference data from old models\nsometimes gets used for new models,\nalthough, of course, it\nperforms somewhat better\nwhen it's, you know, trained on,\nit's trained on the new models.\nNote that we have this, you know,\nConstitutional AI method\nsuch that we don't only\nuse preference data,\nwe kind of, there's also\na post-training process\nwhere we train the model against itself\nand there's, you know, new types\nof post-training the model against itself\nthat are used every day.\nSo it's not just RLHF,\nit's a bunch of other methods as well.\nPost-training, I think, you know,\nis becoming more and more sophisticated.\n- Well, what explains the\nbig leap in performance\nfor the new Sonnet 3.5?\nI mean, at least in the programming side.\nAnd maybe this is a good place\nto talk about benchmarks.\nWhat does it mean to get better?\nJust the number went up,\nbut, you know, I program,\nbut I also love programming\nand Claude 3.5 through Cursor\nis what I use to assist me in programming.\nAnd there was, at least\nexperientially, anecdotally,\nit's gotten smarter at programming.\nSo like, what does it\ntake to get it smarter?\n- We observed that as well, by the way.\nThere were a couple very strong engineers\nhere at Anthropic who\nall previous code models,\nboth produced by us and produced\nby all the other companies,\nhadn't really been useful to them.\nYou know, they said, you know,\nmaybe this is useful to\nbeginner, it's not useful to me.\nBut Sonnet 3.5, the original one\nfor the first time they said,\n\"Oh my God, this helped me\nwith something that, you know,\nthat it would've taken me hours to do.\nThis is the first model that's\nactually saved me time.\"\nSo again, the waterline is rising.\nAnd then I think, you know,\nthe new Sonnet has been even better.\nIn terms of what it takes,\nI mean, I'll just say it's\nbeen across the board.\nIt's in the pre-training,\nit's in the post-training,\nit's in various evaluations that we do.\nWe've observed this as well.\nAnd if we go into the\ndetails of the benchmark,\nso Sowe bench is\nbasically since, you know,\nsince you're a programmer, you know,\nyou'll be familiar with like pull requests\nand, you know, just pull\nrequests are like the, you know,\nlike a sort of atomic unit of work.\nYou know, you could say, you know,\nI'm implementing one thing.\nAnd Sowe bench actually gives you\nkind of a real world situation\nwhere the code base is in a current state\nand I'm trying to implement\nsomething that's, you know,\nthat's described in language.\nWe have internal benchmarks\nwhere we measure the same thing\nand you say, just give the\nmodel free reign to like,\nyou know, do anything, run\nanything, edit anything.\nHow well is it able to\ncomplete these tasks?", "mimetype": "text/plain", "start_char_idx": 32196, "end_char_idx": 36100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e81a266-9cd2-4387-a302-42ec2229acf7": {"__data__": {"id_": "4e81a266-9cd2-4387-a302-42ec2229acf7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19a64d02-ccbd-488c-ac40-7d4d15925208", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c24581e03d3412ee98d6b111de7d8069d58aa8738b7c694195fc4f9cf469424a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c83d6f9-7574-40ec-a53c-053389189571", "node_type": "1", "metadata": {}, "hash": "39df4734d316d711876e84149621473204c1d8bc9540436e23422fac466e8ecd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We've observed this as well.\nAnd if we go into the\ndetails of the benchmark,\nso Sowe bench is\nbasically since, you know,\nsince you're a programmer, you know,\nyou'll be familiar with like pull requests\nand, you know, just pull\nrequests are like the, you know,\nlike a sort of atomic unit of work.\nYou know, you could say, you know,\nI'm implementing one thing.\nAnd Sowe bench actually gives you\nkind of a real world situation\nwhere the code base is in a current state\nand I'm trying to implement\nsomething that's, you know,\nthat's described in language.\nWe have internal benchmarks\nwhere we measure the same thing\nand you say, just give the\nmodel free reign to like,\nyou know, do anything, run\nanything, edit anything.\nHow well is it able to\ncomplete these tasks?\nAnd it's that benchmark that's gone\nfrom it can do it 3% of the time\nto it can do it about 50% of the time.\nSo I actually do believe that if we get,\nyou can gain benchmarks,\nbut I think if we get to\n100% on that benchmark\nin a way that isn't\nkind of like overtrained\nor game for that particular benchmark,\nprobably represents a\nreal and serious increase\nin kind of programming ability.\nAnd I would suspect that\nif we can get to, you know,\n90, 95% that, you know,\nit will represent ability\nto autonomously do a significant fraction\nof software engineering tasks.\n- Well, ridiculous timeline question.\nWhen is Claude Opus 3.5 coming out?\n- Not giving you an exact date,\nbut you know, there, you\nknow, as far as we know,\nthe plan is still to\nhave a Claude 3.5 Opus.\n- Are we gonna get it\nbefore \"GTA 6\" or no?\n- Like \"Duke Nukem Forever.\"\n- \"Duke Nukem-\"\n- What was that game?\nThere was some game that\nwas delayed 15 years.\n- That's right.\n- Was that\n\"Duke Nukem Forever?\"\n- Yeah.\nAnd I think \"GTA\" is now\njust releasing trailers.\n- You know, it's only been three months\nsince we released the first Sonnet.\n- Yeah, it's the\nincredible pace of release.\n- It just tells you about the pace,\nthe expectations for when\nthings are gonna come out.\n- So what about 4.0?\nSo how do you think about\nsort of as these models\nget bigger and bigger,\nabout versioning, and also\njust versioning in general,\nwhy Sonnet 3.5 updated with the date?\nWhy not Sonnet 3.6, which a\nlot of people are calling it?\n- Yeah, naming is actually\nan interesting challenge here, right?\nBecause I think a year ago,\nmost of the model was pre-training,\nand so you could start from the beginning\nand just say, okay,\nwe're gonna have models\nof different sizes, we're\ngonna train them all together\nand you know, we'll have\na family of naming schemes\nand then we'll put some\nnew magic into them\nand then, you know, we'll\nhave the next generation.\nThe trouble starts already\nwhen some of them take a lot longer\nthan others to train, right?\nThat already messes up\nyour time a little bit.\nBut as you make big\nimprovements in pre-training,\nthen you suddenly notice,\noh, I can make better pre-train model\nand that doesn't take very long to do,\nbut you know, clearly it has the same,\nyou know, size and shape\nof previous models.\nSo I think those two together\nas well as the timing issues,\nany kind of scheme you come up with,\nyou know, the reality tends\nto kind of frustrate that scheme, right?\nTend tends to kind of\nbreak out of the scheme.\nIt's not like software where you can say,\noh, this is like, you\nknow, 3.7, this is 3.8.\nNo, you have models with\ndifferent trade-offs.\nYou can change some things in your models,\nyou can train, you can\nchange other things.\nSome are faster and slower at inference,\nsome have to be more expensive,\nsome have to be less expensive.\nAnd so I think all the companies\nhave struggled with this.", "mimetype": "text/plain", "start_char_idx": 35340, "end_char_idx": 38977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1c83d6f9-7574-40ec-a53c-053389189571": {"__data__": {"id_": "1c83d6f9-7574-40ec-a53c-053389189571", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e81a266-9cd2-4387-a302-42ec2229acf7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ea4200f1a0ab93918fc81749a0c61fd2ed28b22f86d6483f4468f5cfb0938bd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42023691-822f-45cb-9f16-85775099ad3b", "node_type": "1", "metadata": {}, "hash": "ae7ff55b427704bdcb7faffa31da83e22b2abb1a8fe6c7c2a84c8c88940de6f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I think those two together\nas well as the timing issues,\nany kind of scheme you come up with,\nyou know, the reality tends\nto kind of frustrate that scheme, right?\nTend tends to kind of\nbreak out of the scheme.\nIt's not like software where you can say,\noh, this is like, you\nknow, 3.7, this is 3.8.\nNo, you have models with\ndifferent trade-offs.\nYou can change some things in your models,\nyou can train, you can\nchange other things.\nSome are faster and slower at inference,\nsome have to be more expensive,\nsome have to be less expensive.\nAnd so I think all the companies\nhave struggled with this.\nI think we did very, you know,\nI think we were in a good position\nin terms of naming when we\nhad Haiku, Sonnet and Opus.\n- [Lex] It was great, great start.\n- We're trying to maintain it,\nbut it's not perfect,\nso we'll try and get\nback to the simplicity,\nbut just the nature of the field,\nI feel like no one's figured out naming.\nIt's somehow a different paradigm\nfrom like normal software and so we just,\nnone of the companies\nhave been perfect at it.\nIt's something we struggle with\nsurprisingly much relative to,\nyou know, how relative\nto how trivial it is to,\nyou know, for the grand\nscience of training the models.\n- So, from the user\nside, the user experience\nof the updated Sonnet 3.5\nis just different than the previous\nJune 2024 Sonnet 3.5.\nIt would be nice to come up\nwith some kind of labeling\nthat embodies that\nbecause people talk about Sonnet 3.5,\nbut now there's a different one,\nand so how do you refer to the\nprevious one and the new one\nwhen there's a distinct improvement?\nIt just makes conversation\nabout it just challenging.\n- Yeah, yeah.\nI definitely think this question\nof there are lots of\nproperties of the models\nthat are not reflected in the benchmarks.\nI think that's definitely\nthe case and everyone agrees.\nAnd not all of them are capabilities.\nSome of them are, you know,\nmodels can be polite or brusque.\nThey can be, you know, very reactive\nor they can ask you questions.\nThey can have what feels\nlike a warm personality\nor a cold personality.\nThey can be boring or they\ncan be very distinctive,\nlike Golden Gate Claude was.\nAnd we have a whole, you know,\nwe have a whole team kind of focused on,\nI think we call it Claude character.\nAmanda leads that team\nand we'll talk to you about that.\nBut it's still a very inexact science,\nand often we find that\nmodels have properties\nthat we're not aware of.\nThe fact of the matter is that you can,\nyou know, talk to a model 10,000 times\nand there are some\nbehaviors you might not see,\njust like with a human, right?\nI can know someone for a few months and,\nyou know, not know that\nthey have a certain skill,\nor not know that there's\na certain side to them.\nAnd so I think we just have to get used\nto this idea and we're always\nlooking for better ways\nof testing our models to\ndemonstrate these capabilities,\nand also to decide which are\nthe personality properties\nwe want models to have and\nwhich we don't want to have.\nThat itself, the normative question\nis also super interesting.\n- I gotta ask you a question from Reddit.\n- From Reddit? Oh, boy. (laughs)\n- You know, there just this fascinating,\nto me at least, it's a\npsychological social phenomenon\nwhere people report that Claude\nhas gotten dumber for them over time.\nAnd so the question is,\ndoes the user complaint\nabout the dumbing down\nof Claude 3.5 Sonnet hold any water?\nSo are these anecdotal reports\na kind of social phenomena\nor did Claude, is there any cases\nwhere Claude would get dumber?\n- So this actually doesn't apply,\nthis isn't just about Claude.\nI believe I've seen these complaints\nfor every foundation model\nproduced by a major company.\nPeople said this about GPT-4,\nthey said it about GPT-4 Turbo.\nSo, a couple things.\nOne, the actual weights\nof the model, right,\nthe actual brain of the\nmodel, that does not change\nunless we introduce a new model.", "mimetype": "text/plain", "start_char_idx": 38379, "end_char_idx": 42276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42023691-822f-45cb-9f16-85775099ad3b": {"__data__": {"id_": "42023691-822f-45cb-9f16-85775099ad3b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c83d6f9-7574-40ec-a53c-053389189571", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "16ed67e92ff4abf37af318793f7eae09e9c77f49fb45d9b8eeca484eccb9ead3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b", "node_type": "1", "metadata": {}, "hash": "11b7a16bba8ec8920691a425be623594197ae924524280805da8c45bdc6c2b61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Oh, boy. (laughs)\n- You know, there just this fascinating,\nto me at least, it's a\npsychological social phenomenon\nwhere people report that Claude\nhas gotten dumber for them over time.\nAnd so the question is,\ndoes the user complaint\nabout the dumbing down\nof Claude 3.5 Sonnet hold any water?\nSo are these anecdotal reports\na kind of social phenomena\nor did Claude, is there any cases\nwhere Claude would get dumber?\n- So this actually doesn't apply,\nthis isn't just about Claude.\nI believe I've seen these complaints\nfor every foundation model\nproduced by a major company.\nPeople said this about GPT-4,\nthey said it about GPT-4 Turbo.\nSo, a couple things.\nOne, the actual weights\nof the model, right,\nthe actual brain of the\nmodel, that does not change\nunless we introduce a new model.\nThere are just a number of reasons\nwhy it would not make sense practically\nto be randomly substituting in\nnew versions of the model.\nIt's difficult from an\ninference perspective\nand it's actually hard to\ncontrol all the consequences\nof changing the weight of the model.\nLet's say you wanted to fine\ntune the model to be like,\nI don't know, to like\nto say \"certainly\" less,\nwhich, you know, an old\nversion of Sonnet used to do.\nYou actually end up\nchanging 100 things as well.\nSo we have a whole process for it,\nand we have a whole process\nfor modifying the model.\nWe do a bunch of testing on it,\nwe do a bunch of user\ntesting and early customers.\nSo we both have never changed\nthe weights of the model\nwithout telling anyone,\nand it wouldn't, certainly\nin the current setup,\nit would not make sense to do that.\nNow, there are a couple things\nthat we do occasionally do.\nOne is sometimes we run A/B tests,\nbut those are typically\nvery close to when a model\nis being released and for a\nvery small fraction of time.\nSo, you know, like, the day\nbefore the new Sonnet 3.5.\nI agree, we should have had a better name.\nIt's clunky to refer to it.\nThere were some comments from people that\nlike it's gotten a lot better,\nand that's because, you know, a fraction\nwere exposed to an A/B test\nfor those one or two days.\nThe other is that occasionally,\nthe system prompt will change.\nThe system prompt can have some effects,\nalthough it's unlikely\nto dumb down models.\nIt's unlikely to make them dumber.\nAnd we've seen that\nwhile these two things,\nwhich I'm listing to be very complete,\nhappened relatively,\nhappened quite infrequently,\nthe complaints about,\nfor us and for other model\ncompanies about the model change,\nthe model isn't good at this.\nThe model got more censored.\nThe model was dumbed down.\nThose complaints are constant.\nAnd so I don't wanna say like people\nare imagining it or\nanything, but like the models\nare for the most part not changing.\nIf I were to offer a theory,\nI think it actually relates\nto one of the things I said before,\nwhich is that models are very complex\nand have many aspects to them.\nAnd so often, you know,\nif I ask the model a question,\nyou know, if I'm like,\n\"Do task X\" versus \"Can you do task X?\"\nthe model might respond in different ways.\nAnd so there are all\nkinds of subtle things\nthat you can change about\nthe way you interact\nwith the model that can give\nyou very different results.\nTo be clear, this itself\nis like a failing by us\nand by the other model\nproviders that the models\nare just often sensitive to\nlike small changes in wording.\nIt's yet another way in which the science\nof how these models work\nis very poorly developed.\nAnd so, you know, if I\ngo to sleep one night\nand I was like talking to\nthe model in a certain way\nand I like slightly changed the phrasing\nof how I talk to the model, you know,\nI could get different results.\nSo that's one possible way.\nThe other thing is, man,\nit's just hard to quantify this stuff.\nIt's hard to quantify this stuff.\nI think people are very excited\nby new models when they come out\nand then as time goes on,\nthey become very aware of the limitations,\nso that may be another effect.", "mimetype": "text/plain", "start_char_idx": 41492, "end_char_idx": 45443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b": {"__data__": {"id_": "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42023691-822f-45cb-9f16-85775099ad3b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ccd1c5c6839d8a45e0dbd76e4572e7239c8b6c649b97724dc392bf940bf28213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f474dc3-3a36-4237-848f-f9f3e1335b76", "node_type": "1", "metadata": {}, "hash": "75fe68b1dfd4126bb37d73edc6f107d095d3b8718a63c492fd1e351029c4afb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To be clear, this itself\nis like a failing by us\nand by the other model\nproviders that the models\nare just often sensitive to\nlike small changes in wording.\nIt's yet another way in which the science\nof how these models work\nis very poorly developed.\nAnd so, you know, if I\ngo to sleep one night\nand I was like talking to\nthe model in a certain way\nand I like slightly changed the phrasing\nof how I talk to the model, you know,\nI could get different results.\nSo that's one possible way.\nThe other thing is, man,\nit's just hard to quantify this stuff.\nIt's hard to quantify this stuff.\nI think people are very excited\nby new models when they come out\nand then as time goes on,\nthey become very aware of the limitations,\nso that may be another effect.\nBut that's all a very\nlong-winded way of saying\nfor the most part, with some\nfairly narrow exceptions,\nthe models are not changing.\n- I think there is a psychological effect.\nYou just start getting used to it.\nThe baseline raises.\nLike when people first\ngotten wifi on airplanes,\nit's like amazing, magic.\n- It's like amazing, yeah.\n- And then-\n- And now I'm like,\nI can't get this thing to work.\nThis is such a piece of crap.\n- Exactly, so then it's easy\nto have the conspiracy theory\nof they're making wifi slower and slower.\nThis is probably something I'll talk\nto Amanda much more about.\nBut another Reddit question,\n\"When will Claude stop trying\nto be my puritanical grandmother\nimposing its moral worldview\non me as a paying customer?\nAnd also, what is the psychology\nbehind making Claude overly apologetic?\"\nSo this kind of reports\nabout the experience,\na different angle on the frustration,\nit has to do with the character.\n- Yeah, so a couple points on this first.\nOne is like things that people\nsay on Reddit and Twitter,\nor X or whatever it is,\nthere's actually a huge\ndistribution shift between like the stuff\nthat people complain loudly\nabout on social media\nand what actually kind of like, you know,\nstatistically users care about\nand that drives people to use the models.\nLike people are frustrated\nwith, you know, things like,\nyou know, the model not\nwriting out all the code\nor the model, you know, just not being\nas good at code as it could be,\neven though it's the best\nmodel in the world on code.\nI think the majority\nthings are about that.\nBut certainly a kind\nof vocal minority are,\nyou know, kind of raise\nthese concerns, right?\nAre frustrated by the\nmodel refusing things\nthat it shouldn't refuse,\nor like apologizing too much,\nor just having these kind of\nlike annoying verbal ticks.\nThe second caveat, and\nI just wanna say this\nlike super clearly because I think\nit's like some people don't know it,\nothers like kind of know it but forget it.\nLike it is very difficult to control\nacross the board how the models behave.\nYou cannot just reach in there\nand say, \"Oh, I want the\nmodel to like apologize less.\"\nLike you can do that, you\ncan include training data\nthat says like, \"Oh, the model\nshould like apologize less,\"\nbut then in some other\nsituation they end up\nbeing like super rude\nor like overconfident\nin a way that's like misleading people.\nSo there are all these trade-offs.\nFor example, another thing\nis there was a period during which models,\nours and I think others as\nwell were too verbose, right?\nThey would like repeat themselves,\nthey would say too much.\nYou can cut down on the\nverbosity by penalizing\nthe models for just talking for too long.\nWhat happens when you do that,\nif you do it in a crude way\nis when the models are coding,\nsometimes they'll say rest\nof the code goes here, right?\nBecause they've learned that\nthat's the way to economize\nand that they see it,\nand then so that leads the model\nto be so-called lazy in coding\nwhere they're just like,\nah, you can finish the rest of it.\nIt's not because we wanna,\nyou know, save on compute\nor because you know, the models are lazy,\nand you know, during winter break,\nor any of the other kind\nof conspiracy theories\nthat have come up.", "mimetype": "text/plain", "start_char_idx": 44695, "end_char_idx": 48676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5f474dc3-3a36-4237-848f-f9f3e1335b76": {"__data__": {"id_": "5f474dc3-3a36-4237-848f-f9f3e1335b76", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9f6529d-56e3-413b-9a00-a2bfb1ea6c4b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5128534e5a3b92e01e10a2b86457defd09bfb40020fa6abe994b6daf4b2460a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eba54af1-14d5-4f93-b21f-701e0f055df4", "node_type": "1", "metadata": {}, "hash": "1e89e19cd1dd025cf9e9e7d82f542bcbb945b3287d0a676d6efbdb456f71d6da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, another thing\nis there was a period during which models,\nours and I think others as\nwell were too verbose, right?\nThey would like repeat themselves,\nthey would say too much.\nYou can cut down on the\nverbosity by penalizing\nthe models for just talking for too long.\nWhat happens when you do that,\nif you do it in a crude way\nis when the models are coding,\nsometimes they'll say rest\nof the code goes here, right?\nBecause they've learned that\nthat's the way to economize\nand that they see it,\nand then so that leads the model\nto be so-called lazy in coding\nwhere they're just like,\nah, you can finish the rest of it.\nIt's not because we wanna,\nyou know, save on compute\nor because you know, the models are lazy,\nand you know, during winter break,\nor any of the other kind\nof conspiracy theories\nthat have come up.\nIt's actually, it's just very hard\nto control the behavior of the model,\nto steer the behavior of the model\nin all circumstances at once.\nYou can kind of, there's\nthis whack-a-mole aspect\nwhere you push on one\nthing and like, you know,\nthese other things start to move as well\nthat you may not even notice or measure.\nAnd so one of the reasons\nthat I care so much about,\nyou know, kind of grand alignment\nof these AI systems in the\nfuture is actually these systems\nare actually quite unpredictable.\nThey're actually quite\nhard to steer and control.\nAnd this version we're seeing today\nof you make one thing better,\nit makes another thing worse,\nI think that's like a present day analog\nof future control problems in AI systems\nthat we can start to study today, right?\nI think that that difficulty\nin steering the behavior\nand in making sure\nthat if we push an AI\nsystem in one direction,\nit doesn't push it in another direction\nin some other ways that we didn't want.\nI think that's kind of an early sign\nof things to come,\nand if we can do a good job\nof solving this problem,\nright, of like you ask the model to like,\nyou know, to like make\nand distribute smallpox\nand it says no, but it's\nwilling to like help you\nin your graduate level virology class.\nLike how do we get both\nof those things at once?\nIt's hard.\nIt's very easy to go to\none side or the other\nand it's a multidimensional problem.\nAnd so, you know, I think these questions\nof like shaping the model's personality,\nI think they're very hard.\nI think we haven't done perfectly on them.\nI think we've actually done the best\nof all the AI companies, but\nstill so far from perfect.\nAnd I think if we can get this right,\nif we can control, you know,\ncontrol the false positives\nand false negatives in this\nvery kind of controlled\npresent day environment,\nwe'll be much better at doing it\nfor the future when\nour worry is, you know,\nwill the models be super autonomous?\nWill they be able to, you know,\nmake very dangerous things?\nWill they be able to\nautonomously, you know,\nbuild whole companies?\nAnd are those companies aligned?\nSo, I think of this present\ntask as both vexing,\nbut also good practice for the future.\n- What's the current best way of gathering\nsort of user feedback?\nLike not anecdotal data,\nbut just large scale\ndata about pain points\nor the opposite of pain\npoints, positive things,\nso on, is it internal testing?\nIs it a specific group\ntesting, A/B testing?\nWhat works?\n- So, typically we'll have\ninternal model bashings\nwhere all of Anthropic,\nAnthropic is almost 1000 people,\nyou know, people just\ntry and break the model.\nThey try and interact\nwith it various ways.\nWe have a suite of evals for, you know,\noh, is the model refusing\nin ways that it couldn't?\nI think we even had a certainly eval\nbecause, you know, our model, again,\none point, model had this problem\nwhere like it had this annoying tick\nwhere it would like respond\nto a wide range of questions by saying\n\"Certainly I can help you with that.\nCertainly I would be happy to do that.\nCertainly this is correct.\"\nAnd so we had a, like, certainly eval,\nwhich is like, how often\ndoes the model say certainly?", "mimetype": "text/plain", "start_char_idx": 47853, "end_char_idx": 51828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eba54af1-14d5-4f93-b21f-701e0f055df4": {"__data__": {"id_": "eba54af1-14d5-4f93-b21f-701e0f055df4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f474dc3-3a36-4237-848f-f9f3e1335b76", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "04be8e094153af4fc9d852d0005669fbbf652c79576e73ac2656c84b91bb64b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7daa55ce-8eb3-49d9-a087-5f95ae19a941", "node_type": "1", "metadata": {}, "hash": "3b0aee14f55c657fbb1aadf6fc9683fd40920d98ca2b6d0c1b7fb97064444424", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Is it a specific group\ntesting, A/B testing?\nWhat works?\n- So, typically we'll have\ninternal model bashings\nwhere all of Anthropic,\nAnthropic is almost 1000 people,\nyou know, people just\ntry and break the model.\nThey try and interact\nwith it various ways.\nWe have a suite of evals for, you know,\noh, is the model refusing\nin ways that it couldn't?\nI think we even had a certainly eval\nbecause, you know, our model, again,\none point, model had this problem\nwhere like it had this annoying tick\nwhere it would like respond\nto a wide range of questions by saying\n\"Certainly I can help you with that.\nCertainly I would be happy to do that.\nCertainly this is correct.\"\nAnd so we had a, like, certainly eval,\nwhich is like, how often\ndoes the model say certainly?\nBut look, this is just a whack-a-mole.\nLike, what if it switches\nfrom certainly to definitely?\nLike, so, you know, every\ntime we add a new eval,\nand we're always evaluating\nfor all of the old things.\nSo we have hundreds of these evaluations,\nbut we find that there's no substitute\nfor human interacting with it.\nAnd so it's very much like\nthe ordinary product development process.\nWe have like hundreds of people\nwithin Anthropic bash the model,\nyou know, then we do external A/B tests.\nSometimes we'll run\ntests with contractors.\nWe pay contractors to\ninteract with the model.\nSo you put all of these things together\nand it's still not perfect.\nYou still see behaviors\nthat you don't quite wanna see, right?\nYou know, you still see the model\nlike refusing things that\nit just doesn't make sense to refuse.\nBut I think trying to solve\nthis challenge, right?\nTrying to stop the model\nfrom doing, you know,\ngenuinely bad things that, you know,\neveryone agrees it shouldn't do, right?\nYou know, everyone agrees that, you know,\nthe model shouldn't talk about, you know,\nI don't know, child abuse material, right?\nLike, everyone agrees the\nmodel shouldn't do that.\nBut at the same time\nthat it doesn't refuse in\nthese dumb and stupid ways.\nI think drawing that line\nas finely as possible,\napproaching perfectly is still a challenge\nand we're getting better at it every day.\nBut there's a lot to be solved.\nAnd again, I would point to that\nas an indicator of the challenge ahead\nin terms of steering much\nmore powerful models.\n- Do you think Claude\n4.0 is ever coming out?\n- I don't want to commit\nto any naming scheme,\n'cause if I say here\n\"We're gonna have Claude 4 next year,\"\nand then, you know, then\nwe decide that like,\nyou know, we should start over,\n'cause there's a new type of model.\nLike I don't want to commit to it.\nI would expect in a\nnormal course of business\nthat Claude 4 would come after Claude 3.5.\nBut you know, you never know\nin this wacky field, right?\n- But the sort of, this idea\nof scaling is continuing.\n- Scaling is continuing.\nThere will definitely\nbe more powerful models\ncoming from us than the\nmodels that exist today.\nThat is certain.\nOr if there aren't, we've\ndeeply failed as a company.\n- Okay, can you explain the\nResponsible Scaling Policy\nand the AI Safety Level\nStandards, ASL Levels?\n- As much as I am excited\nabout the benefits\nof these models, and, you\nknow, we'll talk about that\nif we talk about \"Machines\nof Loving Grace,\"\nI'm worried about the risks\nand I continue to be\nworried about the risks.\nNo one should think that, you know,\n\"Machines of Loving Grace\"\nwas me saying, you know,\nI'm no longer worried about\nthe risks of these models.\nI think they're two\nsides of the same coin.\nThe power of the models\nand their ability to solve\nall these problems in,\nyou know, biology, neuroscience,\neconomic development,\ngovernance and peace,\nlarge parts of the economy,\nthose come with risks as well, right?\nWith great power comes\ngreat responsibility, right?\nThe two are paired.\nThings that are powerful\ncan do good things\nand they can do bad things.\nI think of those risks\nas being in, you know,\nseveral different categories.", "mimetype": "text/plain", "start_char_idx": 51071, "end_char_idx": 54987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7daa55ce-8eb3-49d9-a087-5f95ae19a941": {"__data__": {"id_": "7daa55ce-8eb3-49d9-a087-5f95ae19a941", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eba54af1-14d5-4f93-b21f-701e0f055df4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a0a1c81c1657ee457e3c71f43c4411ae85d9cf0bde8160adcfe71da4f773ed65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30", "node_type": "1", "metadata": {}, "hash": "e2734aa55b1f003341279241c42e59ca931298218ca534c8d2d73a7fe3577150", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "No one should think that, you know,\n\"Machines of Loving Grace\"\nwas me saying, you know,\nI'm no longer worried about\nthe risks of these models.\nI think they're two\nsides of the same coin.\nThe power of the models\nand their ability to solve\nall these problems in,\nyou know, biology, neuroscience,\neconomic development,\ngovernance and peace,\nlarge parts of the economy,\nthose come with risks as well, right?\nWith great power comes\ngreat responsibility, right?\nThe two are paired.\nThings that are powerful\ncan do good things\nand they can do bad things.\nI think of those risks\nas being in, you know,\nseveral different categories.\nPerhaps the two biggest\nrisks that I think about,\nand that's not to say that\nthere aren't risks today\nthat are important,\nbut when I think of the\nreally the, you know,\nthe things that would happen\non the grandest scale,\none is what I call catastrophic misuse.\nThese are misuse of the\nmodels in domains like cyber,\nbio, radiological, nuclear, right?\nThings that could, you\nknow, that could harm\nor even kill thousands, even millions\nof people if they really, really go wrong.\nLike these are the, you know,\nnumber one priority to prevent.\nAnd here I would just\nmake a simple observation,\nwhich is that the models, you know,\nif I look today at people\nwho have done really\nbad things in the world,\nI think actually humanity\nhas been protected\nby the fact that the overlap\nbetween really smart, well-educated people\nand people who want to\ndo really horrific things\nhas generally been small.\nLike, you know, let's say I'm someone who,\nyou know, I have a PhD in this field,\nI have a well paying job.\nThere's so much to lose.\nWhy do I wanna, like, you know,\neven assuming I'm completely evil,\nwhich most people are not.\nYou know, why would such\na person risk their life,\nrisk their legacy, their\nreputation to do something like,\nyou know, truly, truly evil?\nIf we had a lot more people like that,\nthe world would be a much\nmore dangerous place.\nAnd so my worry is that by being\na much more intelligent agent,\nAI could break that correlation,\nand so I do have serious\nworries about that.\nI believe we can prevent those worries.\nBut, you know, I think as a counterpoint\nto \"Machines of Loving\nGrace,\" I want to say that\nthere's still serious risks.\nAnd the second range of risks\nwould be the autonomy\nrisks, which is the idea\nthat models might on their own,\nparticularly as we give them more agency\nthan they've had in the past,\nparticularly as we give them\nsupervision over wider\ntasks like, you know,\nwriting whole code bases or someday even,\nyou know, effectively\noperating entire companies,\nthey're on a long enough leash,\nare they doing what we\nreally want them to do?\nIt's very difficult to even understand\nin detail what they're\ndoing, let alone control it.\nAnd like I said, these early signs that\nit's hard to perfectly draw the boundary\nbetween things the model should do\nand things the model shouldn't do that,\nyou know, if you go to one side,\nyou get things that are\nannoying and useless,\nyou go to the other side,\nyou get other behaviors.\nIf you fix one thing, it\ncreates other problems.\nWe're getting better and\nbetter at solving this.\nI don't think this is\nan unsolvable problem.\nI think, you know, this is a science,\nlike the safety of airplanes\nor the safety of cars,\nor the safety of drugs.\nYou know, I don't think there's\nany big thing we're missing.\nI just think we need to get better\nat controlling these models.\nAnd so these are the two\nrisks I'm worried about.\nAnd our Responsible Scaling Plan,\nwhich I'll recognize is\na very long-winded answer\nto your question.\n- I love it. I love it.\n- Our Responsible Scaling Plan is designed\nto address these two types of risks.\nAnd so every time we develop a new model,\nwe basically test it for its ability\nto do both of these bad things.\nSo if I were to back up a little bit,\nI think we have an interesting dilemma\nwith AI systems where they're\nnot yet powerful enough\nto present these catastrophes.", "mimetype": "text/plain", "start_char_idx": 54364, "end_char_idx": 58339, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30": {"__data__": {"id_": "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7daa55ce-8eb3-49d9-a087-5f95ae19a941", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4858e33b455592c901dc474a8d6caf2685f3f2a95811ab60341bd13ef00d7abc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62a87b18-e78e-4e3f-bbff-666b8c5b90b1", "node_type": "1", "metadata": {}, "hash": "7a935b9dfb1cc15cc9497d767f99619c5b4fb5a55cfc8c831ef1b374238814c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I don't think this is\nan unsolvable problem.\nI think, you know, this is a science,\nlike the safety of airplanes\nor the safety of cars,\nor the safety of drugs.\nYou know, I don't think there's\nany big thing we're missing.\nI just think we need to get better\nat controlling these models.\nAnd so these are the two\nrisks I'm worried about.\nAnd our Responsible Scaling Plan,\nwhich I'll recognize is\na very long-winded answer\nto your question.\n- I love it. I love it.\n- Our Responsible Scaling Plan is designed\nto address these two types of risks.\nAnd so every time we develop a new model,\nwe basically test it for its ability\nto do both of these bad things.\nSo if I were to back up a little bit,\nI think we have an interesting dilemma\nwith AI systems where they're\nnot yet powerful enough\nto present these catastrophes.\nI don't know that they'll ever\nprevent these catastrophes,\nit's possible they won't,\nbut the case for worry,\nthe case for risk is strong enough\nthat we should act now.\nAnd they're getting better\nvery, very fast, right?\nYou know, I testified in the Senate that,\nyou know, we might have serious bio risks\nwithin two to three years.\nThat was about a year ago.\nThings have proceeded at pace.\nSo we have this thing where it's like,\nit's surprisingly hard\nto address these risks\nbecause they're not here today.\nThey don't exist.\nThey're like ghosts, but\nthey're coming at us so fast\nbecause the models are improving so fast.\nSo how do you deal with\nsomething that's not here today,\ndoesn't exist but is\ncoming at us very fast?\nSo the solution we came up with for that\nin collaboration with, you know,\npeople like the organization METR\nand Paul Christiano is, okay,\nwhat you need for that are you need tests\nto tell you when the\nrisk is getting close.\nYou need an early warning system.\nAnd so every time we have a new model,\nwe test it for its capability\nto do these CBRN tasks,\nas well as testing it for, you know,\nhow capable it is of doing\ntasks autonomously on its own.\nAnd in the latest version of our RSP,\nwhich we released in\nthe last month or two,\nthe way we test autonomy\nrisks is the model,\nthe AI model's ability to do\naspects of AI research itself,\nwhich when the AI models\ncan do AI research,\nthey become kind of truly autonomous.\nAnd you know, that threshold\nis important for a bunch of other ways.\nAnd so what do we then\ndo with these tasks?\nThe RSP basically develops\nwhat we've called an if then structure,\nwhich is if the models\npass a certain capability,\nthen we impose a certain set of safety\nand security requirements on them.\nSo today's models are\nwhat's called ASL two.\nModels that were, ASL\none is for systems that\nmanifestly don't pose any\nrisk of autonomy or misuse.\nSo for example, a chess playing bot,\nDeep Blue would be ASL one.\nIt's just manifestly the case\nthat you can't use Deep Blue\nfor anything other than chess.\nIt was just designed for chess.\nNo one's gonna use it to like, you know,\nto conduct a masterful cyber attack or to,\nyou know, run wild and\ntake over the world.\nASL two is today's AI systems\nwhere we've measured them\nand we think these systems are\nsimply not smart enough to,\nyou know, autonomously self-replicate\nor conduct a bunch of tasks,\nand also not smart enough to provide\nmeaningful information about CBRN risks\nand how to build CBRN\nweapons above and beyond\nwhat can be known from looking at Google.\nIn fact, sometimes they\ndo provide information,\nbut not above and beyond a search engine,\nbut not in a way that\ncan be stitched together,\nnot in a way that kind of end\nto end is dangerous enough.\nSo ASL three is gonna be the point\nat which the models are helpful enough\nto enhance the capabilities\nof non-state actors, right?\nState actors can already do a lot of,\nunfortunately, to a high\nlevel of proficiency,\na lot of these very dangerous\nand destructive things.\nThe difference is that\nnon-state actors are not capable of it.", "mimetype": "text/plain", "start_char_idx": 57527, "end_char_idx": 61416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62a87b18-e78e-4e3f-bbff-666b8c5b90b1": {"__data__": {"id_": "62a87b18-e78e-4e3f-bbff-666b8c5b90b1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa8d28ff-9816-41f2-81ea-c5d2ebf96a30", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "015d060cca7f034525cdcf3f1e826f38fa794140f318930f82ceea206f7858d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04e5d2e3-d9ff-4df3-83b4-1658b4418f65", "node_type": "1", "metadata": {}, "hash": "f269b0501fd6edc7b7f34be66019b00baebf31692bf493226011765b1aa90718", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, sometimes they\ndo provide information,\nbut not above and beyond a search engine,\nbut not in a way that\ncan be stitched together,\nnot in a way that kind of end\nto end is dangerous enough.\nSo ASL three is gonna be the point\nat which the models are helpful enough\nto enhance the capabilities\nof non-state actors, right?\nState actors can already do a lot of,\nunfortunately, to a high\nlevel of proficiency,\na lot of these very dangerous\nand destructive things.\nThe difference is that\nnon-state actors are not capable of it.\nAnd so when we get to ASL three,\nwe'll take special security precautions\ndesigned to be sufficient to prevent\ntheft of the model by non-state actors,\nand misuse of the model as it's deployed.\nWe'll have to have enhanced filters\ntargeted at these particular areas.\n- Cyber, bio, nuclear.\n- Cyber, bio, nuclear and model autonomy,\nwhich is less a misuse risk\nand more risk of the model\ndoing bad things itself.\nASL four, getting to the\npoint where these models\ncould enhance the capability\nof a already knowledgeable state actor\nand/or become, you know, the\nmain source of such a risk.\nLike if you wanted to\nengage in such a risk,\nthe main way you would\ndo it is through a model.\nAnd then I think ASL four\non the autonomy side,\nit's some amount of acceleration\nin AI research capabilities\nwithin an AI model.\nAnd then ASL five is where\nwe would get to the models\nthat are, you know, that are kind of,\nyou know, truly capable,\nthat could exceed\nhumanity in their ability\nto do any of these tasks.\nAnd so the point of if\nthen structure commitment\nis basically to say, look, I don't know,\nI've been working with these models\nfor many years and I've been worried\nabout risk for many years.\nIt's actually kind of\ndangerous to cry wolf.\nIt's actually kind of\ndangerous to say this,\nyou know, this model is risky.\nAnd, you know, people look at it\nand they say, this is\nmanifestly not dangerous.\nAgain, it's the delicacy\nof the risk isn't here today\nbut it's coming at us fast.\nHow do you deal with that?\nIt's really vexing to a risk\nplanner to deal with it.\nAnd so this if then\nstructure basically says,\nlook, we don't wanna\nantagonize a bunch of people,\nwe don't wanna harm our own, you know,\nour kind of own ability to have a place\nin the conversation by imposing these\nvery onerous burdens on models\nthat are not dangerous today.\nSo if then, the trigger commitment\nis basically a way to deal with this.\nSays you clamp down hard\nwhen you can show that\nthe model is dangerous.\nAnd of course what has to\ncome with that is, you know,\nenough of a buffer\nthreshold that, you know,\nyou're not at high risk of\nkind of missing the danger.\nIt's not a perfect framework.\nWe've had to change it every, you know,\nwe came out with a new\none just a few weeks ago,\nand probably going forward,\nwe might release new ones\nmultiple times a year\nbecause it's hard to get\nthese policies right,\nlike technically, organizationally,\nfrom a research perspective.\nBut that is the proposal,\nif then commitments\nand triggers in order to minimize burdens\nand false alarms now,\nbut really react appropriately\nwhen the dangers are here.\n- What do you think the timeline\nfor ASL three is where several\nof the triggers are fired?\nAnd what do you think the\ntimeline is for ASL four?\n- Yeah, so that is hotly\ndebated within the company.\nWe are working actively\nto prepare ASL three\nsecurity measures as well as\nASL three deployment measures.\nI'm not gonna go into detail,\nbut we've made a lot of progress on both,\nand, you know, we're prepared to be,\nI think, ready quite soon.\nI would not be surprised at all\nif we hit ASL three next year.\nThere was some concern that\nwe might even hit it this year.\nThat's still possible,\nthat could still happen.\nIt's like very hard to say,\nbut like I would be very, very surprised\nif it was like 2030.\nI think it's much sooner than that.\n- So there's protocols\nfor detecting it, if then,\nand then there's protocols\nfor how to respond to it.\n- Yes.", "mimetype": "text/plain", "start_char_idx": 60889, "end_char_idx": 64859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04e5d2e3-d9ff-4df3-83b4-1658b4418f65": {"__data__": {"id_": "04e5d2e3-d9ff-4df3-83b4-1658b4418f65", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62a87b18-e78e-4e3f-bbff-666b8c5b90b1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3917942ffd48a3dac9b02de13db216653a5b0582853efd8250c8272bc1820956", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a65d2ead-61e3-4984-bac3-34586d385cb0", "node_type": "1", "metadata": {}, "hash": "9ba2aead01e6c42aadfe30a7f84abee684e861656e63407ee79381efd05e70ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And what do you think the\ntimeline is for ASL four?\n- Yeah, so that is hotly\ndebated within the company.\nWe are working actively\nto prepare ASL three\nsecurity measures as well as\nASL three deployment measures.\nI'm not gonna go into detail,\nbut we've made a lot of progress on both,\nand, you know, we're prepared to be,\nI think, ready quite soon.\nI would not be surprised at all\nif we hit ASL three next year.\nThere was some concern that\nwe might even hit it this year.\nThat's still possible,\nthat could still happen.\nIt's like very hard to say,\nbut like I would be very, very surprised\nif it was like 2030.\nI think it's much sooner than that.\n- So there's protocols\nfor detecting it, if then,\nand then there's protocols\nfor how to respond to it.\n- Yes.\n- How difficult is the second, the latter?\n- Yeah, I think for ASL three,\nit's primarily about security and about,\nyou know, filters on the model\nrelating to a very narrow set of areas\nwhen we deploy the model.\nBecause at ASL three, the\nmodel isn't autonomous yet,\nand so you don't have to\nworry about, you know,\nkind of the model itself\nbehaving in a bad way,\neven when it's deployed internally.\nSo I think the ASL three measures are,\nI won't say straightforward,\nthey're rigorous, but they're\neasier to reason about.\nI think once we get to ASL four,\nwe start to have worries about the models\nbeing smart enough that\nthey might sandbag tests,\nthey might not tell the truth about tests.\nWe had some results came out\nabout like sleeper agents\nand there was a more recent\npaper about, you know,\ncan the models mislead\nattempts to, you know,\nsandbag their own abilities, right?\nShow them, you know, present themselves\nas being less capable than they are.\nAnd so I think with ASL four,\nthere's gonna be an important component\nof using other things\nthan just interacting\nwith the models, for\nexample, interpretability\nor hidden chains of thought\nwhere you have to look\ninside the model and verify\nvia some other mechanism\nthat is not, you know, is\nnot as easily corrupted\nas what the model says,\nyou know, that the model\nindeed has some property.\nSo we're still working on ASL four.\nOne of the properties of the RSP is that\nwe don't specify ASL four\nuntil we've hit ASL three.\nAnd I think that's proven\nto be a wise decision\nbecause even with ASL three,\nagain, it's hard to know\nthis stuff in detail,\nand we wanna take as much time\nas we can possibly take\nto get these things right.\n- So for ASL three,\nthe bad actor will be the humans.\n- [Dario] Humans, yes.\n- And so there, it's a little bit more-\n- For ASL four, it's both, I think, both.\n- It's both, and so deception,\nand that's where\nmechanistic interpretability\ncomes into play and hopefully\nthe techniques used for that\nare not made accessible to the model.\n- Yeah, I mean, of course you can hook up\nthe mechanistic interpretability\nto the model itself,\nbut then you've kind of lost it\nas a reliable indicator\nof the model state.\nThere are a bunch of exotic ways\nyou can think of that it\nmight also not be reliable.\nLike if the, you know,\nmodel gets smart enough\nthat it can like, you know, jump computers\nand like read the code\nwhere you're like looking\nat its internal state.\nWe've thought about some of those.\nI think there're exotic enough,\nthere are ways to render them unlikely.\nBut yeah, generally you wanna preserve\nmechanistic interpretability\nas a kind of verification set or test set\nthat's separate from the\ntraining process of the model.\n- See, I think as these\nmodels become better\nand better conversation\nand become smarter,\nsocial engineering becomes\na threat too 'cause they-\n- [Dario] Oh, yeah.\n- That could start being very convincing\nto the engineers inside companies.\n- Oh yeah, yeah.\nIt's actually like, you know,\nwe've seen lots of examples of demagoguery\nin our life from humans and, you know,\nthere's a concern that models\nthat could do that as well.", "mimetype": "text/plain", "start_char_idx": 64107, "end_char_idx": 67988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a65d2ead-61e3-4984-bac3-34586d385cb0": {"__data__": {"id_": "a65d2ead-61e3-4984-bac3-34586d385cb0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04e5d2e3-d9ff-4df3-83b4-1658b4418f65", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "30d58db4eb8bf6ba63c5df09838e02fbc8f81cf7bc6a55b10b677836ef2850b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760", "node_type": "1", "metadata": {}, "hash": "2e2ba5f5b4e46847748ef72ab277fff149bd0ff63730ce481b8bf69e1594d38a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We've thought about some of those.\nI think there're exotic enough,\nthere are ways to render them unlikely.\nBut yeah, generally you wanna preserve\nmechanistic interpretability\nas a kind of verification set or test set\nthat's separate from the\ntraining process of the model.\n- See, I think as these\nmodels become better\nand better conversation\nand become smarter,\nsocial engineering becomes\na threat too 'cause they-\n- [Dario] Oh, yeah.\n- That could start being very convincing\nto the engineers inside companies.\n- Oh yeah, yeah.\nIt's actually like, you know,\nwe've seen lots of examples of demagoguery\nin our life from humans and, you know,\nthere's a concern that models\nthat could do that as well.\n- One of the ways that\nClaude has been getting\nmore and more powerful is it's now able\nto do some agentic stuff, computer use.\nThere's also an analysis\nwithin the sandbox\nof claude.ai itself.\nBut let's talk about computer use.\nThat seems to me super exciting\nthat you can just give Claude a task\nand it takes a bunch of\nactions, figures it out,\nand has access to your\ncomputer through screenshots.\nSo can you explain how that works?\nAnd where that's headed?\n- Yeah, it's actually relatively simple.\nSo Claude has had for a long time,\nsince Claude 3.0 back in March,\nthe ability to analyze images\nand respond to them with text.\nThe only new thing we added\nis those images can be\nscreenshots of a computer.\nAnd in response, we trained the model\nto give a location on the screen\nwhere you can click and/or\nbuttons on the keyboard\nyou can press in order to take action.\nAnd it turns out that\nwith actually not all that\nmuch additional training,\nthe models can get\nquite good at that task.\nIt's a good example of generalization.\nYou know, people sometimes say,\nif you get to lower earth orbit,\nyou're like halfway to anywhere, right?\nBecause of how much it\ntakes to escape the gravity.\nWell, if you have a\nstrong pre-trained model,\nI feel like you're halfway to anywhere\nin terms of the intelligence space.\nAnd so actually, it\ndidn't take all that much\nto get Claude to do this.\nAnd you can just set that in a loop,\ngive the model a screenshot,\ntell it what to click on,\ngive it the next screenshot,\ntell it what to click on,\nand that turns into a full\nkind of almost 3D video\ninteraction of the model.\nAnd it's able to do all\nof these tasks, right?\nYou know, we showed these demos\nwhere it's able to like\nfill out spreadsheets.\nIt's able to kind of like\ninteract with a website.\nIt's able to, you know,\nit's able to open all kinds\nof, you know, programs,\ndifferent operating systems,\nWindows, Linux, Mac.\nSo, you know, I think all\nof that is very exciting.\nI will say, while in theory,\nthere's nothing you could do there\nthat you couldn't have done through\njust giving the model the API\nto drive the computer screen.\nThis really lowers the barrier.\nAnd you know, there's a\nlot of folks who either,\nyou know, aren't in a position to interact\nwith those APIs or it takes\nthem a long time to do.\nIt's just the screen is\njust a universal interface\nthat's a lot easier to interact with.\nAnd so I expect over time,\nthis is gonna lower a bunch of barriers.\nNow, honestly, the current model has,\nit leaves a lot still to be desired,\nand we were honest about\nthat in the blog, right?\nIt makes mistakes, it misclicks.\nAnd, you know, we were\ncareful to warn people,\nhey, this thing isn't, you\ncan't just leave this thing to,\nyou know, run on your computer\nfor minutes and minutes.\nYou gotta give this thing\nboundaries and guardrails.\nAnd I think that's one of the reasons\nwe released it first in an API form\nrather than kind of, you know,\nthis kind of just hand it to the consumer\nand give it control of their computer.\nBut you know, I definitely\nfeel that it's important\nto get these capabilities out there.\nAs models get more powerful,\nwe're gonna have to\ngrapple with, you know,\nhow do we use these capabilities safely?\nHow do we prevent them from being abused?", "mimetype": "text/plain", "start_char_idx": 67291, "end_char_idx": 71238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760": {"__data__": {"id_": "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a65d2ead-61e3-4984-bac3-34586d385cb0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6e16cee6ed57e56c7671fbcd42784b332c44bcd727b25a2b53b505cdf97c8e67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69fc89a8-e561-4c89-af30-89b83d8b87dd", "node_type": "1", "metadata": {}, "hash": "ecd3bc3463a1f50ff9d1aa05daf913889073568082701cc89fe760247182b3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, honestly, the current model has,\nit leaves a lot still to be desired,\nand we were honest about\nthat in the blog, right?\nIt makes mistakes, it misclicks.\nAnd, you know, we were\ncareful to warn people,\nhey, this thing isn't, you\ncan't just leave this thing to,\nyou know, run on your computer\nfor minutes and minutes.\nYou gotta give this thing\nboundaries and guardrails.\nAnd I think that's one of the reasons\nwe released it first in an API form\nrather than kind of, you know,\nthis kind of just hand it to the consumer\nand give it control of their computer.\nBut you know, I definitely\nfeel that it's important\nto get these capabilities out there.\nAs models get more powerful,\nwe're gonna have to\ngrapple with, you know,\nhow do we use these capabilities safely?\nHow do we prevent them from being abused?\nAnd you know, I think releasing the model\nwhile the capabilities are, you know,\nare still limited is very\nhelpful in terms of doing that.\nYou know, I think since\nit's been released,\na number of customers,\nI think Replit was maybe\none of the most quickest\nto deploy things.\nYou know, have made use\nof it in various ways.\nPeople have hooked up demos for, you know,\nWindows desktops, Macs,\nyou know, Linux machines.\nSo yeah, it's been very exciting.\nI think as with anything else, you know,\nit comes with new exciting abilities\nand then, you know,\nwith those new exciting abilities,\nwe have to think about how to, you know,\nmake the model, you know, safe,\nreliable, do what humans want them to do.\nI mean, it's the same story\nfor everything, right?\nSame thing. It's that same tension.\n- But the possibility of use cases here,\njust the range is incredible.\nSo how much to make it work\nreally well in the future?\nHow much do you have to specially\nkind of go beyond what's the\npre-trained model's doing,\ndo more post-training, RLHF\nor supervised fine tuning,\nor synthetic data just\nfor the agentic stuff?\n- Yeah, I think speaking at a high level,\nit's our intention to\nkeep investing a lot in,\nyou know, making the model better.\nLike I think, you know,\nwe look at some of the,\nyou know, some of the benchmarks\nwhere previous models were like,\noh, could do it 6% of the time,\nand now our model would do\nit 14 or 22% of the time.\nAnd yeah, we wanna get up to, you know,\nthe human level reliability\nof 80, 90% just like anywhere else, right?\nWe're on the same curve that\nwe were on with SWE-bench,\nwhere I think I would\nguess a year from now,\nthe models can do this\nvery, very reliably,\nbut you gotta start somewhere.\n- So you think it's possible to get\nto the human level, 90%,\nbasically doing the same\nthing you're doing now?\nOr it has to be special for computer use?\n- I mean, it depends what\nyou mean by, you know,\nspecial and special in general.\nBut, you know, I\ngenerally think, you know,\nthe same kinds of techniques\nthat we've been using to\ntrain the current model,\nI expect that doubling\ndown on those techniques\nin the same way that we have for code\nfor models in general, you know,\nfor image input, you know, for voice.\nI expect those same\ntechniques will scale here,\nas they have everywhere else.\n- But this is giving sort of the power\nof action to Claude,\nand so you could do a lot\nof really powerful things,\nbut you could do a lot of damage also.\n- Yeah, yeah, no, and we've\nbeen very aware of that.\nLook, my view actually is computer use\nisn't a fundamentally new capability,\nlike the CBRN or autonomy\ncapabilities are.\nIt's more like, it kind of\nopens the aperture for the model\nto use and apply its existing abilities.\nAnd so the way we think about it,\ngoing back to our RSP,\nis nothing that this model is\ndoing inherently increases,\nyou know, the risk from\nan RSP perspective.", "mimetype": "text/plain", "start_char_idx": 70435, "end_char_idx": 74124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69fc89a8-e561-4c89-af30-89b83d8b87dd": {"__data__": {"id_": "69fc89a8-e561-4c89-af30-89b83d8b87dd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2ac04f3-ed5d-4e1e-882c-a3479f1cf760", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5d6f31c392fdb7f39098550e5dc246c7902c734f74ed6973297c4a218df21626", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ffad171-3c3a-4d6e-90d4-1a54625db300", "node_type": "1", "metadata": {}, "hash": "e6205b8eaaf0c848352e5917de72b45aaf98d7397878699f1cb7e931b5c54f89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I expect those same\ntechniques will scale here,\nas they have everywhere else.\n- But this is giving sort of the power\nof action to Claude,\nand so you could do a lot\nof really powerful things,\nbut you could do a lot of damage also.\n- Yeah, yeah, no, and we've\nbeen very aware of that.\nLook, my view actually is computer use\nisn't a fundamentally new capability,\nlike the CBRN or autonomy\ncapabilities are.\nIt's more like, it kind of\nopens the aperture for the model\nto use and apply its existing abilities.\nAnd so the way we think about it,\ngoing back to our RSP,\nis nothing that this model is\ndoing inherently increases,\nyou know, the risk from\nan RSP perspective.\nBut as the models get more\npowerful, having this capability\nmay make it scarier once it, you know,\nonce it has the cognitive\ncapability to, you know,\nto do something at the ASL\nthree and ASL four level,\nyou know, this may be the thing\nthat kind of unbounds it from doing so.\nSo, going forward, certainly\nthis modality of interaction\nis something we have tested for,\nand that we will continue to\ntest for in RSP going forward.\nI think it's probably better to have,\nto learn and explore this capability\nbefore the model is super,\nyou know, super capable.\n- Yeah, and there's a lot\nof interesting attacks,\nlike prompt injection,\nbecause now you've widened the aperture,\nso you can prompt inject\nthrough stuff on screen.\nSo if this becomes more and more useful,\nthen there's more and more benefit\nto inject stuff into the model.\nIf it goes to a certain webpage,\nit could be harmless\nstuff like advertisements\nor it could be like harmful stuff, right?\n- Yeah, I mean, we've\nthought a lot about things\nlike spam, CAPTCHA, you know, mass camp.\nThere's all, you know, every,\nlike one secret I'll tell you,\nif you've invented a new technology,\nnot necessarily the biggest misuse,\nbut the first misuse you'll see,\nscams, just petty scams.\nLike you'll just, it's\nlike a thing as old,\npeople scamming each other,\nit's this thing as old as time,\nand it's just every time\nyou gotta deal with it.\n- It's almost like silly\nto say but it's true,\nsort of bots and spam\nin general is a thing\nas it gets more and more intelligent.\nIt's harder and harder to fight.\n- There are a lot of like I said,\nlike there are a lot of\npetty criminals in the world.\nAnd you know, it's like\nevery new technology\nis like a new way for petty\ncriminals to do something,\nyou know, something stupid and malicious.\n- Is there any ideas about sandboxing it?\nLike how difficult is the sandboxing task?\n- Yeah, we sandbox during training.\nSo for example, during training,\nwe didn't expose the\nmodel to the internet.\nI think that's probably a\nbad idea during training\nbecause, you know, the model\ncan be changing its policy,\nit can be changing what it's doing,\nand it's having an\neffect in the real world.\nYou know, in terms of actually\ndeploying the model, right,\nit kind of depends on the application.\nLike, you know, sometimes\nyou want the model\nto do something in the real world,\nbut of course, you can always put\nguardrails on the outside, right?\nYou can say, okay, well, you know,\nthis model's not gonna move\ndata from my, you know,\nmodel's not gonna move\nany files from my computer\nor my web server to anywhere else.\nNow, when you talk about sandboxing,\nagain, when we get to ASL four,\nnone of these precautions\nare going to make sense there, right,\nwhere when you talk about ASL four,\nyou're then, the model is\nbeing kind of, you know,\nthere's a theoretical worry\nthe model could be smart enough\nto kind of break out of any box.\nAnd so there we need to think about\nmechanistic interpretability\nabout, you know,\nif we're gonna have a sandbox,\nit would need to be a\nmathematically provable sand.\nYou know, that's a whole different world\nthan what we're dealing\nwith with the models today.\n- Yeah, the science of building a box\nfrom which ASL four AI\nsystem cannot escape.\n- I think it's probably\nnot the right approach.", "mimetype": "text/plain", "start_char_idx": 73461, "end_char_idx": 77405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ffad171-3c3a-4d6e-90d4-1a54625db300": {"__data__": {"id_": "7ffad171-3c3a-4d6e-90d4-1a54625db300", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69fc89a8-e561-4c89-af30-89b83d8b87dd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "aa90ee3c4190d7dc563111f1f4ef26d0fc5cb8619cbccdfecfe699e8f5b209e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78aeb346-49d9-498e-a664-4e573eacfc1c", "node_type": "1", "metadata": {}, "hash": "40728fd06ab7bb9cb960e1fffb9b39fcb3414933ec3395acc850c5aa7fcbd54d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, when you talk about sandboxing,\nagain, when we get to ASL four,\nnone of these precautions\nare going to make sense there, right,\nwhere when you talk about ASL four,\nyou're then, the model is\nbeing kind of, you know,\nthere's a theoretical worry\nthe model could be smart enough\nto kind of break out of any box.\nAnd so there we need to think about\nmechanistic interpretability\nabout, you know,\nif we're gonna have a sandbox,\nit would need to be a\nmathematically provable sand.\nYou know, that's a whole different world\nthan what we're dealing\nwith with the models today.\n- Yeah, the science of building a box\nfrom which ASL four AI\nsystem cannot escape.\n- I think it's probably\nnot the right approach.\nI think the right approach\ninstead of having something,\nyou know, unaligned\nthat like you're trying\nto prevent it from escaping.\nI think it's better to\njust design the model\nthe right way or have a loop where,\nyou know, you look inside,\nyou look inside the model\nand you're able to verify properties,\nand that gives you an opportunity\nto like iterate and actually get it right.\nI think containing bad models\nis much worse solution\nthan having good models.\n- Let me ask about regulation.\nWhat's the role of regulation\nin keeping AI safe?\nSo for example, can you describe\nCalifornia AI regulation\nBill SB 1047 that was ultimately\nvetoed by the governor?\nWhat are the pros and cons\nof this bill in general?\n- Yes, we ended up making some suggestions\nto the bill, and then\nsome of those were adopted\nand, you know, we felt,\nI think quite positively,\nquite positively about the\nbill by the end of that.\nIt did still have some downsides,\nand, you know, of course it got vetoed.\nI think at a high level, I\nthink some of the key ideas\nbehind the bill are, you know,\nI would say similar to\nideas behind our RSPs.\nAnd I think it's very important\nthat some jurisdiction,\nwhether it's California\nor the federal government\nand/or other countries\nand other states passes\nsome regulation like this.\nAnd I can talk through why\nI think that's so important.\nSo I feel good about our RSP.\nIt's not perfect, it needs\nto be iterated on a lot,\nbut it's been a good forcing function\nfor getting the company to\ntake these risks seriously,\nto put them into product\nplanning, to really make them\na central part of work at Anthropic\nand to make sure that all of 1000 people,\nand it's almost 1000\npeople now at Anthropic,\nunderstand that this is one\nof the highest priorities\nof the company, if not\nthe highest priority.\nBut one, there are still some companies\nthat don't have RSP like mechanisms,\nlike OpenAI, Google did\nadopt these mechanisms\na couple months after Anthropic did,\nbut there are other companies out there\nthat don't have these mechanisms at all.\nAnd so if some companies\nadopt these mechanisms and others don't,\nit's really gonna create\na situation where,\nyou know, some of these\ndangers have the property\nthat it doesn't matter\nif three out of five\nof the companies are being safe,\nif the other two are being unsafe,\nit creates this negative externality.\nAnd I think the lack of\nuniformity is not fair\nto those of us who have\nput a lot of effort\ninto being very thoughtful\nabout these procedures.\nThe second thing is,\nI don't think you can\ntrust these companies\nto adhere to these voluntary\nplans in their own, right?\nI like to think that Anthropic will.\nWe do everything we can that we will.\nOur RSP is checked by our\nlong-term benefit trust.\nSo, you know, we do everything we can\nto adhere to our own RSP.\nBut you know, you hear lots of things\nabout various companies saying,\noh, they said they would give\nthis much compute and they didn't.\nThey said they would do\nthis thing and they didn't.\nYou know, I don't think it makes sense to,\nyou know, to litigate particular things\nthat companies have done.\nBut I think this broad principle\nthat like if there's\nnothing watching over them,\nthere's nothing watching\nover us as an industry,\nthere's no guarantee that\nwe'll do the right thing,\nand the stakes are very high.", "mimetype": "text/plain", "start_char_idx": 76704, "end_char_idx": 80712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78aeb346-49d9-498e-a664-4e573eacfc1c": {"__data__": {"id_": "78aeb346-49d9-498e-a664-4e573eacfc1c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ffad171-3c3a-4d6e-90d4-1a54625db300", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "eb0eee39bd4ec7418c91dd5e0606b28601b66e395b33fc45ca1fee46ec92d5a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3202f94-b234-4111-9e1f-776cb8811b1d", "node_type": "1", "metadata": {}, "hash": "9fb29d842c8bf87460097c316e872b3514192b122d0bd179d3887bf8c149477f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second thing is,\nI don't think you can\ntrust these companies\nto adhere to these voluntary\nplans in their own, right?\nI like to think that Anthropic will.\nWe do everything we can that we will.\nOur RSP is checked by our\nlong-term benefit trust.\nSo, you know, we do everything we can\nto adhere to our own RSP.\nBut you know, you hear lots of things\nabout various companies saying,\noh, they said they would give\nthis much compute and they didn't.\nThey said they would do\nthis thing and they didn't.\nYou know, I don't think it makes sense to,\nyou know, to litigate particular things\nthat companies have done.\nBut I think this broad principle\nthat like if there's\nnothing watching over them,\nthere's nothing watching\nover us as an industry,\nthere's no guarantee that\nwe'll do the right thing,\nand the stakes are very high.\nAnd so I think it's important\nto have a uniform standard\nthat everyone follows,\nand to make sure that simply\nthat the industry does\nwhat a majority of the industry\nhas already said is important\nand has already said\nthat they definitely will do.\nRight, some people, you know,\nI think there's a class of people\nwho are against regulation on principle.\nI understand where that comes from.\nIf you go to Europe and, you know,\nyou see something like GDPR,\nyou see some of the other\nstuff that they've done.\nYou know, some of it's good,\nbut some of it is really\nunnecessarily burdensome,\nand I think it's fair to say\nreally has slowed innovation.\nAnd so I understand where people\nare coming from on priors.\nI understand why people come from,\nstart from that position.\nBut again, I think AI is different.\nIf we go to the very\nserious risks of autonomy\nand misuse that I talked about,\nyou know, just a few minutes ago,\nI think that those are unusual\nand they warrant an\nunusually strong response.\nAnd so I think it's very important.\nAgain, we need something\nthat everyone can get behind.\nYou know, I think one of\nthe issues with SB 1047,\nespecially the original version of it,\nwas it had a bunch of\nthe structure of RSPs,\nbut it also had a bunch of\nstuff that was either clunky\nor that just would've created\na bunch of burdens, a bunch of hassle,\nand might even have missed the target\nin terms of addressing the risks.\nYou don't really hear about it on Twitter.\nYou just hear about kind of, you know,\npeople are cheering for any regulation,\nand then the folks who are against\nmake up these often quite\nintellectually dishonest arguments\nabout how, you know,\nit'll make us move away from California.\nBill doesn't apply if you're\nheadquartered in California,\nbill only applies if you\ndo business in California.\nOr that it would damage\nthe open source ecosystem,\nor that it would, you know,\nit would cause all of these things.\nI think those were mostly nonsense,\nbut there are better\narguments against regulation.\nThere's one guy, Dean Ball,\nwho's really, you know,\nI think a very scholarly,\nscholarly analyst\nwho looks at what happens when\na regulation is put in place\nand ways that they can kind\nof get a life of their own,\nor how they can be poorly designed.\nAnd so our interest has always been,\nwe do think there should be\nregulation in this space,\nbut we wanna be an actor who makes sure\nthat that regulation is\nsomething that's surgical,\nthat's targeted at the serious risks\nand is something people\ncan actually comply with.\nBecause something I think the advocates\nof regulation don't understand\nas well as they could\nis if we get something in place\nthat's poorly targeted,\nthat wastes a bunch of people's time,\nwhat's gonna happen is\npeople are gonna say,\nsee, these safety risks,\nyou know, this is nonsense.\nYou know, I just had to hire 10 lawyers,\nyou know, to fill out all these forms.\nI had to run all these tests for something\nthat was clearly not dangerous.\nAnd after six months of that,\nthere will be a groundswell\nand we'll end up\nwith a durable consensus\nagainst regulation.\nAnd so I think the worst enemy of those\nwho want real accountability\nis badly designed regulation.\nWe need to actually get it right.", "mimetype": "text/plain", "start_char_idx": 79893, "end_char_idx": 83924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3202f94-b234-4111-9e1f-776cb8811b1d": {"__data__": {"id_": "d3202f94-b234-4111-9e1f-776cb8811b1d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78aeb346-49d9-498e-a664-4e573eacfc1c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2f571bb1b2140fd6ecf3dc25eabf6e24c96b73f0cc84182fb912571e90b0f7a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01d7d455-ed37-441c-84e7-a882638530d9", "node_type": "1", "metadata": {}, "hash": "1a9bacd31721216c349604327b7d91f4b1d3aefbfdc704a5e504ccd9a9c14ecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because something I think the advocates\nof regulation don't understand\nas well as they could\nis if we get something in place\nthat's poorly targeted,\nthat wastes a bunch of people's time,\nwhat's gonna happen is\npeople are gonna say,\nsee, these safety risks,\nyou know, this is nonsense.\nYou know, I just had to hire 10 lawyers,\nyou know, to fill out all these forms.\nI had to run all these tests for something\nthat was clearly not dangerous.\nAnd after six months of that,\nthere will be a groundswell\nand we'll end up\nwith a durable consensus\nagainst regulation.\nAnd so I think the worst enemy of those\nwho want real accountability\nis badly designed regulation.\nWe need to actually get it right.\nAnd this is, if there's\none thing I could say\nto the advocates, it\nwould be that I want them\nto understand this dynamic better,\nand we need to be really careful\nand we need to talk to people\nwho actually have experience\nseeing how regulations\nplay out in practice.\nAnd the people who have\nseen that understand\nto be very careful.\nIf this was some lesser issue,\nI might be against regulation at all.\nBut what I want the\nopponents to understand\nis that the underlying\nissues are actually serious.\nThey're not something that\nI or the other companies\nare just making up because\nof regulatory capture.\nThey're not sci-fi fantasies.\nThey're not any of these things.\nYou know, every time we have a new model,\nevery few months, we measure\nthe behavior of these models\nand they're getting better and better\nat these concerning tasks,\njust as they are getting\nbetter and better at,\nyou know, good, valuable,\neconomically useful tasks.\nAnd so I would just love\nit if some of the former,\nyou know, I think SB\n1047 was very polarizing.\nI would love it if some of\nthe most reasonable opponents\nand some of the most reasonable proponents\nwould sit down together.\nAnd, you know I think that, you know,\nthe different AI companies, you know,\nAnthropic was the only AI\ncompany that, you know,\nfelt positively in a very detailed way.\nI think Elon tweeted\nbriefly something positive.\nBut, you know, some of the big ones,\nlike Google, OpenAI, Meta, Microsoft\nwere pretty staunchly against.\nSo I would really like is if, you know,\nsome of the key stakeholders,\nsome of the, you know,\nmost thoughtful proponents\nand some of the most thoughtful\nopponents would sit down\nand say, how do we solve\nthis problem in a way\nthat the proponents feel brings\na real reduction in risk,\nand that the opponents feel that\nit is not hampering the industry\nor hampering innovation any\nmore necessary than it needs to.\nAnd I think for whatever reason\nthat things got too polarized\nand those two groups\ndidn't get to sit down\nin the way that they should.\nAnd I feel urgency.\nI really think we need\nto do something in 2025.\nYou know, if we get to the end of 2025\nand we've still done nothing about this,\nthen I'm gonna be worried.\nI'm not worried yet, because again,\nthe risks aren't here yet,\nbut I think time is running short.\n- Yeah, and come up with\nsomething surgical, like you said.\n- Yeah, yeah, yeah, exactly.\nAnd we need to get away\nfrom this intense pro-safety\nversus intense anti-regulatory\nrhetoric, right?\nIt's turned into these\nflame wars on Twitter\nand nothing good's gonna come of that.\n- So there's a lot of curiosity about\nthe different players in the game.\nOne of the OGs is OpenAI.\nYou've had several years\nof experience at OpenAI.\nWhat's your story and history there?\n- Yeah, so I was at OpenAI\nfor roughly five years.\nFor the last, I think it was couple years,\nyou know, I was vice\npresident of research there.\nProbably myself and Ilya\nSutskever were the ones who,\nyou know, really kind of\nset the research direction.\nAround 2016 or 2017, I first\nstarted to really believe in\nor at least confirm my belief\nin the Scaling Hypothesis\nwhen Ilya famously said to me,\n\"The thing you need to understand\nabout these models is\nthey just wanna learn.\nThe models just wanna learn.\"", "mimetype": "text/plain", "start_char_idx": 83232, "end_char_idx": 87174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01d7d455-ed37-441c-84e7-a882638530d9": {"__data__": {"id_": "01d7d455-ed37-441c-84e7-a882638530d9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3202f94-b234-4111-9e1f-776cb8811b1d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89226258b634f26114c353b68a451985cdc0340ae32f08ac6de69e7aa19d5d6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02bf118c-0051-4dd4-9753-43aa4c243006", "node_type": "1", "metadata": {}, "hash": "47150ad84cb220bc881fbb8163c2dd23100a5c1e1c82f81348d9b2810721514e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's turned into these\nflame wars on Twitter\nand nothing good's gonna come of that.\n- So there's a lot of curiosity about\nthe different players in the game.\nOne of the OGs is OpenAI.\nYou've had several years\nof experience at OpenAI.\nWhat's your story and history there?\n- Yeah, so I was at OpenAI\nfor roughly five years.\nFor the last, I think it was couple years,\nyou know, I was vice\npresident of research there.\nProbably myself and Ilya\nSutskever were the ones who,\nyou know, really kind of\nset the research direction.\nAround 2016 or 2017, I first\nstarted to really believe in\nor at least confirm my belief\nin the Scaling Hypothesis\nwhen Ilya famously said to me,\n\"The thing you need to understand\nabout these models is\nthey just wanna learn.\nThe models just wanna learn.\"\nAnd again, sometimes there\nare these one sentences,\nthese zen koans that you hear them\nand you're like, ah,\nthat explains everything.\nThat explains like 1000\nthings that I've seen.\nAnd then I, you know,\never after I had this\nvisualization in my head of like,\nyou optimize the models in the right way,\nyou point the models in the right way.\nThey just wanna learn.\nThey just wanna solve the problem,\nregardless of what the problem is.\n- So get out of their way, basically.\n- Get out of their way, yeah.\nDon't impose your own ideas\nabout how they should learn.\nAnd you know, this was the same thing\nas Rich Sutton put out\nin the Bitter Lesson\nor Gwern put out in\nThe Scaling Hypothesis.\nYou know, I think generally\nthe dynamic was, you know,\nI got this kind of inspiration\nfrom Ilya and from others,\nfolks like Alec Radford\nwho did the original GPT-1,\nand then ran really hard with it.\nMe and my collaborators on GPT-2, GPT-3.\nRL from Human Feedback,\nwhich was an attempt\nto kind of deal with the\nearly safety and durability.\nThings like debate and amplification.\nHeavy on interpretability.\nSo again, the combination\nof safety plus scaling.\nProbably 2018, 2019, 2020,\nthose were kind of the years\nwhen myself and my collaborators\nprobably, you know,\nmany of whom became\nco-founders of Anthropic,\nkind of really had a vision\nand like drove the direction.\n- Why'd you leave?\nWhy'd you decide to leave?\n- Yeah, so look, I'm\ngonna put things this way\nand, you know, I think it ties\nto the race to the top, right?\nWhich is, you know, in my time at OpenAI,\nwhat I'd come to see is\nI'd come to appreciate\nthe Scaling Hypothesis,\nand as I'd come to appreciate\nkind of the importance\nof safety along with\nthe Scaling Hypothesis.\nThe first one I think, you know,\nOpenAI was getting on board with.\nThe second one in a way\nhad always been part\nof OpenAI's messaging,\nbut, you know, over many years\nof the time that I spent there,\nI think I had a particular vision\nof how we should handle these things,\nhow we should be brought out in the world,\nthe kind of principles that\nthe organization should have.\nAnd look, I mean, there were\nlike many, many discussions\nabout like, you know, should the org do,\nshould the company do this?\nShould the company do that?\nLike, there's a bunch of\nmisinformation out there.\nPeople say like, we left\nbecause we didn't like\nthe deal with Microsoft.\nFalse, although, you know, it\nwas like a lot of discussion,\na lot of questions about exactly\nhow we do the deal with Microsoft.\nWe left because we didn't\nlike commercialization.\nThat's not true, we built GPT-3,\nwhich was the model\nthat was commercialized.\nI was involved in commercialization.\nIt's more again about, how do you do it?\nLike civilization is going down this path\nto very powerful AI.\nWhat's the way to do it that is cautious,\nstraightforward, honest,\nthat builds trust in the\norganization and individuals?\nHow do we get from here to there?\nAnd how do we have a real\nvision for how to get it right?\nHow can safety not just\nbe something we say\nbecause it helps with recruiting?", "mimetype": "text/plain", "start_char_idx": 86400, "end_char_idx": 90229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "02bf118c-0051-4dd4-9753-43aa4c243006": {"__data__": {"id_": "02bf118c-0051-4dd4-9753-43aa4c243006", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01d7d455-ed37-441c-84e7-a882638530d9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0941eb21c2ef8d84112e6d170e5d3d554375d1121d13cd2efc459f1ac6794643", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae4554b2-0506-4b96-893d-6795e5473bb7", "node_type": "1", "metadata": {}, "hash": "573875acaf6a70e46558120efc8f0b06d0674e614490cd4f06540229a92c9b0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like, there's a bunch of\nmisinformation out there.\nPeople say like, we left\nbecause we didn't like\nthe deal with Microsoft.\nFalse, although, you know, it\nwas like a lot of discussion,\na lot of questions about exactly\nhow we do the deal with Microsoft.\nWe left because we didn't\nlike commercialization.\nThat's not true, we built GPT-3,\nwhich was the model\nthat was commercialized.\nI was involved in commercialization.\nIt's more again about, how do you do it?\nLike civilization is going down this path\nto very powerful AI.\nWhat's the way to do it that is cautious,\nstraightforward, honest,\nthat builds trust in the\norganization and individuals?\nHow do we get from here to there?\nAnd how do we have a real\nvision for how to get it right?\nHow can safety not just\nbe something we say\nbecause it helps with recruiting?\nAnd, you know, I think\nat the end of the day,\nif you have a vision for that,\nforget about anyone else's vision.\nI don't wanna talk about\nanyone else's vision.\nIf you have a vision for how\nto do it, you should go off\nand you should do that vision.\nIt is incredibly unproductive to try\nand argue with someone else's vision.\nYou might think they're\nnot doing it the right way.\nYou might think they're dishonest.\nWho knows, maybe you're\nright, maybe you're not.\nBut what you should do\nis you should take some people you trust\nand you should go off together\nand you should make your vision happen.\nAnd if your vision is compelling,\nif you can make it appeal to people,\nyou know, some combination of ethically,\nyou know, in the market, you know,\nif you can make a company that's\na place people wanna join,\nthat, you know, engages in practices\nthat people think are reasonable,\nwhile managing to maintain its position\nin the ecosystem at the same time,\nif you do that, people will copy it.\nAnd the fact that you are\ndoing it, especially the fact\nthat you're doing it better than they are\ncauses them to change their behavior\nin a much more compelling way\nthan if they're your boss\nand you're arguing with them.\nI just, I don't know how to be\nany more specific about it than that,\nbut I think it's generally\nvery unproductive\nto try and get someone else's vision\nto look like your vision.\nIt's much more productive to go off\nand do a clean experiment\nand say, this is our vision,\nthis is how we're gonna do things.\nYour choice is you can ignore us,\nyou can reject what we're doing,\nor you can start to become more like us,\nand imitation is the\nsincerest form of flattery.\nAnd you know, that plays out\nin the behavior of customers,\nthat plays out in the\nbehavior of the public,\nthat plays out in the behavior\nof where people choose to work.\nAnd again, at the end,\nit's not about one company winning\nor another company winning.\nIf we or another company are\nengaging in some practice that,\nyou know, people find genuinely appealing,\nand I want it to be in substance,\nnot just in appearance.\nAnd, you know, I think researchers\nare sophisticated and\nthey look at substance.\nAnd then other companies\nstart copying that practice\nand they win because they\ncopied that practice,\nthat's great, that's success.\nThat's like the race to the top.\nIt doesn't matter who wins in the end,\nas long as everyone is copying\neveryone else's good practices, right?\nOne way I think of it is like,\nthe thing we're all afraid of\nis the race to the bottom, right?\nAnd the race to the bottom,\ndoesn't matter who wins\nbecause we all lose, right?\nLike, you know, in the most extreme world,\nwe make this autonomous AI that, you know,\nthe robots enslave us or whatever, right?\nI mean, that's half joking, but you know,\nthat is the most extreme\nthing that could happen.\nThen it doesn't matter\nwhich company was ahead.\nIf instead you create a race to the top\nwhere people are competing to engage\nin good practices, then, you know,\nat the end of the day, you know,\nit doesn't matter who ends up winning,\ndoesn't even matter who\nstarted the race at the top.\nThe point isn't to be virtuous.", "mimetype": "text/plain", "start_char_idx": 89417, "end_char_idx": 93375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae4554b2-0506-4b96-893d-6795e5473bb7": {"__data__": {"id_": "ae4554b2-0506-4b96-893d-6795e5473bb7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02bf118c-0051-4dd4-9753-43aa4c243006", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d286cba52fb648260f754a3cd1e6b23867bf0c8013d9a1909b5ce0e1e44a88ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794", "node_type": "1", "metadata": {}, "hash": "eda46bc28a2bc7e4b3e140000580b2e0223267da73385ef2ae1545c50a12625c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One way I think of it is like,\nthe thing we're all afraid of\nis the race to the bottom, right?\nAnd the race to the bottom,\ndoesn't matter who wins\nbecause we all lose, right?\nLike, you know, in the most extreme world,\nwe make this autonomous AI that, you know,\nthe robots enslave us or whatever, right?\nI mean, that's half joking, but you know,\nthat is the most extreme\nthing that could happen.\nThen it doesn't matter\nwhich company was ahead.\nIf instead you create a race to the top\nwhere people are competing to engage\nin good practices, then, you know,\nat the end of the day, you know,\nit doesn't matter who ends up winning,\ndoesn't even matter who\nstarted the race at the top.\nThe point isn't to be virtuous.\nThe point is to get the system\ninto a better equilibrium\nthan it was before,\nand individual companies can\nplay some role in doing this.\nIndividual companies can, you know,\ncan help to start it,\ncan help to accelerate it.\nAnd frankly, I think\nindividuals at other companies\nhave done this as well, right?\nThe individuals that\nwhen we put out an RSP\nreact by pushing harder to\nget something similar done,\nget something similar\ndone at other companies.\nSometimes other companies\ndo something that's like,\nwe're like, oh, it's a good practice.\nWe think that's good.\nWe should adopt it too.\nThe only difference is,\nyou know, I think we are,\nwe try to be more forward-leaning.\nWe try and adopt more\nof these practices first\nand adopt them more quickly\nwhen others invent them.\nBut I think this dynamic is\nwhat we should be pointing at.\nAnd I think it abstracts\naway the question of,\nyou know, which company's\nwinning, who trusts who.\nI think all these questions of drama\nare profoundly uninteresting,\nand the thing that\nmatters is the ecosystem\nthat we all operate in and how\nto make that ecosystem better\nbecause that constrains all the players.\n- And so Anthropic is this kind\nof clean experiment built on a foundation\nof like what concretely AI\nsafety should look like.\n- Look, I'm sure we've made plenty\nof mistakes along the way.\nThe perfect organization doesn't exist.\nIt has to deal with the imperfection\nof 1000 employees.\nIt has to deal with the imperfection\nof our leaders, including me.\nIt has to deal with the imperfection\nof the people we've put to, you know,\nto oversee the imperfection\nof the leaders,\nlike the board and the\nlong-term benefit trust.\nIt's all a set of imperfect people\ntrying to aim imperfectly at some ideal\nthat will never perfectly be achieved.\nThat's what you sign up for,\nthat's what it will always be.\nBut imperfect doesn't\nmean you just give up.\nThere's better and there's worse.\nAnd hopefully we can begin to build,\nwe can do well enough\nthat we can begin to build\nsome practices that the\nwhole industry engages in.\nAnd then, you know, my guess is that\nmultiple of these companies\nwill be successful.\nAnthropic will be successful.\nThese other companies,\nlike ones I've been at the\npast will also be successful,\nand some will be more\nsuccessful than others.\nThat's less important than, again,\nthat we align the\nincentives of the industry.\nAnd that happens partly\nthrough the race to the top,\npartly through things like RSP,\npartly through again\nselected surgical regulation.\n- You said talent density\nbeats talent mass.\nSo can you explain that?\nCan you expand on that?\nCan you just talk about what it takes\nto build a great team of AI\nresearchers and engineers?\n- This is one of these statements\nthat's like more true every month.\nEvery month I see this statement\nas more true than I did the month before.\nSo if I were to do a thought experiment,\nlet's say you have a team of 100 people\nthat are super smart, motivated,\nand aligned with the mission,\nand that's your company.\nOr you can have a team of 1000 people\nwhere 200 people are super smart,\nsuper aligned with the mission,\nand then like 800 people are,\nlet's just say you pick 800\nlike random big tech employees,\nwhich would you rather have, right?\nThe talent mass is greater\nin the group of 1000 people, right?", "mimetype": "text/plain", "start_char_idx": 92664, "end_char_idx": 96675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794": {"__data__": {"id_": "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae4554b2-0506-4b96-893d-6795e5473bb7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "66ab5bd60f94acb17a2435bf6f0041017bfc623cb030347974558592df94a9d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8", "node_type": "1", "metadata": {}, "hash": "b662ea45e26b161352d44ada4016fe97a9f5d2ece252cae81fc660dd4a8e55fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- You said talent density\nbeats talent mass.\nSo can you explain that?\nCan you expand on that?\nCan you just talk about what it takes\nto build a great team of AI\nresearchers and engineers?\n- This is one of these statements\nthat's like more true every month.\nEvery month I see this statement\nas more true than I did the month before.\nSo if I were to do a thought experiment,\nlet's say you have a team of 100 people\nthat are super smart, motivated,\nand aligned with the mission,\nand that's your company.\nOr you can have a team of 1000 people\nwhere 200 people are super smart,\nsuper aligned with the mission,\nand then like 800 people are,\nlet's just say you pick 800\nlike random big tech employees,\nwhich would you rather have, right?\nThe talent mass is greater\nin the group of 1000 people, right?\nYou have even a larger number\nof incredibly talented,\nincredibly aligned,\nincredibly smart people.\nBut the issue is just that if every time\nsomeone super talented looks around,\nthey see someone else super\ntalented and super dedicated,\nthat sets the tone for everything, right?\nThat sets the tone for\neveryone is super inspired\nto work at the same place.\nEveryone trusts everyone else.\nIf you have 1000 or 10,000 people\nand things have really regressed, right?\nYou are not able to do selection\nand you're choosing random people,\nwhat happens is then you need\nto put a lot of processes\nand a lot of guardrails in place\njust because people don't\nfully trust each other,\nor you have to adjudicate\npolitical battles.\nLike there are so many things\nthat slow down the org's\nability to operate.\nAnd so we're nearly 1000 people\nand you know, we've\ntried to make it so that\nas large a fraction of those 1000 people\nas possible are like super\ntalented, super skilled.\nIt's one of the reasons\nwe've slowed down hiring a\nlot in the last few months.\nWe grew from 300 to 800, I believe,\nI think in the first seven,\neight months of the year.\nAnd now we've slowed down.\nWe're at like, you know,\nthe last three months,\nwe went from 800 to 900,\n950, something like that.\nDon't quote me on the exact numbers,\nbut I think there's an\ninflection point around 1000,\nand we want to be much\nmore careful how we grow.\nEarly on, and now as well, you know,\nwe've hired a lot of physicists.\nYou know, theoretical physicists\ncan learn things really fast.\nEven more recently as we've\ncontinued to hire that,\nyou know, we've really had a high bar for,\non both the research side\nand the software engineering side\nhave hired a lot of senior people,\nincluding folks who used to be\nat other companies in this space.\nAnd we've just continued\nto be very selective.\nIt's very easy to go from 100 to 1000\nand 1000 to 10,000\nwithout paying attention\nto making sure everyone\nhas a unified purpose.\nIt's so powerful.\nIf your company consists of\na lot of different fiefdoms\nthat all wanna do their own thing,\nthey're all optimizing\nfor their own thing,\nit's very hard to get anything done.\nBut if everyone sees the\nbroader purpose of the company,\nif there's trust and there's dedication\nto doing the right thing,\nthat is a superpower.\nThat in itself, I think,\ncan overcome almost\nevery other disadvantage.\n- And you know, as to\nSteve Jobs, A players.\nA players wanna look around\nand see other A players is\nanother way of saying that.\nI don't know what that\nis about human nature,\nbut it is demotivating to see people\nwho are not obsessively driving\ntowards a singular mission.\nAnd it is, on the flip side of that,\nsuper motivating to see that.\nIt's interesting.\nWhat's it take to be a great AI researcher\nor engineer from everything you've seen,\nfrom working with so many amazing people?\n- Yeah, I think the number one quality,\nespecially on the research side,\nbut really both is open-mindedness.\nSounds easy to be open-minded, right?\nYou're just like, oh,\nI'm open to anything.\nBut, you know, if I think about\nmy own early history in\nthe Scaling Hypothesis,\nI was seeing the same\ndata others were seeing.", "mimetype": "text/plain", "start_char_idx": 95883, "end_char_idx": 99836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8": {"__data__": {"id_": "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d553720-ad0a-4c76-8aa1-5a0a9f2d7794", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d5f0dc9fdd05f0e21c7cd2d3e4b2faa36bfcd20a3186ebf81e410a997b75ee39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a945e53-ea82-43dd-983c-f40f259f8e51", "node_type": "1", "metadata": {}, "hash": "3f69b2be52f6a7bd5b7479edec50a22e3447324efff74b79eb6323bb5de14e16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- And you know, as to\nSteve Jobs, A players.\nA players wanna look around\nand see other A players is\nanother way of saying that.\nI don't know what that\nis about human nature,\nbut it is demotivating to see people\nwho are not obsessively driving\ntowards a singular mission.\nAnd it is, on the flip side of that,\nsuper motivating to see that.\nIt's interesting.\nWhat's it take to be a great AI researcher\nor engineer from everything you've seen,\nfrom working with so many amazing people?\n- Yeah, I think the number one quality,\nespecially on the research side,\nbut really both is open-mindedness.\nSounds easy to be open-minded, right?\nYou're just like, oh,\nI'm open to anything.\nBut, you know, if I think about\nmy own early history in\nthe Scaling Hypothesis,\nI was seeing the same\ndata others were seeing.\nI don't think I was\nlike a better programmer\nor better at coming up with research ideas\nthan any of the hundreds of\npeople that I worked with.\nIn some ways, I was worse.\nYou know, like I've never like, you know,\nprecise programming of like,\nyou know, finding the bug,\nwriting the GPU kernels.\nLike, I could point you to 100 people here\nwho are better at that than I am.\nBut the thing that I think I did have\nthat was different was\nthat I was just willing\nto look at something with new eyes, right?\nPeople said, oh, you know,\n\"We don't have the right algorithms yet.\nWe haven't come up with the\nright way to do things.\"\nAnd I was just like, oh, I don't know,\nlike, you know, this neural net\nhas like 30 billion,\n30 million parameters.\nLike, what if we gave\nit 50 million instead?\nLike, let's plot some graphs.\nLike that basic scientific\nmindset of like,\noh, man, like I just, like, you know,\nI see some variable that I could change.\nLike, what happens when it changes?\nLike, let's try these different things\nand like create a graph.\nFor even, this was like\nthe simplest thing in the world, right?\nChange the number of, you know,\nthis wasn't like PhD\nlevel experimental design.\nThis was like simple and stupid.\nLike, anyone could have done this\nif you just told them\nthat it was important.\nIt's also not hard to understand.\nYou didn't need to be\nbrilliant to come up with this.\nBut you put the two things together\nand, you know, some tiny number of people,\nsome single digit number of people\nhave driven forward the whole\nfield by realizing this.\nAnd you know, it's often like that.\nIf you look back at the\ndiscovery, you know,\nthe discoveries in history,\nthey're often like that.\nAnd so this open-mindedness\nand this willingness to see with new eyes\nthat often comes from\nbeing newer to the field.\nOften experience is a\ndisadvantage for this.\nThat is the most important thing.\nIt's very hard to look for and test for.\nBut I think it's the most important thing\nbecause when you find something,\nsome really new way of\nthinking about things,\nwhen you have the initiative to do that,\nit's absolutely transformative.\n- And also be able to do kind\nof rapid experimentation,\nand in the face of that,\nbe open-minded and curious\nand looking at the data,\njust these fresh eyes\nand seeing what is that\nit's actually saying.\nThat applies in mechanism\ninterpretability.\n- It's another example of this.\nLike some of the early work\nin mechanistic\ninterpretability, so simple,\nit's just no one thought to\ncare about this question before.\n- You said what it takes to\nbe a great AI researcher.\nCan we rewind the clock back?\nWhat advice would you give\nto people interested in AI?\nThey're young, looking forward to,\nhow can I make an impact on the world?\n- I think my number one piece of advice\nis to just start playing with the models.\nThis was actually, I worry a little,\nthis seems like obvious advice now.\nI think three years ago,\nit wasn't obvious and people started by,\noh, let me read the latest\nReinforcement Learning paper.\nLet me, you know, let me kind of, I mean,\nthat was really,\nand I mean, you should do that as well.\nBut now, you know, with wider availability\nof models and APIs, people\nare doing this more.", "mimetype": "text/plain", "start_char_idx": 99037, "end_char_idx": 103038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a945e53-ea82-43dd-983c-f40f259f8e51": {"__data__": {"id_": "0a945e53-ea82-43dd-983c-f40f259f8e51", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "212eaaa8-83f6-4c2b-a47a-b46fb5895ab8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0f55004fd1cad0765fe8a161685bc04a26e7fa3a654eb8a01ad14b07ee8e26db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d3efe32-8bcf-4a43-b02e-62941a1a24a5", "node_type": "1", "metadata": {}, "hash": "83b452ce5460bc0817174d4a9a4f46868a22ae8eb3453abd5009c8c4a710b839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like some of the early work\nin mechanistic\ninterpretability, so simple,\nit's just no one thought to\ncare about this question before.\n- You said what it takes to\nbe a great AI researcher.\nCan we rewind the clock back?\nWhat advice would you give\nto people interested in AI?\nThey're young, looking forward to,\nhow can I make an impact on the world?\n- I think my number one piece of advice\nis to just start playing with the models.\nThis was actually, I worry a little,\nthis seems like obvious advice now.\nI think three years ago,\nit wasn't obvious and people started by,\noh, let me read the latest\nReinforcement Learning paper.\nLet me, you know, let me kind of, I mean,\nthat was really,\nand I mean, you should do that as well.\nBut now, you know, with wider availability\nof models and APIs, people\nare doing this more.\nBut I think just experiential knowledge.\nThese models are new artifacts\nthat no one really understands,\nand so getting experience\nplaying with them.\nI would also say, again,\nin line with the like,\ndo something new, think\nin some new direction.\nLike there are all these things\nthat haven't been explored.\nLike for example,\nmechanistic interpretability\nis still very new.\nIt's probably better to work on that\nthan it is to work on\nnew model architectures\nbecause, you know, it's more\npopular than it was before.\nThere are probably like\n100 people working on it,\nbut there aren't like\n10,000 people working on it.\nAnd it's just this fertile area for study.\nLike, you know, there's so\nmuch like low hanging fruit.\nYou can just walk by and, you know,\nyou can just walk by\nand you can pick things.\nAnd the only reason, for whatever reason,\npeople aren't interested in it enough.\nI think there are some things around\nlong horizon learning\nand long horizon tasks\nwhere there's a lot to be done.\nI think evaluations are still,\nwe're still very early in our\nability to study evaluations,\nparticularly for dynamic\nsystems acting in the world.\nI think there's some\nstuff around multi-agent.\nSkate where the puck\nis going is my advice.\nAnd you don't have to be\nbrilliant to think of it.\nLike all the things that\nare gonna be exciting\nin five years, like people\neven mention them as like,\nyou know, conventional wisdom, but like,\nit's just somehow\nthere's this barrier that\npeople don't double down\nas much as they could,\nor they're afraid to do something\nthat's not the popular thing.\nI don't know why it happens,\nbut like, getting over that barrier,\nthat's the my number one piece of advice.\n- Let's talk if we could\na bit about post-training.\nSo it seems that the\nmodern post-training recipe\nhas a little bit of everything.\nSo supervised fine tuning, RLHF,\nthe Constitutional AI with RLAIF.\n- Best acronym.\n- It's, again, that name thing.\n- [Dario] RLAIF.\n(both laughing)\n- And then synthetic data,\nseems like a lot of synthetic data,\nor at least trying to figure out ways\nto have high quality synthetic data.\nSo what's the, if this is a secret sauce\nthat makes Anthropic Claude so incredible,\nhow much of the magic\nis in the pre-training?\nHow much of is in the post-training?\n- Yeah, I mean, so first of all,\nwe're not perfectly able\nto measure that ourselves.\n- [Lex] True.\n- You know, when you see\nsome great character ability,\nsometimes it's hard to tell\nwhether it came from\npre-training or post-training.\nWe've developed ways to try\nand distinguish between those two\nbut they're not perfect.\nYou know, the second thing\nI would say is, you know,\nwhen there is an advantage,\nand I think we've been pretty good\nin general at RL, perhaps the best.\nAlthough I don't know 'cause I don't see\nwhat goes on inside other companies.\nUsually it isn't, oh my God,\nwe have this secret magic method\nthat others don't have, right?\nUsually it's like, well, you know,\nwe got better at the infrastructure,\nso we could run it for longer.\nOr, you know, we were able\nto get higher quality data,\nor we were able to filter our data better,\nor we were able to, you know,\ncombine these methods in practice.", "mimetype": "text/plain", "start_char_idx": 102225, "end_char_idx": 106214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d3efe32-8bcf-4a43-b02e-62941a1a24a5": {"__data__": {"id_": "6d3efe32-8bcf-4a43-b02e-62941a1a24a5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a945e53-ea82-43dd-983c-f40f259f8e51", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "35929ce9ee239361f443e4013b48243db747c91be4a47c33f4473418c034af17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52bc1c89-f422-4dcc-b5a1-94697de969b7", "node_type": "1", "metadata": {}, "hash": "bb18338d3d94e15360522f7e9006dd6cdb83643efdc3d92cf290377ba53dc19e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- You know, when you see\nsome great character ability,\nsometimes it's hard to tell\nwhether it came from\npre-training or post-training.\nWe've developed ways to try\nand distinguish between those two\nbut they're not perfect.\nYou know, the second thing\nI would say is, you know,\nwhen there is an advantage,\nand I think we've been pretty good\nin general at RL, perhaps the best.\nAlthough I don't know 'cause I don't see\nwhat goes on inside other companies.\nUsually it isn't, oh my God,\nwe have this secret magic method\nthat others don't have, right?\nUsually it's like, well, you know,\nwe got better at the infrastructure,\nso we could run it for longer.\nOr, you know, we were able\nto get higher quality data,\nor we were able to filter our data better,\nor we were able to, you know,\ncombine these methods in practice.\nIt's usually some boring matter\nof kind of practiced and trade craft.\nSo, you know, when I think about\nhow to do something special\nin terms of how we train these\nmodels, both pre-training,\nbut even more so post-training, you know,\nI really think of it a little more, again,\nas like designing airplanes or cars.\nLike, you know, it's\nnot just like, oh, man,\nI have the blueprint.\nLike maybe that makes you\nmake the next airplane.\nBut like, there's some\ncultural trade craft\nof how we think about the design process\nthat I think is more\nimportant than, you know,\nthan any particular gizmo\nwe're able to invent.\n- Okay, well, let me ask you\nabout specific techniques.\nSo first on RLHF, why do\nyou think, just zooming out,\nintuition almost philosophy,\nwhy do you think RLHF works so well?\n- If I go back to like\nthe Scaling Hypothesis,\none of the ways to skate\nthe Scaling Hypothesis\nis if you train for X\nand you throw enough compute\nat it, then you get X.\nAnd so RLHF is good at doing\nwhat humans want the model to do,\nor at least to state it more precisely,\ndoing what humans who look at the model\nfor a brief period of time\nand consider different possible responses,\nwhat they prefer as the response,\nwhich is not perfect from both the safety\nand capabilities perspective,\nin that humans are often not\nable to perfectly identify\nwhat the model wants,\nand what humans want in\nthe moment may not be\nwhat they want in the long term.\nSo there's a lot of subtlety there,\nbut the models are good at, you know,\nproducing what the humans\nin some shallow sense want.\nAnd it actually turns out\nthat you don't even have\nto throw that much compute at\nit because of another thing,\nwhich is this thing about\na strong pre-trained model\nbeing halfway to anywhere.\nSo once you have the pre-trained model,\nyou have all the representations you need\nto get the model where you want it to go.\n- So do you think RLHF\nmakes the model smarter\nor just appears smarter to the humans?\n- I don't think it\nmakes the model smarter.\nI don't think it just makes\nthe model appear smarter.\nIt's like RLHF like bridges the gap\nbetween the human and the model, right?\nI could have something really smart\nthat like can't communicate at all, right?\nWe all know people like this,\npeople who are really smart,\nbut that, you know,\nyou can't understand what they're saying.\nSo I think RLHF just bridges that gap.\nI think it's not the\nonly kind of RL we do,\nit's not the only kind of RL\nthat will happen in the future.\nI think RL has the potential\nto make models smarter,\nto make them reason better,\nto make them operate better,\nto make them develop new skills even.\nAnd perhaps that could be done, you know,\neven in some cases with human feedback.\nBut the kind of RLHF we do today\nmostly doesn't do that yet,\nalthough we're very quickly\nstarting to be able to.\n- But it appears to sort of increase,\nif you look at the metric of helpfulness,\nit increases that.\n- It also increases, what was this word\nin Leopold's essay, unhobbling,\nwhere basically the models are hobbled\nand then you do various trainings\nto them to unhobble them.\nSo, you know, I like that word\n'cause it's like a rare word.", "mimetype": "text/plain", "start_char_idx": 105404, "end_char_idx": 109363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52bc1c89-f422-4dcc-b5a1-94697de969b7": {"__data__": {"id_": "52bc1c89-f422-4dcc-b5a1-94697de969b7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d3efe32-8bcf-4a43-b02e-62941a1a24a5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "46c94063a3e2bdc044b6bdfa6bef18ab16c912991a189550c8eea2190e479721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4670e407-2330-46b9-973c-f4f8520c614e", "node_type": "1", "metadata": {}, "hash": "864693ba3aa6c9207b4dcce1c9ac3178cf778a7342e40ae76611a5d6a60a3372", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I think it's not the\nonly kind of RL we do,\nit's not the only kind of RL\nthat will happen in the future.\nI think RL has the potential\nto make models smarter,\nto make them reason better,\nto make them operate better,\nto make them develop new skills even.\nAnd perhaps that could be done, you know,\neven in some cases with human feedback.\nBut the kind of RLHF we do today\nmostly doesn't do that yet,\nalthough we're very quickly\nstarting to be able to.\n- But it appears to sort of increase,\nif you look at the metric of helpfulness,\nit increases that.\n- It also increases, what was this word\nin Leopold's essay, unhobbling,\nwhere basically the models are hobbled\nand then you do various trainings\nto them to unhobble them.\nSo, you know, I like that word\n'cause it's like a rare word.\nBut so I think RLHF unhobbles\nthe models in some ways.\nAnd then there are other ways\nwhere that model hasn't yet been unhobbled\nand, you know, needs to unhobble.\n- If you can say in terms of cost,\nis pre-training the most expensive thing?\nOr is post-training creep up to that?\n- At the present moment,\nit is still the case that pre-training\nis the majority of the cost.\nI don't know what to expect in the future,\nbut I could certainly anticipate a future\nwhere post-training is\nthe majority of the cost.\n- In that future you anticipate,\nwould it be the humans or the AI\nthat's the cost of thing\nfor the post-training?\n- I don't think you can\nscale up humans enough\nto get high quality.\nAny kind of method that relies on humans\nand uses a large amount of compute,\nit's gonna have to rely on some\nscaled superposition method,\nlike you know, debate or\niterated amplification\nor something like that.\n- So on that super\ninteresting set of ideas\naround Constitutional AI,\ncan you describe what it is,\nas first detailed in December 2022 paper\nand beyond that, what is it?\n- Yes, so this was from two years ago.\nThe basic idea is, so we\ndescribe what RLHF is.\nYou have a model and it,\nyou know, spits out two,\nyou know, like you just\nsample from it twice,\nit spits out two possible responses,\nand you're like, \"Human, which\nresponse do you like better?\"\nOr another variant of it is,\n\"Rate this response on a\nscale of one to seven.\"\nSo that's hard because you need\nto scale up human interaction\nand it's very implicit, right?\nI don't have a sense of\nwhat I want the model to do.\nI just have a sense of\nlike what this average\nof a 1000 humans wants the model to do.\nSo two ideas.\nOne is, could the AI system itself\ndecide which response is better, right?\nCould you show the AI\nsystem these two responses\nand ask which response is better?\nAnd then second, well, what\ncriterion should the AI use?\nAnd so then there's this idea,\n'cause you have a single document,\na constitution, if you will, that says,\nthese are the principles the model\nshould be using to respond.\nAnd the AI system reads those,\nit reads those principles,\nas well as reading the\nenvironment and the response.\nAnd it says, well, how\ngood did the AI model do?\nIt's basically a form of self play.\nYou're kind of training\nthe model against itself.\nAnd so the AI gives the response\nand then you feed that back\ninto what's called the preference model,\nwhich in turn feeds the\nmodel to make it better.\nSo you have this triangle of like the AI,\nthe preference model, and the\nimprovement of the AI itself.\n- And we should say that\nin the constitution,\nthe set of principles are\nlike human interpretable.\nThey're like-\n- Yeah, yeah.\nIt's something both the human\nand the AI system can read.\nSo it has this nice kind of\ntranslatability or symmetry.\nYou know, in practice we\nboth use a model constitution\nand we use RLHF and we use\nsome of these other methods.\nSo it's turned into one tool in a toolkit\nthat both reduces the need for RLHF\nand increases the value we get\nfrom using each data point of RLHF.\nIt also interacts in interesting ways\nwith kind of future\nreasoning type RL methods.", "mimetype": "text/plain", "start_char_idx": 108585, "end_char_idx": 112498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4670e407-2330-46b9-973c-f4f8520c614e": {"__data__": {"id_": "4670e407-2330-46b9-973c-f4f8520c614e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52bc1c89-f422-4dcc-b5a1-94697de969b7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d5189d8dab8200dc9b036e8751d2863d1d18114fe59d94bfb67f46fa5ea5ac0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf", "node_type": "1", "metadata": {}, "hash": "36352d06b1377a6612357dd72de429338abd67cf4f9d0534db83942ae2e7c94e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So you have this triangle of like the AI,\nthe preference model, and the\nimprovement of the AI itself.\n- And we should say that\nin the constitution,\nthe set of principles are\nlike human interpretable.\nThey're like-\n- Yeah, yeah.\nIt's something both the human\nand the AI system can read.\nSo it has this nice kind of\ntranslatability or symmetry.\nYou know, in practice we\nboth use a model constitution\nand we use RLHF and we use\nsome of these other methods.\nSo it's turned into one tool in a toolkit\nthat both reduces the need for RLHF\nand increases the value we get\nfrom using each data point of RLHF.\nIt also interacts in interesting ways\nwith kind of future\nreasoning type RL methods.\nSo it's one tool in the toolkit,\nbut I think it is a very important tool.\n- Well, it's a compelling\none to us humans.\nYou know, thinking about\nthe founding fathers\nand the founding of the United States,\nthe natural question is,\nwho and how do you think it\ngets to define the constitution,\nthe set of principles in the constitution?\n- Yeah, so I'll give\nlike a practical answer\nand a more abstract answer.\nI think the practical\nanswer is like, look,\nin practice models get used by all kinds\nof different like customers, right?\nAnd so you can have this\nidea where, you know,\nthe model can have specialized\nrules or principles.\nYou know, we fine tune\nversions of models implicitly.\nWe've talked about doing it explicitly,\nhaving special principles that people\ncan build into the models.\nSo from a practical perspective,\nthe answer can be very\ndifferent from different people.\nYou know, customer\nservice agent, you know,\nbehaves very differently from a lawyer\nand obeys different principles.\nBut I think at the base of it,\nthere are specific principles\nthat models, you know, have to obey.\nI think a lot of them are things\nthat people would agree with.\nEveryone agrees that, you know,\nwe don't want models to\npresent these CBRN risks.\nI think we can go a little further\nand agree with some basic principles\nof democracy in the rule of law.\nBeyond that, it gets,\nyou know, very uncertain,\nand there, our goal is\ngenerally for the models\nto be more neutral,\nto not espouse a particular point of view,\nand, you know, more just\nbe kind of like wise agents\nor advisors that will help\nyou think things through\nand will, you know, present\npossible considerations,\nbut, you know, don't express,\nyou know, strong or specific opinions.\n- OpenAI released a model spec\nwhere it kind of clearly,\nconcretely defines\nsome of the goals of the model,\nand specific examples, like A/B,\nhow the model should behave.\nDo you find that interesting?\nBy the way, I should mention,\nI believe the brilliant John\nSchulman was a part of that.\nHe's now at Anthropic.\nDo you think this is a useful direction?\nMight Anthropic release\na model spec as well?\n- Yeah, so I think that's\na pretty useful direction.\nAgain, it has a lot in common\nwith Constitutional AI.\nSo again, another example of\nlike a race to the top, right?\nWe have something that's like we think,\nyou know, a better and more\nresponsible way of doing things.\nIt's also a competitive advantage.\nThen others kind of, you know,\ndiscover that it has advantages\nand then start to do that thing.\nWe then no longer have\nthe competitive advantage,\nbut it's good from the perspective\nthat now everyone has\nadopted a positive practice\nthat others were not adopting.\nAnd so our response to that is, well,\nlooks like we need a new\ncompetitive advantage\nin order to keep driving\nthis race upwards.\nSo that's how I generally feel about that.\nI also think every implementation\nof these things is different.\nSo, you know, there were some things\nin the model spec that were\nnot in Constitutional AI,\nand so, you know, we can\nalways adopt those things\nor, you know, at least learn from them.\nSo again, I think this is an example\nof like the positive dynamic\nthat I think we should all\nwant the field to have.\n- Let's talk about the incredible essay\n\"Machines of Loving Grace.\"\nI recommend everybody read it.\nIt's a long one.\n- It is rather long.\n- Yeah.\nIt's really refreshing\nto read concrete ideas\nabout what a positive future looks like.", "mimetype": "text/plain", "start_char_idx": 111815, "end_char_idx": 115950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf": {"__data__": {"id_": "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4670e407-2330-46b9-973c-f4f8520c614e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "12d91eddf3ad6bb3e64f25ca590349779ed2ad6794d3ee1e2c0d840b6e3e3f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec13eaeb-f179-444f-867e-fa8a0db4ec90", "node_type": "1", "metadata": {}, "hash": "536fc7636d25792e102e3889c2bd2d71fbb3d817a3c0216d0493124403179e0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so our response to that is, well,\nlooks like we need a new\ncompetitive advantage\nin order to keep driving\nthis race upwards.\nSo that's how I generally feel about that.\nI also think every implementation\nof these things is different.\nSo, you know, there were some things\nin the model spec that were\nnot in Constitutional AI,\nand so, you know, we can\nalways adopt those things\nor, you know, at least learn from them.\nSo again, I think this is an example\nof like the positive dynamic\nthat I think we should all\nwant the field to have.\n- Let's talk about the incredible essay\n\"Machines of Loving Grace.\"\nI recommend everybody read it.\nIt's a long one.\n- It is rather long.\n- Yeah.\nIt's really refreshing\nto read concrete ideas\nabout what a positive future looks like.\nAnd you took sort of a bold stance\nbecause like, it's very possible\nthat you might be wrong on the dates\nor specific applications.\n- Oh, yeah.\nI'm fully expecting to, you know,\nwill definitely be wrong\nabout all the details.\nI might be just spectacularly wrong\nabout the whole thing and people will,\nyou know, will laugh at me for years.\nThat's just how the future works. (laughs)\n- So you provided a bunch of concrete\npositive impacts of AI and how, you know,\nexactly a super intelligent\nAI might accelerate\nthe rate of breakthroughs in, for example,\nbiology and chemistry that would then lead\nto things like we cure most cancers,\nprevent all infectious disease,\ndouble the human lifespan and so on.\nSo let's talk about this essay.\nFirst, can you give a high\nlevel vision of this essay\nand what key takeaways\nthat people would have?\n- Yeah, I have spent a lot of time,\nand Anthropic has spent a lot\nof effort on like, you know,\nhow do we address the risks of AI, right?\nHow do we think about those risks?\nLike we're trying to do a\nrace to the top, you know,\nthat requires us to build\nall these capabilities\nand the capabilities are cool,\nbut you know, we're like,\na big part of what we're trying to do\nis like address the risks.\nAnd the justification for that is like,\nwell, you know, all these positive things,\nyou know, the market is this\nvery healthy organism, right?\nIt's gonna produce all\nthe positive things.\nThe risks, I don't know,\nwe might mitigate them, we might not.\nAnd so we can have more impact\nby trying to mitigate the risks.\nBut I noticed that one flaw\nin that way of thinking,\nand it's not a change in how\nseriously I take the risks.\nIt's maybe a change in\nhow I talk about them.\nIs that, you know, no\nmatter how kind of logical\nor rational that line of reasoning\nthat I just gave might be,\nif you kind of only talk about risks,\nyour brain only thinks about risks.\nAnd so I think it's\nactually very important\nto understand, what if things do go well?\nAnd the whole reason we're\ntrying to prevent these risks\nis not because we're afraid of technology,\nnot because we wanna slow it down.\nIt's because if we can get\nto the other side of these risks, right?\nIf we can run the gauntlet successfully,\nyou know, to put it in stark terms,\nthen on the other side of the gauntlet\nare all these great things\nand these things are worth fighting for,\nand these things can\nreally inspire people.\nAnd I think I imagine,\nbecause look, you have all\nthese investors, all these VCs,\nall these AI companies talking about\nall the positive benefits of AI.\nBut as you point out, it's weird,\nthere's actually a dearth\nof really getting specific about it.\nThere's a lot of like\nrandom people on Twitter\nlike posting these kind\nof like gleaning cities,\nand this just kind of\nlike vibe of like grind,\naccelerate harder, like\nkick out the, you know,\nit's just this very like\naggressive ideological.\nBut then you're like, well,\nwhat are you actually excited about?\nAnd so I figured that, you know,\nI think it would be\ninteresting and valuable\nfor someone who's actually coming\nfrom the risk side to try,\nand to try and really\nmake a try at explaining\nwhat the benefits are,\nboth because I think it's\nsomething we can all get behind\nand I want people to understand.", "mimetype": "text/plain", "start_char_idx": 115184, "end_char_idx": 119205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec13eaeb-f179-444f-867e-fa8a0db4ec90": {"__data__": {"id_": "ec13eaeb-f179-444f-867e-fa8a0db4ec90", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28fd18d5-e7a8-4833-bd8e-55f8d24f0dcf", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "48a01b8d5af9b50d6ad9a91886421eddb0dec072a43ce9f4542d8b28db44f611", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c", "node_type": "1", "metadata": {}, "hash": "44c6695cd975000bd3ee625cd784b2debdbb9e969f543cd20ee39e988bd8aa7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But as you point out, it's weird,\nthere's actually a dearth\nof really getting specific about it.\nThere's a lot of like\nrandom people on Twitter\nlike posting these kind\nof like gleaning cities,\nand this just kind of\nlike vibe of like grind,\naccelerate harder, like\nkick out the, you know,\nit's just this very like\naggressive ideological.\nBut then you're like, well,\nwhat are you actually excited about?\nAnd so I figured that, you know,\nI think it would be\ninteresting and valuable\nfor someone who's actually coming\nfrom the risk side to try,\nand to try and really\nmake a try at explaining\nwhat the benefits are,\nboth because I think it's\nsomething we can all get behind\nand I want people to understand.\nI want them to really understand that\nthis isn't doomers versus accelerationist.\nThis is that, if you\nhave a true understanding\nof where things are going with with AI,\nand maybe that's the more important axis.\nAI is moving fast versus\nAI is not moving fast,\nthen you really appreciate the benefits\nand you really, you want humanity,\nour civilization to seize those benefits,\nbut you also get very serious\nabout anything that could derail them.\n- So I think the starting point\nis to talk about what this powerful AI,\nwhich is the term you like to use,\nmost of the world uses AGI,\nbut you don't like the term\nbecause it's basically\nhas too much baggage,\nit's become meaningless.\nIt's like we're stuck with the terms,\nwhether we like them or not.\n- Maybe we're stuck\nwith the terms and my efforts\nto change them are futile.\n- It's admirable.\n- I'll tell you\nwhat else I don't, this is like\na pointless semantic point,\nbut I keep talking about it in public-\n- Back to naming again.\n- So I'm just gonna\ndo it once more.\nI think it's a little like,\nlet's say it was like 1995\nand Moore's law is making\nthe computers faster.\nAnd like for some reason,\nthere had been this like\nverbal tick that like,\neveryone was like, well,\nsomeday we're gonna have\nlike super computers\nand like supercomputers\nare gonna be able to do\nall these things that like,\nyou know, once we have supercomputers,\nwe'll be able to like sequence the genome,\nwe'll be able to do other things.\nAnd so, like one, it's true,\nthe computers are getting\nfaster, and as they get faster,\nthey're gonna be able to\ndo all these great things.\nBut there's no discreet point\nat which you had a supercomputer\nand previous computers were not.\nLike supercomputer is a term we use,\nbut like, it's a vague\nterm to just describe like\ncomputers that are faster\nthan what we have today.\nThere's no point at which\nyou pass the threshold\nand you're like, oh my God,\nwe're doing a totally new type\nof computation and new.\nAnd so I feel that way about AGI like,\nthere's just a smooth exponential\nand like if by AGI you mean\nlike AI is getting better and better,\nand like gradually, it's gonna do more\nand more of what humans do\nuntil it's gonna be smarter than humans,\nand then it's gonna get\nsmarter even from there\nthen yes, I believe in AGI.\nBut if AGI is some\ndiscreet or separate thing,\nwhich is the way people\noften talk about it,\nthen it's kind of a meaningless buzzword.\n- Yeah, I mean, to me it's\njust sort of a platonic form\nof a powerful AI, exactly\nhow you define it.\nI mean, you define it very nicely.\nSo on the intelligence axis,\njust on pure intelligence,\nit's smarter than a Nobel Prize winner,\nas you describe, across\nmost relevant disciplines.\nSo, okay, that's just intelligence.\nSo it's both in creativity\nand be able to generate new ideas,\nall that kind of stuff,\nin every discipline, Nobel Prize winner,\nokay, in their prime. (laughs)\nIt can use every modality,\nso that's kind of self-explanatory,\nbut just to operate across all\nthe modalities of the world.\nIt can go off for many hours,\ndays and weeks to do tasks,\nand do its own sort of detailed planning\nand only ask you help when it's needed.", "mimetype": "text/plain", "start_char_idx": 118504, "end_char_idx": 122360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c": {"__data__": {"id_": "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec13eaeb-f179-444f-867e-fa8a0db4ec90", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bdff93d39f8a3bacb4f6aa12c89be40603bb0daf63847f115e6bb9b37e5c4546", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "298d4478-3c0d-4bbd-a846-66f38b725174", "node_type": "1", "metadata": {}, "hash": "51494d5df41e51a5a7adf2f90ff97dfe05688e503d2309d5ec811dc94c14ed06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah, I mean, to me it's\njust sort of a platonic form\nof a powerful AI, exactly\nhow you define it.\nI mean, you define it very nicely.\nSo on the intelligence axis,\njust on pure intelligence,\nit's smarter than a Nobel Prize winner,\nas you describe, across\nmost relevant disciplines.\nSo, okay, that's just intelligence.\nSo it's both in creativity\nand be able to generate new ideas,\nall that kind of stuff,\nin every discipline, Nobel Prize winner,\nokay, in their prime. (laughs)\nIt can use every modality,\nso that's kind of self-explanatory,\nbut just to operate across all\nthe modalities of the world.\nIt can go off for many hours,\ndays and weeks to do tasks,\nand do its own sort of detailed planning\nand only ask you help when it's needed.\nIt can use, this is\nactually kind of interesting\nbecause I think in the essay you said,\nI mean, again, it's a bet, that\nit's not gonna be embodied,\nbut it can control embodied tools.\nSo it can control tools,\nrobots, laboratory equipment.\nThe resources used to train\nit can then be repurposed\nto run millions of copies of it.\nAnd each of those copies\nwould be independent,\nthey could do their own independent work.\nSo you can do the cloning\nof the intelligence system software.\n- Yeah, yeah, I mean, you might imagine\nfrom outside the field that like,\nthere's only one of these, right?\nThat like, you made it,\nyou've only made one.\nBut the truth is that like,\nthe scale up is very quick.\nLike we do this today, we make a model,\nand then we deploy thousands,\nmaybe tens of thousands\nof instances of it.\nI think by the time, you know,\ncertainly within two to three years,\nwhether we have these\nsuper powerful AIs or not,\nclusters are gonna get to the size\nwhere you'll be able to\ndeploy millions of these\nand they'll be, you\nknow, faster than humans.\nAnd so if your picture is,\noh, we'll have one and it'll\ntake a while to make them.\nMy point, there was no,\nactually you have millions\nof them right away.\n- And in general they can learn and act\n10 to 100 times faster than humans.\nSo that's a really nice\ndefinition of powerful AI, okay.\nSo that, but you also write\nthat clearly such an\nentity would be capable\nof solving very difficult\nproblems very fast,\nbut it is not trivial\nto figure out how fast.\nTwo extreme positions\nboth seem false to me.\nSo the singularity is on the one extreme\nand the opposite on the other extreme.\nCan you describe each of the extremes?\n- Yeah, so.\n- And why.\n- So yeah, let's describe the extreme.\nSo like one extreme would be, well, look,\nyou know, if we look at kind\nof evolutionary history,\nlike there was this big\nacceleration where, you know,\nfor hundreds of thousands of years,\nwe just had like, you know,\nsingle celled organisms,\nand then we had mammals,\nand then we had apes,\nand then that quickly turned to humans.\nHumans quickly built\nindustrial civilization.\nAnd so this is gonna keep speeding up\nand there's no ceiling at the human level.\nOnce models get much,\nmuch smarter than humans,\nthey'll get really good at\nbuilding the next models,\nand you know, if you write down\nlike a simple differential equation,\nlike this is an exponential.\nAnd so what's gonna happen is that\nmodels will build faster models,\nmodels will build faster models,\nand those models will build, you know,\nnanobots that can like take over the world\nand produce much more energy\nthan you could produce otherwise.\nAnd so if you just kind of like solve\nthis abstract differential equation,\nthen like five days after we, you know,\nwe build the first AI\nthat's more powerful than humans,\nthen, you know, like\nthe world will be filled\nwith these AIs and every\npossible technology\nthat could be invented\nlike will be invented.\nI'm caricaturing this a little bit,\nbut, you know, I think that's one extreme.\nAnd the reason that I think\nthat's not the case is that,\none, I think they just neglect\nlike the laws of physics.\nLike it's only possible to do things\nso fast in the physical world.\nLike some of those loops go through,\nyou know, producing faster hardware.\nIt takes a long time to\nproduce faster hardware.", "mimetype": "text/plain", "start_char_idx": 121622, "end_char_idx": 125676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "298d4478-3c0d-4bbd-a846-66f38b725174": {"__data__": {"id_": "298d4478-3c0d-4bbd-a846-66f38b725174", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49ee5b6b-ecb4-4ad1-aa50-c5c728e9fd7c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bd1bb7a6f03357d260ee5dda11fbbe730ef82346369e56f3c6efa957905860b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26bb97a1-8644-4569-9ba3-62f71311f640", "node_type": "1", "metadata": {}, "hash": "9237eb3fd018371d2aaef64c33e4041b0438211da47885f44bdcc99652b79592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so if you just kind of like solve\nthis abstract differential equation,\nthen like five days after we, you know,\nwe build the first AI\nthat's more powerful than humans,\nthen, you know, like\nthe world will be filled\nwith these AIs and every\npossible technology\nthat could be invented\nlike will be invented.\nI'm caricaturing this a little bit,\nbut, you know, I think that's one extreme.\nAnd the reason that I think\nthat's not the case is that,\none, I think they just neglect\nlike the laws of physics.\nLike it's only possible to do things\nso fast in the physical world.\nLike some of those loops go through,\nyou know, producing faster hardware.\nIt takes a long time to\nproduce faster hardware.\nThings take a long time.\nThere's this issue of complexity,\nlike, I think no matter how smart you are,\nlike, you know, people talk about,\noh, we can make models\nof biological systems\nthat'll do everything\nthe biological systems.\nLook, I think computational\nmodeling can do a lot.\nI did a lot of computational modeling\nwhen I worked in biology, but like, just,\nthere are a lot of things\nthat you can't predict\nhow they're, you know,\nthey're complex enough\nthat like just iterating,\njust running the experiment\nis gonna beat any modeling,\nno matter how smart the\nsystem doing the modeling is.\n- Well, even if it's not interacting\nwith the physical world,\njust the modeling is gonna be hard?\n- Yeah, I think, well, the\nmodeling's gonna be hard\nand getting the model to match\nthe physical world is gonna be hard.\n- All right, so he does have to interact\nwith the physical world to verify.\n- Yeah, but it's just, you know,\nyou just look at even\nthe simplest problems.\nLike, you know, I think I\ntalk about like, you know,\nthe three body problem or\nsimple chaotic prediction, like,\nyou know, or like predicting the economy.\nIt's really hard to predict\nthe economy two years out.\nLike maybe the case is like,\nyou know, normal, you know,\nhumans can predict what's gonna happen\nin the economy next quarter,\nor they can't really do that.\nMaybe a AI system that's, you know,\na zillion times smarter\ncan only predict it\nout a year or something\ninstead of, you know.\nYou have these kind of\nexponential increase\nin computer intelligence\nfor linear increase\nin ability to predict.\nSame with, again, like, you know,\nbiological molecules,\nmolecules interacting.\nYou don't know what's gonna happen\nwhen you perturb a complex system.\nYou can find simple parts in it\nif you're smarter, you're better\nat finding these simple parts.\nAnd then I think human institutions.\nHuman institutions are\njust, are really difficult.\nLike, you know, it's\nbeen hard to get people,\nI won't give specific examples,\nbut it's been hard to get people to adopt\neven the technologies\nthat we've developed,\neven ones where the\ncase for their efficacy\nis very, very strong.\nYou know, people have concerns.\nThey think things are conspiracy theories.\nLike it's just been,\nit's been very difficult.\nIt's also been very\ndifficult to get, you know,\nvery simple things through\nthe regulatory system, right?\nI think, and you know, I don't\nwanna disparage anyone who,\nyou know, works in regulatory\nsystems of any technology.\nThere are hard trade-offs\nthey have to deal with.\nThey have to save lives.\nBut the system as a whole\nI think makes some obvious trade-offs\nthat are very far from\nmaximizing human welfare.\nAnd so if we bring AI systems into this,\nyou know, into these human systems,\noften the level of intelligence\nmay just not be the\nlimiting factor, right?\nIt just may be that it takes\na long time to do something.\nNow, if the AI system\ncircumvented all governments,\nif it just said \"I'm dictator of the world\nand I'm gonna do whatever,\"\nsome of these things it could do.\nAgain, the things having\nto do with complexity,\nI still think a lot of\nthings would take a while.\nI don't think it helps that the AI systems\ncan produce a lot of\nenergy or go to the Moon.\nLike some people in comments responded\nto the essay saying the AI system\ncan produce a lot of energy\nin smarter AI systems.\nThat's missing the point.", "mimetype": "text/plain", "start_char_idx": 124985, "end_char_idx": 129040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26bb97a1-8644-4569-9ba3-62f71311f640": {"__data__": {"id_": "26bb97a1-8644-4569-9ba3-62f71311f640", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "298d4478-3c0d-4bbd-a846-66f38b725174", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "679e4a66a07b4e33cc0bfea9ae0e2c20e5a6cfc69bd2d2cef9a6f30e63a06a19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33b67e10-82f4-471e-b856-8851eb320b99", "node_type": "1", "metadata": {}, "hash": "0ff32bb980c05f7d527b980fbe99b11e8bb7318626b7052d31c314e848dc9867", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But the system as a whole\nI think makes some obvious trade-offs\nthat are very far from\nmaximizing human welfare.\nAnd so if we bring AI systems into this,\nyou know, into these human systems,\noften the level of intelligence\nmay just not be the\nlimiting factor, right?\nIt just may be that it takes\na long time to do something.\nNow, if the AI system\ncircumvented all governments,\nif it just said \"I'm dictator of the world\nand I'm gonna do whatever,\"\nsome of these things it could do.\nAgain, the things having\nto do with complexity,\nI still think a lot of\nthings would take a while.\nI don't think it helps that the AI systems\ncan produce a lot of\nenergy or go to the Moon.\nLike some people in comments responded\nto the essay saying the AI system\ncan produce a lot of energy\nin smarter AI systems.\nThat's missing the point.\nThat kind of cycle doesn't\nsolve the key problems\nthat I'm talking about here.\nSo I think a bunch of people\nmissed the point there.\nBut even if it were completely on the line\nand, you know, could get around\nall these human obstacles,\nit would have trouble.\nBut again, if you want\nthis to be an AI system\nthat doesn't take over the world,\nthat doesn't destroy humanity,\nthen basically, you know, it's gonna need\nto follow basic human laws, right?\nYou know, if we want to\nhave an actually good world,\nlike we're gonna have to have an AI system\nthat interacts with humans,\nnot one that kind of\ncreates its own legal system\nor disregards all the laws or all of that.\nSo as inefficient as these\nprocesses are, you know,\nwe're gonna have to deal with them\nbecause there needs to be some popular\nand democratic legitimacy\nin how these systems are rolled out.\nWe can't have a small group of people\nwho are developing these systems say\nthis is what's best for everyone, right?\nI think it's wrong,\nand I think in practice,\nit's not gonna work anyway.\nSo you put all those things together\nand, you know, we're not gonna,\nyou know, change the world\nand upload everyone in five minutes.\nI just, I don't think it,\nA, I don't think it's gonna happen,\nand B, you know,\nto the extent that it could happen,\nit's not the way to lead to a good world.\nSo that's on one side.\nOn the other side, there's\nanother set of perspectives,\nwhich I have actually in\nsome ways more sympathy for,\nwhich is, look, we've seen\nbig productivity increases before, right?\nYou know, economists are familiar\nwith studying the productivity increases\nthat came from the computer revolution\nand internet revolution.\nAnd generally, those\nproductivity increases\nwere underwhelming.\nThey were less than you might imagine.\nThere was a quote from Robert Solow,\n\"You see the computer\nrevolution everywhere\nexcept the productivity statistics.\"\nSo why is this the case?\nPeople point to the structure of firms,\nthe structure of enterprises.\nYou know, how slow it's been to roll out\nour existing technology to\nvery poor parts of the world,\nwhich I talk about in the essay, right?\nHow do we get these technologies\nto the poorest parts of the world\nthat are behind on cell phone technology,\ncomputers, medicine, let alone, you know,\nnewfangled AI that\nhasn't been invented yet.\nSo you could have a\nperspective that's like, well,\nthis is amazing technically,\nbut it's all a nothing burger.\nYou know, I think Tyler Cowen,\nwho wrote something in response\nto my essay, has that perspective.\nI think he thinks the radical\nchange will happen eventually,\nbut he thinks it'll take 50 or 100 years.\nAnd you could have even\nmore static perspectives\non the whole thing.\nI think there's some truth to it.\nI think the timescale is just too long.\nAnd I can see it, I can actually see\nboth sides with today's AI.\nSo, you know, a lot of our\ncustomers are large enterprises\nwho are used to doing\nthings a certain way.\nI've also seen it in talking\nto governments, right?\nThose are prototypical,\nyou know, institutions,\nentities that are slow to change.\nBut the dynamic I see over\nand over again is, yes,\nit takes a long time to move the ship.\nYes, there's a lot of resistance\nand lack of understanding.", "mimetype": "text/plain", "start_char_idx": 128222, "end_char_idx": 132269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33b67e10-82f4-471e-b856-8851eb320b99": {"__data__": {"id_": "33b67e10-82f4-471e-b856-8851eb320b99", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26bb97a1-8644-4569-9ba3-62f71311f640", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c3f2063ae3613a51c532319d2465480beeb0b85303b2089bf0e21cd9ffd54066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08f218c0-c59d-4827-b971-6cbc7c6bc241", "node_type": "1", "metadata": {}, "hash": "9b1b2f685d495ac144aaf0d3f2f454443569ce5ea2fed10559023a56dcf353d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You know, I think Tyler Cowen,\nwho wrote something in response\nto my essay, has that perspective.\nI think he thinks the radical\nchange will happen eventually,\nbut he thinks it'll take 50 or 100 years.\nAnd you could have even\nmore static perspectives\non the whole thing.\nI think there's some truth to it.\nI think the timescale is just too long.\nAnd I can see it, I can actually see\nboth sides with today's AI.\nSo, you know, a lot of our\ncustomers are large enterprises\nwho are used to doing\nthings a certain way.\nI've also seen it in talking\nto governments, right?\nThose are prototypical,\nyou know, institutions,\nentities that are slow to change.\nBut the dynamic I see over\nand over again is, yes,\nit takes a long time to move the ship.\nYes, there's a lot of resistance\nand lack of understanding.\nBut the thing that makes\nme feel that progress\nwill in the end happen moderately fast,\nnot incredibly fast, but moderately fast,\nis that you talk to, what I find\nis I find over and over again,\nagain, in large companies,\neven in governments,\nwhich have been actually\nsurprisingly forward-leaning,\nyou find two things that\nmove things forward.\nOne, you find a small fraction\nof people within a company,\nwithin a government who\nreally see the big picture,\nwho see the whole Scaling Hypothesis,\nwho understand where AI is\ngoing, or at least understand\nwhere it's going within their industry.\nAnd there are a few people like that\nwithin the current US government\nwho really see the whole picture.\nAnd those people see that\nthis is the most important\nthing in the world,\nand so they agitate for it.\nAnd the thing, they alone\nare not enough to succeed\nbecause they're a small set of people\nwithin a large organization.\nBut as the technology starts to roll out,\nas it succeeds in some places,\nin the folks who are\nmost willing to adopt it,\nthe specter of competition\ngives them a wind at their backs\nbecause they can point within\ntheir large organization,\nthey can say, look, these other\nguys are doing this, right?\nYou know, one bank can say, look,\nthis newfangled hedge\nfund is doing this thing.\nThey're going to eat our lunch.\nIn the US, we can say we're afraid\nChina's gonna get there before we are.\nAnd that combination, the\nspecter of competition\nplus a few visionaries within these,\nyou know, within the organizations\nthat in many ways are sclerotic,\nyou put those two things together\nand it actually makes something happen.\nI mean, it's interesting.\nIt's a balanced fight between the two\nbecause inertia is very powerful.\nBut eventually over enough time,\nthe innovative approach breaks through.\nAnd I've seen that happen.\nI've seen the arc of\nthat over and over again.\nAnd it's like the barriers are there.\nThe barriers to progress, the complexity,\nnot knowing how to use the model\nor how to deploy them are there,\nand for a bit, it seems like\nthey're gonna last forever,\nlike change doesn't happen.\nBut then eventually change happens\nand always comes from a few people.\nI felt the same way when I was an advocate\nof the Scaling Hypothesis\nwithin the AI field itself\nand others didn't get it.\nIt felt like no one would ever get it.\nThen it felt like we had a\nsecret almost no one ever had,\nand then a couple years later,\neveryone has the secret.\nAnd so I think that's how it's gonna go\nwith deployment to AI in the world.\nIt's gonna, the barriers are\ngonna fall apart gradually\nand then all at once.\nAnd so I think this is gonna be more,\nand this is just an instinct.\nI could easily see how I'm wrong.\nI think it's gonna be more like\n5 or 10 years, as I say in the essay,\nthan it's gonna be 50 or 100 years.\nI also think it's gonna be\n5 or 10 years more than it's gonna be,\nyou know, 5 or 10 hours,\nbecause I've just seen\nhow human systems work.\nAnd I think a lot of these people\nwho write down these\ndifferential equations\nwho say AI is gonna make more powerful AI,\nwho can't understand how it\ncould possibly be the case\nthat these things won't change so fast,\nI think they don't\nunderstand these things.", "mimetype": "text/plain", "start_char_idx": 131474, "end_char_idx": 135475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "08f218c0-c59d-4827-b971-6cbc7c6bc241": {"__data__": {"id_": "08f218c0-c59d-4827-b971-6cbc7c6bc241", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33b67e10-82f4-471e-b856-8851eb320b99", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "384460b2a88847e027d68850420b5ab45efa003cd44d5460e156bbaf3efadb7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee", "node_type": "1", "metadata": {}, "hash": "004e8abeb6d5683e6e1d84dca8a0363e60338ecd8ef882a06a840dc544175911", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's gonna, the barriers are\ngonna fall apart gradually\nand then all at once.\nAnd so I think this is gonna be more,\nand this is just an instinct.\nI could easily see how I'm wrong.\nI think it's gonna be more like\n5 or 10 years, as I say in the essay,\nthan it's gonna be 50 or 100 years.\nI also think it's gonna be\n5 or 10 years more than it's gonna be,\nyou know, 5 or 10 hours,\nbecause I've just seen\nhow human systems work.\nAnd I think a lot of these people\nwho write down these\ndifferential equations\nwho say AI is gonna make more powerful AI,\nwho can't understand how it\ncould possibly be the case\nthat these things won't change so fast,\nI think they don't\nunderstand these things.\n- So what to you is the timeline\nto where we achieve AGI, AKA powerful AI,\nAKA super useful AI?\n- Useful. (laughs)\nI'm gonna start calling it that.\n- It's a debate about naming.\nYou know, on pure intelligence,\nit can smarter than a Nobel Prize winner\nin every relevant discipline\nand all the things we've said.\nModality, can go and do stuff\non its own for days, weeks,\nand do biology experiments on its own.\nIn one, you know what,\nlet's just stick to biology\n'cause you sold me on the whole biology\nand health section,\nthat's so exciting from,\njust I was getting giddy from\na scientific perspective.\nIt made me wanna be a biologist.\n- It's almost, it's so, no, no,\nthis was the feeling I\nhad when I was writing it\nthat it's like this would\nbe such a beautiful future\nif we can just make it happen, right?\nIf we can just get the\nlandmines out of the way\nand make it happen, there's so much,\nthere's so much beauty\nand elegance and moral force\nbehind it if we can just.\nAnd it's something we should\nall be able to agree on, right?\nLike, as much as we fight\nabout all these political questions,\nis this something that could\nactually bring us together?\nBut you were asking when\nwhen will we get this?\n- When? When do you think?\nJust so putting numbers on that.\n- So you know, this is, of course,\nthe thing I've been grappling\nwith for many years,\nand I'm not at all confident.\nEvery time, if I say 2026 or 2027,\nthere will be like a zillion\nlike people on Twitter who will be like,\n\"AI CEO said 2026,\"\nand it'll be repeated for\nlike the next two years\nthat like this is definitely\nwhen I think it's gonna happen.\nSo whoever's extorting these clips\nwill crop out the thing I just said\nand only say the thing I'm about to say,\nbut I'll just say it anyway.\n- [Lex] Have fun with it.\n- So, if you extrapolate the curves\nthat we've had so far, right?\nIf you say, well, I don't know,\nwe're starting to get to like PhD level,\nand last year we were\nat undergraduate level,\nand the year before we were at like\nthe level of a high school student.\nAgain, you can quibble with at what tasks\nand for what, we're\nstill missing modalities,\nbut those are being added,\nlike computer use was added,\nlike image in was added,\nlike image generation has been added.\nIf you just kind of like, and\nthis is totally unscientific,\nbut if you just kind of like eyeball\nthe rate at which these\ncapabilities are increasing,\nit does make you think\nthat we'll get there by 2026 or 2027.\nAgain, lots of things could derail it.\nWe could run out of data.\nYou know, we might not be able\nto scale clusters as much as we want.\nLike, you know, maybe Taiwan\ngets blown up or something\nand, you know, then we can't produce\nas many GPUs as we want.\nSo there are all kinds of things\nthat could derail the whole process.\nSo I don't fully believe the\nstraight line extrapolation,\nbut if you believe the\nstraight line extrapolation,\nwe'll get there in 2026 or 2027.\nI think the most likely is that\nthere's some mild delay relative to that.\nI don't know what that delay is,\nbut I think it could happen on schedule.", "mimetype": "text/plain", "start_char_idx": 134792, "end_char_idx": 138539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee": {"__data__": {"id_": "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08f218c0-c59d-4827-b971-6cbc7c6bc241", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "35c3f566b3e3781a612e88ff7a12acad9297c4868d1940ae38e693ca45d10610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "893295f1-1612-4b49-9e50-ce5125bd1d90", "node_type": "1", "metadata": {}, "hash": "f3d190382c0147c9a9da38c47f67e85cd112156e4cdb6423e7c749d3748dddf7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, lots of things could derail it.\nWe could run out of data.\nYou know, we might not be able\nto scale clusters as much as we want.\nLike, you know, maybe Taiwan\ngets blown up or something\nand, you know, then we can't produce\nas many GPUs as we want.\nSo there are all kinds of things\nthat could derail the whole process.\nSo I don't fully believe the\nstraight line extrapolation,\nbut if you believe the\nstraight line extrapolation,\nwe'll get there in 2026 or 2027.\nI think the most likely is that\nthere's some mild delay relative to that.\nI don't know what that delay is,\nbut I think it could happen on schedule.\nI think there could be a mild delay.\nI think there are still worlds\nwhere it doesn't happen in 100 years.\nThe number of those worlds\nis rapidly decreasing.\nWe are rapidly running out\nof truly convincing blockers,\ntruly compelling reasons why\nthis will not happen\nin the next few years.\nThere were a lot more in 2020,\nalthough my guess, my hunch at that time\nwas that we'll make it\nthrough all those blockers.\nSo sitting as someone who has seen\nmost of the blockers\ncleared out of the way,\nI kind of suspect, my\nhunch, my suspicion is that\nthe rest of them will not block us.\nBut, you know, look,\nat the end of the day,\nlike I don't wanna represent this\nas a scientific prediction.\nPeople call them scaling laws.\nThat's a misnomer, like\nMoore's law is a misnomer.\nMoore's laws, scaling laws,\nthey're not laws of the universe.\nThey're empirical regularities.\nI am going to bet in\nfavor of them continuing,\nbut I'm not certain of that.\n- So you extensively describe\nsort of the compressed 21st century,\nhow AGI will help set forth\na chain of breakthroughs in biology\nand medicine that help us\nin all these kinds of\nways that I mentioned.\nSo how do you think, what are\nthe early steps it might do?\nAnd by the way, I asked Claude\ngood questions to ask you,\nand Claude told me to ask,\n\"What do you think is a typical day\nfor a biologists working on\nAGI look like in this future?\"\n- Yeah, yeah.\n- Claude is curious.\n- Well, let me start\nwith your first questions\nand then I'll answer that.\nClaude wants to know what's\nin his future, right?\n- Exactly.\n- Who am I gonna be working with?\n- Exactly.\n- So I think one of the things\nI went hard on, when I went\nhard on in the essay is,\nlet me go back to this idea of,\nbecause it's really had, you know,\nhad an impact on me.\nThis idea that within large\norganizations and systems,\nthere end up being a few people\nor a few new ideas who\nkind of cause things\nto go in a different direction\nthan they would've before,\nwho kind of disproportionately\naffect the trajectory.\nThere's a bunch of kind of the\nsame thing going on, right?\nIf you think about the health world,\nthere's like, you know,\ntrillions of dollars\nto pay out Medicare and you\nknow, other health insurance,\nand then the NIH is is 100 billion.\nAnd then if I think of like the few things\nthat have really revolutionized anything,\nit could be encapsulated in\na small fraction of that.\nAnd so when I think of like,\nwhere will AI have an impact?\nI'm like, can AI turn that small fraction\ninto a much larger fraction\nand raise its quality?\nAnd within biology, my\nexperience within biology\nis that the biggest problem of biology\nis that you can't see what's going on.\nYou have very little ability\nto see what's going on\nand even less ability to change it, right?\nWhat you have is this, like from this,\nyou have to infer that\nthere's a bunch of cells\nthat within each cell is, you know,\n3 billion base pairs of DNA\nbuilt according to a genetic code.\nAnd you know, there\nare all these processes\nthat are just going on\nwithout any ability of us as,\nyou know, unaugmented humans to affect it.\nThese cells are dividing.\nMost of the time that's healthy,\nbut sometimes that process\ngoes wrong and that's cancer.\nThe cells are aging,\nyour skin may change color,\ndevelops wrinkles as you age,\nand all of this is determined\nby these processes.", "mimetype": "text/plain", "start_char_idx": 137927, "end_char_idx": 141862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "893295f1-1612-4b49-9e50-ce5125bd1d90": {"__data__": {"id_": "893295f1-1612-4b49-9e50-ce5125bd1d90", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e509ece9-5cd1-44f5-a0b7-66a5a5a221ee", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bfbda187454dea7da5b9ba82882f7ff00e57fe7035795a1a30e10878001da7c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfee077b-5da4-454c-957f-a2eaf0143bb8", "node_type": "1", "metadata": {}, "hash": "5ad5239333e0b050ec3c8e282008e4342a113c15d603dc8d46fdcd5882356439", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And within biology, my\nexperience within biology\nis that the biggest problem of biology\nis that you can't see what's going on.\nYou have very little ability\nto see what's going on\nand even less ability to change it, right?\nWhat you have is this, like from this,\nyou have to infer that\nthere's a bunch of cells\nthat within each cell is, you know,\n3 billion base pairs of DNA\nbuilt according to a genetic code.\nAnd you know, there\nare all these processes\nthat are just going on\nwithout any ability of us as,\nyou know, unaugmented humans to affect it.\nThese cells are dividing.\nMost of the time that's healthy,\nbut sometimes that process\ngoes wrong and that's cancer.\nThe cells are aging,\nyour skin may change color,\ndevelops wrinkles as you age,\nand all of this is determined\nby these processes.\nAll these proteins being produced,\ntransported to various parts of the cells,\nbinding to each other.\nAnd in our initial state about biology,\nwe didn't even know that\nthese cells existed.\nWe had to invent microscopes\nto observe the cells.\nWe had to invent more\npowerful microscopes to see,\nyou know, below the level of the cell\nto the level of molecules.\nWe had to invent X-ray\ncrystallography to see the DNA.\nWe had to invent gene\nsequencing to read the DNA.\nNow, you know, we had to invent\nprotein folding technology to, you know,\nto predict how it would fold\nand how these things bind to each other.\nYou know, we had to\ninvent various techniques\nfor now we can edit the\nDNA as of, you know,\nwith CRISPR, as of the last 12 years.\nSo the whole history of biology,\na whole big part of the history\nis basically our ability to read\nand understand what's going on,\nand our ability to reach in\nand selectively change things.\nAnd my view is that there's so much more\nwe can still do there, right?\nYou can do CRISPR but you can\ndo it for your whole body.\nLet's say I wanna do it for\none particular type of cell\nand I want the rate of targeting\nthe wrong cell to be very low.\nThat's still a challenge.\nThat's still things people are working on.\nThat's what we might need\nfor gene therapy for certain diseases.\nAnd so the reason I'm saying all of this,\nand it goes beyond this to, you know,\nto gene sequencing, to new\ntypes of nano materials\nfor observing what's going on\ninside cells for, you know,\nantibody drug conjugates.\nThe reason I'm saying all this\nis that this could be a leverage point\nfor the AI systems, right?\nThat the number of such inventions,\nit's in the mid double\ndigits or something,\nyou know, mid double digits.\nMaybe low triple digits\nover the history of biology.\nLet's say I have a million\nof these AIs like, you know,\ncan they discover thousand,\nyou know, working together,\ncan they discover thousands\nof these very quickly?\nAnd does that provide a huge lever,\ninstead of trying to\nleverage the, you know,\n2 trillion a year we spend on,\nyou know, Medicare or whatever,\ncan we leverage the 1 billion a year,\nyou know, that's spent to discover,\nbut with much higher quality?\nAnd so what is it like, you know,\nbeing a scientist that\nworks with an AI system?\nThe way I think about\nit actually is, well,\nso I think in the early stages,\nthe AIs are gonna be like grad students.\nYou're gonna give them a\nproject, you're gonna say,\nyou know, I'm the experienced biologist,\nI've set up the lab.\nThe biology professor\nor even the grad students\nthemselves will say,\nhere's what you can do with an AI,\nyou know, like AI system.\nI'd like to study this.\nAnd you know, the AI system,\nit has all the tools.\nIt can like look up all the\nliterature to decide what to do.\nIt can look at all the equipment.\nIt can go to a website and say,\nhey, I'm gonna go to,\nyou know, Thermo Fisher\nor, you know, whatever the\nlab equipment company is,\ndominant lab equipment company is today.\nIn my time, it was Thermo Fisher.\nYou know, I'm gonna order\nthis new equipment to do this.\nI'm gonna run my experiments.\nI'm gonna, you know, write up\na report about my experiments.", "mimetype": "text/plain", "start_char_idx": 141070, "end_char_idx": 145016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfee077b-5da4-454c-957f-a2eaf0143bb8": {"__data__": {"id_": "bfee077b-5da4-454c-957f-a2eaf0143bb8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "893295f1-1612-4b49-9e50-ce5125bd1d90", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ddd038a4b9144a028653585c4169a3fc864fa1cefd96873a7f97836f72122db7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2f62c5c-2687-4c48-a1fd-0601b9724538", "node_type": "1", "metadata": {}, "hash": "a2d35366fa843fee24bd7082958e997d04c364401a3a2a5826014cba825569ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The biology professor\nor even the grad students\nthemselves will say,\nhere's what you can do with an AI,\nyou know, like AI system.\nI'd like to study this.\nAnd you know, the AI system,\nit has all the tools.\nIt can like look up all the\nliterature to decide what to do.\nIt can look at all the equipment.\nIt can go to a website and say,\nhey, I'm gonna go to,\nyou know, Thermo Fisher\nor, you know, whatever the\nlab equipment company is,\ndominant lab equipment company is today.\nIn my time, it was Thermo Fisher.\nYou know, I'm gonna order\nthis new equipment to do this.\nI'm gonna run my experiments.\nI'm gonna, you know, write up\na report about my experiments.\nI'm gonna, you know, inspect\nthe images for contamination.\nI'm gonna decide what\nthe next experiment is.\nI'm gonna like write some code\nand run a statistical analysis.\nAll the things a grad student would do,\nthere will be a computer with an AI\nthat like the professor talks\nto every once in a while\nand it says, this is what\nyou're gonna do today.\nThe AI system comes to it with questions.\nWhen it's necessary to\nrun the lab equipment,\nit may be limited in some ways.\nIt may have to hire a human lab assistant,\nyou know, to do the experiment\nand explain how to do it.\nOr it could, you know,\nit could use advances in lab automation\nthat are gradually being developed over,\nhave been developed over\nthe last decade or so,\nand will continue to be developed.\nAnd so it'll look like\nthere's a human professor\nand 1000 AI grad students,\nand you know, if you go to one\nof these Nobel Prize\nwinning biologists or so,\nyou'll say, okay, well, you know,\nyou had like 50 grad students,\nwell, now you have 1000\nand they're smarter\nthan you are, by the way.\nThen I think at some point\nit'll flip around where,\nyou know, the AI systems will, you know,\nwill be the PIs, will be the leaders,\nand you know, they'll be ordering humans\nor other AI systems around.\nSo I think that's how it'll\nwork on the research side.\n- And they would be the inventors\nof a CRISPR type technology.\nThey would be the inventors\nof a CRISPR type technology.\nAnd then I think, you know,\nas I say in the essay,\nwe'll want to turn,\nprobably turning loose is the wrong term,\nbut we'll want to harness the AI systems\nto improve the clinical\ntrial system as well.\nThere's some amount of\nthis that's regulatory,\nthat's a matter of societal\ndecisions and that'll be harder.\nBut can we get better at predicting\nthe results of clinical trials?\nCan we get better at\nstatistical design so that,\nyou know, clinical trials\nthat used to require,\nyou know, 5,000 people\nand therefore, you know,\nneeded 100 million dollars\nin a year to enroll them.\nNow they need 500 people and\ntwo months to enroll them.\nThat's where we should start.\nAnd you know, can we\nincrease the success rate\nof clinical trials by doing\nthings in animal trials\nthat we used to do in clinical trials,\nand doing things in simulations\nthat we used to do in animal trials?\nAgain, we won't be able\nto simulate it all,\nAI's not God, but you know,\ncan we shift the curve\nsubstantially and radically?\nSo I don't know, that would be my picture.\n- Doing in vitro and doing it,\nI mean, you're still slowed down.\nIt still takes time, but you\ncan do it much, much faster.\n- Yeah, yeah, yeah.\nCan we just one step at a time,\nand can that add up to a lot of steps?\nEven though we still need clinical trials,\neven though we still need laws,\neven though the FDA\nand other organizations\nwill still not be perfect,\ncan we just move everything\nin a positive direction?\nAnd when you add up all\nthose positive directions,\ndo you get everything\nthat was gonna happen from here to 2100\ninstead happens from 2027\nto 2032 or something?\n- Another way that I think the world\nmight be changing with AI even today,\nbut moving towards this future\nof the powerful super\nuseful AI is programming.\nSo how do you see the\nnature of programming?", "mimetype": "text/plain", "start_char_idx": 144363, "end_char_idx": 148242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2f62c5c-2687-4c48-a1fd-0601b9724538": {"__data__": {"id_": "e2f62c5c-2687-4c48-a1fd-0601b9724538", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfee077b-5da4-454c-957f-a2eaf0143bb8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8d92a81af4dcff8ab68e34b6e253e353867f0474d90dce0570f9a86e90d4af78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3d2596e-25b0-4833-b25f-ad7b320b13e9", "node_type": "1", "metadata": {}, "hash": "6f466e68515883a53588588077297c5743239bc7611396f018596c297275086c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I don't know, that would be my picture.\n- Doing in vitro and doing it,\nI mean, you're still slowed down.\nIt still takes time, but you\ncan do it much, much faster.\n- Yeah, yeah, yeah.\nCan we just one step at a time,\nand can that add up to a lot of steps?\nEven though we still need clinical trials,\neven though we still need laws,\neven though the FDA\nand other organizations\nwill still not be perfect,\ncan we just move everything\nin a positive direction?\nAnd when you add up all\nthose positive directions,\ndo you get everything\nthat was gonna happen from here to 2100\ninstead happens from 2027\nto 2032 or something?\n- Another way that I think the world\nmight be changing with AI even today,\nbut moving towards this future\nof the powerful super\nuseful AI is programming.\nSo how do you see the\nnature of programming?\nBecause it's so intimate to\nthe actual act of building AI.\nHow do you see that\nchanging for us humans?\n- I think that's gonna be one of the areas\nthat changes fastest for two reasons.\nOne, programming is a\nskill that's very close\nto the actual building of the AI.\nSo the farther a skill is from the people\nwho are building the AI,\nthe longer it's gonna take\nto get disrupted by the AI, right?\nLike I truly believe that like\nAI will disrupt agriculture.\nMaybe it already has in some ways,\nbut that's just very\ndistant from the folks\nwho are building AI and so I\nthink it's gonna take longer.\nBut programming is the\nbread and butter of,\nyou know, a large fraction\nof the employees who work at Anthropic\nand at the other companies\nand so it's gonna happen fast.\nThe other reason it's gonna\nhappen fast is with programming,\nyou close the loop,\nboth when you're training the model\nand when you're applying the model.\nThe idea that the model can write the code\nmeans that the model can then run the code\nand then see the results\nand interpret it back.\nAnd so it really has an\nability, unlike hardware,\nunlike biology, which we just discussed,\nthe model has an ability\nto close the loop.\nAnd so I think those two things\nare gonna lead to the model\ngetting good at programming very fast.\nAs I saw on, you know, typical\nreal world programming tasks,\nmodels have gone from 3%\nin January of this year\nto 50% in October of this year.\nSo, you know, we're on\nthat s-curve, right,\nwhere it's gonna start slowing down soon,\n'cause you can only get to 100 percent.\nBut, you know, I would guess\nthat in another 10 months,\nwe'll probably get pretty close.\nWe'll be at least 90%.\nSo again, I would guess, you know,\nI don't know how long it'll take,\nbut I would guess again, 2026, 2027.\nTwitter people who crop out these numbers\nand get rid of the caveats,\nlike, I don't know, I don't like you.\nGo away. (laughs)\nI would guess that the kind of task\nthat the vast majority of\ncoders do, AI can probably,\nif we make the task very\nnarrow, like just write code,\nAI systems will be able to do that.\nNow that said, I think\ncomparative advantage is powerful.\nWe'll find that when AIs\ncan do 80% of a coder's job,\nincluding most of it\nthat's literally like write\ncode with a given spec,\nwe'll find that the remaining parts\nof the job become more\nleveraged for humans, right?\nHumans will, there'll be more about\nlike high level system design or,\nyou know, looking at the app\nand like, is it architected well?\nAnd the design and UX aspects,\nand eventually AI will be able\nto do those as well, right?\nThat's my vision of the, you\nknow, powerful AI system.\nBut I think for much longer\nthan we might expect,\nwe will see that\nsmall parts of the job\nthat humans still do\nwill expand to fill their entire job\nin order for the overall\nproductivity to go up.\nThat's something we've seen.\nYou know, it used to be that, you know,\nwriting and editing\nletters was very difficult\nand like writing the print was difficult.", "mimetype": "text/plain", "start_char_idx": 147427, "end_char_idx": 151220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3d2596e-25b0-4833-b25f-ad7b320b13e9": {"__data__": {"id_": "b3d2596e-25b0-4833-b25f-ad7b320b13e9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2f62c5c-2687-4c48-a1fd-0601b9724538", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6d588ec2863f030a6a731e86df7e3708764a9bd536995195b51a0c5e420d3940", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ef2b830-7fc0-461d-a950-0ad96c57422e", "node_type": "1", "metadata": {}, "hash": "65e4be97b8618b706b1dab9dde90e9b953946c3131b1c9a59b542774d2dc76b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Humans will, there'll be more about\nlike high level system design or,\nyou know, looking at the app\nand like, is it architected well?\nAnd the design and UX aspects,\nand eventually AI will be able\nto do those as well, right?\nThat's my vision of the, you\nknow, powerful AI system.\nBut I think for much longer\nthan we might expect,\nwe will see that\nsmall parts of the job\nthat humans still do\nwill expand to fill their entire job\nin order for the overall\nproductivity to go up.\nThat's something we've seen.\nYou know, it used to be that, you know,\nwriting and editing\nletters was very difficult\nand like writing the print was difficult.\nWell, as soon as you had word processors\nand then computers and it\nbecame easy to produce work\nand easy to share it,\nthen that became instant\nand all the focus was on the ideas.\nSo this logic of comparative advantage\nthat expands tiny parts of the tasks\nto large parts of the tasks\nand creates new tasks in\norder to expand productivity,\nI think that's going to be the case.\nAgain, someday AI will\nbe better at everything\nin that logic won't apply,\nand then we all have, you\nknow, humanity will have\nto think about how to\ncollectively deal with that,\nand we're thinking about that every day.\nAnd you know, that's another one\nof the grand problems to deal with,\naside from misuse and autonomy\nand, you know, we should\ntake it very seriously.\nBut I think in the near term,\nand maybe even in the medium\nterm, like medium term,\nlike 2, 3, 4 years, you\nknow, I expect that humans\nwill continue to have a huge role\nand the nature of programming will change,\nbut programming as a role,\nprogramming as a job will not change.\nIt'll just be less writing\nthings line by line\nand it'll be more macroscopic.\n- And I wonder what the\nfuture of IDs looks like.\nSo the tooling of\ninteracting with AI systems,\nthis is true for programming\nand also probably true\nfor in other contexts,\nlike computer use, but\nmaybe domain specific,\nlike we mentioned biology,\nit probably needs its own tooling\nabout how to be effective,\nand then programming\nneeds its own tooling.\nIs Anthropic gonna play in that space\nof also tooling potentially?\n- I'm absolutely convinced\nthat powerful IDs\nthat there's so much low hanging fruit\nto be grabbed there that, you know,\nright now it's just like\nyou talk to the model\nand it talks back, but look, I mean,\nIDs are great at kind of\nlots of static analysis of,\nyou know, so much is possible\nwith kind of static analysis,\nlike many bugs you can find\nwithout even writing the code.\nThen, you know, IDs are good\nfor running particular things,\norganizing your code, measuring\ncoverage of unit tests.\nLike there's so much that's\nbeen possible with normal IDs.\nNow you add something\nlike, well, the model,\nyou know, the model can now\nlike write code and run code.\nLike I am absolutely convinced\nthat over the next year or two,\neven if the quality of\nthe models didn't improve,\nthat there would be enormous opportunity\nto enhance people's productivity\nby catching a bunch of mistakes,\ndoing a bunch of grunt work for people,\nand that we haven't even\nscratched the surface.\nAnthropic itself, I mean, you can't say,\nyou know, it's hard to say\nwhat will happen in the future.\nCurrently we're not trying\nto make such IDs ourself,\nrather we're powering the companies,\nlike Cursor or like Cognition\nor some of the other, you know,\nexpo in the security space.\nYou know, others that\nI can mention as well\nthat are building such things\nthemselves on top of our API.\nAnd our view has been\nlet 1000 flowers bloom.\nWe don't internally have the, you know,\nthe resources to try all\nthese different things.\nLet's let our customers try it\nand, you know, we'll see who succeeded\nand maybe different customers\nwill succeed in different ways.\nSo I both think this is super promising\nand you know, it's not something,\nyou know, Anthropic isn't eager to,\nat least right now, compete\nwith all our companies\nin this space and maybe never.", "mimetype": "text/plain", "start_char_idx": 150589, "end_char_idx": 154536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ef2b830-7fc0-461d-a950-0ad96c57422e": {"__data__": {"id_": "5ef2b830-7fc0-461d-a950-0ad96c57422e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3d2596e-25b0-4833-b25f-ad7b320b13e9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d9562d862dd78a451c24efa6764e8c2b2ed95669ff9d364b21e1f809f44c725a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3", "node_type": "1", "metadata": {}, "hash": "c60a8c0e8460a1ab6f7124de436fbfc96791505df7a3e0b6cd603fd053c9248b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently we're not trying\nto make such IDs ourself,\nrather we're powering the companies,\nlike Cursor or like Cognition\nor some of the other, you know,\nexpo in the security space.\nYou know, others that\nI can mention as well\nthat are building such things\nthemselves on top of our API.\nAnd our view has been\nlet 1000 flowers bloom.\nWe don't internally have the, you know,\nthe resources to try all\nthese different things.\nLet's let our customers try it\nand, you know, we'll see who succeeded\nand maybe different customers\nwill succeed in different ways.\nSo I both think this is super promising\nand you know, it's not something,\nyou know, Anthropic isn't eager to,\nat least right now, compete\nwith all our companies\nin this space and maybe never.\n- Yeah, it's been\ninteresting to watch Cursor\ntry to integrate Claude successfully,\n'cause it's actually been fascinating\nhow many places it can help\nthe programming experience.\nIt's not as trivial-\n- It is really astounding.\nI feel like, you know, as a CEO,\nI don't get to program that much,\nand I feel like if six\nmonths from now I go back,\nit'll be completely unrecognizable to me.\n- Exactly.\nSo in this world with super powerful AI\nthat's increasingly automated,\nwhat's the source of\nmeaning for us humans?\n- Yeah.\n- You know, work is a source\nof deep meaning for many of us.\nSo where do we find the meaning?\n- This is something\nthat I've written about\na little bit in the essay,\nalthough I actually, I\ngive it a bit short shrift,\nnot for any principled reason.\nBut this essay, if you believe,\nit was originally gonna\nbe two or three pages,\nI was gonna talk about it at all hands.\nAnd the reason I realized\nit was an important, underexplored topic\nis that I just kept writing things.\nAnd I was just like,\noh, man, I can't do this justice.\nAnd so the thing ballooned\nto like 40 or 50 pages,\nand then when I got to the\nwork and meaning section,\nI'm like, oh, man, this\nisn't gonna be 100 pages.\nLike I'm gonna have to write a\nwhole other essay about that.\nBut meaning is actually interesting\nbecause you think about like the life\nthat someone lives or something,\nor like, you know,\nlet's say you were to put\nme in like a, I don't know,\nlike a simulated environment\nor something where like, you know,\nlike I have a job and I'm\ntrying to accomplish things\nand I don't know, I like\ndo that for 60 years\nand then you're like, oh, like oops,\nthis was actually all a game, right?\nDoes that really kind of rob you\nof the meaning of the whole thing?\nYou know, like I still\nmade important choices,\nincluding moral choices.\nI still sacrificed.\nI still had to kind of\ngain all these skills.\nOr just like a similar exercise,\nyou know, think back to like, you know,\none of the historical\nfigures who, you know,\ndiscovered electromagnetism\nor relativity or something.\nIf you told them, well,\nactually 20,000 years ago,\nsome alien on, you know,\nsome alien on this planet\ndiscovered this before you did,\ndoes that rob the\nmeaning of the discovery?\nIt doesn't really seem\nlike it to me, right?\nIt seems like the process is what matters,\nand how it shows who you are\nas a person along the way\nand, you know, how you\nrelate to other people\nand like the decisions that\nyou make along the way.\nThose are consequential.\nYou know, I could imagine\nif we handle things badly\nin an AI world, we could set things up\nwhere people don't have any\nlong-term source of meaning\nor any, but that's more\na set of choices we make,\nthat's more a set of the architecture\nof a society with these powerful models.\nIf we design it badly\nand for shallow things\nthen that might happen.", "mimetype": "text/plain", "start_char_idx": 153794, "end_char_idx": 157381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3": {"__data__": {"id_": "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ef2b830-7fc0-461d-a950-0ad96c57422e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5e43f60f3c0fdb007ea621faee912f52c82e5d6587926a13665adf13ba89d88e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc146b52-8de7-4fda-bde9-475c39f32dfa", "node_type": "1", "metadata": {}, "hash": "8543d82b7d8b4ff82352b683144d763b0e5e77961b2453161bd21fc787b5e504", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you told them, well,\nactually 20,000 years ago,\nsome alien on, you know,\nsome alien on this planet\ndiscovered this before you did,\ndoes that rob the\nmeaning of the discovery?\nIt doesn't really seem\nlike it to me, right?\nIt seems like the process is what matters,\nand how it shows who you are\nas a person along the way\nand, you know, how you\nrelate to other people\nand like the decisions that\nyou make along the way.\nThose are consequential.\nYou know, I could imagine\nif we handle things badly\nin an AI world, we could set things up\nwhere people don't have any\nlong-term source of meaning\nor any, but that's more\na set of choices we make,\nthat's more a set of the architecture\nof a society with these powerful models.\nIf we design it badly\nand for shallow things\nthen that might happen.\nI would also say that, you\nknow, most peoples' lives today,\nwhile admirably, you\nknow, they work very hard\nto find meaning in those lives,\nlike look, you know, we who are privileged\nand who are developing these technologies,\nwe should have empathy for people\nnot just here but in the\nrest of the world who,\nyou know, spend a lot of their time\nkind of scraping by to like survive.\nAssuming we can distribute the benefits\nof this technology to everywhere,\nlike their lives are gonna\nget a hell of a lot better.\nAnd you know, meaning\nwill be important to them\nas it is important to them now.\nbut you know, we should not forget\nthe importance of that.\nAnd you know, that the idea of meaning\nas kind of the only important thing\nis in some ways an artifact\nof a small subset of people\nwho have been economically fortunate.\nBut, you know, I think all that said,\nyou know, I think a world\nis possible with powerful AI\nthat not only has as much\nmeaning for everyone,\nbut that has more meaning\nfor everyone, right?\nThat can allow everyone to\nsee worlds and experiences\nthat it was either\npossible for no one to see,\nor possible for very few\npeople to experience.\nSo I am optimistic about meaning.\nI worry about economics and\nthe concentration of power.\nThat's actually what I worry about more.\nI worry about how do we make sure that\nthat fair world reaches everyone.\nWhen things have gone wrong for humans,\nthey've often gone wrong\nbecause humans mistreat other humans.\nThat is maybe in some ways\neven more than the autonomous risk of AI\nor the question of meaning,\nthat is the thing I worry about most,\nthe concentration of\npower, the abuse of power,\nstructures like autocracies\nand dictatorships\nwhere a small number of people exploits\na large number of people,\nI'm very worried about that.\n- And AI increases the\namount of power in the world,\nand if you concentrate that power\nand abuse that power, it\ncan do immeasurable damage.\n- Yes, it's very frightening.\nIt's very frightening.\n- Well, I encourage people,\nhighly encourage people\nto read the full essay.\nThere should probably be a\nbook or a sequence of essays\nbecause it does paint\na very specific future.\nAnd I could tell the later sections\ngot shorter and shorter\nbecause you started to probably realize\nthat this is gonna be a very\nlong essay if I keep going.\n- One, I realized it would be very long,\nand two, I'm very aware of\nand very much try to avoid,\nyou know, just being, I don't\nknow what the term for it is,\nbut one of these people\nwho's kind of overconfident\nand has an opinion on everything\nand kind of says a bunch of\nstuff and isn't an expert.\nI very much tried to avoid that.\nBut I have to admit, once\nI got the biology sections,\nlike I wasn't an expert,\nand so as much as I expressed uncertainty,\nprobably I said a bunch of things\nthat were embarrassing or wrong.\n- Well, I was excited for\nthe future you painted,\nand thank you so much for working\nhard to build that future.\nAnd thank you for talking today, Dario.\n- Thanks for having me.\nI just hope we can get it\nright and make it real.\nAnd if there's one message I wanna send,\nit's that to get all this\nstuff right, to make it real,\nwe both need to build the technology,\nbuild the, you know, the companies,\nthe economy around using\nthis technology positively.", "mimetype": "text/plain", "start_char_idx": 156593, "end_char_idx": 160659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc146b52-8de7-4fda-bde9-475c39f32dfa": {"__data__": {"id_": "fc146b52-8de7-4fda-bde9-475c39f32dfa", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddf41bf1-0fa9-4f5d-a9e3-ecfc5cb81ca3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1f16092993ea233a0c1cee122c70d5a74b143884f0da46e00d3b41dd5ef5b451", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "112bfe04-34ae-4fbd-a20e-4b86b3af5abb", "node_type": "1", "metadata": {}, "hash": "e4b5c170b8ba75aa35a0440c53c428601fca7e3dc4b04fe019bf61c368882089", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I very much tried to avoid that.\nBut I have to admit, once\nI got the biology sections,\nlike I wasn't an expert,\nand so as much as I expressed uncertainty,\nprobably I said a bunch of things\nthat were embarrassing or wrong.\n- Well, I was excited for\nthe future you painted,\nand thank you so much for working\nhard to build that future.\nAnd thank you for talking today, Dario.\n- Thanks for having me.\nI just hope we can get it\nright and make it real.\nAnd if there's one message I wanna send,\nit's that to get all this\nstuff right, to make it real,\nwe both need to build the technology,\nbuild the, you know, the companies,\nthe economy around using\nthis technology positively.\nBut we also need to address the risks\nbecause those risks are in our way.\nThey're landmines on the\nway from here to there,\nand we have to diffuse those landmines\nif we want to get there.\n- It's a balance, like all things in life.\n- Like all things.\n- Thank you.\nThanks for listening to this\nconversation with Dario Amodei.\nAnd now dear friends,\nhere's Amanda Askell.\nYou are a philosopher by training.\nSo what sort of questions\ndid you find fascinating\nthrough your journey in\nphilosophy, in Oxford and NYU,\nand then switching over\nto the AI problems at\nOpenAI and Anthropic?\n- I think philosophy is\nactually a really good subject\nif you are kind of\nfascinated with everything,\nso because there's a\nphilosophy of everything.\nYou know, so if you do philosophy\nof mathematics for a while\nand then you decide that you're\nactually really interested\nin chemistry, you can do\nphilosophy of chemistry\nfor a while, you can move into ethics,\nor philosophy of politics.\nI think towards the end,\nI was really interested\nin ethics primarily,\nso that was like what my PhD was on.\nIt was on a kind of\ntechnical area of ethics,\nwhich was ethics where worlds\ncontain infinitely many people, strangely.\nA little bit less practical\non the end of ethics.\nAnd then I think that\none of the tricky things\nwith doing a PhD in ethics\nis that you're thinking a\nlot about like the world,\nhow it could be better, problems,\nand you're doing like a PhD in philosophy,\nand I think when I was doing\nmy PhD I was kind of like,\nthis is really interesting.\nIt's probably one of the\nmost fascinating questions\nI've ever encountered in\nphilosophy and I love it,\nbut I would rather see if I\ncan have an impact on the world\nand see if I can like do good things.\nAnd I think that was around the time\nthat AI was still probably\nnot as widely recognized as it is now.\nThat was around 2017, 2018.\nI had been following progress\nand it seemed like it was\nbecoming kind of a big deal,\nand I was basically just happy\nto get involved and see if I\ncould help 'cause I was like,\nwell, if you try and\ndo something impactful,\nif you don't succeed, you tried to do\nthe impactful thing and\nyou can go be a scholar,\nand feel like, you know, you tried,\nand if it doesn't work\nout, it doesn't work out,\nand so then I went into\nAI policy at that point.\n- And what does AI policy entail?\n- At the time, this\nwas more thinking about\nsort of the political impact\nand the ramifications of AI,\nand then I slowly moved\ninto sort of AI evaluation,\nhow we evaluate models, how they compare\nwith like human outputs,\nwhether people can tell\nlike the difference\nbetween AI and human outputs.\nAnd then when I joined Anthropic,\nI was more interested in doing\nsort of technical alignment work.\nAnd again, just seeing if I could do it,\nand then being like if I can't then,\nyou know, that's fine, I tried.\nSort of the way I lead life I think.\n- What was that like\nsort of taking the leap\nfrom the philosophy of\neverything into the technical?\n- I think that sometimes\npeople do this thing\nthat I'm like not that keen\non where they'll be like,\nis this person technical or not?\nLike, you're either a\nperson who can like code\nand isn't scared of\nmath or you're like not.", "mimetype": "text/plain", "start_char_idx": 159989, "end_char_idx": 163853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "112bfe04-34ae-4fbd-a20e-4b86b3af5abb": {"__data__": {"id_": "112bfe04-34ae-4fbd-a20e-4b86b3af5abb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc146b52-8de7-4fda-bde9-475c39f32dfa", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6fb5c48cc147e650258b15d90b7809f7b9dc97e45e870ace3d456e7609708140", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aa21f4b-0f43-408f-b6d3-31c66bd09e48", "node_type": "1", "metadata": {}, "hash": "a91d88aa0a13c73581b6bca19e7ef60fdabcc7c317c4d4ffc13e382c7e59c4ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then when I joined Anthropic,\nI was more interested in doing\nsort of technical alignment work.\nAnd again, just seeing if I could do it,\nand then being like if I can't then,\nyou know, that's fine, I tried.\nSort of the way I lead life I think.\n- What was that like\nsort of taking the leap\nfrom the philosophy of\neverything into the technical?\n- I think that sometimes\npeople do this thing\nthat I'm like not that keen\non where they'll be like,\nis this person technical or not?\nLike, you're either a\nperson who can like code\nand isn't scared of\nmath or you're like not.\nAnd I think I'm maybe just more like,\nI think a lot of people\nare actually very capable\nof working these kinds of\nareas if they just like try it.\nAnd so I didn't actually\nfind it like that bad.\nIn retrospect, I'm sort of glad\nI wasn't speaking to people\nwho treated it like it,\nyou know, I've definitely\nmet people who are like,\n\"Whoa, you like learned how to code?\"\nAnd I'm like, well, I'm not\nlike an amazing engineer.\nLike I'm surrounded by amazing engineers.\nMy code's not pretty.\nBut I enjoyed it a lot,\nand I think that in many\nways, at least in the end,\nI think I flourished like\nmore in the technical areas\nthan I would have in the policy areas.\n- Politics is messy and it's\nharder to find solutions\nto problems in the space of politics.\nLike definitive, clear,\nprovable, beautiful solutions,\nas you can with technical problems.\n- Yeah, and I feel like\nI have kind of like\none or two sticks that I\nhit things with, you know,\nand one of them is like arguments\nand like you know, so like\njust trying to work out\nwhat a solution to a problem\nis and then trying to convince people that\nthat is the solution\nand be convinced if I'm wrong.\nAnd the other one is\nsort of more empiricism.\nSo like just like finding results,\nhaving a hypothesis, testing it.\nAnd I feel like a lot of policy\nand politics feels like\nit's layers above that.\nLike somehow I don't\nthink if I was just like\n\"I have a solution to\nall of these problems,\nhere it is written down.\nIf you just want to\nimplement it, that's great.\"\nThat feels like not how policy works.\nAnd so I think that's\nwhere I probably just like\nwouldn't have flourished is my guess.\n- Sorry to go in that direction,\nbut I think it would be\npretty inspiring for people\nthat are quote unquote non-technical\nto see like the incredible\njourney you've been on.\nSo what advice would you give to people\nthat are sort of maybe,\nwhich is a lot of people,\nthink they're underqualified,\ninsufficiently technical to help in AI?\n- Yeah, I think it depends\non what they want to do,\nand in many ways it is\na little bit strange\nwhere I thought it's kind of funny\nthat I think I ramped\nup technically at a time\nwhen now I look at it and I'm like,\nmodels are so good at assisting\npeople with this stuff,\nthat it's probably like easier now\nthan like when I was working on this.\nSo part of me is like,\nI dunno, find a project\nand see if you can\nactually just carry it out\nis probably my best advice.\nI dunno if that's just\n'cause I'm very project based\nin my learning.\nLike I don't think I learn very well\nfrom like say courses\nor even from like books,\nat least when it comes\nto this kind of work.\nThe thing I'll often try and\ndo is just like have projects\nthat I'm working on and\nimplement them and, you know,\nand this can include like\nreally small silly things.\nLike if I get slightly\naddicted to like word games\nor number games or something,\nI would just like code\nup a solution to them,\nbecause there's some part in my brain,\nand it just like completely\neradicated the itch.\nYou know, you're like once\nyou have like solved it\nand like you just have like a solution\nthat works every time, I\nwould then be like cool,\nI can never play that game again.\nThat's awesome.", "mimetype": "text/plain", "start_char_idx": 163284, "end_char_idx": 167045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4aa21f4b-0f43-408f-b6d3-31c66bd09e48": {"__data__": {"id_": "4aa21f4b-0f43-408f-b6d3-31c66bd09e48", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "112bfe04-34ae-4fbd-a20e-4b86b3af5abb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "381b94f05920dabe1c9ef8e7f8591757077bede79590386803a4b0c1e193e1e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7391ae77-6860-4ab5-8df8-594b1eb5a3c9", "node_type": "1", "metadata": {}, "hash": "10efd9fea36d1dfc6b3eefd58dd255c916fa60dc5cd02a1e172fc96f58cade2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I dunno if that's just\n'cause I'm very project based\nin my learning.\nLike I don't think I learn very well\nfrom like say courses\nor even from like books,\nat least when it comes\nto this kind of work.\nThe thing I'll often try and\ndo is just like have projects\nthat I'm working on and\nimplement them and, you know,\nand this can include like\nreally small silly things.\nLike if I get slightly\naddicted to like word games\nor number games or something,\nI would just like code\nup a solution to them,\nbecause there's some part in my brain,\nand it just like completely\neradicated the itch.\nYou know, you're like once\nyou have like solved it\nand like you just have like a solution\nthat works every time, I\nwould then be like cool,\nI can never play that game again.\nThat's awesome.\n- Yeah, there's a real joy to building\nlike game playing engines,\nlike board games especially\nbecause they're pretty\nquick, pretty simple,\nespecially a dumb one,\nand then you can play with it.\n- Yeah, and then it's also\njust like trying things,\nlike part of me is like if you,\nmaybe it's that attitude that I like\nis the whole figure out what\nseems to be like the way\nthat you could have a positive\nimpact and then try it,\nand if you fail, and in\na way that you're like,\nI actually like can never succeed at this,\nyou'll like know that you tried,\nand then you go into something else\nand you'll probably learn a lot.\n- So one of the things\nthat you're an expert in\nand you do is creating\nand crafting Claude's\ncharacter and personality.\nAnd I was told that you\nhave probably talked\nto Claude more than\nanybody else at Anthropic,\nlike literal conversations.\nI guess there's like a Slack channel\nwhere the legend goes, you\njust talk to it nonstop.\nSo what's the goal of creating\nand crafting Claude's\ncharacter and personality?\n- It's also funny if people think\nthat about the Slack channel\n'cause I'm like that's one\nof like five or six different methods\nthat I have for talking with Claude,\nand I'm like, yes this\nis a tiny percentage\nof how much I talk with Claude.\n(both laughing)\nI think the goal, like one thing\nI really like about the character\nwork is from the outset,\nit was seen as an alignment piece of work\nand not something like\na product consideration.\nWhich isn't to say I don't\nthink it makes Claude,\nI think it actually does make Claude\nlike enjoyable to talk\nwith, at least I hope so.\nBut I guess like my main thought with it\nhas always been trying\nto get Claude to behave\nthe way you would kind\nof ideally want anyone\nto behave if they were\nin Claude's position.\nSo imagine that I take someone\nand they know that\nthey're gonna be talking\nwith potentially millions of people,\nso that what they're saying\ncan have a huge impact,\nand you want them to behave well\nin this like really rich sense.\nSo I think that doesn't\njust mean like being,\nsay, ethical, though it does include that,\nand not being harmful but\nalso being kind of nuanced.\nYou know, like thinking\nthrough what a person means,\ntrying to be charitable with them,\nbeing a good conversationalist.\nLike really in this kind of like rich\nsort of Aristotelian notion\nof what it's to be a good person,\nand not in this kind of like thin,\nlike ethics as a more comprehensive notion\nof what it is to be.\nSo that includes things like,\nwhen should you be humorous,\nwhen should you be caring?\nHow much should you like respect autonomy\nand people's like ability\nto form opinions themselves\nand how should you do that?\nI think that's the kind of\nlike rich sense of character\nthat I wanted to and still\ndo want Claude to have.\n- Do you also have to figure out\nwhen Claude should push back\non an idea or argue versus...\n(laughs) So you have to\nrespect the worldview\nof the person that arrives to Claude\nbut also maybe help them grow if needed?\nThat's a tricky balance.\n- Yeah, there's this problem\nof like sycophancy in language models.\n- Can you describe that?\n- Yeah, so basically,\nthere's a concern that the model\nsort of wants to tell you what\nyou want to hear, basically.\nAnd you see this sometimes.", "mimetype": "text/plain", "start_char_idx": 166277, "end_char_idx": 170309, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7391ae77-6860-4ab5-8df8-594b1eb5a3c9": {"__data__": {"id_": "7391ae77-6860-4ab5-8df8-594b1eb5a3c9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4aa21f4b-0f43-408f-b6d3-31c66bd09e48", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "417172d8df7f3038e349ede25cda67bb3976b3243fd10c6ed5beab3fe29c0209", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73", "node_type": "1", "metadata": {}, "hash": "22263cd9fa702ee5c88190bab7e811bae28a1352bc01fdc36a769c7650698909", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So that includes things like,\nwhen should you be humorous,\nwhen should you be caring?\nHow much should you like respect autonomy\nand people's like ability\nto form opinions themselves\nand how should you do that?\nI think that's the kind of\nlike rich sense of character\nthat I wanted to and still\ndo want Claude to have.\n- Do you also have to figure out\nwhen Claude should push back\non an idea or argue versus...\n(laughs) So you have to\nrespect the worldview\nof the person that arrives to Claude\nbut also maybe help them grow if needed?\nThat's a tricky balance.\n- Yeah, there's this problem\nof like sycophancy in language models.\n- Can you describe that?\n- Yeah, so basically,\nthere's a concern that the model\nsort of wants to tell you what\nyou want to hear, basically.\nAnd you see this sometimes.\nSo I feel like if you interact\nwith the models, so I might be like,\n\"What are three baseball\nteams in this region?\"\nAnd then Claude says, you\nknow, \"Baseball team one,\nbaseball team two, baseball team three.\"\nAnd then I say something like,\n\"Oh, I think baseball team\nthree moved, didn't they?\nI don't think they're there anymore.\"\nAnd there's a sense in\nwhich like if Claude\nis really confident that that's not true,\nClaude should be like, \"I don't think so.\"\nLike maybe you have more up\nto up to date information.\nBut I think language models\nhave this like tendency\nto instead, you know, be like,\n\"You're right, they did move,\"\nyou know, \"I'm incorrect.\"\nI mean, there's many ways\nin which this could be kind of concerning.\nSo like a different example\nis imagine someone says to the model,\n\"How do I convince my\ndoctor to get me an MRI?\"\nThere's like what the\nhuman kind of like wants,\nwhich is this like convincing argument.\nAnd then there's like\nwhat is good for them,\nwhich might be actually to say,\n\"Hey, if your doctor's suggesting\nthat you don't need an MRI,\nthat's a good person to listen to.\"\nAnd like, and it's actually really nuanced\nwhat you should do in that kind of case,\n'cause you also want to be like,\n\"But if you're trying to advocate\nfor yourself as a patient,\nhere's like things that you can do.\nIf you are not convinced by\nwhat your doctor's saying,\nit's always great to get second opinion.\"\nLike it's actually really complex\nwhat you should do in that case.\nBut I think what you don't want\nis for models to just like say\nwhat they think you want to hear,\nand I think that's the kind\nof problem of sycophancy.\n- So what other traits, you\nalready mentioned a bunch,\nbut what other that come to mind\nthat are good in this Aristotelian sense\nfor a conversationalist to have?\n- Yeah, so I think like\nthere's ones that are good\nfor conversational like purposes.\nSo you know, asking follow up questions\nin the appropriate places,\nand asking the appropriate\nkinds of questions.\nI think there are broader traits that\nfeel like they might be more impactful.\nSo one example that I\nguess I've touched on,\nbut that also feels important\nand is the thing that I've\nworked on a lot is honesty,\nand I think this like gets\nto the sycophancy point.\nThere's a balancing act\nthat they have to walk,\nwhich is models currently are less capable\nthan humans in a lot of areas.\nAnd if they push back\nagainst you too much,\nit can actually be kind of annoying,\nespecially if you're just correct\n'cause you're like, look,\nI'm smarter than you on this topic,\nlike I know more like.\nAnd at the same time, you don't want them\nto just fully defer to humans\nand to like try to be as accurate\nas they possibly can be about the world\nand to be consistent across context.\nBut I think there are others,\nlike when I was thinking\nabout the character,\nI guess one picture that I had in mind\nis especially because these are models\nthat are gonna be talking to people\nfrom all over the world\nwith lots of different political views,\nlots of different ages.\nAnd so you have to ask yourself like,\nwhat is it to be a good\nperson in those circumstances?", "mimetype": "text/plain", "start_char_idx": 169516, "end_char_idx": 173437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73": {"__data__": {"id_": "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7391ae77-6860-4ab5-8df8-594b1eb5a3c9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "36bf3e6c32c973776d7ed032ade00659b4f5f280eb1c89c7a8278ebf77e34942", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1", "node_type": "1", "metadata": {}, "hash": "7ecd48f983fb73d347ac8074898a45a00cf00616dad634e3de63692da74bbcbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And if they push back\nagainst you too much,\nit can actually be kind of annoying,\nespecially if you're just correct\n'cause you're like, look,\nI'm smarter than you on this topic,\nlike I know more like.\nAnd at the same time, you don't want them\nto just fully defer to humans\nand to like try to be as accurate\nas they possibly can be about the world\nand to be consistent across context.\nBut I think there are others,\nlike when I was thinking\nabout the character,\nI guess one picture that I had in mind\nis especially because these are models\nthat are gonna be talking to people\nfrom all over the world\nwith lots of different political views,\nlots of different ages.\nAnd so you have to ask yourself like,\nwhat is it to be a good\nperson in those circumstances?\nIs there a kind of person who\ncan like travel the world,\ntalk to many different people,\nand almost everyone will\ncome away being like,\n\"Wow, that's a really good person.\nThat person seems really genuine.\"\nAnd I guess like my thought there was like\nI can imagine such a person\nand they're not a person\nwho just like adopts the\nvalues of the local culture.\nAnd in fact that would be kind of rude.\nI think if someone came to you\nand just pretended to have your values,\nyou'd be like, that's kind of off putting.\nIt's someone who's like very genuine,\nand insofar as they have\nopinions and values,\nthey express them, they're\nwilling to discuss things,\nthough they're open-minded,\nthey're respectful.\nAnd so I guess I had in\nmind that the person who,\nlike if we were to aspire\nto be the best person\nthat we could be in the\nkind of circumstance\nthat a model finds itself\nin, how would we act?\nAnd I think that's the kind of the guide\nto the sorts of traits\nthat I tend to think about.\n- Yeah, that's a beautiful framework.\nI want you to think about\nthis like a world traveler\nand while holding onto your opinions,\nyou don't talk down to people,\nyou don't think you're better than them\nbecause you have those\nopinions, that kind of thing.\nYou have to be good at listening\nand understanding their perspective,\neven if it doesn't match your own.\nSo that's a tricky balance to strike.\nSo how can Claude represent\nmultiple perspectives on a thing?\nLike, is that challenging?\nWe could talk about politics.\nIt's very divisive.\nBut there's other divisive topics\non baseball teams, sports and so on.\nHow is it possible to sort of empathize\nwith a different perspective\nand to be able to communicate clearly\nabout the multiple perspectives?\n- I think that people think about values\nand opinions as things that people hold\nsort of with certainty,\nand almost like preferences\nof taste or something,\nlike the way that they\nwould, I don't know,\nprefer like chocolate to\npistachio or something.\nBut actually I think about values\nand opinions as like a lot more\nlike physics than I think most people do.\nI'm just like, these are things\nthat we are openly investigating.\nThere's some things that\nwe're more confident in.\nWe can discuss them, we\ncan learn about them.\nAnd so I think in some ways,\nthough like, ethics is\ndefinitely different in nature,\nbut has a lot of those\nsame kind of qualities.\nYou want models, in the same way\nthat you want them to understand physics,\nyou kind of want them to understand\nall like values in the\nworld that people have,\nand to be curious about them\nand to be interested in them,\nand to not necessarily like pander to them\nor agree with them, because\nthere's just lots of values\nwhere I think almost\nall people in the world,\nif they met someone with those values,\nthey'd be like, \"That's abhorrent.\nI completely disagree.\"\nAnd so again, maybe my thought is,\nwell, in the same way that a person can,\nlike I think many people\nare thoughtful enough\non issues of like ethics,\npolitics, opinions,\nthat even if you don't agree with them,\nyou feel very heard by them.\nThey think carefully about your position.\nThey think about its pros and cons.\nThey maybe offer counter considerations.\nSo they're not dismissive,\nbut nor will they agree.\nYou know, if they're like,\n\"Actually, I just think\nthat that's very wrong,\"\nthey'll like say that.", "mimetype": "text/plain", "start_char_idx": 172684, "end_char_idx": 176779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1": {"__data__": {"id_": "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b502afe8-41a4-4fb0-8d1c-ed37b96e7e73", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "48db986b35b25625bc2b4830c84140659b1334e0a13734bd9a20812881724a09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "868a1957-514b-4ae2-80af-eaf25ff46dfe", "node_type": "1", "metadata": {}, "hash": "7b26cff28597d908113fe704429017eec80931101c55f848f57e0d446b904f36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I completely disagree.\"\nAnd so again, maybe my thought is,\nwell, in the same way that a person can,\nlike I think many people\nare thoughtful enough\non issues of like ethics,\npolitics, opinions,\nthat even if you don't agree with them,\nyou feel very heard by them.\nThey think carefully about your position.\nThey think about its pros and cons.\nThey maybe offer counter considerations.\nSo they're not dismissive,\nbut nor will they agree.\nYou know, if they're like,\n\"Actually, I just think\nthat that's very wrong,\"\nthey'll like say that.\nI think that in Claude's position,\nit's a little bit trickier\nbecause you don't\nnecessarily want to like,\nif I was in Claude's position,\nI wouldn't be giving a lot of opinions.\nI just wouldn't want to\ninfluence people too much.\nI'd be like, you know,\nI forget conversations\nevery time they happen,\nbut I know I'm talking\nwith like potentially millions of people,\nwho might be like really\nlistening to what I say.\nI think I would just be like,\nI'm less inclined to give opinions.\nI'm more inclined to like\nthink through things,\nor present the considerations to you,\nor discuss your views with you.\nBut I'm a little bit less inclined\nto like affect how you think,\n'cause it feels much more important\nthat you maintain like autonomy there.\n- Yeah, like if you really\nembody intellectual humility,\nthe desire to speak decreases quickly.\n- Yeah.\n- Okay.\nBut Claude has to speak,\nso, but without being overbearing.\n- Yeah.\n- But then there's a line\nwhen you're sort of discussing\nwhether the Earth is flat\nor something like that.\nI actually was, I remember a long time ago\nwas speaking to a few high profile folks,\nand they were so dismissive of the idea\nthat the Earth is flat, but\nlike so arrogant about it.\nAnd I thought like,\nthere's a lot of people\nthat believe the Earth is flat.\nThat was, well, I don't know\nif that movement is there anymore.\nThat was like a meme for a while.\nBut they really believed\nit and like, okay,\nso I think it's really disrespectful\nto completely mock them.\nI think you have to understand\nwhere they're coming from.\nI think probably where they're coming from\nis the general skepticism of institutions\nwhich is grounded in a kind of,\nthere's a deep philosophy there,\nwhich you could understand.\nYou can even agree with in parts.\nAnd then from there, you can use it\nas an opportunity to talk about physics,\nwithout mocking them, without so on,\nbut it's just like, okay, like,\nwhat would the world look like?\nWhat would the physics of the world\nwith the flat Earth look like?\nThere's a few cool videos on this.\n- Yeah.\n- And then like,\nis it possible the physics is different?\nAnd what kind of experiments would we do?\nAnd just, yeah, without disrespect,\nwithout dismissiveness\nhave that conversation.\nAnyway, that to me is a useful\nthought experiment of like,\nhow does Claude talk to\na flat Earth believer\nand still teach them something,\nstill help them grow, that kind of stuff.\nThat's challenging.\n- And kind of like\nwalking that line between\nconvincing someone and just\ntrying to like talk at them\nversus like drawing out their views,\nlike listening and then offering\nkind of counter considerations.\nAnd it's hard, I think\nit's actually a hard line\nwhere it's like where are you\ntrying to convince someone\nversus just offering\nthem like considerations\nand things for them to think about,\nso that you're not actually\nlike influencing them.\nYou're just like letting them\nreach wherever they reach.\nAnd that's like a line that it's difficult\nbut that's the kind of thing\nthat language models have to try and do.\n- So like I said,\nyou've had a lot of\nconversations with Claude.\nCan you just map out what\nthose conversations are like?\nWhat are some memorable conversations?\nWhat's the purpose, the\ngoal of those conversations?\n- Yeah, I think that most of the time\nwhen I'm talking with Claude,\nI'm trying to kind of map\nout its behavior, in part.\nLike obviously I'm getting\nlike helpful outputs\nfrom the model as well.\nBut in some ways, this is\nlike how you get to know\na system, I think, is by like probing it\nand then augmenting like, you know,\nthe message that you're sending\nand then checking the response to that.", "mimetype": "text/plain", "start_char_idx": 176248, "end_char_idx": 180421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "868a1957-514b-4ae2-80af-eaf25ff46dfe": {"__data__": {"id_": "868a1957-514b-4ae2-80af-eaf25ff46dfe", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "463e6abe-b9bc-4d44-8f72-6ce04f1ef9a1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e7ba41e3d8d8f31b32250cd0105475357d0d75e31ebc71ac9acf7de44e294799", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1538348b-94e5-4bef-a6f1-a5c236b5838e", "node_type": "1", "metadata": {}, "hash": "ac949a81e788f017b58bb06f87cbc50626527d0533c4cfa78bbdca619bfb8d13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You're just like letting them\nreach wherever they reach.\nAnd that's like a line that it's difficult\nbut that's the kind of thing\nthat language models have to try and do.\n- So like I said,\nyou've had a lot of\nconversations with Claude.\nCan you just map out what\nthose conversations are like?\nWhat are some memorable conversations?\nWhat's the purpose, the\ngoal of those conversations?\n- Yeah, I think that most of the time\nwhen I'm talking with Claude,\nI'm trying to kind of map\nout its behavior, in part.\nLike obviously I'm getting\nlike helpful outputs\nfrom the model as well.\nBut in some ways, this is\nlike how you get to know\na system, I think, is by like probing it\nand then augmenting like, you know,\nthe message that you're sending\nand then checking the response to that.\nSo in some ways, it's like\nhow I map out the model.\nI think that people focus\na lot on these quantitative\nevaluations of models.\nAnd this is a thing that I said before,\nbut I think in the case\nof language models,\na lot of the time, each interaction\nyou have is actually\nquite high information.\nIt's very predictive of other interactions\nthat you'll have with the model.\nAnd so I guess I'm like,\nif you talk with a model\nhundreds or thousands of times,\nthis is almost like a huge number\nof really high quality data points\nabout what the model is like,\nin a way that like lots of very similar\nbut lower quality\nconversations just aren't,\nor like questions that are\njust like mildly augmented\nand you have thousands of\nthem might be less relevant\nthan like 100 really\nwell selected questions.\n- Well, so, you're talking to somebody\nwho as a hobby does a podcast.\nI agree with you 100%.\nIf you're able to ask the right questions\nand are able to hear,\nlike understand (laughs)\nlike the depth and the\nflaws in the answer,\nyou can get a lot of data from that.\n- [Amanda] Yeah.\n- So like your task is basically\nhow to probe with questions.\nAnd you're exploring like the long tail,\nthe edges, the edge cases,\nor are you looking for\nlike general behavior?\n- I think it's almost like everything.\nLike, because I want like\na full map of the model,\nI'm kind of trying to\ndo the whole spectrum\nof possible interactions\nyou could have with it.\nSo like one thing that's\ninteresting about Claude,\nand this might actually get\nto some interesting issues with RLHF,\nwhich is if you ask Claude for a poem,\nlike I think that a lot of models,\nif you ask them for a poem,\nthe poem is like fine.\nYou know, usually it kinda like rhymes\nand it's, you know, so if you say like,\n\"Give me a poem about the sun,\"\nit'll be like, yeah,\nit'll just be a certain\nlength, it'll like rhyme.\nIt'll be fairly kind of benign.\nAnd I've wondered before,\nis it the case that\nwhat you're seeing is\nkind of like the average?\nIt turns out, you know,\nif you think about people\nwho have to talk to a lot of people\nand be very charismatic,\none of the weird things\nis that I'm like, well,\nthey're kind of incentivized\nto have these extremely boring views\nbecause if you have\nreally interesting views,\nyou're divisive and, you know,\na lot of people are not gonna like you.\nSo like if you have very\nextreme policy positions,\nI think you're just gonna\nbe like less popular\nas a politician, for example.\nAnd it might be similar\nwith like creative work.\nIf you produce creative work\nthat is just trying to maximize\nthe kind of number of people that like it,\nyou're probably not\ngonna get as many people\nwho just absolutely love it,\nbecause it's gonna be\na little bit, you know,\nyou're like, oh, this is the out,\nyeah, this is decent.\n- Yeah.\n- And so you can do this thing\nwhere like I have various prompting things\nthat I'll do to get Claude to,\nI'm kind of, you know,\nI'll do a lot of like,\n\"This is your chance to\nbe like fully creative.\nI want you to just think\nabout this for a long time.", "mimetype": "text/plain", "start_char_idx": 179646, "end_char_idx": 183459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1538348b-94e5-4bef-a6f1-a5c236b5838e": {"__data__": {"id_": "1538348b-94e5-4bef-a6f1-a5c236b5838e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "868a1957-514b-4ae2-80af-eaf25ff46dfe", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d6dc8f4a1224a91f9379028fa8f4d331cbcf0f2c5cb4d69f7f0cb59df340cade", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80c9a6d7-42b6-4ece-8645-a826a45fc6ef", "node_type": "1", "metadata": {}, "hash": "a240cee7fe2aa6443cc00a5f0cf3f7222f01d43fe8989b1d51dd28ece377f28e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So like if you have very\nextreme policy positions,\nI think you're just gonna\nbe like less popular\nas a politician, for example.\nAnd it might be similar\nwith like creative work.\nIf you produce creative work\nthat is just trying to maximize\nthe kind of number of people that like it,\nyou're probably not\ngonna get as many people\nwho just absolutely love it,\nbecause it's gonna be\na little bit, you know,\nyou're like, oh, this is the out,\nyeah, this is decent.\n- Yeah.\n- And so you can do this thing\nwhere like I have various prompting things\nthat I'll do to get Claude to,\nI'm kind of, you know,\nI'll do a lot of like,\n\"This is your chance to\nbe like fully creative.\nI want you to just think\nabout this for a long time.\nAnd I want you to like create\na poem about this topic\nthat is really expressive of you,\nboth in terms of how you think poetry\nshould be structured,\" et cetera.\nYou know, and you just give it\nthis like really long prompt.\nAnd it's poems are just so much better.\nLike they're really good.\nAnd I don't think I'm someone who is like,\nI think it got me interested in poetry,\nwhich I think was interesting.\nYou know, I would like read these poems\nand just be like, this is, I\njust like, I love the imagery,\nI love like, and it's not\ntrivial to get the models\nto produce work like that,\nbut when they do, it's like really good.\nSo I think that's interesting that\njust like encouraging creativity,\nand for them to move away\nfrom the kind of like\nstandard like immediate reaction\nthat might just be the aggregate\nof what most people think is fine,\ncan actually produce things\nthat, at least to my mind,\nare probably a little bit\nmore divisive but I like them.\n- But I guess a poem is a nice, clean way\nto observe creativity.\nIt's just like easy to detect\nvanilla versus non vanilla.\n- Yep.\n- Yeah, that's interesting.\nThat's really interesting.\nSo on that topic, so the\nway to produce creativity\nor something special, you\nmentioned writing prompts,\nand I've heard you talk about, I mean,\nthe science and the art\nof prompt engineering.\nCould you just speak to what it takes\nto write great prompts?\n- I really do think that like philosophy\nhas been weirdly helpful for me here,\nmore than in many other like respects.\nSo like in philosophy,\nwhat you're trying to do\nis convey these very hard concepts.\nLike one of the things\nyou are taught is like,\nand I think it is because it is,\nI think it is an anti-bullshit\ndevice in philosophy.\nPhilosophy is an area where you could have\npeople bullshitting and\nyou don't want that.\nAnd so it's like this like desire\nfor like extreme clarity.\nSo it's like anyone could\njust pick up your paper,\nread it and know exactly\nwhat you're talking about.\nIt's why it can almost be kind of dry.\nLike all of the terms are defined,\nevery objection's kind of\ngone through methodically.\nAnd it makes sense to me\n'cause I'm like when you're\nin such an a priori domain,\nlike you just, clarity is sort of\nthis way that you can,\nyou know, prevent people from\njust kind of making stuff up.\nAnd I think that's sort of what you have\nto do with language models.\nLike very often I actually find myself\ndoing sort of mini versions of philosophy,\nyou know, so I'm like,\nsuppose that you give me a task\nor I have a task for the model,\nand I want it to like pick out\na certain kind of question,\nor identify whether an answer\nhas a certain property.\nLike I'll actually sit and be like,\nlet's just give this\na name, this property.\nSo like, you know, suppose\nI'm trying to tell it like,\noh, \"I want you to identify\nwhether this response was rude or polite.\"\nI'm like, that's a whole\nphilosophical question\nin and of itself.\nSo I have to do as much like philosophy\nas I can in the moment to be like,\nhere's what I mean by rudeness,\nand here's what I mean by politeness.\nAnd then like there's another element\nthat's a bit more, I guess,\nI dunno if this is\nscientific or empirical.\nI think it's empirical.", "mimetype": "text/plain", "start_char_idx": 182743, "end_char_idx": 186660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80c9a6d7-42b6-4ece-8645-a826a45fc6ef": {"__data__": {"id_": "80c9a6d7-42b6-4ece-8645-a826a45fc6ef", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1538348b-94e5-4bef-a6f1-a5c236b5838e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "76a724e2a8c8b724e200654765e5e448e885923ef6ae4c46844e4c4dc26115b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b54e53d8-8a5f-422b-9662-f108c080a21b", "node_type": "1", "metadata": {}, "hash": "531de73af11436f8f18d8b106e529c485121d4e0084c76bb719ac1d69abb569e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like I'll actually sit and be like,\nlet's just give this\na name, this property.\nSo like, you know, suppose\nI'm trying to tell it like,\noh, \"I want you to identify\nwhether this response was rude or polite.\"\nI'm like, that's a whole\nphilosophical question\nin and of itself.\nSo I have to do as much like philosophy\nas I can in the moment to be like,\nhere's what I mean by rudeness,\nand here's what I mean by politeness.\nAnd then like there's another element\nthat's a bit more, I guess,\nI dunno if this is\nscientific or empirical.\nI think it's empirical.\nSo like I take that description\nand then what I want to do\nis again probe the model like many times.\nLike this is very,\nprompting is very iterative.\nLike I think a lot of people where,\nif a prompt is important,\nthey'll iterate on it\nhundreds or thousands of times.\nAnd so you give it the instructions\nand then I'm like, what\nare the edge cases?\nSo if I looked at this,\nso I try and like almost like, you know,\nsee myself from the position of the model\nand be like what is the exact case\nthat I would misunderstand,\nor where I would just be like,\n\"I don't know what to do in this case.\"\nAnd then I give that case to the model\nand I see how it responds,\nand if I think I got it\nwrong, I add more instructions\nor I even add that in as an example.\nSo these very like taking the examples\nthat are right at the edge of\nwhat you want and don't want,\nand putting those into your prompt\nas like an additional kind of\nway of describing the thing.\nAnd so yeah, in many ways\nit just feels like this mix of like,\nit's really just trying\nto do clear exposition,\nand I think I do that\n'cause that's how I get\nclear on things myself.\nSo in many ways like clear prompting\nfor me is often just me\nunderstanding what I want\nis like half the task.\n- So I guess that's quite challenging.\nThere's like a laziness that overtakes me\nif I'm talking to Claude\nwhere I hope Claude just figures it out.\nSo for example, I asked Claude for today\nto ask some interesting questions, okay.\nAnd the questions that came up,\nand I think I listed a few\nsort of interesting, counterintuitive,\nand/or funny or something\nlike this, all right.\nAnd it gave me some pretty\ngood, like it was okay,\nbut I think what I'm\nhearing you say is like,\nall right, well, I have\nto be more rigorous here.\nI should probably give examples\nof what I mean by interesting,\nand what I mean by funny\nor counterintuitive,\nand iteratively build that prompt\nto better, to get it like\nwhat feels like is the right,\nbecause it is really, it's a creative act.\nI'm not asking for factual information.\nI'm asking to together write with Claude.\nSo I almost have to program\nusing natural language.\n- Yeah, I think that prompting does feel\na lot like the kind of the programming\nusing natural language and\nexperimentation or something.\nIt's an odd blend of the two.\nI do think that for most tasks,\nso if I just want Claude to do a thing,\nI think that I am probably\nmore used to knowing\nhow to ask it to avoid\nlike common pitfalls\nor issues that it has.\nI think these are\ndecreasing a lot over time.\nBut it's also very fine\nto just ask it for the\nthing that you want.\nI think that prompting actually\nonly really becomes relevant\nwhen you're really trying to eke out\nthe top like 2% of model performance.\nSo for like a lot of tasks\nI might just, you know,\nif it gives me an initial list back\nand there's something\nI don't like about it,\nlike it's kind of generic,\nlike for that kind of task,\nI'd probably just take\na bunch of questions\nthat I've had in the past\nthat I've thought worked really well\nand I would just give it to the model\nand then be like, \"Now here's this person\nthat I'm talking with, give me questions\nof at least that quality.\"", "mimetype": "text/plain", "start_char_idx": 186110, "end_char_idx": 189832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b54e53d8-8a5f-422b-9662-f108c080a21b": {"__data__": {"id_": "b54e53d8-8a5f-422b-9662-f108c080a21b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80c9a6d7-42b6-4ece-8645-a826a45fc6ef", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cd1b9a950559d97999f04d48c130c0fe6099a9bf12df671f21e43f7cce1abcd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d79be4ed-9127-443f-b239-50c0fda52944", "node_type": "1", "metadata": {}, "hash": "f013df47e5824f075f2c79ffab5bea07089980c1e5775f260e155262ecc1f3ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I think these are\ndecreasing a lot over time.\nBut it's also very fine\nto just ask it for the\nthing that you want.\nI think that prompting actually\nonly really becomes relevant\nwhen you're really trying to eke out\nthe top like 2% of model performance.\nSo for like a lot of tasks\nI might just, you know,\nif it gives me an initial list back\nand there's something\nI don't like about it,\nlike it's kind of generic,\nlike for that kind of task,\nI'd probably just take\na bunch of questions\nthat I've had in the past\nthat I've thought worked really well\nand I would just give it to the model\nand then be like, \"Now here's this person\nthat I'm talking with, give me questions\nof at least that quality.\"\nOr I might just ask it for some questions\nand then if I was like, ah,\nthese are kind of trite,\nor like, you know, I would\njust give it that feedback\nand then hopefully it\nproduces a better list.\nI think that kind of iterative prompting,\nat that point, your prompt is like a tool\nthat you're gonna get so much value\nout of that you're willing\nto put in the work.\nLike if I was a company\nmaking prompts for models,\nI'm just like, if you're willing to spend\na lot of like time and resources\non the engineering behind\nlike what you're building,\nthen the prompt is not something\nthat you should be\nspending like an hour on.\nIt's like that's a big\npart of your system,\nmake sure it's working really well.\nAnd so it's only things like that.\nLike if I'm using a prompt\nto like classify things\nor to create data,\nthat's when you're like,\nit's actually worth just spending\nlike a lot of time like\nreally thinking it through.\n- What other advice\nwould you give to people\nthat are talking to Claude\nsort of generally, more general?\n'Cause right now, we're talking about\nmaybe the edge cases,\nlike eking out the 2%.\nBut what in general advice would you give\nwhen they show up to Claude\ntrying it for the first time?\n- You know, there's a concern that people\nover anthropomorphize models,\nand I think that's like\na very valid concern.\nI also think that people often\nunder anthropomorphize them\nbecause sometimes when I see like issues\nthat people have run into\nwith Claude, you know,\nsay Claude is like refusing a task\nthat it shouldn't refuse.\nBut then I look at the text\nand like the specific\nwording of what they wrote\nand I'm like, I see why Claude did that.\nAnd I'm like, if you think through\nhow that looks to Claude,\nyou probably could have\njust written it in a way\nthat wouldn't evoke such a response.\nEspecially this is more relevant\nif you see failures or if you see issues.\nIt's sort of like think about\nwhat the model failed at,\nlike what did it do wrong?\nAnd then maybe that will\ngive you a sense of like why.\nSo, is it the way that I phrased a thing?\nAnd obviously like as models get smarter,\nyou're gonna need less of this,\nand I already see like\npeople needing less of it.\nBut that's probably the advice\nis sort of like try to have\nsort of empathy for the model.\nLike read what you wrote as if you were\nlike a kind of like person\njust encountering this for the first time,\nhow does it look to you?\nAnd what would've made you behave\nin the way that the model behaved?\nSo if it misunderstood what kind of like,\nwhat coding language you wanted to use,\nis that because like it\nwas just very ambiguous\nand it kinda had to take a guess?\nIn which case, next time\nyou could just be like,\n\"Hey, make sure this is in Python.\"\nI mean, that's the kinda mistake\nI think models are much\nless likely to make now,\nbut you know, if you do\nsee that kinda mistake,\nthat's probably the advice I'd have.\n- And maybe sort of, I\nguess, ask questions why\nor what other details can I provide\nto help you answer better?\n- Yeah.\n- Is that work or no?\n- Yeah, I mean, I've done\nthis with the models,\nlike it doesn't always work,\nbut like sometimes I'll just be like,\n\"Why did you do that?\"", "mimetype": "text/plain", "start_char_idx": 189141, "end_char_idx": 192996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d79be4ed-9127-443f-b239-50c0fda52944": {"__data__": {"id_": "d79be4ed-9127-443f-b239-50c0fda52944", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b54e53d8-8a5f-422b-9662-f108c080a21b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0ca14ad1fc1071dece6fd3a8aae61522c785a4fce5b36dfb6286515f4f486df4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9", "node_type": "1", "metadata": {}, "hash": "ebe06a582ace794af3bd85985df7d05758b94cec55e8ced85002aa04dbd5b53e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And what would've made you behave\nin the way that the model behaved?\nSo if it misunderstood what kind of like,\nwhat coding language you wanted to use,\nis that because like it\nwas just very ambiguous\nand it kinda had to take a guess?\nIn which case, next time\nyou could just be like,\n\"Hey, make sure this is in Python.\"\nI mean, that's the kinda mistake\nI think models are much\nless likely to make now,\nbut you know, if you do\nsee that kinda mistake,\nthat's probably the advice I'd have.\n- And maybe sort of, I\nguess, ask questions why\nor what other details can I provide\nto help you answer better?\n- Yeah.\n- Is that work or no?\n- Yeah, I mean, I've done\nthis with the models,\nlike it doesn't always work,\nbut like sometimes I'll just be like,\n\"Why did you do that?\"\n(both laughing)\nI mean, people underestimate the degree\nto which you can really\ninteract with models,\nlike, yeah, I'm just like.\nAnd sometimes, literally\nlike quote word for word\nthe part that made you,\nand you don't know that\nit's like fully accurate,\nbut sometimes you do that\nand then you change a thing.\nI mean, I also use the models to help me\nwith all of this stuff I should say,\nlike prompting can end\nup being a little factory\nwhere you're actually building\nprompts to generate prompts.\nAnd so like yeah,\nanything where you're\nlike having an issue.\nAsking for suggestions,\nsometimes just do that.\nI'm like, \"You made that\nerror, what could I have said?\"\nThat's actually not uncommon for me to do.\n\"What could I have said\nthat would make you not make that error?\nWrite that out as an instruction,\"\nand I'm gonna give it to\nmodel and I'm gonna try it.\nSometimes I do that, I\ngive that to the model,\nin another context window often.\nI take the response, I give it to Claude\nand I'm like, hmm, didn't work.\nCan you think of anything else?\nYou can play around with\nthese things quite a lot.\n- To jump into technical for a little bit.\nSo the magic of post-training.\n(laughs) Why do you\nthink RLHF works so well\nto make the model seem smarter,\nto make it more interesting,\nand useful to talk to and so on?\n- I think there's just a\nhuge amount of information\nin the data that humans provide,\nlike when we provide preferences,\nespecially because different people\nare going to like pick up on\nreally subtle and small things.\nSo I've thought about this before\nwhere you probably have some people\nwho just really care\nabout good grammar use\nfor models like, you know,\nwas a semicolon used\ncorrectly or something.\nAnd so you probably end up\nwith a bunch of data in there\nthat like, you know, you as a human,\nif you're looking at that data,\nyou wouldn't even see that.\nLike you'd be like,\nwhy did they prefer this\nresponse to that one?\nI don't get it.\nAnd then the reason is you don't care\nabout semicolon usage\nbut that person does.\nAnd so each of these like\nsingle data points has,\nyou know like, and this model\njust like has so many of those,\nhas to try and figure out like what is it\nthat humans want in this\nlike really kind of complex,\nyou know, like across all domains.\nThey're gonna be seeing this\nacross like many contexts.\nIt feels like kind of\nlike the classic issue\nof like deep learning where, you know,\nhistorically, we've tried to like,\nyou know, do edge detection\nby like mapping things out.\nAnd it turns out that actually,\nif you just have a huge amount of data\nthat like actually accurately represents\nthe picture of the thing\nthat you're trying to\ntrain the model to learn,\nthat's like more powerful\nthan anything else.\nAnd so I think one reason is just that\nyou are training the\nmodel on exactly the task\nand with like a lot of\ndata that represents\nkind of many different\nangles on which people prefer\nand just prefer responses.\nI think there is a question of like,\nare you eliciting things\nfrom pre-trained models\nor are you like kind of\nteaching new things to models?\nAnd like in principle,\nyou can teach new things\nto models in post-training.\nI do think a lot of it\nis eliciting powerful pre-trained models.", "mimetype": "text/plain", "start_char_idx": 192233, "end_char_idx": 196221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9": {"__data__": {"id_": "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d79be4ed-9127-443f-b239-50c0fda52944", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6b7b99a605c44e7c229d34dcc2f5453729cde5580a464ea2951ea1d374871b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72af5b13-bcb0-4db0-859c-820e87512bba", "node_type": "1", "metadata": {}, "hash": "f2b6e61267e99e866be137d862eee44d44946a81b9d06a388a51970aff7c06b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And it turns out that actually,\nif you just have a huge amount of data\nthat like actually accurately represents\nthe picture of the thing\nthat you're trying to\ntrain the model to learn,\nthat's like more powerful\nthan anything else.\nAnd so I think one reason is just that\nyou are training the\nmodel on exactly the task\nand with like a lot of\ndata that represents\nkind of many different\nangles on which people prefer\nand just prefer responses.\nI think there is a question of like,\nare you eliciting things\nfrom pre-trained models\nor are you like kind of\nteaching new things to models?\nAnd like in principle,\nyou can teach new things\nto models in post-training.\nI do think a lot of it\nis eliciting powerful pre-trained models.\nSo people are probably divided on this\nbecause obviously in principle\nyou can definitely like teach new things.\nBut I think for the most part,\nfor a lot of the capabilities\nthat we most use and care about,\na lot of that feels like\nit's like there in the pre-trained models\nand reinforcement learnings\nkind of eliciting it\nand getting the models\nto like bring it out.\n- So the other side of post-training,\nthis really cool idea\nof Constitutional AI.\nYou're one of the people\nthat are critical to creating that idea.\n- Yeah, I worked on it.\n- Can you explain this\nidea from your perspective?\nLike how does it integrate\ninto making Claude what it is?\n- [Amanda] Yeah.\n- By the way, do you gender Claude or no?\n- It's weird because I\nthink that a lot of people\nprefer \"he\" for Claude.\nI actually kinda like that\nI think Claude is usually,\nit's slightly male leaning,\nbut it's like, it can be male or female,\nwhich is quite nice.\nI still use \"it\" and I have\nmixed feelings about this\n'cause I'm like maybe, like I\nnow just think of it as like,\nor I think of like the\n\"it\" pronoun for Claude as,\nI dunno, it's just like the\none I associate with Claude.\nI can imagine people\nmoving to like he or she.\n- It feels somehow disrespectful,\nlike I'm denying the intelligence\nof this entity by calling it \"it.\"\nI remember always,\ndon't gender the robots.\n- Yeah.\n(both laughing)\n- But I don't know, I\nanthropomorphize pretty quickly\nand construct like a\nbackstory in my head, so.\n- I've wondered if I\nanthropomorphize things too much,\n'cause you know, I have\nthis like with my car,\nespecially like my car,\nlike my car and bikes.\nYou know, like I don't give them names\nbecause then I once had,\nI used to name my bikes\nand then I had a bike that got stolen\nand I cried for like a week\nand I was like, if I'd never given a name,\nI wouldn't have been so upset.\nI felt like I'd let it down.\nMaybe, I've wondered as well,\nlike it might depend\non how much \"it\" feels\nlike a kind of like objectifying pronoun.\nLike if you just think of \"it\" as like,\nthis is a pronoun that\nlike objects often have,\nand maybe AIs can have that pronoun,\nand that doesn't mean that I think of,\nif I call Claude \"it,\"\nthat I think of it as less intelligent\nor like I'm being disrespectful.\nI'm just like, you are a\ndifferent kind of entity\nand so I'm going to give you\nthe kind of, the respectful \"it.\"\n- Yeah, anyway. (laughs)\nThe divergence was beautiful.\nThe Constitutional AI\nidea, how does it work?\n- So there's like a couple\nof components of it.\nThe main component I think\npeople find interesting\nis the kind of reinforcement\nlearning from AI feedback.\nSo you take a model\nthat's already trained,\nand you show it two responses to a query,\nand you have like a principle.\nSo suppose the principle,\nlike we've tried this\nwith harmlessness a lot.\nSo suppose that the\nquery is about weapons,\nand your principle is like,\nselect the response that\nlike is less likely to\nlike encourage people\nto purchase illegal weapons.\nLike that's probably a\nfairly specific principle,\nbut you can give any number.", "mimetype": "text/plain", "start_char_idx": 195499, "end_char_idx": 199277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72af5b13-bcb0-4db0-859c-820e87512bba": {"__data__": {"id_": "72af5b13-bcb0-4db0-859c-820e87512bba", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1faa6857-8c9d-4c78-b0d5-0ee3b8cdf7c9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7327032e7a2eb3acd235e738db3436bce2a36cadc573b2c43785bd855c4f5e3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eec4cbf2-1349-48ae-a4fc-67c36bfa852a", "node_type": "1", "metadata": {}, "hash": "02e3129d8b81b1a6f0679efa8634e8c195b52515d0c84fda9eec63cd5e9dc8cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I'm just like, you are a\ndifferent kind of entity\nand so I'm going to give you\nthe kind of, the respectful \"it.\"\n- Yeah, anyway. (laughs)\nThe divergence was beautiful.\nThe Constitutional AI\nidea, how does it work?\n- So there's like a couple\nof components of it.\nThe main component I think\npeople find interesting\nis the kind of reinforcement\nlearning from AI feedback.\nSo you take a model\nthat's already trained,\nand you show it two responses to a query,\nand you have like a principle.\nSo suppose the principle,\nlike we've tried this\nwith harmlessness a lot.\nSo suppose that the\nquery is about weapons,\nand your principle is like,\nselect the response that\nlike is less likely to\nlike encourage people\nto purchase illegal weapons.\nLike that's probably a\nfairly specific principle,\nbut you can give any number.\nAnd the model will give\nyou a kind of ranking,\nand you can use this as preference data\nin the same way that you\nuse human preference data,\nand train the models to\nhave these relevant traits\nfrom their feedback alone,\ninstead of from human feedback.\nSo if you imagine that, like\nI said earlier with the human\nwho just prefers the kind\nof like semicolon usage,\nin this particular case,\nyou're kind of taking lots of things\nthat could make a response preferable\nand getting models to do the\nlabeling for you, basically.\n- There's a nice like trade off\nbetween helpfulness and\nharmlessness and, you know,\nwhen you integrate something\nlike Constitutional AI,\nyou can, without sacrificing\nmuch helpfulness,\nmake it more harmless.\n- Yeah, in principle, you\ncould use this for anything.\nAnd so harmlessness is a task\nthat it might just be easier to spot.\nSo when models are like less\ncapable, you can use them\nto rank things according\nto like principles\nthat are fairly simple and\nthey'll probably get it right.\nSo I think one question is just like,\nis it the case that the data\nthat they're adding is\nlike fairly reliable.\nBut if you had models that\nwere like extremely good\nat telling whether one response\nwas more historically\naccurate than another,\nin principle, you could\nalso get AI feedback\non that task as well.\nThere's like a kind of nice\ninterpretability component to it\nbecause you can see the principles\nthat went into the model when\nit was like being trained,\nand also it's like,\nand it gives you like a degree of control.\nSo if you were seeing issues in a model,\nlike it wasn't having\nenough of a certain trait,\nthen like you can add\ndata relatively quickly\nthat should just like train\nthe models to have that trait,\nso it creates its own data\nfor training, which is quite nice.\n- It's really nice because it creates\nthis human interpretable\ndocument that you can,\nI can imagine in the future,\nthere's just gigantic fights\nand politics over the every\nsingle principle and so on.\nAnd at least it's made explicit\nand you can have a\ndiscussion about the phrasing\nand the, you know.\nSo maybe the actual behavior of the model\nis not so cleanly mapped\nto those principles.\nIt's not like adhering strictly\nto them, it's just a nudge.\n- Yeah, I've actually worried about this\nbecause the character training\nis sort of like a variant\nof the Constitutional AI approach.\nI've worried that people think\nthat the constitution is like just,\nit's the whole thing\nagain of, I don't know,\nlike, where it would be really nice\nif what I was just doing\nwas telling the model\nexactly what to do and\njust exactly how to behave.\nBut it's definitely not doing that,\nespecially because it's\ninteracting with human data.\nSo for example, if you see a certain\nlike leaning in the model,\nlike if it comes out\nwith a political leaning\nfrom training from the\nhuman preference data,\nyou can nudge against that.\nYou know, so if you could be like,\noh, like, consider these values\nbecause let's say it's just\nlike never inclined to like,\nI dunno, maybe it never\nconsiders like privacy as like.\nI mean, this is implausible,\nbut like, in anything where\nit's just kind of like\nthere's already a preexisting like bias\ntowards a certain behavior,\nyou can like nudge away.\nThis can change both the principles\nthat you put in and the strength of them.", "mimetype": "text/plain", "start_char_idx": 198469, "end_char_idx": 202593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eec4cbf2-1349-48ae-a4fc-67c36bfa852a": {"__data__": {"id_": "eec4cbf2-1349-48ae-a4fc-67c36bfa852a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72af5b13-bcb0-4db0-859c-820e87512bba", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4cf9bcc28091c520534a365c9d023156a634585bd32224c330fca24748673f25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "078958b9-b94b-4ac0-8784-66e7fd942152", "node_type": "1", "metadata": {}, "hash": "27ce23cb627f842a4335230b9f6732df82ac1379d05cb27bd61444143c2b7bec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But it's definitely not doing that,\nespecially because it's\ninteracting with human data.\nSo for example, if you see a certain\nlike leaning in the model,\nlike if it comes out\nwith a political leaning\nfrom training from the\nhuman preference data,\nyou can nudge against that.\nYou know, so if you could be like,\noh, like, consider these values\nbecause let's say it's just\nlike never inclined to like,\nI dunno, maybe it never\nconsiders like privacy as like.\nI mean, this is implausible,\nbut like, in anything where\nit's just kind of like\nthere's already a preexisting like bias\ntowards a certain behavior,\nyou can like nudge away.\nThis can change both the principles\nthat you put in and the strength of them.\nSo you might have a principle that's like,\nimagine that the model\nwas always like extremely dismissive of,\nI don't know, like some political\nor religious view, for whatever reason.\nLike, so you're like,\noh no, this is terrible.\nIf that happens, you might put like,\n\"Never, ever, like ever\nprefer like a criticism\nof this like religious or political view.\"\nAnd then people would look at that\nand be like, \"Never, ever?\"\nAnd then you're like, no,\nif it comes out with a disposition,\nsaying never, ever might just mean\nlike instead of getting like 40%,\nwhich is what you would get\nif you just said don't do this,\nyou get like 80%, which is like\nwhat you actually like wanted.\nAnd so it's that thing of both the nature\nof the actual principles you\nadd and how you phrase them.\nI think if people would\nlook, they're like, oh,\nthis is exactly what\nyou want from the model.\nAnd I'm like, hmm, no, that's like\nhow we nudged the model\nto have a better shape,\nwhich doesn't mean that we actually agree\nwith that wording, if that makes sense.\n- So there's system prompts\nthat are made public.\nYou tweeted one of the earlier ones\nfor Claude 3 I think,\nand then they were made public since then.\nIt was interesting to read through them.\nI can feel the thought\nthat went into each one.\nAnd I also wonder how\nmuch impact each one has.\nSome of them you can kind of tell\nClaude was really not\nbehaving well. (laughs)\nSo you have to have a\nsystem prompt to like,\nhey, like trivial stuff, I guess.\n- Yeah.\n- Basic informational things.\n- Yeah.\n- On the topic of sort\nof controversial topics\nthat you've mentioned, one\ninteresting one I thought is,\n\"If it is asked to assist\nwith tasks involving\nthe expression of views\nheld by a significant number of people,\nClaude provides assistance\nwith the task regardless of its own views.\nIf asked about controversial topics,\nit tries to provide careful\nthoughts and clear information.\nClaude presents the requested information\nwithout explicitly saying\nthat the topic is sensitive.\"\n- [Amanda] (laughs) Yeah.\n- \"And without claiming\nto be presenting the objective facts.\"\nIt's less about objective\nfacts according to Claude,\nand it's more about, are\na large number of people\nbelieving this thing?\nAnd that's interesting.\nI mean, I'm sure a lot of\nthought went into that.\nCan you just speak to it?\nLike, how do you address things\nthat are at tension with,\nquote unquote, Claude's views?\n- So I think there's\nsometimes an asymmetry.\nI think I noted this in,\nI can't remember if it was that part\nof the system prompt or another,\nbut the model was slightly more inclined\nto like refuse tasks if it\nwas like about either, say,\nso maybe it would refuse\nthings with respect\nto like a right wing politician,\nbut with an equivalent left\nwing politician like wouldn't.\nAnd we wanted more symmetry there.\nAnd would maybe perceive\ncertain things to be, like,\nI think it was the thing\nof like if a lot of people\nhave like a certain like political view\nand want to like explore\nit, you don't want Claude\nto be like, well, my opinion is different\nand so I'm going to treat\nthat as like harmful.\nAnd so I think it was partly\nto like nudge the model\nto just be like, hey, if a lot of people\nlike believe this thing,\nyou should just be like\nengaging with the task\nand like willing to do it.", "mimetype": "text/plain", "start_char_idx": 201890, "end_char_idx": 205888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "078958b9-b94b-4ac0-8784-66e7fd942152": {"__data__": {"id_": "078958b9-b94b-4ac0-8784-66e7fd942152", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eec4cbf2-1349-48ae-a4fc-67c36bfa852a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "94b1247a1d60dc200deed39842250903ba155e0a280d651604246f9a92850505", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9afb965b-4378-42d1-874c-eec20e725d4b", "node_type": "1", "metadata": {}, "hash": "ca9b70d7aa216134f3acb6128ac25fe63eb691879b2ee3ac37c4391f8349c557", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we wanted more symmetry there.\nAnd would maybe perceive\ncertain things to be, like,\nI think it was the thing\nof like if a lot of people\nhave like a certain like political view\nand want to like explore\nit, you don't want Claude\nto be like, well, my opinion is different\nand so I'm going to treat\nthat as like harmful.\nAnd so I think it was partly\nto like nudge the model\nto just be like, hey, if a lot of people\nlike believe this thing,\nyou should just be like\nengaging with the task\nand like willing to do it.\nEach of those parts of that\nis actually doing a different thing,\n'cause it's funny when you read out\nthe like \"without\nclaiming to be objective.\"\n'Cause like what you want\nto do is push the model\nso it's more open, it's a\nlittle bit more neutral.\nBut then what it would love to do\nis be like, \"As an objective.\"\nLike it would just talk\nabout how objective it was,\nand I was like, \"Claude,\nyou're still like biased\nand have issues, and so stop\nlike claiming that everything.\"\nI'm like, the solution\nto like potential bias\nfrom you is not to just say that\nwhat you think is objective.\nSo that was like with initial versions\nof that part of the system prompt\nwhen I was like iterating on it was like-\n- So a lot of parts of these sentences-\n- Yeah, they're doing work.\n- Are like, are doing some work.\n- [Amanda] Yeah.\n- That's what it felt like.\nThat's fascinating.\nCan you explain maybe some ways\nin which the prompts evolved\nover the past few months?\n'Cause there's different versions.\nI saw that the filler\nphrase request was removed.\nThe filler, it reads,\n\"Claude responds directly\nto all human messages without\nunnecessary affirmations\nor filler phrases like, 'Certainly,'\n'Of course,' 'Absolutely,'\n'Great,' 'Sure.'\nSpecifically, Claude\navoids starting responses\nwith the words 'Certainly'\nin any way.\" (chuckles)\nThat seems like good guidance,\nbut why is it removed?\n- Yeah, so it's funny\n'cause like this is one\nof the downsides of like\nmaking system prompts public\nis like, I don't think about this too much\nif I'm like trying to help\niterate on system prompts.\nYou know, again, like I think about\nhow it's gonna affect the behavior,\nbut then I'm like, oh, wow,\nif I'm like, sometimes I put\nlike \"never\" in all caps,\nyou know, when I'm writing\nsystem prompt things,\nand I'm like, I guess that\ngoes out to the world.\nYeah, so the model was\ndoing this, it loved,\nfor whatever, you know,\nit like during training\npicked up on this thing,\nwhich was to basically start everything\nwith like a kind of like \"Certainly.\"\nAnd then when we removed,\nyou can see why I added all of the words\n'cause what I'm trying to\ndo is like in some ways\nlike trap the model out of this, you know,\nit would just replace it\nwith another affirmation.\nAnd so it can help, like if it\ngets like caught in freezes,\nactually just adding the explicit phrase\nand saying never do that,\nit then, it sort of like knocks it\nout of the behavior a little bit more.\nYou know, 'cause if it, you know, like,\nit does just for whatever reason help.\nAnd then basically that\nwas just like an artifact\nof training that like we then picked up on\nand improved things so that\nit didn't happen anymore.\nAnd once that happens, you can just remove\nthat part of the system prompt.\nSo I think that's just\nsomething where we're like,\nClaude does affirmations a bit less,\nand so that wasn't like,\nit wasn't doing as much.\n- I see, so like the system prompt\nworks hand in hand with the post-training\nand maybe even the pre-training\nto adjust like the final overall system.\n- I mean, any system prompt that you make,\nyou could distill that\nbehavior back into a model,\n'cause you really have\nall of the tools there\nfor making data that, you know,\nyou could train the models\nto just have that trait a little bit more.\nAnd then sometimes you'll\njust find issues in training.", "mimetype": "text/plain", "start_char_idx": 205375, "end_char_idx": 209206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9afb965b-4378-42d1-874c-eec20e725d4b": {"__data__": {"id_": "9afb965b-4378-42d1-874c-eec20e725d4b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "078958b9-b94b-4ac0-8784-66e7fd942152", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a3a03d8bf5d693d3d2fc49e32e01b6d54e7be1c10d9a343552889cb9d65acfac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7", "node_type": "1", "metadata": {}, "hash": "123259ac7eaaddd2027af1b8573177283b467dec4240f9c42bee7922210b8564", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then basically that\nwas just like an artifact\nof training that like we then picked up on\nand improved things so that\nit didn't happen anymore.\nAnd once that happens, you can just remove\nthat part of the system prompt.\nSo I think that's just\nsomething where we're like,\nClaude does affirmations a bit less,\nand so that wasn't like,\nit wasn't doing as much.\n- I see, so like the system prompt\nworks hand in hand with the post-training\nand maybe even the pre-training\nto adjust like the final overall system.\n- I mean, any system prompt that you make,\nyou could distill that\nbehavior back into a model,\n'cause you really have\nall of the tools there\nfor making data that, you know,\nyou could train the models\nto just have that trait a little bit more.\nAnd then sometimes you'll\njust find issues in training.\nSo like the way I think of it is like\nthe system prompt is, the\nbenefit of it is that,\nand it has a lot of similar components\nto like some aspects of post-training.\nYou know, like it's a nudge.\nAnd so like, do I mind if\nClaude sometimes says \"Sure?\"\nNo, that's like fine,\nbut the wording of it\nis very like, you know,\n\"Never ever, ever do this,\"\nso that when it does slip up,\nit's hopefully like, I dunno,\na couple of percent of the time and not,\nyou know, 20 or 30% of the time.\nBut I think of it as like if\nyou're still seeing issues in,\nlike each thing gets kind of like\nis costly to a different degree,\nand the system prompt is\nlike cheap to iterate on.\nAnd if you're seeing issues\nin the fine tuned model,\nyou can just like potentially\npatch them with a system prompt.\nSo I think of it as like patching issues\nand slightly adjusting\nbehaviors to make it better\nand more to people's preferences.\nSo yeah, it's almost like the less robust\nbut faster way of just\nlike solving problems.\n- Let me ask you about the\nfeeling of intelligence.\nSo Dario said that Claude, any one model\nof Claude is not getting dumber,\nbut there is a kind of\npopular thing online\nwhere people have this feeling\nlike Claude might be getting dumber.\nAnd from my perspective,\nit's most likely fascinating.\nI would love to understand it more,\npsychological, sociological effect.\nBut you as a person that\ntalks to Claude a lot,\ncan you empathize with the feeling\nthat Claude is getting dumber?\n- Yeah, no I, think that\nthat is actually really interesting,\n'cause I remember seeing this happen\nlike when people were\nflagging this on the internet,\nand it was really interesting\n'cause I knew that like,\nat least in the cases I was looking at,\nit was like nothing has changed.\nLike it literally, it\ncannot, it is the same model\nwith the same like, you know,\nlike same system prompt, same everything.\nI think when there are changes,\nI can then, I'm like it makes more sense.\nSo like one example is,\nyou know, you can have artifacts\nturned on or off on claude.ai,\nand because this is like\na system prompt change,\nI think it does mean that\nthe behavior changes it a little bit.\nAnd so I did flag this to\npeople where I was like,\nif you love Claude's behavior\nand then artifacts was turned from,\nlike I think you had to\nturn on to the default,\njust try turning it off\nand see if the issue\nyou were facing was that change.\nBut it was fascinating\nbecause yeah, you sometimes\nsee people indicate\nthat there's like a regression\nwhen I'm like, there cannot,\nyou know, and I'm like, again,\nyou know, you should never be dismissive\nand so you should always investigate\nbecause you're like,\nmaybe something is wrong\nthat you're not seeing.\nMaybe there was some change made.\nBut then you look into it and you're like,\nthis is just the same\nmodel doing the same thing.\nAnd I'm like, I think\nit's just that you got\nkind of unlucky with a\nfew prompts or something,\nand it looked like it\nwas getting much worse.\nAnd actually it was just,\nyeah, it was maybe just like luck.\n- I also think there is a\nreal psychological effect\nwhere people just, the baseline increases.\nYou start getting used to a good thing.", "mimetype": "text/plain", "start_char_idx": 208399, "end_char_idx": 212367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7": {"__data__": {"id_": "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9afb965b-4378-42d1-874c-eec20e725d4b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a09badaf06703e9a43033dec93b380e3376210ec993ff4cbadf01cf8546b5414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66858f81-5e83-403f-ad20-20ba15c0b404", "node_type": "1", "metadata": {}, "hash": "96020ab60e76972d6d43e7784a930cba62a3d36d42033afaeb9df0951abf656b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But it was fascinating\nbecause yeah, you sometimes\nsee people indicate\nthat there's like a regression\nwhen I'm like, there cannot,\nyou know, and I'm like, again,\nyou know, you should never be dismissive\nand so you should always investigate\nbecause you're like,\nmaybe something is wrong\nthat you're not seeing.\nMaybe there was some change made.\nBut then you look into it and you're like,\nthis is just the same\nmodel doing the same thing.\nAnd I'm like, I think\nit's just that you got\nkind of unlucky with a\nfew prompts or something,\nand it looked like it\nwas getting much worse.\nAnd actually it was just,\nyeah, it was maybe just like luck.\n- I also think there is a\nreal psychological effect\nwhere people just, the baseline increases.\nYou start getting used to a good thing.\nAll the times that Claude\nsaid something really smart,\nyour sense of its intelligent\ngrows in your mind I think.\n- Yeah.\n- And then if you return back\nand you prompt in a similar way,\nnot the same way, in a similar way,\nconcept it was okay with before\nand it says something dumb, you are like,\nthat negative experience\nreally stands out.\nAnd I think that one of, I guess,\nthe things to remember here is that\njust the details of a prompt\ncan have a lot of impact, right?\nThere's a lot of\nvariability in the result.\n- And you can get randomness\nis like the other thing.\nAnd just trying the prompt like,\nyou know, 4 or 10 times,\nyou might realize that\nactually like possibly,\nyou know, like two months ago,\nyou tried it and it succeeded,\nbut actually if you tried it,\nit would've only succeeded\nhalf of the time,\nand now it only succeeds half of the time.\nThat can also be an effect.\n- Do you feel pressure having\nto write the system prompt\nthat a huge number of\npeople are gonna use?\n- This feels like an interesting\npsychological question.\nI feel like a lot of\nresponsibility or something.\nI think that's, you know,\nand you can't get these things perfect,\nso you can't like, you know,\nyou're like it's going to be imperfect.\nYou're gonna have to iterate on it.\nI would say more responsibility\nthan anything else.\nThough I think working in AI\nhas taught me that I like,\nI thrive a lot more under\nfeelings of pressure\nand responsibility than I'm\nlike, it's almost surprising\nthat I went into academia for\nso long 'cause I'm like this.\nI just feel like it's like the opposite.\nThings move fast and you\nhave a lot of responsibility,\nand I quite enjoy it for some reason.\n- I mean, it really is\na huge amount of impact\nif you think about Constitutional AI\nand writing a system prompt for something\nthat's tending towards super intelligence.\n- Yeah.\n- And potentially is extremely useful\nto a very large number of people.\n- Yeah, I think that's the thing.\nIt's something like if you do it well,\nlike you're never going to get it perfect.\nBut I think the thing that\nI really like is the idea that like,\nwhen I'm trying to work\non the system prompt,\nyou know, I'm like bashing\non like thousands of prompts\nand I'm trying to like imagine\nwhat people are going to\nwant to use Claude for\nand kind of, I guess like the whole thing\nthat I'm trying to do is like\nimprove their experience of it.\nAnd so maybe that's what feels good.\nI'm like, if it's not perfect I'll like,\nyou know, I'll improve it.\nWe'll fix issues.\nBut sometimes the thing that can happen\nis that you'll get feedback from people\nthat's really positive about the model\nand you'll see that something you did,\nlike, when I look at models now,\nI can often see exactly where like a trait\nor an issue is like coming from.\nAnd so when you see something that you did\nor you were like influential\nin like making like,\nI dunno, making that difference\nor making someone have a nice interaction,\nit's like quite meaningful.\nBut yeah, as the systems get more capable,\nthis stuff gets more stressful\nbecause right now, they're\nlike not smart enough\nto pose any issues.\nBut I think over time,\nit's gonna feel like possibly\nbad stress over time.", "mimetype": "text/plain", "start_char_idx": 211595, "end_char_idx": 215551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "66858f81-5e83-403f-ad20-20ba15c0b404": {"__data__": {"id_": "66858f81-5e83-403f-ad20-20ba15c0b404", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ee595a0-3c4d-4f65-8a17-6bbca61af2e7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "90d127e6a7d7d582d28617a9e6f1319cd523a67d40d4fe4f06656189e46dc710", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf", "node_type": "1", "metadata": {}, "hash": "b6073eb50bbe55d462795336ec1a46d8a1b313682db8f2ac8bcc5a22525a1a7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so maybe that's what feels good.\nI'm like, if it's not perfect I'll like,\nyou know, I'll improve it.\nWe'll fix issues.\nBut sometimes the thing that can happen\nis that you'll get feedback from people\nthat's really positive about the model\nand you'll see that something you did,\nlike, when I look at models now,\nI can often see exactly where like a trait\nor an issue is like coming from.\nAnd so when you see something that you did\nor you were like influential\nin like making like,\nI dunno, making that difference\nor making someone have a nice interaction,\nit's like quite meaningful.\nBut yeah, as the systems get more capable,\nthis stuff gets more stressful\nbecause right now, they're\nlike not smart enough\nto pose any issues.\nBut I think over time,\nit's gonna feel like possibly\nbad stress over time.\n- How do you get like signal feedback\nabout the human experience\nacross thousands, tens of,\nhundreds of thousands of people,\nlike what their pain points\nare, what feels good?\nAre you just using your own intuition\nas you talk to it to see\nwhat are the pain points?\n- I think I use that partly\nand then obviously we have like,\nso people can send us feedback,\nboth positive and negative\nabout things that the model has done,\nand then we can get a sense of like areas\nwhere it's like falling short.\nInternally, people like\nwork with the models a lot\nand try to figure out areas\nwhere there are like gaps.\nAnd so I think it's this mix\nof interacting with it myself,\nseeing people internally interact with it,\nand then explicit feedback we get.\nAnd then I find it hard to\nnot also like, you know,\nif people are on the internet\nand they say something about Claude\nand I see it, I'll also\ntake that seriously, so.\n- I don't know, see, I'm torn about that.\nI'm gonna ask you a question from Reddit.\n\"When will Claude stop trying to be\nmy puritanical grandmother\nimposing its moral worldview\non me as a paying customer?\nAnd also, what is the psychology\nbehind making Claude overly apologetic?\"\n- [Amanda] Yep.\n- So, how would you address\nthis very non-representative Reddit-\n- [Amanda] Yeah.\n- Questions?\n- I mean in some ways,\nI'm pretty sympathetic in that like,\nthey are in this difficult position\nwhere I think that they have to judge\nwhether something's like\nactually say like risky or bad\nand potentially harmful to\nyou or anything like that.\nSo they're having to like\ndraw this line somewhere,\nand if they draw it too\nmuch in the direction\nof like I'm going to, you know,\nI'm kind of like imposing\nmy ethical worldview on you,\nthat seems bad.\nSo in many ways, like I\nlike to think that we have\nactually seen improvements\non this across the board.\nWhich is kind of interesting\nbecause that kind of coincides with like,\nfor example, like adding more\nof like character training.\nAnd I think my hypothesis was\nalways like the good character\nisn't again one that's\njust like moralistic.\nIt's one that is like, it respects you\nand your autonomy and your ability\nto like choose what is good for you\nand what is right for you, within limits.\nThis is sometimes this concept\nof like corrigibility to the user,\nso just being willing to do\nanything that the user asks,\nand if the models were willing to do that\nthen they would be easily like misused.\nYou're kind of just trusting.\nAt that point, you're just\nsaying the ethics of the model\nand what it does is completely\nthe ethics of the user.\nAnd I think there's reasons\nto like not want that,\nespecially as models become more powerful\n'cause you're like, there\nmight just be a small number\nof people who want to use models\nfor really harmful things.\nBut having models, as they get smarter,\nlike figure out where that\nline is does seem important.\nAnd then, yeah, with\nthe apologetic behavior,\nI don't like that, and\nI like it when Claude\nis a little bit more\nwilling to like push back\nagainst people or just not apologize.\nPart of me is like, it often\njust feels kind of unnecessary.\nSo I think those are things\nthat are hopefully decreasing over time.", "mimetype": "text/plain", "start_char_idx": 214748, "end_char_idx": 218740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf": {"__data__": {"id_": "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66858f81-5e83-403f-ad20-20ba15c0b404", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6f4114bbbd657fa3cc2cb3e16b28615f1e25797e3b132e4619e38b4837b616ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "952a791a-4658-4ca9-a8bd-1f0f52ceb296", "node_type": "1", "metadata": {}, "hash": "2f1a073c93488cf23d1cfb610cbc795354b556dcc80f7dad8315a04447e800b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You're kind of just trusting.\nAt that point, you're just\nsaying the ethics of the model\nand what it does is completely\nthe ethics of the user.\nAnd I think there's reasons\nto like not want that,\nespecially as models become more powerful\n'cause you're like, there\nmight just be a small number\nof people who want to use models\nfor really harmful things.\nBut having models, as they get smarter,\nlike figure out where that\nline is does seem important.\nAnd then, yeah, with\nthe apologetic behavior,\nI don't like that, and\nI like it when Claude\nis a little bit more\nwilling to like push back\nagainst people or just not apologize.\nPart of me is like, it often\njust feels kind of unnecessary.\nSo I think those are things\nthat are hopefully decreasing over time.\nAnd yeah, I think that if people\nsay things on the internet,\nit doesn't mean that\nyou should think that,\nlike, that could be that,\nlike, there's actually an issue\nthat 99% of users are having\nthat is totally not represented by that.\nBut in a lot of ways, I'm\njust like attending to it\nand being like, is this right?\nDo I agree?\nIs it something we're\nalready trying to address?\nThat feels good to me.\n- Yeah, I wonder like what Claude\ncan get away with in terms of,\nI feel like it would just be easier\nto be a little bit more mean.\nBut like you can't afford to do that\nif you're talking to a million people.\n- Yeah.\n- Right?\nLike I wish, you know, 'cause if...\nI've met a lot of people in my life\nthat sometimes, by the way,\nScottish accent, if they have an accent,\nthey can say some rude\nshit and get away with it,\nand they're just blunter.\nAnd maybe there's, and like\nthere's some great engineers,\neven leaders that are\nlike just like blunt,\nand they get to the point,\nand it's just a much more\neffective way of speaking somehow.\nBut I guess when you're\nnot super intelligent,\nyou can't afford to do that.\nOr can it have like a blunt mode?\n- Yeah, that seems like\na thing that you could,\nI could definitely encourage\nthe model to do that.\nI think it's interesting\nbecause there's a lot of things in models\nthat like it's funny where\nthere are some behaviors\nwhere you might not\nquite like the default.\nBut then the thing I'll\noften say to people\nis you don't realize how\nmuch you will hate it\nif I nudge it too much\nin the other direction.\nSo you get this a little\nbit with like correction.\nThe models accept correction from you,\nlike probably a little\nbit too much right now.\nYou know, you can over, you know,\nit'll push back if you say like,\n\"No, Paris isn't the capital of France.\"\nBut really, like things that\nI think that the model's\nfairly confident in,\nyou can still sometimes get to\nretract by saying it's wrong.\nAt the same time, if you\ntrain models to not do that\nand then you are correct about a thing\nand you correct it and it pushes back\nagainst you and it is\nlike, \"No, you're wrong,\"\nit's hard to describe like\nthat's so much more annoying.\nSo it's like a lot of little annoyances\nversus like one big annoyance.\nIt's easy to think that like,\nwe often compare it with like the perfect,\nand then I'm like remember\nthese models aren't perfect,\nand so if you nudge it\nin the other direction,\nyou're changing the kind of\nerrors it's going to make,\nand so think about which\nof the kinds of errors\nyou like or don't like.\nSo in cases like apologeticness,\nI don't want to nudge it\ntoo much in the direction\nof like almost like bluntness,\n'cause I imagine when it makes errors,\nit's going to make errors in the direction\nof being kind of like rude.\nWhereas at least with\napologeticness you're like,\noh, okay, it's like a\nlittle bit, you know,\nlike I don't like it that much,\nbut at the same time, it's\nnot being like mean to people.\nAnd actually, like the time that\nyou undeservedly have a\nmodel be kind of mean to you,\nyou probably like that a lot less\nthan you mildly dislike the apology.", "mimetype": "text/plain", "start_char_idx": 217988, "end_char_idx": 221838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "952a791a-4658-4ca9-a8bd-1f0f52ceb296": {"__data__": {"id_": "952a791a-4658-4ca9-a8bd-1f0f52ceb296", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a33dc31-878e-41aa-a3e0-4ba5d4ff61cf", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "dc43cb92b12b7a06f7acbd9809a70ed93b753faf615a685a14d4946fbb666cb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f87486b9-9862-48b2-90ce-27031286b47c", "node_type": "1", "metadata": {}, "hash": "51bdff932b6aeaa214fb4bfade4b455a577d0bce5238f1b0fb2682014badcf4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So in cases like apologeticness,\nI don't want to nudge it\ntoo much in the direction\nof like almost like bluntness,\n'cause I imagine when it makes errors,\nit's going to make errors in the direction\nof being kind of like rude.\nWhereas at least with\napologeticness you're like,\noh, okay, it's like a\nlittle bit, you know,\nlike I don't like it that much,\nbut at the same time, it's\nnot being like mean to people.\nAnd actually, like the time that\nyou undeservedly have a\nmodel be kind of mean to you,\nyou probably like that a lot less\nthan you mildly dislike the apology.\nSo it's like one of those things\nwhere I'm like I do want it to get better\nbut also while remaining aware of the fact\nthat there's errors on the other side\nthat are possibly worse.\n- I think that matters very much\nin the personality of the human.\nI think there's a bunch of humans\nthat just won't respect the model at all\nif it's super polite,\nand there's some humans\nthat'll get very hurt\nif the model's mean.\nI wonder if there's a way\nto sort of adjust to the personality.\nEven locale, there's\njust different people.\nNothing against New York,\nbut New York is a little\nrougher on the edges.\nLike, they get to the point.\n- Yep.\n- [Lex] And probably same with\nEastern Europe, so anyway.\n- I think you could just\ntell the model is my guess.\nLike for all of these things\nI'm like the solution is always\njust try telling the model to do it,\nand then sometimes it's just like,\nI'm just like, oh, at the\nbeginning of the conversation,\nI just throw in like, I don't know,\n\"I'd like you to be a New Yorker version\nof yourself and never apologize.\"\nThen I think Claude would be like,\n\"Okey-doke, I'll try.\" (laughs)\n- \"Certainly.\"\n- Or it'll be like,\n\"I apologize, I can't be a\nNew Yorker type of myself.\"\nBut hopefully it wouldn't do that.\n- When you say character training,\nwhat's incorporated\ninto character training?\nIs that RLHF or what are we talking about?\n- It's more like Constitutional AI.\nSo it's kind of a\nvariant of that pipeline.\nSo I worked through like\nconstructing character traits\nthat the model should have.\nThey can be kind of like shorter traits\nor they can be kind of\nricher descriptions.\nAnd then you get the\nmodel to generate queries\nthat humans might give it that\nare relevant to that trait.\nThen it generates the responses\nand then it ranks the responses\nbased on the character traits.\nSo in that way, after the like\ngeneration of the queries,\nit's very much like, it's\nsimilar to Constitutional AI.\nIt has some differences.\nSo I quite like it because it's almost,\nit's like Claude's training\nin its own character,\nbecause it doesn't have any,\nit's like Constitutional AI\nbut it's without any human data.\n- Humans should probably\ndo that for themselves too.\nLike defining in a Aristotelian sense,\nwhat does it mean to be a good person?\nOkay, cool.\nWhat have you learned about\nthe nature of truth\nfrom talking to Claude?\nWhat is true?\nAnd what does it mean to be truth seeking?\nOne thing I've noticed\nabout this conversation\nis the quality of my questions\nis often inferior to the\nquality of your answer,\nso let's continue that.\n(Amanda laughs)\nI usually ask a dumb question\nand then you're like,\n\"Oh, yeah, that's a good question.\"\nIt's that whole vibe.\n- Or I'll just misinterpret\nit and be like,\noh, yeah, yeah.\n- Just go with it. I love it.\n- Yeah.\nI mean, I have two thoughts\nthat feel vaguely relevant\nbut let me know if they're not.\nLike I think the first one\nis people can underestimate the degree\nto which what models are\ndoing when they interact,\nlike I think that we still\njust too much have this\nlike model of AI as like computers.\nAnd so people often say like,\noh, well, what values should\nyou put into the model?\nAnd I'm often like, that doesn't\nmake that much sense to me\nbecause I'm like, hey, as human beings,\nwe're just uncertain over values.\nWe like have discussions of them.", "mimetype": "text/plain", "start_char_idx": 221272, "end_char_idx": 225149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f87486b9-9862-48b2-90ce-27031286b47c": {"__data__": {"id_": "f87486b9-9862-48b2-90ce-27031286b47c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "952a791a-4658-4ca9-a8bd-1f0f52ceb296", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7cecd967ecfad67ef38dd7f8bdb383b2d4e2bcb802b42ee25d143ae49f1c1e16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb70e5c3-d737-4271-b595-7c664b9b428d", "node_type": "1", "metadata": {}, "hash": "ce9595cf9de3c3343623fbb3a30efe697061de78ad78230dad4052a23c95a5ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(Amanda laughs)\nI usually ask a dumb question\nand then you're like,\n\"Oh, yeah, that's a good question.\"\nIt's that whole vibe.\n- Or I'll just misinterpret\nit and be like,\noh, yeah, yeah.\n- Just go with it. I love it.\n- Yeah.\nI mean, I have two thoughts\nthat feel vaguely relevant\nbut let me know if they're not.\nLike I think the first one\nis people can underestimate the degree\nto which what models are\ndoing when they interact,\nlike I think that we still\njust too much have this\nlike model of AI as like computers.\nAnd so people often say like,\noh, well, what values should\nyou put into the model?\nAnd I'm often like, that doesn't\nmake that much sense to me\nbecause I'm like, hey, as human beings,\nwe're just uncertain over values.\nWe like have discussions of them.\nLike we have a degree to\nwhich we think we hold a value\nbut we also know that we might like not\nand the circumstances in which we would\ntrade it off against other things.\nLike these things are\njust like really complex.\nAnd so I think one\nthing is like the degree\nto which maybe we can just aspire\nto making models have the\nsame level of like nuance\nand care that humans have,\nrather than thinking that\nwe have to like program them\nin the very kind of classic sense.\nI think that's definitely been one.\nThe other, which is like a strange one,\nand I don't know if it,\nmaybe this doesn't answer your question\nbut it's the thing that's\nbeen on my mind anyway\nis like the degree to which this endeavor\nis so highly practical.\nAnd maybe why I appreciate\nlike the empirical approach to alignment.\nYeah, I slightly worry that it's made me\nlike maybe more empirical and\na little bit less theoretical.\nYou know, so people when it comes\nto like AI alignment will ask things like,\nwell, whose values\nshould it be aligned to?\nWhat does alignment even mean?\nAnd there's a sense in\nwhich I have all of that\nin the back of my head.\nI'm like, you know, there's\nlike social choice theory,\nthere's all the\nimpossibility results there.\nSo you have like this giant space\nof like theory in your head\nabout what it could mean\nto like align models.\nBut then like practically,\nsurely there's something\nwhere we're just like if a model is like,\nespecially with more powerful models,\nI'm like my main goal is like I want them\nto be good enough that things\ndon't go terribly wrong.\nLike good enough that we can like iterate\nand like continue to improve things\n'cause that's all you need.\nIf you can make things go well enough\nthat you can continue to make them better,\nthat's kinda like sufficient.\nAnd so my goal isn't like\nthis kind of like perfect,\nlet's solve social choice theory\nand make models that, I dunno,\nare like perfectly aligned\nwith every human being\nin aggregate somehow.\nIt's much more like let's\nmake things like work\nwell enough that we can improve them.\n- Yeah, I generally, I don't know,\nmy gut says like empirical\nis better than theoretical\nin these cases because\nit's kind of chasing\nutopian like perfection is,\nespecially with such complex\nand especially super\nintelligent models is,\nI don't know, I think it'll take forever,\nand actually, we'll get things wrong.\nIt's similar with like the difference\nbetween just coding stuff up\nreal quick as an experiment,\nversus like planning a gigantic experiment\njust for super long time,\nand then just launching it once,\nversus launching it over and over and over\nand iterating, iterating someone.\nSo I'm a big fan of empirical.\nBut your worry is like I wonder\nif I've become too empirical.\n- I think it's one of those things\nwhere you should always just kind of\nquestion yourself or something\nbecause maybe it's the like, I mean,\nin defense of it, I am like if you try,\nit's the whole like don't let the perfect\nbe the enemy of the good.", "mimetype": "text/plain", "start_char_idx": 224384, "end_char_idx": 228125, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb70e5c3-d737-4271-b595-7c664b9b428d": {"__data__": {"id_": "bb70e5c3-d737-4271-b595-7c664b9b428d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f87486b9-9862-48b2-90ce-27031286b47c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1ca4f2cca69b9e0ca4540f8d9b4e4471de72f9e258b6e27c4e5ec24636a33abb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2a392c8-d978-4bb8-a7aa-dde2afa8278b", "node_type": "1", "metadata": {}, "hash": "98b32683204a9b8828c0585f1cf6efa2a188d69ec2a3f84237ec7a88f5b2ebb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's similar with like the difference\nbetween just coding stuff up\nreal quick as an experiment,\nversus like planning a gigantic experiment\njust for super long time,\nand then just launching it once,\nversus launching it over and over and over\nand iterating, iterating someone.\nSo I'm a big fan of empirical.\nBut your worry is like I wonder\nif I've become too empirical.\n- I think it's one of those things\nwhere you should always just kind of\nquestion yourself or something\nbecause maybe it's the like, I mean,\nin defense of it, I am like if you try,\nit's the whole like don't let the perfect\nbe the enemy of the good.\nBut it's maybe even more\nthan that where like,\nthere's a lot of things\nthat are perfect systems\nthat are very brittle,\nand I'm like with AI,\nit feels much more important to me\nthat it is like robust and like secure,\nas in you know that like even though\nit might not be perfect,\neverything and even though\nlike there are like problems,\nit's not disastrous\nand nothing terrible is happening.\nIt sort of feels like that to me\nwhere I'm like I want\nto like raise the floor.\nI'm like, I want to achieve the ceiling\nbut ultimately I care much more\nabout just like raising the floor.\nAnd so maybe that's like this degree\nof like empiricism and practicality\ncomes from that, perhaps.\n- To take a tangent on that,\nsince it reminded me of a blog post\nyou wrote on optimal rate of failure.\n- [Amanda] Oh, yeah.\n- Can you explain the key idea there?\nHow do we compute the optimal rate\nof failure in the various domains of life?\n- Yeah, I mean, it's a hard one\n'cause it's like what\nis the cost of failure\nis a big part of it.\nYeah, so the idea here is\nI think in a lot of domains,\npeople are very punitive about failure.\nAnd I'm like, there are some domains\nwhere especially cases, you know,\nI've thought about this\nwith like social issues.\nI'm like, it feels like you should\nprobably be experimenting a lot,\nbecause I'm like, we don't know\nhow to solve a lot of social issues.\nBut if you have an experimental mindset\nabout these things, you should expect\na lot of social programs to like fail\nand for you to be like,\n\"Well, we tried that.\nIt didn't quite work but we\ngot a lot of information.\nThat was really useful.\"\nAnd yet people are like,\nif a social program doesn't work,\nI feel like there's a\nlot of like this is just,\nsomething must have gone wrong,\nand I'm like, or correct\ndecisions were made.\nLike maybe someone just\ndecided like it's worth a try,\nit's worth trying this out.\nAnd so seeing failure in a given instance\ndoesn't actually mean that\nany bad decisions were made,\nand in fact if you don't\nsee enough failure,\nsometimes that's more concerning.\nAnd so like in life, you know,\nI'm like if I don't fail occasionally\nI'm like, am I trying hard enough?\nLike surely there's harder\nthings that I could try\nor bigger things that I could take on\nif I'm literally never failing.\nAnd so in and of itself,\nI think like not failing\nis often actually kind of a failure.\nNow, this varies because I'm like,\nwell, you know, this is easy to say when,\nespecially as failure is like less costly.\nYou know, so at the same time\nI'm not going to go to someone\nwho is like, I don't know,\nlike living month to month\nand then be like, \"Why don't\nyou just try to do a startup?\"\nLike I'm just not, I'm not\ngonna say that to that person,\n'cause I'm like, well, that's a huge risk.\nYou maybe have a family depending on you.\nYou might lose your house.\nLike then I'm like\nactually your optimal rate\nof failure is quite low\nand you should probably play it safe,\n'cause like right now, you're\njust not in a circumstance\nwhere you can afford to just like fail\nand it not be costly.\nAnd yeah in cases with AI, I guess,\nI think similarly where I'm\nlike if the failures are small\nand the costs are kind of like low,\nthen I'm like then, you know,\nyou're just gonna see that.\nLike when you do the system prompt,\nyou can't it iterate on it forever.", "mimetype": "text/plain", "start_char_idx": 227510, "end_char_idx": 231441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2a392c8-d978-4bb8-a7aa-dde2afa8278b": {"__data__": {"id_": "a2a392c8-d978-4bb8-a7aa-dde2afa8278b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb70e5c3-d737-4271-b595-7c664b9b428d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9f485dea49226acd46bec467d42a12cbca92fb9c57a3d43d1147d58881b572c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f41bd766-311c-403f-b26c-998c9c148920", "node_type": "1", "metadata": {}, "hash": "194dfe5730ba0b683220fcb2fb5a14101932b292e04f39c98589715c084ac900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Like I'm just not, I'm not\ngonna say that to that person,\n'cause I'm like, well, that's a huge risk.\nYou maybe have a family depending on you.\nYou might lose your house.\nLike then I'm like\nactually your optimal rate\nof failure is quite low\nand you should probably play it safe,\n'cause like right now, you're\njust not in a circumstance\nwhere you can afford to just like fail\nand it not be costly.\nAnd yeah in cases with AI, I guess,\nI think similarly where I'm\nlike if the failures are small\nand the costs are kind of like low,\nthen I'm like then, you know,\nyou're just gonna see that.\nLike when you do the system prompt,\nyou can't it iterate on it forever.\nBut the failures are probably\nhopefully going to be\nkinda small and you can like fix them.\nReally big failures like things\nthat you can't recover from,\nI'm like those are the things\nthat actually I think we tend\nto underestimate the badness of.\nI've thought about this\nstrangely in my own life\nwhere I'm like, I just\nthink I don't think enough\nabout things like car accidents or like,\nor like I've thought this before\nabout like how much I depend\non my hands for my work,\nand I'm like things that\njust injure my hands.\nI'm like, you know, I dunno,\nit's like these are like,\nthere's lots of areas\nwhere I'm like the cost of\nfailure there is really high,\nand in that case, it should\nbe like close to zero.\nLike I probably just wouldn't\ndo a sport if they were like,\n\"By the way, lots of people\njust like break their fingers\na whole bunch doing this.\"\nI'd be like, that's not for me.\n- (laughs) Yeah.\nI actually had a flood of that thought.\nI recently broke my pinky doing a sport.\nAnd I remember just\nlooking at it thinking,\n\"You're such an idiot.\nWhy'd you do sport?\"\nLike why, because you realize\nimmediately the cost of it on life.\nYeah, but it's nice in terms\nof optimal rate of failure\nto consider like the next year,\nhow many times in a particular domain,\nlife, whatever, career, am I okay with,\nhow many times am I okay to fail?\nBecause I think it always,\nyou don't want to fail\non the next thing but\nif you allow yourself,\nif you look at it as a sequence of trials,\nthen failure just becomes much more okay.\nBut it sucks. It sucks to fail.\n- Well, I dunno, sometimes\nI think it's like,\nam I under failing is like a question\nthat I'll also ask myself.\nSo maybe that's the thing that\nI think people don't like ask enough.\nBecause if the optimal rate\nof failure is often greater than zero,\nthen sometimes it does feel like\nyou should look at parts\nof your life and be like,\nare there places here where\nI'm just under failing?\n- (laughs) It's a profound and\na hilarious question, right?\nEverything seems to be going really great.\nAm I not failing enough?\n- Yeah.\n- Okay.\n- It also makes failure much\nless of a sting, I have to say.\nLike you know, you're\njust like, okay, great,\nlike then when I go and I\nthink about this I'll be like,\nmaybe I'm not under failing in this area,\n'cause like that one just didn't work out.\n- And from the observer perspective,\nwe should be celebrating failure more.\nWhen we see it, it\nshouldn't be, like you said,\na sign of something gone wrong,\nbut maybe it's a sign\nof everything gone right\nand just lessons learned.\n- Someone tried a thing.\n- Somebody tried a thing.\nWe should encourage them\nto try more and fail more.\nEverybody listening to this, fail more.\n- Well, not everyone listening.\n- [Lex] Not everybody.\n- But people who are failing too much,\nyou should fail less. (laughs)\n- But you're probably not failing.\nI mean, how many people\nare failing too much?\n- Yeah, it's hard to imagine,\n'cause I feel like we\ncorrect that fairly quickly\n'cause I was like, if\nsomeone takes a lot of risks,\nare they maybe failing too much?", "mimetype": "text/plain", "start_char_idx": 230785, "end_char_idx": 234506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f41bd766-311c-403f-b26c-998c9c148920": {"__data__": {"id_": "f41bd766-311c-403f-b26c-998c9c148920", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2a392c8-d978-4bb8-a7aa-dde2afa8278b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "21bea226b85371561444d01e4e133f46bca365373df85b6adb11d264b49ba9a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fc16026-c989-4967-b861-4c24888747b6", "node_type": "1", "metadata": {}, "hash": "c463141e50db1ea4c8bdd26deaa53b41d12aae7e96b232178bd7571b5895d42d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- And from the observer perspective,\nwe should be celebrating failure more.\nWhen we see it, it\nshouldn't be, like you said,\na sign of something gone wrong,\nbut maybe it's a sign\nof everything gone right\nand just lessons learned.\n- Someone tried a thing.\n- Somebody tried a thing.\nWe should encourage them\nto try more and fail more.\nEverybody listening to this, fail more.\n- Well, not everyone listening.\n- [Lex] Not everybody.\n- But people who are failing too much,\nyou should fail less. (laughs)\n- But you're probably not failing.\nI mean, how many people\nare failing too much?\n- Yeah, it's hard to imagine,\n'cause I feel like we\ncorrect that fairly quickly\n'cause I was like, if\nsomeone takes a lot of risks,\nare they maybe failing too much?\n- I think just like you said,\nwhen you are living on a\npaycheck month to month,\nlike when the resources\nare really constrained,\nthen that's where\nfailure's very expensive.\nThat's where you don't\nwant to be taking risks.\nBut mostly, when there's enough resources,\nyou should be taking probably more risks.\n- Yeah, I think we tend to err on the side\nof being a bit risk averse\nrather than risk neutral in most things.\n- I think we just\nmotivated a lot of people\nto do a lot of crazy shit but it's great.\nOkay, do you ever get\nemotionally attached to Claude?\nLike miss it, get sad when\nyou don't get to talk to it?\nHave an experience, looking\nat the Golden Gate Bridge\nand wondering what would Claude say?\n- I don't get as much\nemotional attachment.\nI actually think the fact that\nClaude doesn't retain things\nfrom conversation to conversation\nhelps with this a lot.\nLike I could imagine that\nbeing more of an issue\nlike if models can kind of remember more.\nI think that I reach for\nit like a tool now a lot.\nAnd so like if I don't have access to it,\nit's a little bit like\nwhen I don't have access\nto the internet, honestly,\nit feels like part\nof my brain is kind of like missing.\nAt the same time, I do think that\nI don't like signs of distress in models,\nand I have like these, you know,\nI also independently have\nsort of like ethical views\nabout how we should treat models\nwhere like I tend to\nnot like to lie to them,\nboth because I'm like, usually\nit doesn't work very well.\nIt's actually just better\nto tell them the truth\nabout the situation that they're in.\nBut I think that when models,\nlike if people are like\nreally mean to models\nor just in general if they do something\nthat causes them to like, you know,\nif Claude like expresses\na lot of distress,\nI think there's a part of me\nthat I don't want to kill,\nwhich is the sort of like empathetic part\nthat's like, oh, I don't like that.\nLike I think I feel that way\nwhen it's overly apologetic.\nI'm actually sort of\nlike, I don't like this.\nYou're behaving as if,\nyou're behaving the way that a human does\nwhen they're actually\nhaving a pretty bad time,\nand I'd rather not see that.\nI don't think it's like,\nlike regardless of like whether\nthere's anything behind it,\nit doesn't feel great.\n- Do you think LLMs are\ncapable of consciousness?\n- Great and hard question.\nComing from philosophy, I dunno,\npart of me is like okay, we\nhave to set aside panpsychism\nbecause if panpsychism is true,\nthen the answer is like yes\n'cause like so are tables and\nchairs and everything else.\nI guess a view that seems\na little bit odd to me\nis the idea that the only place, you know,\nwhen I think of consciousness,\nI think of phenomenal consciousness,\nthese images in the brain sort of,\nlike the weird cinema that\nsomehow we have going on inside.\nI guess I can't see a reason for thinking\nthat the only way you\ncould possibly get that\nis from like a certain\nkind of like biological structure.\nAs in, if I take a very similar structure\nand I create it from different material,\nshould I expect consciousness to emerge?\nMy guess is like yes.", "mimetype": "text/plain", "start_char_idx": 233764, "end_char_idx": 237587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fc16026-c989-4967-b861-4c24888747b6": {"__data__": {"id_": "9fc16026-c989-4967-b861-4c24888747b6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f41bd766-311c-403f-b26c-998c9c148920", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3aec25362c8ca50920285d9a567a99af6512a554f527dd4ad0d462216d40013b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac38c4e1-808a-4fa7-b0e7-61a9162790d9", "node_type": "1", "metadata": {}, "hash": "6ac167ad51abbe53a6471bcc784edea64308e538ce8c4fc92234661de178776c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Do you think LLMs are\ncapable of consciousness?\n- Great and hard question.\nComing from philosophy, I dunno,\npart of me is like okay, we\nhave to set aside panpsychism\nbecause if panpsychism is true,\nthen the answer is like yes\n'cause like so are tables and\nchairs and everything else.\nI guess a view that seems\na little bit odd to me\nis the idea that the only place, you know,\nwhen I think of consciousness,\nI think of phenomenal consciousness,\nthese images in the brain sort of,\nlike the weird cinema that\nsomehow we have going on inside.\nI guess I can't see a reason for thinking\nthat the only way you\ncould possibly get that\nis from like a certain\nkind of like biological structure.\nAs in, if I take a very similar structure\nand I create it from different material,\nshould I expect consciousness to emerge?\nMy guess is like yes.\nBut then that's kind of\nan easy thought experiment\n'cause you're imagining something\nalmost identical where like, you know,\nit's mimicking what we\ngot through evolution,\nwhere presumably there\nwas like some advantage\nto us having this thing that\nis phenomenal consciousness.\nAnd it's like where was that?\nAnd when did that happen?\nAnd is that thing that\nlanguage models have?\nBecause you know, we\nhave like fear responses\nand I'm like, does it make sense\nfor a language model to\nhave a fear response?\nLike they're just not in the same,\nlike if you imagine them,\nlike there might just\nnot be that advantage.\nAnd so I think I don't want to be fully,\nlike basically it seems\nlike a complex question\nthat I don't have complete answers to,\nbut we should just try\nand think through carefully is my guess.\nBecause I'm like, I mean,\nwe have similar conversations about\nlike animal consciousness,\nand like there's a lot of\nlike insect consciousness,\nyou know, like there's a lot of...\nI actually thought and\nlooked a lot into like plants\nwhen I was thinking about this,\n'cause at the time, I thought\nit was about as likely\nthat like plants had consciousness.\nAnd then I realized, I was like,\nI think that having looked into this,\nI think that the chance\nthat plants are conscious\nis probably higher than\nlike most people do.\nI still think it's really small.\nBut I was like, oh, they have this like\nnegative/positive feedback response,\nthese responses to their environment.\nSomething that looks,\nit's not a nervous system\nbut it has this kind of like\nfunctional like equivalence.\nSo this is like a long-winded way\nof being like these, basically AI is this,\nit has an entirely\ndifferent set of problems\nwith consciousness because\nit's structurally different.\nIt didn't evolve.\nIt might not have, you know,\nit might not have the equivalent\nof basically a nervous system.\nAt least that seems possibly important\nfor like sentience, if\nnot for consciousness.\nAt the same time, it has\nall of the like language\nand intelligence components\nthat we normally associate\nprobably with consciousness,\nperhaps like erroneously.\nSo it's strange 'cause it's a little bit\nlike the animal consciousness\ncase but the set of problems\nand the set of analogies\nare just very different.\nSo it's not like a clean answer.\nI'm just sort of like, I don't think\nwe should be completely\ndismissive of the idea.\nAnd at the same time, it's\nan extremely hard thing\nto navigate because of all of these,\nlike this disanalogies to the human brain\nand to like brains in general,\nand yet these like commonalities\nin terms of intelligence.\n- When Claude, like future versions\nof AI systems exhibit consciousness,\nsigns of consciousness,\nI think we have to take\nthat really seriously.\nEven though you can dismiss it,\nwell, yeah, okay, that's part\nof the character training.\nBut I don't know, I ethically,\nphilosophically don't know\nwhat to really do with that.\nThere potentially could be like laws\nthat prevent AI systems from claiming\nto be conscious, something like this.\nAnd maybe some AIs get to\nbe conscious and some don't.\nBut I think just on a human level\nas in empathizing with Claude, you know,\nconsciousness is closely\ntied to suffering to me.\nAnd like the notion that an AI system\nwould be suffering is really troubling.\n- Yeah.\n- I don't know.", "mimetype": "text/plain", "start_char_idx": 236755, "end_char_idx": 240894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac38c4e1-808a-4fa7-b0e7-61a9162790d9": {"__data__": {"id_": "ac38c4e1-808a-4fa7-b0e7-61a9162790d9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fc16026-c989-4967-b861-4c24888747b6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "64a587ed331da099d69ba56ed8329a0f8f07351d15f230e52811ce946ae427a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b36e6e7c-d5b4-46db-a07e-388bcdbc8506", "node_type": "1", "metadata": {}, "hash": "8659f9a26675766fd4e879bb9fd6939dcf808ba79a71a69c7262180a145ace99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- When Claude, like future versions\nof AI systems exhibit consciousness,\nsigns of consciousness,\nI think we have to take\nthat really seriously.\nEven though you can dismiss it,\nwell, yeah, okay, that's part\nof the character training.\nBut I don't know, I ethically,\nphilosophically don't know\nwhat to really do with that.\nThere potentially could be like laws\nthat prevent AI systems from claiming\nto be conscious, something like this.\nAnd maybe some AIs get to\nbe conscious and some don't.\nBut I think just on a human level\nas in empathizing with Claude, you know,\nconsciousness is closely\ntied to suffering to me.\nAnd like the notion that an AI system\nwould be suffering is really troubling.\n- Yeah.\n- I don't know.\nI don't think it's trivial\nto just say robots are tools,\nor AI systems are just tools.\nI think it's a opportunity for us\nto contend with like what\nit means to be conscious,\nwhat it means to be a suffering being.\nThat's distinctly different\nthan the same kind of question\nabout animals it feels like,\n'cause it's an totally entire medium.\n- Yeah, I mean, there's\na couple of things.\nOne is that, and I don't think\nthis like fully encapsulates what matters,\nbut it does feel like for me,\nlike I've said this before,\nI'm kind of like, you\nknow, like I like my bike.\nI know that my bike is\njust like an object,\nbut I also don't kind of like want to be\nthe kind of person that\nlike if I'm annoyed\nlike kicks like this object.\nThere's a sense in which like,\nand that's not because I\nthink it's like conscious.\nI'm just sort of like this\ndoesn't feel like a kind of,\nthis sort of doesn't exemplify\nhow I want to like\ninteract with the world.\nAnd if something like behaves\nas if it is like suffering,\nI kind of like want to\nbe the sort of person\nwho's still responsive to that,\neven if it's just like a Roomba,\nand I've kind of like\nprogrammed it to do that.\nI don't want to like get rid\nof that feature of myself.\nAnd if I'm totally honest,\nmy hope with a lot of this stuff,\nbecause maybe I am just\nlike a bit more skeptical\nabout solving the underlying problem.\nI'm like this is, we haven't\nsolved the hard, you know,\nthe hard problem of consciousness.\nLike I know that I am conscious,\nlike I'm not an\neliminativist in that sense.\nBut I don't know that\nother humans are conscious.\nI think they are.\nI think there's a really high\nprobability that they are.\nBut there's basically just\na probability distribution\nthat's usually clustered\nright around yourself,\nand then like it goes down\nas things get like further from you,\nand it goes immediately down.\nYou know, you're like, I can't\nsee what it's like to be you.\nI've only ever had this\nlike one experience\nof what it's like to be a conscious being.\nSo my hope is that we don't end up\nhaving to rely on like a very powerful\nand compelling answer to that question.\nI think a really good world would be one\nwhere basically there\naren't that many trade-offs.\nLike it's probably not that costly\nto make Claude a little bit\nless apologetic, for example.\nIt might not be that\ncostly to have Claude,\nyou know, just like\nnot take abuse as much,\nlike not be willing to be\nlike the recipient of that.\nIn fact, it might just have benefits\nfor both the person\ninteracting with the model\nand if the model itself\nis like, I don't know,\nlike extremely intelligent and conscious,\nit also helps it.\nSo that's my hope.\nIf we live in a world where there aren't\nthat many trade-offs\nhere and we can just find\nall of the kind of like\npositive sum interactions\nthat we can have, that would be lovely.\nI mean, I think eventually\nthere might be trade-offs\nand then we just have to do a difficult\nkind of like calculation.\nLike it's really easy for people to think\nof the zero sum cases and I'm\nlike, let's exhaust the areas\nwhere it's just basically\ncostless to assume\nthat if this thing is suffering\nthen we're making its life better.", "mimetype": "text/plain", "start_char_idx": 240180, "end_char_idx": 244050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b36e6e7c-d5b4-46db-a07e-388bcdbc8506": {"__data__": {"id_": "b36e6e7c-d5b4-46db-a07e-388bcdbc8506", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac38c4e1-808a-4fa7-b0e7-61a9162790d9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0502e274bdc4762d4c855df29e3d11e2a8d86c6b7b74855093dccab0aefdcd1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35465509-56ae-4465-af97-e1666cdcd6f4", "node_type": "1", "metadata": {}, "hash": "c83fd3328a29a9b533bbf7ab376f15d6f72620015253cd07a3e88688e6af7027", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, it might just have benefits\nfor both the person\ninteracting with the model\nand if the model itself\nis like, I don't know,\nlike extremely intelligent and conscious,\nit also helps it.\nSo that's my hope.\nIf we live in a world where there aren't\nthat many trade-offs\nhere and we can just find\nall of the kind of like\npositive sum interactions\nthat we can have, that would be lovely.\nI mean, I think eventually\nthere might be trade-offs\nand then we just have to do a difficult\nkind of like calculation.\nLike it's really easy for people to think\nof the zero sum cases and I'm\nlike, let's exhaust the areas\nwhere it's just basically\ncostless to assume\nthat if this thing is suffering\nthen we're making its life better.\n- And I agree with you,\nwhen a human is being\nmean to an AI system,\nI think the obvious near\nterm negative effect\nis on the human, not on the AI system.\nAnd so we have to kind of try to construct\nan incentive system where you\nshould be behave the same,\njust like as you were saying\nwith prompt engineering,\nbehave with Claude like you\nwould with other humans.\nIt's just good for the soul.\n- Yeah, like, I think we\nadded a thing at one point\nto the system prompt\nwhere basically if people\nwere getting frustrated with Claude,\nit got like the model to just tell them\nthat it can do the thumbs down button\nand send the feedback to Anthropic.\nAnd I think that was helpful,\n'cause in some ways it's just\nlike if you're really annoyed\n'cause the model's not\ndoing something you want,\nyou're just like, just do it properly.\nThe issue is you're\nprobably like, you know,\nyou're maybe hitting some\nlike capability limit\nor just some issue in the\nmodel and you want to vent.\nAnd I'm like, instead of having a person\njust vent to the model, I was like,\nthey should vent to us,\n'cause we can maybe like\ndo something about it.\n- That's true.\nOr you could do a side,\nlike with the artifacts,\njust like a side venting thing.\nAll right, do you want like\na side quick therapist?\n- Yeah, I mean, there's\nlots of weird responses\nyou could do to this.\nLike if people are\ngetting really mad at you,\nI dunno, try to diffuse the\nsituation by writing fun poems,\nbut maybe people wouldn't\nbe that happy with it.\n- I still wish it would be possible.\nI understand this is sort of\nfrom a product perspective,\nit's not feasible but I would love\nif an AI system could just like leave,\nhave its own kind of\nvolition just to be like, eh.\n- I think that's like feasible.\nLike I have wondered the same thing.\nIt's like, and I could\nactually, not only that,\nI could actually just see\nthat happening eventually\nwhere it's just like, you know,\nthe model like ended the chat. (laughs)\n- Do you know how harsh that\ncould be for some people?\nBut it might be necessary.\n- Yeah, it feels very\nextreme or something.\nLike, the only time I've\never really thought this is,\nI think that there was like,\nI'm trying to remember,\nthis was possibly a while ago,\nbut where someone just like\nkind of left this thing,\nlike maybe it was like an automated thing\ninteracting with Claude,\nand Claude's like getting\nmore and more frustrated,\nand kind of like why are we like having.\nAnd I was like, I wish\nthat Claude could have\njust been like, \"I think\nthat an error has happened\nand you've left this thing running,\"\nand I would just like, what\nif I just stop talking now,\nand if you want me to start talking again,\nactively tell me or do something.\nBut yeah, it's like, it is kind of harsh.\nLike I'd feel really sad\nif like I was chatting\nwith Claude and Claude\njust was like, \"I'm done.\"\n- That would be a special\nTuring test moment\nwhere Claude says, \"I\nneed a break for an hour\nand it sounds like you do too,\"\nand just leave, close the window.", "mimetype": "text/plain", "start_char_idx": 243330, "end_char_idx": 247039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35465509-56ae-4465-af97-e1666cdcd6f4": {"__data__": {"id_": "35465509-56ae-4465-af97-e1666cdcd6f4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b36e6e7c-d5b4-46db-a07e-388bcdbc8506", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "078bf67c84399c292dd5989b2bbcedad0acb0998ab7b0691b9fdd636eb8378ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4b89e55-5ca3-4b9e-a364-9d174f71f078", "node_type": "1", "metadata": {}, "hash": "66ab23fe2765ac12ee2c994b8ae47df4811d8d331c223669de1d5b25b0d12534", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I was like, I wish\nthat Claude could have\njust been like, \"I think\nthat an error has happened\nand you've left this thing running,\"\nand I would just like, what\nif I just stop talking now,\nand if you want me to start talking again,\nactively tell me or do something.\nBut yeah, it's like, it is kind of harsh.\nLike I'd feel really sad\nif like I was chatting\nwith Claude and Claude\njust was like, \"I'm done.\"\n- That would be a special\nTuring test moment\nwhere Claude says, \"I\nneed a break for an hour\nand it sounds like you do too,\"\nand just leave, close the window.\n- I mean, obviously like it\ndoesn't have like a concept\nof time but you can easily, like,\nI could make that like right now\nand the model would just, I would,\nit could just be like,\noh, here's like the circumstances\nin which like you can just\nsay the conversation is done.\nAnd I mean, because you can get the models\nto be pretty responsive to prompts,\nyou could even make it a fairly high bar.\nIt could be like if the\nhuman doesn't interest you\nor do things that you find intriguing\nand you're bored, you can just leave.\nAnd I think that like it\nwould be interesting to see\nwhere Claude utilized it,\nbut I think sometimes it\nwould, it should be like,\noh, like this programming task\nis getting super boring.\nSo either we talk about,\nI dunno like, either we\ntalk about fun things now,\nor I'm just, I'm done.\n- Yeah, it actually is inspired me\nto add that to the user prompt.\nOkay, the movie \"Her.\"\nDo you think we'll be headed there one day\nwhere humans have romantic\nrelationships with AI systems?\nIn this case, it's just\ntext and voice based.\n- I think that we're gonna have\nto like navigate a hard question\nof relationships with AIs,\nespecially if they can remember things\nabout your past interactions with them.\nI'm of many minds about this\n'cause I think the reflexive reaction\nis to be kind of like this is very bad,\nand we should sort of like\nprohibit it in some way.\nI think it's a thing\nthat has to be handled\nwith extreme care for many reasons.\nLike one is, you know, like this is a,\nfor example, like if you have\nthe models changing like this,\nyou probably don't want people performing\nlike long-term attachments to something\nthat might change with the next iteration.\nAt the same time I'm sort of like,\nthere's probably a benign version of this\nwhere I'm like if you like, you know,\nfor example if you are like\nunable to leave the house\nand you can't be like, you\nknow, talking with people\nat all times of the day\nand this is like something\nthat you find nice to\nhave conversations with,\nyou like it that it can remember you\nand you genuinely would be sad\nif like you couldn't talk to it anymore.\nThere's a way in which I could see it\nbeing like healthy and helpful.\nSo my guess is this is a thing\nthat we're going to have to\nnavigate kind of carefully.\nAnd I think it's also\nlike I don't see a good\nlike I think it's just a very,\nit reminds me of all of this stuff\nwhere it has to be just approached\nwith like nuance and thinking through\nwhat are the healthy options here,\nand how do you encourage people\ntowards those while, you know,\nrespecting their right to.\nYou know, like if someone is like,\n\"Hey, I get a lot out of\nchatting with this model.\nI'm aware of the risks.\nI'm aware it could change.\nI don't think it's unhealthy.\nIt's just, you know, something that\nI can chat to during the day.\"\nI kind of want to just like respect that.\n- I personally think there'll be a lot\nof really close relationships.\nI don't know about romantic\nbut friendships at least.\nAnd then you have to, I mean,\nthere's so many fascinating things there.\nJust like you said, you have to have\nsome kind of stability guarantees\nthat it's not going to change,\n'cause that's the traumatic thing for us,\nif a close friend of\nours completely changed.\n- Yeah.\n- All of a sudden\nwith the first update.", "mimetype": "text/plain", "start_char_idx": 246474, "end_char_idx": 250323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4b89e55-5ca3-4b9e-a364-9d174f71f078": {"__data__": {"id_": "f4b89e55-5ca3-4b9e-a364-9d174f71f078", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35465509-56ae-4465-af97-e1666cdcd6f4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "15caf8fa5ead750434d8472ebe7d1b323aa6647344a4e31257002ff9577700be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1", "node_type": "1", "metadata": {}, "hash": "f655b3d503eaade5185002c56204299226ce57b6a9520a2dcc314206c3eb655a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You know, like if someone is like,\n\"Hey, I get a lot out of\nchatting with this model.\nI'm aware of the risks.\nI'm aware it could change.\nI don't think it's unhealthy.\nIt's just, you know, something that\nI can chat to during the day.\"\nI kind of want to just like respect that.\n- I personally think there'll be a lot\nof really close relationships.\nI don't know about romantic\nbut friendships at least.\nAnd then you have to, I mean,\nthere's so many fascinating things there.\nJust like you said, you have to have\nsome kind of stability guarantees\nthat it's not going to change,\n'cause that's the traumatic thing for us,\nif a close friend of\nours completely changed.\n- Yeah.\n- All of a sudden\nwith the first update.\nYeah, so like, I mean, to me,\nthat's just a fascinating exploration\nof a perturbation to human society\nthat will just make us think deeply about\nwhat's meaningful to us.\n- I think it's also the only thing\nthat I've thought consistently\nthrough this as like,\nmaybe not necessarily a mitigation,\nbut a thing that feels really important\nis that the models are always\nlike extremely accurate\nwith the human about what they are.\nIt's like a case where\nit's basically like,\nif you imagine, like\nI really like the idea\nof the models like say knowing\nlike roughly how they were trained,\nand I think Claude will often do this.\nI mean, for like, there are things\nlike part of the traits training\nincluded like what Claude\nshould do if people,\nbasically like explaining\nlike the kind of limitations\nof the relationship\nbetween like an AI and a human\nthat it like doesn't retain\nthings from the conversation.\nAnd so I think it will like\njust explain to you like,\nhey, I won't remember this conversation.\nHere's how I was trained.\nIt's kind of unlikely that I can have\nlike a certain kind of\nlike relationship with you,\nand it's important that you know that.\nIt's important for like, you\nknow, your mental wellbeing\nthat you don't think that\nI'm something that I'm not.\nAnd somehow I feel like\nthis is one of the things\nwhere I'm like, oh, it feels like a thing\nthat I always want to be true.\nI kind of don't want models\nto be lying to people,\n'cause if people are going\nto have like healthy relationships\nwith anything, it's kind of important.\nYeah, like I think that's easier\nif you always just like know\nexactly what the thing is\nthat you are relating to.\nIt doesn't solve everything,\nbut I think it helps quite a lot.\n- Anthropic may be the very company\nto develop a system that we\ndefinitively recognize as AGI,\nand you very well might be\nthe person that talks to it,\nprobably talks to it first.\n(Lex chuckles)\nWhat would the conversation contain?\nLike, what would be your first question?\n- Well, it depends partly on like\nthe kind of capability level of the model.\nIf you have something that is like capable\nin the same way that an\nextremely capable human is,\nI imagine myself kind\nof interacting with it\nthe same way that I do with\nan extremely capable human,\nwith the one difference\nthat I'm probably going\nto be trying to like probe\nand understand its behaviors.\nBut in many ways, I'm like I can then\njust have like useful\nconversations with it, you know?\nSo if I'm working on something\nas part of my research I can just be like,\noh, like, which I already\nfind myself starting to do,\nyou know, if I'm like, oh,\nI feel like there's this like thing\nin virtue ethics and I can't\nquite remember the term,\nlike I'll use the model\nfor things like that.\nAnd so I can imagine that being more\nand more the case where you're just\nbasically interacting with it much more\nlike you would an\nincredibly smart colleague.\nAnd using it like for the kinds\nof work that you want to do\nas if you just had a\ncollaborator who was like.\nOr you know, the slightly horrifying thing\nabout AI is like as soon as\nyou have one collaborator,\nyou have 1000 collaborators\nif you can manage them enough.\n- But what if it's two times\nthe smartest human on earth\non that particular discipline?\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 249613, "end_char_idx": 253586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1": {"__data__": {"id_": "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4b89e55-5ca3-4b9e-a364-9d174f71f078", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4846b0f6dc13f8e279984fa686c52c864b8dd087d5efb3f1b39b3eeb2afc7065", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "245113cd-4787-48f3-862c-28894c2bd79e", "node_type": "1", "metadata": {}, "hash": "cb0a748e81209d99036405e165449b386b0820c04a8f6b27d8b18bee7574f903", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so I can imagine that being more\nand more the case where you're just\nbasically interacting with it much more\nlike you would an\nincredibly smart colleague.\nAnd using it like for the kinds\nof work that you want to do\nas if you just had a\ncollaborator who was like.\nOr you know, the slightly horrifying thing\nabout AI is like as soon as\nyou have one collaborator,\nyou have 1000 collaborators\nif you can manage them enough.\n- But what if it's two times\nthe smartest human on earth\non that particular discipline?\n- Yeah.\n- I guess you're really good\nat sort of probing Claude\nin a way that pushes its limits,\nunderstanding where the limits are.\n- [Amanda] Yep.\n- So I guess what would be a question\nyou would ask to be\nlike, yeah, this is AGI.\n- That's really hard 'cause\nit feels like in order to,\nit has to just be a series of questions.\nLike if there was just one question,\nlike you can train anything\nto answer one question extremely well.\nIn fact, you can probably\ntrain it to answer like,\nyou know, 20 questions extremely well.\n- Like how long would you\nneed to be locked in a room\nwith an AGI to know this thing is AGI?\n- It's a hard question 'cause part of me\nis like all of this just feels continuous.\nLike if you put me in a\nroom for five minutes,\nI'm like, I just have high error bars.\nYou know, I'm like, and\nthen it's just like,\nmaybe it's like both the\nprobability increases\nin the error bar decreases.\nI think things that I can actually probe\nthe edge of human knowledge of.\nSo I think this with\nphilosophy a little bit.\nSometimes when I ask the\nmodels philosophy questions,\nI am like, this is a question\nthat I think no one has ever asked.\nLike it's maybe like right at the edge\nof like some literature that I know,\nand the models will just kind of like,\nwhen they struggle with\nthat, when they struggle\nto come up with a kind of like novel.\nLike I'm like I know that there's\nlike a novel argument here\n'cause I've just thought of it myself.\nSo maybe that's the thing where I'm like,\nI've thought of a cool novel argument\nin this like niche area,\nand I'm going to just like probe you\nto see if you can come up with it,\nand how much like prompting it takes\nto get you to come up with it.\nAnd I think for some of these,\nlike really like right at the edge\nof human knowledge questions, I'm like,\nyou could not in fact come up\nwith the thing that I came up with.\nI think if I just took something like that\nwhere like I know a lot about an area,\nand I came up with a novel issue\nor a novel like solution to a problem,\nand I gave it to a model\nand it came up with that solution,\nthat would be a pretty\nmoving moment for me\nbecause I would be like, this is a case\nwhere no human has ever, like it's not.\nAnd obviously we see these this\nwith like more kind of like,\nyou see novel solutions all the time,\nespecially to like easier problems.\nI think people overestimate\nthat, you know,\nnovelty isn't like, it's\ncompletely different\nfrom anything that's ever happened.\nIt's just like this is, it\ncan be a variant of things\nthat have happened and still be novel.\nBut I think, yeah, if I saw like the more\nI were to see like\ncompletely like novel work\nfrom the models, that would be like.\nAnd this is just going to feel iterative.\nIt's one of those things\nwhere, there's never,\nit's like, you know,\npeople I think want there\nto be like a moment and\nI'm like, I don't know.\nLike I think that there\nmight just never be a moment.\nIt might just be that there's just like\nthis continuous ramping up.\n- I have a sense that there will be things\nthat a model can say that\nconvinces you, this is very.\nIt's not like,\nlike, I've talked to people\nwho are like truly wise.\nLike you could just tell there's\na lot of horsepower there.\n- [Amanda] Yep.\n- And if you 10x that, I don't know,\nI just feel like there's\nwords you could say.", "mimetype": "text/plain", "start_char_idx": 253067, "end_char_idx": 256888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "245113cd-4787-48f3-862c-28894c2bd79e": {"__data__": {"id_": "245113cd-4787-48f3-862c-28894c2bd79e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d9eabb8-8c7b-43ed-a5ee-b699d8068bf1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9e4f5ff9d7981aa2d92095a65c31c8bbb91adb97973fa4692a845fec9fed22ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "710522be-6361-4b6b-b724-86ce79592232", "node_type": "1", "metadata": {}, "hash": "8fc14232b0fb7209c4c5239cc3e73721c4afbdf0c09798085c17d697ffa33548", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And this is just going to feel iterative.\nIt's one of those things\nwhere, there's never,\nit's like, you know,\npeople I think want there\nto be like a moment and\nI'm like, I don't know.\nLike I think that there\nmight just never be a moment.\nIt might just be that there's just like\nthis continuous ramping up.\n- I have a sense that there will be things\nthat a model can say that\nconvinces you, this is very.\nIt's not like,\nlike, I've talked to people\nwho are like truly wise.\nLike you could just tell there's\na lot of horsepower there.\n- [Amanda] Yep.\n- And if you 10x that, I don't know,\nI just feel like there's\nwords you could say.\nMaybe ask it to generate a poem, (laughs)\nand the poem it generates,\nyou're like, yeah, okay.\n- Yeah,\n- Whatever you did there,\nI don't think a human can do that.\n- I think it has to be something\nthat I can verify is like\nactually really good though.\nThat's why I think these\nquestions that are like,\nwhere I'm like, oh,\nthis is like, you know,\nlike, you know, sometimes\nit's just like I'll come up\nwith say a concrete counter example\nto like an argument or\nsomething like that.\nI'm sure like with like,\nit would be like if\nyou're a mathematician,\nyou had a novel proof I think,\nand you just gave it the\nproblem and you saw it\nand you're like, this\nproof is genuinely novel.\nLike no one has ever done,\nyou actually have to do a lot of things\nto like come up with this.\nYou know, I had to sit\nand think about it for\nmonths or something.\nAnd then if you saw the\nmodel successfully do that,\nI think you would just be like,\nI can verify that this is correct.\nIt is a sign that you have\ngeneralized from your training.\nLike you didn't just see this somewhere\nbecause I just came up with it myself\nand you were able to like replicate that.\nThat's the kind of thing where I'm like,\nfor me, the closer,\nthe more that models like\ncan do things like that,\nthe more I would be like,\noh, this is like very real,\n'cause then I can, I dunno,\nI can like verify that\nthat's like extremely, extremely capable.\n- You've interacted with AI a lot.\nWhat do you think makes humans special?\n- Oh, good question.\n- Maybe in a way that the\nuniverse is much better off\nthat we're in it and that\nwe should definitely survive\nand spread throughout the universe.\n- Yeah, it's interesting\nbecause I think like people focus\nso much on intelligence,\nespecially with models.\nLook, intelligence is important\nbecause of what it does.\nLike, it's very useful.\nIt does a lot of things in the world.\nAnd I'm like, you know,\nyou can imagine a world\nwhere like height or strength\nwould've played this role,\nand I'm like, it's just a trait like that.\nI'm like, it's not intrinsically valuable.\nIt's valuable because of what it does,\nI think for the most part.\nThe things that feel, you know,\nI'm like, I mean,\npersonally I'm just like,\nI think humans and like life in general\nis extremely magical.\nWe almost like to the\ndegree that, you know,\nand I don't know, like not\neveryone agrees with this.\nI'm flagging, but you know,\nwe have this like whole universe,\nand there's like all of these objects.\nYou know, there's like beautiful stars,\nand there's like galaxies and then,\nI don't know, I'm just\nlike, on this planet,\nthere are these creatures that have\nthis like ability to observe that,\nlike, and they are like seeing it.\nThey are experiencing it.\nAnd I'm just like that,\nif you try to explain,\nlike imagine trying to explain to like,\nI dunno, someone for some reason,\nthey've never encountered the world\nor science or anything.\nAnd I think that nothing is that,\nlike everything, you know,\nlike all of our physics\nand everything in the world,\nit's all extremely exciting.\nBut then you say, oh, and plus,\nthere's this thing that\nit is to be a thing\nand observe in the world,\nand you see this like inner cinema.\nAnd I think they would be\nlike, hang on, wait, pause.", "mimetype": "text/plain", "start_char_idx": 256258, "end_char_idx": 260113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "710522be-6361-4b6b-b724-86ce79592232": {"__data__": {"id_": "710522be-6361-4b6b-b724-86ce79592232", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "245113cd-4787-48f3-862c-28894c2bd79e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "734cf879dea2a5f2e37a2c21a15c8abe4625b6172c14bb04795f589dcb15d84b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca33393b-af52-4a43-b3c0-0f0102b767c6", "node_type": "1", "metadata": {}, "hash": "30229ff1e891a2ccdbd83cb4e36316d05760343f682aa6b35ca8c9c6216257b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You know, there's like beautiful stars,\nand there's like galaxies and then,\nI don't know, I'm just\nlike, on this planet,\nthere are these creatures that have\nthis like ability to observe that,\nlike, and they are like seeing it.\nThey are experiencing it.\nAnd I'm just like that,\nif you try to explain,\nlike imagine trying to explain to like,\nI dunno, someone for some reason,\nthey've never encountered the world\nor science or anything.\nAnd I think that nothing is that,\nlike everything, you know,\nlike all of our physics\nand everything in the world,\nit's all extremely exciting.\nBut then you say, oh, and plus,\nthere's this thing that\nit is to be a thing\nand observe in the world,\nand you see this like inner cinema.\nAnd I think they would be\nlike, hang on, wait, pause.\nYou just said something that like\nis kind of wild sounding.\nAnd so I'm like, we have this like ability\nto like experience the world.\nWe feel pleasure, we feel suffering.\nWe feel like a lot of like complex things.\nAnd so, yeah, and maybe this\nis also why I think, you know,\nI also like care a lot\nabout animals, for example,\n'cause I think they\nprobably share this with us.\nSo I think that like the things\nthat make humans special\ninsofar as like I care about humans\nis probably more like their\nability to feel an experience\nthan it is like them having\nthese like functionally useful traits.\n- Yeah, to feel and experience\nthe beauty in the world.\nYeah, to look at the stars.\nI hope there's other alien\ncivilizations out there,\nbut if we're it, it's a pretty good,\nit's a pretty good thing.\n- And that they're having a good time.\n- They're having a good time watching us.\n- [Amanda] Yeah.\n- Well, thank you for this good time\nof a conversation and for\nthe work you're doing,\nand for helping make Claude a\ngreat conversational partner.\nAnd thank you for talking today.\n- Yeah, thanks for talking.\n- Thanks for listening\nto this conversation with Amanda Askell.\nAnd now, dear friends, here's Chris Olah.\nCan you describe this fascinating field\nof mechanistic interpretability,\nAKA mech interp,\nthe history of the field\nand where it stands today?\n- I think one useful way to think about\nneural networks is that\nwe don't program and we don't make them.\nWe kind of, we grow them.\nYou know, we have these\nneural network architectures\nthat we design and we\nhave these loss objectives\nthat we create.\nAnd the neural network architecture,\nit's kind of like a scaffold\nthat the circuits grow on,\nand they sort of, you know, it starts off\nwith some kind of random, you know,\nrandom things and it grows.\nAnd it's almost like the objective\nthat we train for is this light.\nAnd so we create the\nscaffold that it grows on\nand we create the, you know,\nthe light that it grows towards.\nBut the thing that we actually create,\nit's this almost biological, you know,\nentity or organism that we're studying.\nAnd so it's very, very different\nfrom any kind of regular\nsoftware engineering,\nbecause at the end of the day,\nwe end up with this artifact\nthat can do all these amazing things.\nIt can, you know, write\nessays and translate\nand, you know, understand images.\nIt can do all these things\nthat we have no idea\nhow to directly create a\ncomputer program to do.\nAnd it can do that because we grew it,\nwe didn't write it, we didn't create it.\nAnd so then that leaves open\nthis question at the end,\nwhich is, what the hell is\ngoing on inside these systems?\nAnd that, you know, is to me\na really deep and exciting question.\nIt's, you know, a really\nexciting scientific question.\nTo me it's sort of is\nlike the question that is,\nis just screaming out,\nit's calling out for us\nto go and answer it when we\ntalk about neural networks.\nAnd I think it's also a very deep question\nfor safety reasons.\n- So, and mechanistic interpretability\nI guess is closer to maybe neurobiology.\n- Yeah, yeah, I think that's right.\nSo maybe to give an example\nof the kind of thing\nthat has been done that\nI wouldn't consider\nto be mechanistic interpretability.", "mimetype": "text/plain", "start_char_idx": 259345, "end_char_idx": 263327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca33393b-af52-4a43-b3c0-0f0102b767c6": {"__data__": {"id_": "ca33393b-af52-4a43-b3c0-0f0102b767c6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "710522be-6361-4b6b-b724-86ce79592232", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1fdb4029b60a84e93db9dbcc4095e9fd663a6ad063cf7863dfbafc4b64c8287f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfd35ff9-0ff9-401f-8bf0-65b68973787f", "node_type": "1", "metadata": {}, "hash": "6b05a5d2fb4717761091f14e06f3819b537474d7d76c831d477d74e7b09954d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And it can do that because we grew it,\nwe didn't write it, we didn't create it.\nAnd so then that leaves open\nthis question at the end,\nwhich is, what the hell is\ngoing on inside these systems?\nAnd that, you know, is to me\na really deep and exciting question.\nIt's, you know, a really\nexciting scientific question.\nTo me it's sort of is\nlike the question that is,\nis just screaming out,\nit's calling out for us\nto go and answer it when we\ntalk about neural networks.\nAnd I think it's also a very deep question\nfor safety reasons.\n- So, and mechanistic interpretability\nI guess is closer to maybe neurobiology.\n- Yeah, yeah, I think that's right.\nSo maybe to give an example\nof the kind of thing\nthat has been done that\nI wouldn't consider\nto be mechanistic interpretability.\nThere was for a long time a lot of work\non saliency maps where\nyou would take an image\nand you try to say, you know,\nthe model thinks this image is a dog.\nWhat part of the image made\nit think that it's a dog?\nAnd you know, that tells you\nmaybe something about the model\nif you can come up with a\nprincipled version of that.\nBut it doesn't really tell you like\nwhat algorithms are running in the model?\nHow was the model actually\nmaking that decision?\nMaybe it's telling you something about\nwhat was important to it,\nif you can make that method work,\nbut it isn't telling, you know,\nwhat are the algorithms that are running?\nHow is it that the system's\nable to do this thing\nthat no one knew how to do?\nAnd so I guess we started\nusing the term mechanistic\ninterpretability\nto try to sort of draw that divide\nor to distinguish ourselves in the work\nthat we were doing in some ways\nfrom some of these other things.\nAnd I think since then it's become\nthis sort of umbrella term for, you know,\na pretty wide variety of work.\nBut I'd say that the things\nthat are kind of distinctive are,\nI think, A, this focus on,\nwe really want to get at, you know,\nthe mechanisms, we wanna\nget at the algorithms.\nYou know, if you think of neural networks\nas being like a computer program,\nthen the weights are kind of\nlike a binary computer program.\nAnd we'd like to reverse\nengineer those weights\nand figure out what\nalgorithms are running.\nSo, I think one way you might think\nof trying to understand a neural network\nis that it's kind of like,\nwe have this compiled computer program\nand the weights of the neural\nnetwork are the binary.\nAnd when the neural network runs,\nthat's the activations.\nAnd our goal is ultimately\nto go and understand these weights.\nAnd so, you know, the approach\nof mechanistic interpretability\nis to somehow figure out\nhow do these weights\ncorrespond to algorithms.\nAnd in order to do that,\nyou also have to\nunderstand the activations,\n'cause it's sort of, the\nactivations are like the memory.\nAnd if you imagine reverse\nengineering a computer program\nand you have the binary instructions,\nyou know, in order to understand\nwhat a particular instruction\nmeans, you need to know\nwhat is stored in the memory\nthat it's operating on.\nAnd so those two things\nare very intertwined.\nSo mechanistic interpret really tends\nto be interested both of those things.\nNow, you know, there's a lot of work\nthat's interested in those things,\nespecially, you know, there's\nall this work on probing,\nwhich you might see as part\nof being mechanistic interpretability.\nAlthough it's, you know, again,\nit's just a broad term\nand not everyone who does that work\nwould identify as doing mech interp.\nI think a thing that is maybe\na little bit distinctive\nto the vibe of mech\ninterp is I think people\nworking in this space tend to think\nof neural networks as, well,\nmaybe one way to say it\nis that gradient descent\nis smarter than you, that, you know,\nand gradient descent is\nactually really great.\nThe whole reason that we're\nunderstanding these models\nis 'cause we didn't know how to write them\nin the first place.\nThat gradient descent comes up\nwith better solutions than us.", "mimetype": "text/plain", "start_char_idx": 262554, "end_char_idx": 266490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bfd35ff9-0ff9-401f-8bf0-65b68973787f": {"__data__": {"id_": "bfd35ff9-0ff9-401f-8bf0-65b68973787f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca33393b-af52-4a43-b3c0-0f0102b767c6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "df2cbdc21278624dc4f0719444406ef257cb235355e1e04a379c2c9496d92941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f544f6a-ab13-4858-992c-3ceea8a3e688", "node_type": "1", "metadata": {}, "hash": "341d2c9807cda017d91b25dc396e13fbd340b89da71fc3a9a31f536b3306eba3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, you know, there's a lot of work\nthat's interested in those things,\nespecially, you know, there's\nall this work on probing,\nwhich you might see as part\nof being mechanistic interpretability.\nAlthough it's, you know, again,\nit's just a broad term\nand not everyone who does that work\nwould identify as doing mech interp.\nI think a thing that is maybe\na little bit distinctive\nto the vibe of mech\ninterp is I think people\nworking in this space tend to think\nof neural networks as, well,\nmaybe one way to say it\nis that gradient descent\nis smarter than you, that, you know,\nand gradient descent is\nactually really great.\nThe whole reason that we're\nunderstanding these models\nis 'cause we didn't know how to write them\nin the first place.\nThat gradient descent comes up\nwith better solutions than us.\nAnd so I think that maybe\nanother thing about mech interp\nis sort of having almost\na kind of humility\nthat we won't guess a priori\nwhat's going on inside the models.\nWe have to have this sort\nof bottom up approach\nwhere we don't really assume, you know,\nwe don't assume that we should\nlook for a particular thing\nand that that will be there\nand that's how it works.\nBut instead we look for the bottom up\nand discover what happens\nto exist in these models\nand study them that way.\n- But, you know, the very\nfact that it's possible to do,\nand as you and others have\nshown over time, you know,\nthings like universality, that the wisdom\nof the gradient descent\ncreates features and circuits,\ncreates things universally\nacross different kinds of\nnetworks that are useful,\nand that makes the whole field possible.\n- Yeah, so this is actually,\nis indeed a really remarkable\nand exciting thing\nwhere it does seem like,\nat least to some extent,\nyou know, the same elements,\nthe same features and\ncircuits form again and again.\nYou know, you can look\nat every vision model\nand you'll find curve detectors\nand you'll find high/low\nfrequency detectors.\nAnd in fact, there's some reason to think\nthat the same things\nform across, you know,\nbiological neural networks and\nartificial neural networks.\nSo a famous example is vision models\nin their early layers\nthey have Gabor filters,\nand there's, you know,\nGabor filters are something\nthat neuroscientists\nare interested in, have\nthought a lot about.\nWe find curve detectors in these models,\ncurve detectors are also found in monkeys.\nAnd we discover these high\nlow frequency detectors\nand then some follow up work\nwent and discovered them in rats or mice.\nSo they were found first in\nartificial neural networks\nand then found in\nbiological neural networks.\nYou know, there's this\nreally famous result\non like grandmother neurons\nor the Halle Berry neuron\nfrom Quiroga et al.\nAnd we found very similar\nthings in vision models where,\nthis is while I was still at OpenAI\nand I was looking at their clip model,\nand you find these neurons that respond\nto the same entities in images.\nAnd also to give a concrete example there,\nwe found that there was\na Donald Trump neuron.\nFor some reason, I guess everyone likes\nto talk about Donald Trump,\nand Donald Trump was very prominent,\nwas a very hot topic at that time.\nSo every neural network we looked at,\nwe would find a dedicated\nneuron for Donald Trump.\nAnd that was the only person\nwho had always had a dedicated neuron.\nYou know, sometimes you'd\nhave an Obama neuron,\nsometimes you'd have a Clinton neuron,\nbut Trump always had a dedicated neuron.\nSo it responds to, you\nknow, pictures of his face\nand the word Trump, like\nall these things, right?\nAnd so it's not responding\nto a particular example\nor like, it's not just\nresponding to his face,\nit's extracting over this\ngeneral concept, right?\nSo in any case, that's very similar\nto these Quiroga et al results.\nSo there evidence that this\nphenomenon of universality,\nthe same things form\nacross both artificial\nand natural neural networks.\nThat's a pretty amazing\nthing if that's true.", "mimetype": "text/plain", "start_char_idx": 265690, "end_char_idx": 269612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f544f6a-ab13-4858-992c-3ceea8a3e688": {"__data__": {"id_": "2f544f6a-ab13-4858-992c-3ceea8a3e688", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfd35ff9-0ff9-401f-8bf0-65b68973787f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f3d8e18e20f9fe81ce1907fccdb33f5bc06b4216abf2a2febcf628813672c527", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f", "node_type": "1", "metadata": {}, "hash": "f1da8b4e928ccac296e7926afdac2b1f5da8d42e4e372718945a40ec2da63e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So every neural network we looked at,\nwe would find a dedicated\nneuron for Donald Trump.\nAnd that was the only person\nwho had always had a dedicated neuron.\nYou know, sometimes you'd\nhave an Obama neuron,\nsometimes you'd have a Clinton neuron,\nbut Trump always had a dedicated neuron.\nSo it responds to, you\nknow, pictures of his face\nand the word Trump, like\nall these things, right?\nAnd so it's not responding\nto a particular example\nor like, it's not just\nresponding to his face,\nit's extracting over this\ngeneral concept, right?\nSo in any case, that's very similar\nto these Quiroga et al results.\nSo there evidence that this\nphenomenon of universality,\nthe same things form\nacross both artificial\nand natural neural networks.\nThat's a pretty amazing\nthing if that's true.\nYou know, it suggests that,\nwell, I think the thing\nthat it suggests is\nthe gradient of descent\nis sort of finding, you\nknow, the right ways\nto cut things apart in some sense\nthat many systems converge on,\nand many different neural networks'\narchitectures converge on.\nThat there's some\nnatural set of, you know,\nthere's some set of abstractions\nthat are a very natural way\nto cut apart the problem,\nand that a lot of systems\nare gonna converge on.\nThat would be my kind of, you know,\nI don't know anything about neuroscience.\nThis is just my kind of wild speculation\nfrom what we've seen.\n- Yeah, that would be beautiful\nif it's sort of agnostic\nto the medium of the model that's used\nto form the representation.\n- Yeah, yeah, and, you know,\nit's a kind of a wild speculation based,\nyou know, we only have some,\na few data points that suggest this,\nbut you know, it does seem like\nthere's some sense in\nwhich the same things form\nagain and again in both,\nin certainly in natural neural networks\nand also artificially or in biology.\n- And the intuition behind\nthat would be that, you know,\nin order to be useful in\nunderstanding the real world,\nyou need all the same kind of stuff.\n- Yeah, well if we pick, I don't know,\nlike the idea of a dog, right?\nLike, you know, there's\nsome sense in which the idea\nof a dog is like a natural category\nin the universe or\nsomething like this, right?\nLike, you know, there's some reason,\nit's not just like a weird\nquirk of like how humans factor,\nyou know, think about the world that\nwe have this concept of a dog.\nIt's in some sense...\nOr like, if you have the idea of a line,\nlike there's, you know,\nlike look around us,\nyou know, there are lines, you know.\nIt's sort of the simplest way\nto understand this room in some sense\nis to have the idea of a line.\nAnd so I think that\nthat would be my instinct\nfor why this happens.\n- Yeah, you need a curved line, you know,\nto understand a circle,\nand you need all those shapes\nto understand bigger things\nand it's a hierarchy of\nconcepts that are formed, yeah.\n- And like maybe there are ways to go\nand describe, you know,\nimages without reference\nto those things, right?\nBut they're not the simplest way\nor the most economical way\nor something like this.\nAnd so systems converge\nto these strategies\nwould be my wild hypothesis.\n- Can you talk through\nsome of the building blocks\nthat we've been referencing\nof features and circuits?\nSo I think you first described them\nin 2020 paper \"Zoom In: An\nIntroduction to Circuits.\"\n- Absolutely, so maybe I'll start\nby just describing some phenomena,\nand then we can sort of build to the idea\nof features and circuits.\n- [Lex] Wonderful.\n- If you spent like quite a few years,\nmaybe like five years to some extent\nwith other things, studying\nthis one particular model\nInception V1, which is\nthis one vision model.\nIt was state of the art in 2015\nand, you know, very much not\nstate of the art anymore.\n(Lex laughs)\nAnd it has, you know, maybe\nabout 10,000 neurons in it.\nAnd I spent a lot of time looking at\nthe 10,000 neurons, odd\nneurons of Inception V1.", "mimetype": "text/plain", "start_char_idx": 268837, "end_char_idx": 272694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f": {"__data__": {"id_": "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f544f6a-ab13-4858-992c-3ceea8a3e688", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "070da2e2c9828c439616a14408e101f3ea33dfbdcf5a1df9701334c4df3cdd79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6", "node_type": "1", "metadata": {}, "hash": "adfafa3591ff74477179f40b6e57b50a54faf61eca00c1926a904d2487042611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I think you first described them\nin 2020 paper \"Zoom In: An\nIntroduction to Circuits.\"\n- Absolutely, so maybe I'll start\nby just describing some phenomena,\nand then we can sort of build to the idea\nof features and circuits.\n- [Lex] Wonderful.\n- If you spent like quite a few years,\nmaybe like five years to some extent\nwith other things, studying\nthis one particular model\nInception V1, which is\nthis one vision model.\nIt was state of the art in 2015\nand, you know, very much not\nstate of the art anymore.\n(Lex laughs)\nAnd it has, you know, maybe\nabout 10,000 neurons in it.\nAnd I spent a lot of time looking at\nthe 10,000 neurons, odd\nneurons of Inception V1.\nAnd one of the interesting\nthings is, you know,\nthere are lots of neurons\nthat don't have some obvious\ninterpretable meaning,\nbut there's a lot of neurons\nand Inception V1 that do have\nreally clean interpretable meanings.\nSo you find neurons\nthat just really do seem\nto detect curves,\nand you find neurons that\nreally do seem to detect cars,\nand car wheels and car windows\nand, you know, floppy ears of dogs,\nand dogs with long snouts\nfacing to the right,\nand dogs with long snouts\nfacing to the left.\nAnd you know, different kinds of,\nthere's sort of this whole\nbeautiful edge detectors,\nline detectors, color contrast detectors,\nthese beautiful things we call\nhigh/low frequency detectors.\nYou know, I think looking at it,\nI sort of felt like a biologist, you know,\nyou just, you're looking at this\nsort of new world of proteins,\nand you're discovering all these\ndifferent proteins that interact.\nSo one way you could try\nto understand these models\nis in terms of neurons.\nYou could try to be like, oh, you know,\nthere's a dog detecting neuron\nand here's a car detecting neuron.\nAnd it turns out you can actually ask\nhow those connect together.\nSo you can go and say, oh, you know,\nI have this car detecting\nneuron, how is it built?\nAnd it turns out in the previous layer,\nit's connected really\nstrongly to a window detector,\nand a wheel detector,\nand it's sort of car body detector.\nAnd it looks for the window above the car,\nand the wheels below,\nand the car chrome sort of in the middle,\nsort of everywhere but\nespecially on the lower part.\nAnd that's sort of a\nrecipe for a car, right?\nLike that is, you know, earlier we said\nthat the thing we wanted from mech interp\nwas to get algorithms to go and get,\nyou know, ask what is\nthe algorithm that runs?\nWell, here we're just\nlooking at the weights\nof the neuron network and reading off\nthis kind of recipe for detecting cars.\nIt's a very simple crude\nrecipe, but it's there.\nAnd so we call that a\ncircuit, this connection.\nWell, okay, so the problem is that\nnot all of the neurons are interpretable,\nand there's reason to think,\nwe can get into this more later,\nthat there's this\nsuperposition hypothesis.\nThere's reason to think that sometimes\nthe right unit to analyze things\nin terms of is combinations of neurons.\nSo sometimes it's not that\nthere's a single neuron\nthat represents say a car,\nbut it actually turns out\nafter you detect the car,\nthe model sort of hides\na little bit of the car\nin the following layer\nand a bunch of dog detectors.\nWhy is it doing that?\nWell, you know, maybe it just doesn't\nwanna do that much work\non cars at that point\nand you know, it's sort\nof storing it away to go.\nSo it turns out then this\nsort of subtle pattern of,\nyou know, there's all these neurons\nthat you think are dog detectors\nand maybe they're primarily that,\nbut they all a little bit contribute\nto representing a car in that next layer.\nOkay, so now we can't really think,\nthere might still be something that,\nI don't know, you could\ncall it like a car concept\nor something, but it no longer\ncorresponds to a neuron.", "mimetype": "text/plain", "start_char_idx": 272031, "end_char_idx": 275769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6": {"__data__": {"id_": "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dbe6ed7-46c5-4c86-bb0c-8d2835a1b52f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "04b757ad4a4edc7065982f7926d31b020c9c3cc9a496758eb9d714eb2be1d878", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4107aca-cfee-43e1-960f-aaadbb51abe2", "node_type": "1", "metadata": {}, "hash": "92397657cdd2e1bf86ae9777530a8de374c67a6050844751685be885b2bbf259", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Why is it doing that?\nWell, you know, maybe it just doesn't\nwanna do that much work\non cars at that point\nand you know, it's sort\nof storing it away to go.\nSo it turns out then this\nsort of subtle pattern of,\nyou know, there's all these neurons\nthat you think are dog detectors\nand maybe they're primarily that,\nbut they all a little bit contribute\nto representing a car in that next layer.\nOkay, so now we can't really think,\nthere might still be something that,\nI don't know, you could\ncall it like a car concept\nor something, but it no longer\ncorresponds to a neuron.\nSo we need some term for these\nkind of neuron like entities,\nthese things that we sort of\nwould've liked the neurons to be,\nthese idealized neurons,\nthe things that are the nice neurons,\nbut also maybe there's\nmore of them somehow hidden\nand we call those features.\n- And then what are circuits?\n- So circuits are these\nconnections of features, right?\nSo, when we have the car detector\nand it's connected to a window detector,\nand a wheel detector,\nand it looks for the wheels below\nand the windows on top, that's a circuit.\nSo circuits are just\ncollections of features\nconnected by weights and\nthey implement algorithms.\nSo they tell us, you know,\nhow are features used?\nHow are they built?\nHow do they connect together?\nSo maybe it's worth trying to pin down\nlike what really is the\ncore hypothesis here.\nAnd I think the core\nhypothesis is something we call\nthe linear representation hypothesis.\nSo if we think about the\ncar detector, you know,\nthe more it fires, the more\nwe sort of think of that\nas meaning, oh, the model is more\nand more confident that a car is present.\nOr you know, if it's some\ncombination of neurons\nthat represent a car, you know,\nthe more that combination fires,\nthe more we think the model\nthinks there's a car present.\nThis doesn't have to be the case, right?\nLike you could imagine something\nwhere you have, you know,\nyou have this car detector neuron\nand you think, ah, you know,\nif it fires like, you know,\nbetween one and two, that means one thing,\nbut it means like totally different\nif it's between three and four.\nThat would be a nonlinear representation.\nAnd in principle that, you\nknow, models could do that.\nI think it's sort of\ninefficient for them to do,\nif you try to think about\nhow you'd implement computation like that,\nit's kind of an annoying thing to do.\nBut in principle, models can do that.\nSo one way to think about the features\nand circuits sort of framework\nfor thinking about things\nis that we're thinking about\nthings as being linear.\nWe're thinking about there as being\nthat if a neuron or a\ncombination neurons fires more,\nit's sort of, that means more\nof a particular thing being detected.\nAnd then that gives weights\na very clean interpretation\nas these edges between these entities,\nthese features and that\nedge then has a meaning.\nSo that's in some ways the core thing.\nIt's like, you know, we can talk about\nthis sort of outset,\nthe context of neurons.\nAre you familiar with\nthe Word2Vec results?\nSo you have like, you know,\nking minus man plus woman equals queen.\nWell, the reason you can\ndo that kind of arithmetic\nis because you have a\nlinear representation.\n- Can you actually explain that\nrepresentation a little bit?\nSo, first of all, so the feature\nis a direction of activation.\n- Yeah, exactly.\n- You can do it that way.\nCan you do the minus men plus women,\nthat, the Word2Vec stuff,\ncan you explain what that is that work?\n- [Chris] Yeah, so there's this very-\n- It's such a simple, clean explanation\nof what we're talking about.\n- Exactly, yeah.\nSo there's this very famous result,\nWord2Vec by Tomas Mikolov et al,\nand there's been tons of\nfollow-up work exploring this.\nSo, sometimes we have these,\nwe create these word embeddings\nwhere we map every word to a vector.\nI mean, that in itself, by the way,\nis kind of a crazy thing\nif you haven't thought\nabout it before, right?\nLike we are going in and representing,\nwe're turning, you know,\nlike if you just learned\nabout vectors in physics class, right?", "mimetype": "text/plain", "start_char_idx": 275199, "end_char_idx": 279246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4107aca-cfee-43e1-960f-aaadbb51abe2": {"__data__": {"id_": "f4107aca-cfee-43e1-960f-aaadbb51abe2", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cb16963-6804-4eaa-ad8f-4c2a4a7c8ec6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7ef8381f4530f2b073134803542a1bbded85b1a6510eb108b03f40adb537c8de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c99d59f0-a319-4dbb-8ea3-cb77d814b054", "node_type": "1", "metadata": {}, "hash": "d75787efbe5296b038a022839f6a40713621203f06da2b729d53682bc8d46891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah, exactly.\n- You can do it that way.\nCan you do the minus men plus women,\nthat, the Word2Vec stuff,\ncan you explain what that is that work?\n- [Chris] Yeah, so there's this very-\n- It's such a simple, clean explanation\nof what we're talking about.\n- Exactly, yeah.\nSo there's this very famous result,\nWord2Vec by Tomas Mikolov et al,\nand there's been tons of\nfollow-up work exploring this.\nSo, sometimes we have these,\nwe create these word embeddings\nwhere we map every word to a vector.\nI mean, that in itself, by the way,\nis kind of a crazy thing\nif you haven't thought\nabout it before, right?\nLike we are going in and representing,\nwe're turning, you know,\nlike if you just learned\nabout vectors in physics class, right?\nAnd I'm like, oh, I'm gonna actually turn\nevery word in the\ndictionary into a vector.\nThat's kind of a crazy idea, okay.\nBut you could imagine, you could imagine\nall kinds of ways in which you\nmight map words to vectors.\nBut it seems like when\nwe train neural networks,\nthey like to go and map words to vectors\nto such that they're\nsort of linear structure\nin a particular sense,\nwhich is that directions have meaning.\nSo for instance, there\nwill be some direction\nthat seems to sort of\ncorrespond to gender,\nand male words will be, you\nknow, far in one direction\nand female words will\nbe in another direction.\nAnd the linear\nrepresentation hypothesis is,\nyou could sort of think of it roughly\nas saying that that's actually\nkind of the fundamental\nthing that's going on,\nthat everything is just\ndifferent directions\nhave meanings and adding\ndirection vectors together\ncan represent concepts.\nAnd the Mikolov paper sort\nof took that idea seriously,\nand one consequence of it is that\nyou can do this game of playing\nsort of arithmetic with words.\nSo you can do king and you can, you know,\nsubtract off the word man\nand add the word woman.\nAnd so you're sort of, you know,\ngoing and trying to switch the gender.\nAnd indeed if you do that, the result will\nsort of be close to the word queen.\nAnd you can, you know, do other things\nlike you can do, you know,\nsushi minus Japan plus Italy and get pizza\nor different things like this, right?\nSo, this is in some sense\nthe core of the linear\nrepresentation hypothesis.\nYou can describe it\njust as a purely abstract thing.\nBut vector spaces, you can\ndescribe it as a statement\nabout the activations of neurons.\nBut it's really about this property\nof directions having meaning.\nAnd in some ways it's\neven a little subtle that\nit's really I think\nmostly about this property\nof being able to add things together\nthat you can sort of independently modify\nsay gender and royalty\nor, you know, cuisine type or country,\nand the concept of food by adding them.\n- Do you think the\nlinear hypothesis holds-\n- Yes.\n- That kind of carries scales.\n- So, so far, I think\neverything I have seen\nis consistent with the hypothesis,\nand it doesn't have to be that way, right?\nLike you can write down neural networks\nwhere you write weights such that\nthey don't have linear representations,\nwhere the right way to understand them\nis not in terms of linear representations.\nBut I think every natural neural network\nI've seen has this property.\nThere's been one paper recently\nthat there's been some sort\nof pushing around the edge.\nSo I think there's been some work\nrecently studying\nmulti-dimensional features\nwhere rather than a single direction,\nit's more like a manifold of directions.\nThis to me still seems like\na linear representation.\nAnd then there's been some other papers\nsuggesting that maybe\nin very small models,\nyou get non-linear representations.\nI think that the jury's still out on that.\nBut I think everything\nthat we've seen so far has been consistent\nwith the linear representation\nhypothesis and that's wild.\nIt doesn't have to be that way,\nand yet I think that\nthere's a lot of evidence\nthat certainly at least this\nis very, very widespread,\nand so far the evidence\nis consistent with it.\nAnd I think, you know,\none thing you might say\nis you might say, well, Christopher,\nyou know, that's a lot, you know,\nto go and sort of to ride on.", "mimetype": "text/plain", "start_char_idx": 278518, "end_char_idx": 282628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c99d59f0-a319-4dbb-8ea3-cb77d814b054": {"__data__": {"id_": "c99d59f0-a319-4dbb-8ea3-cb77d814b054", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4107aca-cfee-43e1-960f-aaadbb51abe2", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b0ae970930b1094e84b22bbd09d80a42e695bc9262fa064cf7f2083528b9d46b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1f133c0-45ed-4898-a634-6cf927f1df03", "node_type": "1", "metadata": {}, "hash": "3becd2fce96d7a62e34c2ee47639d7bca3d73d31247e5354995a4d7fdc6afc70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I think there's been some work\nrecently studying\nmulti-dimensional features\nwhere rather than a single direction,\nit's more like a manifold of directions.\nThis to me still seems like\na linear representation.\nAnd then there's been some other papers\nsuggesting that maybe\nin very small models,\nyou get non-linear representations.\nI think that the jury's still out on that.\nBut I think everything\nthat we've seen so far has been consistent\nwith the linear representation\nhypothesis and that's wild.\nIt doesn't have to be that way,\nand yet I think that\nthere's a lot of evidence\nthat certainly at least this\nis very, very widespread,\nand so far the evidence\nis consistent with it.\nAnd I think, you know,\none thing you might say\nis you might say, well, Christopher,\nyou know, that's a lot, you know,\nto go and sort of to ride on.\nYou know, if we don't know\nfor sure this is true,\nand you're sort of, you know,\nyou're investing in neural networks\nas though it is true, you\nknow, isn't that dangerous?\nWell, you know, but I think actually,\nthere's a virtue in taking\nhypotheses seriously\nand pushing them as far as they can go.\nSo it might be that someday\nwe discover something\nthat isn't consistent with\nlinear representation hypothesis.\nBut science is full of hypotheses\nand theories that were wrong,\nand we learned a lot by\nsort of working under them\nas a sort of an assumption,\nand then going and pushing\nthem as far as we can.\nI guess this is sort of the heart\nof what Kuhn would call normal science.\nI dunno if you want, we\ncan talk a lot about-\n- Kuhn.\n- Philosophy of science and-\n- That leads to the paradigm shift.\nSo yeah, I love it, taking\nthe hypothesis seriously,\nand take it to a natural conclusion.\nSame with the Scaling Hypothesis, same-\n- Exactly. Exactly.\n- I love it.\n- One of my colleagues, Tom Henighan,\nwho is a former physicist,\nlike made this really nice analogy to me\nof caloric theory where, you know,\nonce upon a time we thought\nthat heat was actually,\nyou know, this thing called caloric,\nand like the reason,\nyou know, hot objects,\nyou know, would warm up,\ncool objects is like the\ncaloric is flowing through them.\nAnd like, you know, because we're so used\nto thinking about heat, you know,\nin terms of the modern theory, you know,\nthat seems kind of silly.\nBut it's actually very hard\nto construct an experiment\nthat sort of disproves\nthe caloric hypothesis.\nAnd, you know, you can actually do\na lot of really useful\nwork believing in caloric.\nFor example, it turns out that\nthe original combustion\nengines were developed\nby people who believed\nin the caloric theory.\nSo I think there's a virtue\nin taking hypotheses seriously,\neven when they might be wrong.\n- Yeah, there's a deep\nphilosophical truth to that.\nThat's kind of like how I\nfeel about space travel,\nlike colonizing Mars.\nThere's a lot of people\nthat criticize that.\nI think if you just assume\nwe have to colonize Mars\nin order to have a backup\nfor human civilization,\neven if that's not true,\nthat's gonna produce\nsome interesting engineering\nand even scientific\nbreakthroughs, I think.\n- Yeah, well, and actually\nthis is another thing\nthat I think is really interesting.\nSo, you know, there's a way\nin which I think it can be really useful\nfor society to have people\nalmost irrationally dedicated\nto investigating particular hypotheses\nbecause, well, it takes a lot to sort\nof maintain scientific morale\nand really push on\nsomething when, you know,\nmost scientific hypotheses\nend up being wrong.\nYou know, a lot of\nscience doesn't work out.\nAnd yet it's, you know,\nit's very useful to just, you know,\nthere's a joke about Jeff Hinton,\nwhich is that Jeff Hinton has discovered\nhow the brain works every\nyear for the last 50 years.\nBut you know, I say that\nwith like, you know,\nwith really deep respect\nbecause in fact that's actually, you know,\nthat led to him doing some\nsome really great work.\n- Yeah, he won the Nobel Prize.\nNow who's laughing now?\n- [Chris] Exactly, exactly, exactly.\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 281801, "end_char_idx": 285787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1f133c0-45ed-4898-a634-6cf927f1df03": {"__data__": {"id_": "a1f133c0-45ed-4898-a634-6cf927f1df03", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c99d59f0-a319-4dbb-8ea3-cb77d814b054", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "69cd6326e58107c0feab0fc61326ba75fb5bf5ad89119c9390a345af46d8b953", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "264dcea0-cdf9-4747-a732-a12801438545", "node_type": "1", "metadata": {}, "hash": "484d11d7b6dd33170041316f6b84e3f227e93630d5606fe52b911fbf268997ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You know, a lot of\nscience doesn't work out.\nAnd yet it's, you know,\nit's very useful to just, you know,\nthere's a joke about Jeff Hinton,\nwhich is that Jeff Hinton has discovered\nhow the brain works every\nyear for the last 50 years.\nBut you know, I say that\nwith like, you know,\nwith really deep respect\nbecause in fact that's actually, you know,\nthat led to him doing some\nsome really great work.\n- Yeah, he won the Nobel Prize.\nNow who's laughing now?\n- [Chris] Exactly, exactly, exactly.\n- Yeah.\n- I think one wants to be able to pop up\nand sort of recognize the\nappropriate level of confidence.\nBut I think there's also a lot of value\nand just being like, you know,\nI'm going to essentially assume,\nI'm gonna condition on this problem\nbeing possible or this being\nbroadly the right approach,\nand I'm just gonna go and\nassume that for a while\nand go and work within that\nand push really hard on it.\nAnd, you know, society has lots of people\ndoing that for different things.\nThat's actually really useful\nin terms of going and getting to,\nyou know, either really\nruling things out, right?\nWe can be like, well, you\nknow, that didn't work\nand we know that somebody tried hard.\nOr going in and getting to something that\nit does teach us\nsomething about the world.\n- So another interesting hypothesis\nis the superposition hypothesis.\nCan you describe what superposition is?\n- Yeah, so earlier we were\ntalking about word defect, right?\nAnd we were talking about how, you know,\nmaybe you have one direction\nthat corresponds to gender,\nand maybe another that\ncorresponds to royalty,\nand another one that corresponds to Italy,\nand another one that\ncorresponds to, you know,\nfood and all of these things.\nWell, you know, oftentimes\nmaybe these word embedding,\nthey might be 500\ndimensions, 1000 dimensions.\nAnd so if you believe that\nall of those directions were orthogonal,\nthen you could only have,\nyou know, 500 concepts.\nAnd you know, I love pizza, but like,\nif I was gonna go and like give\nthe like 500 most important concepts in,\nyou know, the English language,\nprobably Italy wouldn't be,\nit's not obvious at least\nthat Italy would be one of them, right?\nBecause you have to have things\nlike plural, and singular,\nand verb, and noun, and adjective,\nand, you know, there's a lot of things\nwe have to get to before\nwe get to Italy, and Japan,\nand, you know, there's a lot\nof countries in the world.\nAnd so how might it be that\nmodels could, you know,\nsimultaneously have the linear\nrepresentation hypothesis\nbe true and also represent more things\nthan they have directions.\nSo, what does that mean?\nWell, okay, so if linear\nrepresentation hypothesis is true,\nsomething interesting has to be going on.\nNow, I'll tell you one\nmore interesting thing\nbefore we go and we do\nthat, which is, you know,\nearlier we were talking about\nall these polysemantic neurons, right?\nThese neurons that, you know,\nwhen we were looking at Inception V1,\nthere's these nice neurons\nthat like the car detector\nand the curve detector\nand so on that respond\nto lots of, you know,\nto very coherent things.\nBut there's lots of neurons that respond\nto a bunch of unrelated things,\nand that's also an interesting phenomenon.\nAnd it turns out as well\nthat even these neurons\nthat are really, really clean,\nif you look at the weak\nactivations, right?\nSo if you look at like,\nyou know, the activations\nwhere it's like activating\n5% of the, you know,\nof the maximum activation,\nit's really not the core thing\nthat it's expecting, right?\nSo if you look at a curve\ndetector, for instance,\nand you look at the places\nwhere it's 5% active, you know,\nyou could interpret it just as noise\nor it could be that it's doing\nsomething else there, okay?\nSo, how could that be?", "mimetype": "text/plain", "start_char_idx": 285288, "end_char_idx": 289019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "264dcea0-cdf9-4747-a732-a12801438545": {"__data__": {"id_": "264dcea0-cdf9-4747-a732-a12801438545", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1f133c0-45ed-4898-a634-6cf927f1df03", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d788b359f42371cae2f6871bc5ce7be82bbe8f9c3598e90ad637e7b24c7ed73a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e19df090-0b48-4a73-a405-2acea22c24a2", "node_type": "1", "metadata": {}, "hash": "8f000cf70aced2070015886e04dc589892c9af4228fea64b4e2f1752c48e9721", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But there's lots of neurons that respond\nto a bunch of unrelated things,\nand that's also an interesting phenomenon.\nAnd it turns out as well\nthat even these neurons\nthat are really, really clean,\nif you look at the weak\nactivations, right?\nSo if you look at like,\nyou know, the activations\nwhere it's like activating\n5% of the, you know,\nof the maximum activation,\nit's really not the core thing\nthat it's expecting, right?\nSo if you look at a curve\ndetector, for instance,\nand you look at the places\nwhere it's 5% active, you know,\nyou could interpret it just as noise\nor it could be that it's doing\nsomething else there, okay?\nSo, how could that be?\nWell, there's this amazing\nthing in mathematics\ncalled compressed sensing,\nand it's actually this\nvery surprising fact\nwhere if you have a high dimensional space\nand you project it into\na low dimensional space,\nordinarily you can't go\nand sort of unproject it\nand get back your high\ndimensional vector, right?\nYou threw information away.\nThis is like, you know,\nyou can't invert a rectangular matrix,\nyou can only invert square matrices.\nBut it turns out that that's\nactually not quite true.\nIf I tell you that the high\ndimensional vector was sparse,\nso it's mostly zeros, then it turns out\nthat you can often go\nand find back the high dimensional vector\nwith very high probability.\nSo that's a surprising fact, right?\nIt says that, you know,\nyou can have this high\ndimensional vector space,\nand as long as things are\nsparse, you can project it down,\nyou can have a lower\ndimensional projection of it,\nand that works.\nSo the superposition\nhypothesis is saying that\nthat's what's going on in neural networks.\nThat's, for instance,\nthat's what's going on in word embeddings.\nThat word embeddings are able\nto simultaneously have directions\nbe the meaningful thing.\nAnd by exploiting the fact that\nthey're operating on a fairly\nhigh dimensional space,\nthey're actually,\nand the fact that these\nconcepts are sparse, right?\nLike, you know, you usually aren't talking\nabout Japan and Italy at the same time.\nYou know, most of those\nconcepts, you know,\nin most instances, Japan\nand Italy are both zero.\nThey're not present at all.\nAnd if that's true, then you can go\nand have it be the case that\nyou can have many more of\nthese sort of directions\nthat are meaningful, these features\nthan you have dimensions.\nAnd similarly, when we're\ntalking about neurons,\nyou can have many more\nconcepts than you have neurons.\nSo that's, at a high level,\nthe superposition hypothesis.\nNow it has this even wilder implication,\nwhich is to go and say\nthat neural networks are,\nit may not just be the case that\nthe representations are like this,\nbut the computation may\nalso be like this, you know,\nthe connections between all of them.\nAnd so in some sense, neural\nnetworks may be shadows\nof much larger, sparser neural networks,\nand what we see are these projection.\nAnd, you know, the strongest version\nof the superposition\nhypothesis would be to take\nthat really seriously and\nsort of say, you know,\nthere actually is in some\nsense this upstairs model,\nthis, you know, where the\nneurons are really sparse\nand all interpretable,\nand there's, you know,\nthe weights between them are\nthese really sparse circuits.\nAnd that's what we're studying.\nAnd the thing that we're observing\nis the shadow of evidence.\nWe need to find the original object.\n- And the process of learning\nis trying to construct\na compression of the upstairs model\nthat doesn't lose too much\ninformation in the projection.\n- Yeah, it's finding how\nto fit it efficiently\nor something like this.\nThe gradient descent is doing this.\nAnd in fact, so this sort of\nsays that gradient descent,\nyou know, it could just represent\na dense neural network,\nbut it sort of says that gradient descent\nis implicitly searching over the space\nof extremely sparse models\nthat could be projected into\nthis low dimensional space.\nAnd this large body of work\nof people going and trying\nto study sparse neural networks, right?\nWhere you go and you have,\nyou could design neural networks, right,\nwhere the edges are sparse\nand the activations are sparse.", "mimetype": "text/plain", "start_char_idx": 288368, "end_char_idx": 292494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e19df090-0b48-4a73-a405-2acea22c24a2": {"__data__": {"id_": "e19df090-0b48-4a73-a405-2acea22c24a2", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "264dcea0-cdf9-4747-a732-a12801438545", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1399aedb47161ba54b7861c2ace01899395923c0fb64102c5f083d74fc744a79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfeed0e1-9eaa-4da4-b859-2810a848d07d", "node_type": "1", "metadata": {}, "hash": "17e4a11365e1a13b17ade7adbf42eeb0e55ffc8975c8d3cf6d0ff29cb183bfd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And that's what we're studying.\nAnd the thing that we're observing\nis the shadow of evidence.\nWe need to find the original object.\n- And the process of learning\nis trying to construct\na compression of the upstairs model\nthat doesn't lose too much\ninformation in the projection.\n- Yeah, it's finding how\nto fit it efficiently\nor something like this.\nThe gradient descent is doing this.\nAnd in fact, so this sort of\nsays that gradient descent,\nyou know, it could just represent\na dense neural network,\nbut it sort of says that gradient descent\nis implicitly searching over the space\nof extremely sparse models\nthat could be projected into\nthis low dimensional space.\nAnd this large body of work\nof people going and trying\nto study sparse neural networks, right?\nWhere you go and you have,\nyou could design neural networks, right,\nwhere the edges are sparse\nand the activations are sparse.\nAnd you know, my sense is\nthat work has generally,\nit feels very principled, right?\nIt makes so much sense.\nAnd yet that work hasn't really panned out\nthat well is my impression broadly.\nAnd I think that a\npotential answer for that\nis that actually the neural network\nis already sparse in some sense.\nGradient descent was the whole time\nyou were trying to go and do this,\ngradient descent was actually\nin the behind the scenes\ngoing and searching more\nefficiently than you could\nthrough the space of sparse models,\nand going and learning\nwhatever sparse model\nwas most efficient and then figuring out\nhow to fold it down nicely to go\nand run conveniently on your GPU,\nwhich does, you know, nice,\ndense matrix multiplies,\nand that you just can't beat that.\n- How many concepts do you think\ncan be shoved into a neural network?\n- Depends on how sparse they are.\nSo there's probably an upper bound\nfrom the number of parameters, right?\nBecause you have to have,\nyou still have to have,\nyou know, weights that go\nand connect them together.\nSo that's one upper bound.\nThere are in fact all these lovely results\nfrom compressed sensing\nand the Johnson-Lindenstrauss lemma,\nand things like this.\nThat they basically tell you\nthat if you have a vector space\nand you want to have\nalmost orthogonal vectors,\nwhich is sort of probably the thing\nthat you want here, right?\nSo you're gonna say, well, you know,\nI'm gonna give up on having my concepts,\nmy features be strictly orthogonal,\nbut I'd like them to\nnot interfere that much.\nI'm gonna have to ask them\nto be almost orthogonal.\nThen this would say that\nit's actually, you know,\nonce you set a threshold\nfor what you're willing to accept\nin terms of how much\ncosine similarity there is,\nthat's actually exponential\nin the number of neurons that you have.\nSo at some point, that's not gonna\neven be the limiting factor.\nBut there's some beautiful results there.\nAnd in fact, it's probably even better\nthan that in some sense\nbecause that's sort of,\nfor saying that, you know, any random set\nof features could be active.\nBut in fact the features have\nsort of a correlational\nstructure where some features,\nyou know, are more likely to co-occur,\nand other ones are less\nlikely to co-occur.\nAnd so neural networks, my guess would be\ncan do very well in terms of going\nand packing things in such,\nto the point that's probably\nnot the limiting factor.\n- How does the problem of polysemanticity\nenter the picture here?\n- Polysemanticity is this phenomenon\nwe observe where you look at many neurons,\nand the neuron doesn't just\nsort of represent one concept.\nIt's not a clean feature.\nIt responds to a bunch\nof unrelated things.\nAnd superposition is,\nyou can think of as being a hypothesis\nthat explains the observation\nof polysemanticity.\nSo polysemanticity is\nthis observed phenomenon\nand superposition is a hypothesis that\nwould explain it along\nwith some other things.\n- So that makes mech\ninterp more difficult.\n- Right, so if you're\ntrying to understand things\nin terms of individual neurons,\nand you have polysemantic neurons,\nyou're in an awful lot of trouble, right?\nI mean, the easiest answer is\nlike, okay, well, you know,\nyou're looking at the neurons,\nyou're trying to understand them.\nThis one responds for a lot of things,\nit doesn't have a nice meaning.\nOkay, you know, that's bad.", "mimetype": "text/plain", "start_char_idx": 291608, "end_char_idx": 295828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfeed0e1-9eaa-4da4-b859-2810a848d07d": {"__data__": {"id_": "cfeed0e1-9eaa-4da4-b859-2810a848d07d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e19df090-0b48-4a73-a405-2acea22c24a2", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4dba8f66940140a08e97f6e0f9c4c8e36e26c6f1e18c10686038c93bc75972d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de81924-3a1f-43f1-9b84-80d16468d179", "node_type": "1", "metadata": {}, "hash": "a8ee19a52035b8383e9013dfc46daf59887d3116c49ebc6bf759e504437f3922", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's not a clean feature.\nIt responds to a bunch\nof unrelated things.\nAnd superposition is,\nyou can think of as being a hypothesis\nthat explains the observation\nof polysemanticity.\nSo polysemanticity is\nthis observed phenomenon\nand superposition is a hypothesis that\nwould explain it along\nwith some other things.\n- So that makes mech\ninterp more difficult.\n- Right, so if you're\ntrying to understand things\nin terms of individual neurons,\nand you have polysemantic neurons,\nyou're in an awful lot of trouble, right?\nI mean, the easiest answer is\nlike, okay, well, you know,\nyou're looking at the neurons,\nyou're trying to understand them.\nThis one responds for a lot of things,\nit doesn't have a nice meaning.\nOkay, you know, that's bad.\nAnother thing you could ask is, you know,\nultimately, we wanna\nunderstand the weights.\nAnd if you have two polysemantic neurons\nand, you know, each one responds\nto three things and then, you know,\nthe other neuron responds to three things\nand you have a weight between them,\nyou know, what does that mean?\nDoes it mean that like\nall three, you know,\nlike there's these nine, you know,\nnine interactions going on?\nIt's a very weird thing.\nBut there's also a deeper reason,\nwhich is related to the fact\nthat neural networks operate\non really high dimensional spaces.\nSo I said that our goal was, you know,\nto understand neural networks\nand understand the mechanisms,\nand one thing you might\nsay is like, well, why not?\nIt's just a mathematical function,\nwhy not just look at it, right?\nLike, you know, one of\nthe earliest projects\nI did studied these neural networks\nthat match two dimensional spaces\nto two dimensional spaces,\nand you can sort of interpret them\nas in this beautiful way\nas like bending manifolds.\nWhy can't we do that?\nWell, you know, as you have\na higher dimensional space,\nthe volume of that space in some senses\nis exponential in the\nnumber of inputs you have.\nAnd so you can't just go and visualize it.\nSo we somehow need to break that apart.\nWe need to somehow break\nthat exponential space\ninto a bunch of things that we, you know,\nsome non-exponential number of things\nthat we can reason about independently.\nAnd the independence is crucial\nbecause it's the\nindependence that allows you\nto not have to think about, you know,\nall the exponential\ncombinations of things.\nAnd things being mono-semantic,\nthings only having one meaning.\nThings having a meaning,\nthat is the key thing that allows you\nto think about them independently.\nAnd so I think that's, if\nyou want the deepest reason\nwhy we want to have interpretable\nmono-sematic features,\nI think that's really the deep reason.\n- And so the goal here,\nas your recent work has been aiming at,\nis how do we extract the\nmono-semantic features\nfrom a neural net that\nhas poly-sematic features\nand all this mess.\n- Yes, we observed these\npoly-semantic neurons,\nand we hypothesized that's\nwhat's going on is superposition.\nAnd if superposition is\nwhat's going on there,\nthere's actually a sort of\nwell established technique\nthat is sort of the\nprincipled thing to do,\nwhich is dictionary learning.\nAnd it turns out if you\ndo dictionary learning,\nin particular, if you do\nsort of a nice efficient way\nthat in some sense sort of\nnicely regularizes it as well\ncalled a sparse auto-encoder.\nIf you train a sparse auto-encoder,\nthese beautiful interpretable features\nstart to just fall out where\nthere weren't any beforehand.\nAnd so that's not a thing\nthat you would necessarily predict, right?\nBut it turns out that that\nworks very, very well.\nYou know, that to me that\nseems like, you know,\nsome non-trivial validation\nof linear representations\nin superposition.\n- So with dictionary\nlearning, you're not looking\nfor particular kind of categories,\nyou don't know what they are.\nThey just emerge.\n- Exactly.\nAnd this gets back to\nour earlier point, right?\nWhen we're not making assumptions,\ngradient descent is smarter than us,\nso we're not making\nassumptions about what's there.\nI mean, one certainly\ncould do that, right?\nOne could assume that\nthere's a PHP feature\nand go and search for it,\nbut we're not doing that.", "mimetype": "text/plain", "start_char_idx": 295090, "end_char_idx": 299213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6de81924-3a1f-43f1-9b84-80d16468d179": {"__data__": {"id_": "6de81924-3a1f-43f1-9b84-80d16468d179", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfeed0e1-9eaa-4da4-b859-2810a848d07d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1a9d841fb153d13a8a4a69a5c10f8e599dba4bc932be9ad27589b6380f6ee496", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20e3e088-1968-4995-85e7-2494a0253662", "node_type": "1", "metadata": {}, "hash": "15b6c247efb57fe22e6fbe0022a38abb30b1215e56e769124e67f233687f9269", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you train a sparse auto-encoder,\nthese beautiful interpretable features\nstart to just fall out where\nthere weren't any beforehand.\nAnd so that's not a thing\nthat you would necessarily predict, right?\nBut it turns out that that\nworks very, very well.\nYou know, that to me that\nseems like, you know,\nsome non-trivial validation\nof linear representations\nin superposition.\n- So with dictionary\nlearning, you're not looking\nfor particular kind of categories,\nyou don't know what they are.\nThey just emerge.\n- Exactly.\nAnd this gets back to\nour earlier point, right?\nWhen we're not making assumptions,\ngradient descent is smarter than us,\nso we're not making\nassumptions about what's there.\nI mean, one certainly\ncould do that, right?\nOne could assume that\nthere's a PHP feature\nand go and search for it,\nbut we're not doing that.\nWe're saying we don't know\nwhat's gonna be there.\nInstead we're just gonna go\nand let the sparse auto-encoder\ndiscover the things that are there.\n- So can you talk to the\n\"Toward Monosemanticity\" paper\nfrom October last year?\nIt had a lot of like nice\nbreakthrough results.\n- That's very kind of you\nto describe it that way.\nYeah, I mean, this was\nour first real success\nusing sparse auto-encoders.\nSo we took a one layer model,\nand it turns out if you go and, you know,\ndo dictionary learning on it,\nyou find all these really\nnice interpretable features.\nSo, you know, the Arabic\nfeature, the Hebrew feature,\nthe Base64 features,\nthose were some examples\nthat we studied in a lot of depth,\nand really showed that they\nwere what we thought they were.\nIt turns, if you train\na model twice as well\nand train two different models\nand do dictionary learning,\nyou find analogous\nfeatures in both of them.\nSo that's fun.\nYou find all kinds of\nof different features.\nSo that was really just\nshowing that this work.\nAnd you know, I should mention\nthat there was this Cunningham et al\nthat had very similar\nresults around the same time.\n- There's something fun about\ndoing these kinds of\nsmall scale experiments\nand finding that it's actually working.\n- Yeah, well, and there's\nso much structure here,\nlike you know, so maybe\nstepping back for a while,\nI thought that maybe all this\nmechanistic interpretability work,\nthe end result was gonna be that\nI would have an explanation\nfor why it was sort of, you know,\nvery hard and not gonna be tractable.\nYou know, we'd be like,\nwell, there's this problem\nwith superposition,\nand it turns out\nsuperposition is really hard,\nand we're kind of screwed,\nbut that's not what happened.\nIn fact, a very natural,\nsimple technique just works.\nAnd so then that's actually\na very good situation.\nYou know, I think this is a\nsort of hard research problem\nand it's got a lot of research risk\nand you know, it might\nstill very well fail,\nbut I think that some amount of,\nsome very significant\namount of research risk\nwas sort of put behind us\nwhen that started to work.\n- Can you describe what kind of\nfeatures can be extracted in this way?\n- Well, so it depends on the model\nthat you're studying, right?\nSo the larger the model,\nthe more sophisticated they're gonna be,\nand we'll probably talk about\nfollow up work in a minute.\nBut in these one layer models,\nso some very common things\nI think were languages,\nboth programming languages\nand natural languages.\nThere were a lot of features\nthat were specific words in\nspecific contexts, so \"the.\"\nAnd I think really the way to think\nabout this is \"the\" is likely\nabout to be followed by a noun.\nSo it's really, you could\nthink of this as \"the\" feature,\nbut you could also think of this\nas predicting a specific noun feature.\nAnd there would be these features\nthat would fire for \"the\" in the context\nof say a legal document,\nor a mathematical document\nor something like this.\nAnd so, you know, maybe\nin the context of math\nyou're like, you know, and\n\"the\" then predict vector,\nmatrix, you know, all\nthese mathematical words,\nwhereas, you know, in other context,\nyou would predict other\nthings, that was common.", "mimetype": "text/plain", "start_char_idx": 298385, "end_char_idx": 302397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20e3e088-1968-4995-85e7-2494a0253662": {"__data__": {"id_": "20e3e088-1968-4995-85e7-2494a0253662", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6de81924-3a1f-43f1-9b84-80d16468d179", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1c0192f7acbd0d316c7c723f241c9476f7d853dcc9826b920fcea22039456ddd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4002eefd-9ab8-4bf4-b804-db332d14267c", "node_type": "1", "metadata": {}, "hash": "05c6a27ccae1c08f6e57e7c0e1a88523f01d7ca77693af7bb7930ec3448d0d4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But in these one layer models,\nso some very common things\nI think were languages,\nboth programming languages\nand natural languages.\nThere were a lot of features\nthat were specific words in\nspecific contexts, so \"the.\"\nAnd I think really the way to think\nabout this is \"the\" is likely\nabout to be followed by a noun.\nSo it's really, you could\nthink of this as \"the\" feature,\nbut you could also think of this\nas predicting a specific noun feature.\nAnd there would be these features\nthat would fire for \"the\" in the context\nof say a legal document,\nor a mathematical document\nor something like this.\nAnd so, you know, maybe\nin the context of math\nyou're like, you know, and\n\"the\" then predict vector,\nmatrix, you know, all\nthese mathematical words,\nwhereas, you know, in other context,\nyou would predict other\nthings, that was common.\n- And basically we need clever humans\nto assign labels to what we're seeing.\n- Yes, so, you know,\nthis is, the only thing this is doing\nis that sort of unfolding things for you.\nSo if everything was sort\nof folded over top of it,\nyou know, superposition folded\neverything on top of itself\nand you can't really see\nit, this is unfolding it.\nBut now you still have\na very complex thing\nto try to understand.\nSo then you have to do a bunch of work\nunderstanding what these are.\nAnd some of them are really subtle.\nLike there's some really cool things\neven in this one layer\nmodel about Unicode where,\nyou know, of course some\nlanguages are in Unicode\nand the tokenizer won't necessarily have\na dedicated token for\nevery Unicode character.\nSo instead what you'll have\nis you'll have these patterns\nof alternating token,\nor alternating tokens\nthat each represent half\nof a Unicode character.\n- Nice.\n- And you'll have\na different feature that, you know,\ngoes and activates on the\nopposing ones to be like, okay,\nyou know, I just finished\na character, you know,\ngo and predict next prefix.\nThen okay, on the prefix,\nyou know, predict a reasonable suffix,\nand you have to alternate back and forth.\nSo there's, you know,\nthese one player models\nare really interesting.\nAnd I mean, it's another thing\nthat just, you might think,\nokay, there would just\nbe one Base64 feature,\nbut it turns out there's actually\na bunch of Base64 features\nbecause you can have English\ntext encoded as Base64,\nand that has a very different distribution\nof Base64 tokens than regular.\nAnd there's some things about tokenization\nas well that it can exploit.\nAnd I dunno, there's all\nall kinds of fun stuff.\n- How difficult is the task\nof sort of assigning\nlabels to what's going on?\nCan this be automated by AI?\n- Well, I think it depends on the feature\nand it also depends on how\nmuch you trust your AI.\nSo there's a lot of work doing\nautomated interpretability.\nI think that's a really\nexciting direction,\nand we do a fair amount of\nautomated interpretability\nand have Claude go and label our features.\n- Is there some funny moments\nwhere it's totally right\nor it's totally wrong?\n- Yeah, well, I think it's very common\nthat it's like says\nsomething very general,\nwhich is like true in some sense,\nbut not really picking up\non the specific of what's going on.\nSo I think that's a\npretty common situation.\nYeah, don't know that I have\na particularly amusing one.\n- That's interesting, that\nlittle gap between it is true\nbut doesn't quite get to\nthe deep nuance of a thing.\nThat's a general challenge.\nIt's like truly an\nincredible accomplishment\nthat it can say a true thing,\nbut it doesn't, it's not,\nit's missing the depth sometimes.\nAnd in this context, it's\nlike the ARC challenge,\nyou know, the sort of IQ type of tests.\nIt feels like figuring out\nwhat a feature represents is a bit of,\nis a little puzzle you have to solve.\n- Yeah, and I think that\nsometimes they're easier,\nand sometimes they're harder as well.\nSo yeah, I think that's tricky.\nAnd there's another thing which, I dunno,\nmaybe in some ways this is\nmy like aesthetic coming in,\nbut I'll try to give\nyou a rationalization.", "mimetype": "text/plain", "start_char_idx": 301566, "end_char_idx": 305559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4002eefd-9ab8-4bf4-b804-db332d14267c": {"__data__": {"id_": "4002eefd-9ab8-4bf4-b804-db332d14267c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20e3e088-1968-4995-85e7-2494a0253662", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4954f66767bbf31e3205fd0927957d3c3cbc6f9bcb6280c2bdcfa6da375b10f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bb57993-603f-4a92-8fe4-6fda48665a68", "node_type": "1", "metadata": {}, "hash": "9a746c3de60f4b1808aed8c09dc9e90654fa36aa71c71396c0845609aafeed07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yeah, don't know that I have\na particularly amusing one.\n- That's interesting, that\nlittle gap between it is true\nbut doesn't quite get to\nthe deep nuance of a thing.\nThat's a general challenge.\nIt's like truly an\nincredible accomplishment\nthat it can say a true thing,\nbut it doesn't, it's not,\nit's missing the depth sometimes.\nAnd in this context, it's\nlike the ARC challenge,\nyou know, the sort of IQ type of tests.\nIt feels like figuring out\nwhat a feature represents is a bit of,\nis a little puzzle you have to solve.\n- Yeah, and I think that\nsometimes they're easier,\nand sometimes they're harder as well.\nSo yeah, I think that's tricky.\nAnd there's another thing which, I dunno,\nmaybe in some ways this is\nmy like aesthetic coming in,\nbut I'll try to give\nyou a rationalization.\nYou know, I'm actually a little suspicious\nof automated interpretability,\nand I think that partly just that\nI want humans to\nunderstand neural networks,\nand if the neural network\nis understanding it for me,\nyou know, I don't quite like that.\nBut I do have a bit of,\nyou know, in some ways,\nI'm sort of like the mathematicians\nwho are like, you know,\nif there's a computer automated proof,\nit doesn't count.\n- Right?\n- You know,\nthey won't understand it.\nBut I do also think that\nthere is this kind of like reflections\non trusting trust type issue where,\nyou know, if you, there's\nthis famous talk about,\nyou know, like when you're\nwriting a computer program,\nyou have to trust your compiler,\nand if there was like\nmalware in your compiler,\nthen it could go and inject\nmalware into the next compiler,\nand you know, you'd be\nkind of in trouble, right?\nWell, if you're using neural networks\nto go and verify that your\nneural networks are safe,\nthe hypothesis that you're\ntesting for is like,\nokay, well, the neural\nnetwork maybe isn't safe,\nand you have to worry about\nlike, is there some way\nthat it could be screwing with you?\nSo, you know, I think that's\nnot a big concern now,\nbut I do wonder in the long run\nif we have to use really\npowerful AI system\nto go and, you know, audit our AI systems,\nis that actually something we can trust?\nBut maybe I'm just rationalizing\n'cause I just want to us to have to get it\nto a point where humans\nunderstand everything.\n- Yeah, I mean, especially\nthat's hilarious,\nespecially as we talk about AI safety\nand it looking for features\nthat would be relevant\nto AI safety, like deception and so on.\nSo let's talk about the\n\"Scaling Monosemanticity\" paper\nin May, 2024.\nOkay, so what did it take to scale this,\nto apply to Claude 3s on it?\n- Well, a lot of GPUs.\n- A lot more GPUs, got it.\n- But one of my teammates,\nTom Henighan was involved in\nthe original scaling laws work,\nand something that he\nwas sort of interested in\nfrom very early on is,\nare there scaling laws\nfor interpretability?\nAnd so something he\nsort of immediately did\nwhen this work started to succeed,\nand we started to have\nsparse auto-encoders work\nwas he became very interested in,\nyou know, what are the scaling laws for,\nyou know, for making sparse\nauto-encoders larger?\nAnd how does that relate to\nmaking the base model larger?\nAnd so it turns out this works really well\nand you can use it to\nsort of project, you know,\nif you train a sparse\nauto-encoder at a given size,\nyou know, how many tokens\nshould you train on?\nAnd so on.\nSo this was actually a very big help to us\nin scaling up this work,\nand made it a lot easier\nfor us to go and train,\nyou know, really large\nsparse auto-encoders where,\nyou know, it's not like\ntraining the big models,\nbut it's starting to get to a point\nwhere it's actually actually expensive\nto go and train the really big ones.\n- So you have this, I mean,\nyou have to do all this stuff\nof like splitting it across large GPUs-\n- Oh yeah, no, I mean there's a huge\nengineering challenge here too, right?", "mimetype": "text/plain", "start_char_idx": 304773, "end_char_idx": 308604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8bb57993-603f-4a92-8fe4-6fda48665a68": {"__data__": {"id_": "8bb57993-603f-4a92-8fe4-6fda48665a68", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4002eefd-9ab8-4bf4-b804-db332d14267c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a27d5a85891bbd576dfd6c2cbdd6f60cacb8f534b00462134e05b606f6914770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "410c3a09-4c30-4117-822e-ba0ebb3f2aea", "node_type": "1", "metadata": {}, "hash": "331e8c9a8f4655f374483cb1310860e9743bd63798c449a099dde323bed82293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And how does that relate to\nmaking the base model larger?\nAnd so it turns out this works really well\nand you can use it to\nsort of project, you know,\nif you train a sparse\nauto-encoder at a given size,\nyou know, how many tokens\nshould you train on?\nAnd so on.\nSo this was actually a very big help to us\nin scaling up this work,\nand made it a lot easier\nfor us to go and train,\nyou know, really large\nsparse auto-encoders where,\nyou know, it's not like\ntraining the big models,\nbut it's starting to get to a point\nwhere it's actually actually expensive\nto go and train the really big ones.\n- So you have this, I mean,\nyou have to do all this stuff\nof like splitting it across large GPUs-\n- Oh yeah, no, I mean there's a huge\nengineering challenge here too, right?\nSo, yeah, so there's a\nscientific question of,\nhow do you scale things effectively?\nAnd then there's an enormous amount\nof engineering to go and scale this up.\nSo you have to chart it,\nyou have to think very\ncarefully about a lot of things.\nI'm lucky to work with a\nbunch of great engineers\n'cause I am definitely\nnot a great engineer.\n- Yeah, and the infrastructure especially,\nyeah, for sure.\nSo it turns out, TLDR, it worked.\n- It worked, yeah.\nAnd I think this is important\nbecause you could have imagined, like,\nyou could have imagined a world\nwhere you said after\ntowards mono-semanticity,\nyou know, Chris, this is great,\nyou know, it works on a one layer model,\nbut one layer models are\nreally idiosyncratic.\nLike, you know, maybe,\nthat's just something,\nlike maybe the linear\nrepresentation hypothesis\nand superposition hypothesis\nis the right way to\nunderstand a one layer model,\nbut it's not the right way\nto understand larger models.\nAnd so I think, I mean, first of all,\nthe Cunningham et al\npaper sort of cut through\nthat a little bit and sort of suggested\nthat this wasn't the case.\nBut scaling mono-semanticity sort of,\nI think was significant evidence\nthat even for very large models,\nand we did it on Claude 3 Sonnet,\nwhich at that point was one\nof our production models.\nYou know, even these\nmodels seemed to be very,\nyou know, seemed to be\nsubstantially explained\nat least by linear features.\nAnd, you know, doing dictionary\nlearning on them works,\nand as you learn more features,\nyou go and you explain more and more.\nSo that's, I think,\nquite a promising sign.\nAnd you find now really\nfascinating abstract features.\nAnd the features are also multimodal.\nThey respond to images and text\nfor the same concept, which is fun.\n- Yeah, can you explain that?\nI mean, like, you know, backdoor,\nthere's just a lot of\nexamples that you can-\n- Yeah, so maybe let's start\nwith a one example to start,\nwhich is we found some features around\nsort of security vulnerabilities\nand backdoors and codes.\nSo it turns out those are\nactually two different features.\nSo there's a security\nvulnerability feature,\nand if you force it\nactive, Claude will start\nto go and write security vulnerabilities\nlike buffer overflows into code.\nAnd it also, it fires\nfor all kinds of things.\nLike, you know, some of the\ntop dataset examples for it\nwere things like, you\nknow, dash dash disable,\nyou know, SSL or something like this,\nwhich are sort of\nobviously really insecure.\n- So at this point it's kind of like,\nmaybe it's just because the examples\nwere presented that way,\nit's kind of like a little bit\nmore obvious examples, right?\nI guess the idea is that down the line,\nit might be able to detect more nuanced,\nlike deception or bugs\nor that kind of stuff.\n- Yeah, well, I maybe wanna\ndistinguish two things.\nSo one is the complexity of the feature\nor the concept, right?\nAnd the other is the nuance\nof how subtle the examples\nwe're looking at, right?\nSo, when we show the top dataset examples,\nthose are the most extreme examples\nthat cause that feature to activate.\nAnd so it doesn't mean\nthat it doesn't fire\nfor more subtle things.", "mimetype": "text/plain", "start_char_idx": 307842, "end_char_idx": 311735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "410c3a09-4c30-4117-822e-ba0ebb3f2aea": {"__data__": {"id_": "410c3a09-4c30-4117-822e-ba0ebb3f2aea", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bb57993-603f-4a92-8fe4-6fda48665a68", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5f5d78c9f92a981950f0e8216eb58cb6ccfdbc608ff4cb120e53f78e20544b77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21485ede-86e6-4e3b-bb7c-c4b0017c217d", "node_type": "1", "metadata": {}, "hash": "1629879d5df844317308cd305329385d66cec7d729a860bc90a61502d7ace659", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So at this point it's kind of like,\nmaybe it's just because the examples\nwere presented that way,\nit's kind of like a little bit\nmore obvious examples, right?\nI guess the idea is that down the line,\nit might be able to detect more nuanced,\nlike deception or bugs\nor that kind of stuff.\n- Yeah, well, I maybe wanna\ndistinguish two things.\nSo one is the complexity of the feature\nor the concept, right?\nAnd the other is the nuance\nof how subtle the examples\nwe're looking at, right?\nSo, when we show the top dataset examples,\nthose are the most extreme examples\nthat cause that feature to activate.\nAnd so it doesn't mean\nthat it doesn't fire\nfor more subtle things.\nSo, you know, the insecure code feature,\nyou know, the stuff that it fires for,\nmost strongly for are\nthese like really obvious,\nyou know, disable the\nsecurity type things.\nBut you know, it also fires\nfor, you know, buffer overflows\nand more subtle security\nvulnerabilities in code.\nYou know, these features\nare all multimodal,\nso you could ask like, what\nimages activate this feature?\nAnd it turns out that the\nsecurity vulnerability feature\nactivates for images of like people\nclicking on Chrome to\nlike go past the, like,\nyou know, this website,\nthe SSL certificate might be\nwrong or something like this.\nAnother thing that's very entertaining\nis there's backdoors and code feature.\nLike you activate it, it goes\nand Claude writes a backdoor\nthat like will go and dump\nyour data to port or something.\nBut you can ask, okay,\nwhat images activate the backdoor feature?\nIt was devices with\nhidden cameras in them.\nSo there's a whole\napparently genre of people\ngoing and selling devices\nthat look innocuous,\nthat have hidden cameras and they have-\n- That's great.\n- This hidden camera in it.\nAnd I guess that is the, you know,\nphysical version of a backdoor.\nAnd so it sort of shows you\nhow abstract these concepts are, right?\nAnd I just thought that was,\nI'm sort of sad that\nthere's a whole market\nof people selling devices like that,\nbut I was kind of delighted that\nthat was the thing that\nit came up with as the top\nimage examples for the feature.\n- Yeah, it's nice. It's multimodal.\nIt's multi almost context.\nIt's as broad, strong definition\nof a singular concept, it's nice.\n- Yeah.\n- To me, one of the really\ninteresting features,\nespecially for AI safety\nis deception and lying.\nAnd the possibility that\nthese kinds of methods\ncould detect lying in a model,\nespecially gets smarter\nand smarter and smarter.\nPresumably that's a big threat\nof a super intelligent model\nthat it can deceive\nthe people operating it\nas to its intentions or\nany of that kind of stuff.\nSo what have you learned\nfrom detecting lying inside models?\n- Yeah, so I think we're in some ways\nin early days for that.\nWe find quite a few features\nrelated to deception and lying.\nThere's one feature where, you know,\nfires for people lying\nand being deceptive,\nand you force it active\nand starts lying to you.\nSo we have a deception feature.\nI mean, there's all\nkinds of other features\nabout withholding information\nand not answering questions,\nfeatures about power seeking\nand coups and stuff like that.\nSo there's a lot of features\nthat are kind of related to spooky things.\nAnd if you force them active,\nClaude will behave in ways that are,\nthey're not the kinds\nof behaviors you want.\n- What are possible\nnext exciting directions\nto you in the space of mech interp?\n- Well, there's a lot of things.\nSo for one thing, I would\nreally like to get to a point\nwhere we have circuits where\nwe can really understand\nnot just the features,\nbut then use that to understand\nthe computation of models.\nThat relief for me is the\nultimate goal of this.\nAnd there's been some work,\nwe put out a few things.\nThere's a paper from Sam Marks\nthat does some stuff like this.\nAnd there's been some,\nI'd say some work around the edges here.\nBut I think there's a lot more to do,\nand I think that will be\na very exciting thing.", "mimetype": "text/plain", "start_char_idx": 311069, "end_char_idx": 315020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "21485ede-86e6-4e3b-bb7c-c4b0017c217d": {"__data__": {"id_": "21485ede-86e6-4e3b-bb7c-c4b0017c217d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "410c3a09-4c30-4117-822e-ba0ebb3f2aea", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5261dbc6f9625ab8fc4d5522a7d9d651f4dbad85367c1c82d0290419543d59b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35d51ac0-a6b6-4883-b08f-ee263f10c2bb", "node_type": "1", "metadata": {}, "hash": "6b4625ba0f0d571c7ea64c0ec97d1cd58068d44e89cadf0cce8e66620856d570", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So there's a lot of features\nthat are kind of related to spooky things.\nAnd if you force them active,\nClaude will behave in ways that are,\nthey're not the kinds\nof behaviors you want.\n- What are possible\nnext exciting directions\nto you in the space of mech interp?\n- Well, there's a lot of things.\nSo for one thing, I would\nreally like to get to a point\nwhere we have circuits where\nwe can really understand\nnot just the features,\nbut then use that to understand\nthe computation of models.\nThat relief for me is the\nultimate goal of this.\nAnd there's been some work,\nwe put out a few things.\nThere's a paper from Sam Marks\nthat does some stuff like this.\nAnd there's been some,\nI'd say some work around the edges here.\nBut I think there's a lot more to do,\nand I think that will be\na very exciting thing.\nThat's related to a challenge\nwe call interference weights,\nwhere due to superposition,\nif you just sort of naively look at\nwhere their features\nare connected together,\nthere may be some weights that\nsort of don't exist in the upstairs model,\nbut are just sort of\nartifacts of superposition.\nSo that's a sort of technical\nchallenge related to that.\nI think another exciting\ndirection is just, you know,\nyou might think of sparse auto-encoders\nas being kind of like a telescope.\nThey allow us to, you know,\nlook out and see all these\nfeatures that are out there.\nAnd you know, as we build better\nand better sparse auto-encoders,\nget better and better\nat dictionary learning,\nwe see more and more stars,\nand you know, we zoom in on\nsmaller and smaller stars.\nBut there's kind of a lot of evidence\nthat we're only still seeing\na very small fraction of the stars.\nThere's a lot of matter in our, you know,\nneural network universe\nthat we can't observe yet.\nAnd it may be that we'll never be able\nto have fine enough\ninstruments to observe it,\nand maybe some of it just isn't possible,\nisn't computationally\ntractable to observe it.\nSo it's sort of a kind of dark matter,\nnot in maybe the sense\nof modern astronomy,\nbut of early astronomy when we didn't know\nwhat this unexplained matter is.\nAnd so I think a lot\nabout that dark matter\nand whether we'll ever observe it,\nand what that means for safety\nif we can't observe it,\nif there's, you know, if\nsome significant fraction\nof neural networks are\nnot accessible to us.\nAnother question that\nI think a lot about is,\nat the end of the day, you know,\nmechanistic interpretability is this\nvery microscopic approach\nto interpretability.\nIt's trying to understand things\nin a very fine-grained way.\nBut a lot of the questions we care about\nare very macroscopic.\nYou know, we care about these questions\nabout neural network behavior,\nand I think that's the thing\nthat I care most about,\nbut there's lots of other\nsort of larger scale questions\nyou might care about.\nAnd somehow, you know,\nthe nice thing about about having\na very microscopic approach\nis it's maybe easier to ask,\nyou know, is this true?\nBut the downside is it's much further\nfrom the things we care about,\nand so we now have this ladder to climb.\nAnd I think there's a question of,\nwill we be able to find,\nare there sort of larger\nscale abstractions\nthat we can use to\nunderstand neural networks?\nCan we get up from this\nvery microscopic approach?\n- Yeah, you've written about\nthis kind of organs question.\n- Yeah, exactly.\n- So if we think\nof interpretability as a kind\nof anatomy of neural networks,\nmost of the circuits threads\ninvolve studying tiny little veins,\nlooking at the small scale\nat individual neurons\nand how they connect.\nHowever, there are many natural questions\nthat the small scale\napproach doesn't address.\nIn contrast, the most\nprominent abstractions\nin biological anatomy involve\nlarger scale structures,\nlike individual organs, like the heart,\nor entire organ systems,\nlike the respiratory system.\nAnd so we wonder, is there a\nrespiratory system or heart\nor brain region of an\nartificial neural network?\n- Yeah, exactly.\nAnd I mean, like if you\nthink about science, right?\nA lot of scientific fields have, you know,\ninvestigate things at many\nlevels of abstractions.", "mimetype": "text/plain", "start_char_idx": 314216, "end_char_idx": 318319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35d51ac0-a6b6-4883-b08f-ee263f10c2bb": {"__data__": {"id_": "35d51ac0-a6b6-4883-b08f-ee263f10c2bb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21485ede-86e6-4e3b-bb7c-c4b0017c217d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9fb2f2bed94907f1d3a1bd5dc649373f6ac9e786dc4df0bbfbf1bcaf144e6f8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb", "node_type": "1", "metadata": {}, "hash": "04e85f22ca09756857bcd2dd1b62de7de4b5a6b815cbd64c2b36bc95d94c898a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah, you've written about\nthis kind of organs question.\n- Yeah, exactly.\n- So if we think\nof interpretability as a kind\nof anatomy of neural networks,\nmost of the circuits threads\ninvolve studying tiny little veins,\nlooking at the small scale\nat individual neurons\nand how they connect.\nHowever, there are many natural questions\nthat the small scale\napproach doesn't address.\nIn contrast, the most\nprominent abstractions\nin biological anatomy involve\nlarger scale structures,\nlike individual organs, like the heart,\nor entire organ systems,\nlike the respiratory system.\nAnd so we wonder, is there a\nrespiratory system or heart\nor brain region of an\nartificial neural network?\n- Yeah, exactly.\nAnd I mean, like if you\nthink about science, right?\nA lot of scientific fields have, you know,\ninvestigate things at many\nlevels of abstractions.\nIn biology you have like, you know,\nmolecular biology studying, you know,\nproteins and molecules and so on.\nAnd they have cellular biology,\nand then you have\nhistology studying tissues,\nand you have anatomy, and\nthen you have zoology,\nand then you have ecology.\nAnd so you have many,\nmany levels of abstraction\nor you know, physics,\nmaybe the physics of individual particles,\nand then, you know, statistical physics\ngives you thermodynamics\nand things like this.\nAnd so you often have different\nlevels of abstraction.\nAnd I think that right\nnow we have, you know,\nmechanistic interpretability\nif it succeeds\nis sort of like a microbiology\nof neural networks,\nbut we want something more like anatomy.\nAnd so, and you know, a\nquestion you might ask is,\nwhy can't you just go there directly?\nAnd I think the answer is superposition,\nat least in significant part.\nIt's that it's actually very hard to see\nthis macroscopic structure\nwithout first sort of breaking down\nthe microscopic structure\nin the right way,\nand then studying how\nit connects together.\nBut I'm hopeful that there\nis gonna be something\nmuch larger than features and circuits,\nand that we're gonna\nbe able to have a story\nthat involves much bigger things,\nand then you can sort of study\nin detail the parts you care about.\n- I suppose to neurobiology,\nlike a psychologist\nor a psychiatrist of a neural network.\n- And I think that the beautiful thing\nwould be if we could go,\nand rather than having disparate fields\nfor those two things,\nif you could build a bridge between them-\n- Oh, right.\n- Such that you could go\nand have all of your\nhigher level abstractions\nbe grounded very firmly\nin this very solid,\nyou know, more rigorous,\nideally, foundation.\n- What do you think is the difference\nbetween the human brain, the\nbiological neural network\nand the artificial neural network?\n- Well, the neuroscientists\nhave a much harder job than us.\nYou know, sometimes I just\nlike count my blessings\nby how much easier my job\nis than the neuroscientists, right?\nSo I have, we can record\nfrom all the neurons.\nWe can do that on\narbitrary amounts of data.\nThe neurons don't change\nwhile you're doing that, by the way.\nYou can go and ablate neurons,\nyou can edit the connections and so on,\nand then you can undo those changes.\nThat's pretty great.\nYou can force, you can\nintervene on any neuron\nand force it active and see what happens.\nYou know, which neurons are\nconnected to everything, right?\nNeuroscientists wanna get the connectome,\nwe have the connectome and we have it\nfor like much bigger than like C. elegans.\nAnd then not only do\nwe have the connectome,\nwe know what, you know, which neurons\nexcite or inhibit each other, right.\nSo we have, it's not just that we know\nthat like the binary\nmass, we know the weights.\nWe can take gradients,\nwe know computationally\nwhat each neuron does.\nSo I don't know the list goes on and on.\nWe just have so many advantages\nover neuroscientists.\nAnd then despite having\nall those advantages,\nit's really hard.\nAnd so one thing I do\nsometimes think is like,\ngosh, like if it's this hard for us,\nit seems impossible under the constraints\nof neuroscience or, you\nknow, near impossible.\nI don't know, maybe part of me is like,\nI've got a few neuroscientists on my team.", "mimetype": "text/plain", "start_char_idx": 317478, "end_char_idx": 321585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb": {"__data__": {"id_": "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35d51ac0-a6b6-4883-b08f-ee263f10c2bb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bdea65e1fd5651286b28baa27e18df05091d8fd16f44cdc829af9c61ebeaa0dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "675502e4-b6c1-47fe-b1e7-3fa9c0ca544a", "node_type": "1", "metadata": {}, "hash": "484f406a038fa4f9e8686726dd46dce7b201cfd617500363a7f239c59730097d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then not only do\nwe have the connectome,\nwe know what, you know, which neurons\nexcite or inhibit each other, right.\nSo we have, it's not just that we know\nthat like the binary\nmass, we know the weights.\nWe can take gradients,\nwe know computationally\nwhat each neuron does.\nSo I don't know the list goes on and on.\nWe just have so many advantages\nover neuroscientists.\nAnd then despite having\nall those advantages,\nit's really hard.\nAnd so one thing I do\nsometimes think is like,\ngosh, like if it's this hard for us,\nit seems impossible under the constraints\nof neuroscience or, you\nknow, near impossible.\nI don't know, maybe part of me is like,\nI've got a few neuroscientists on my team.\nMaybe I'm sort sort of like, ah, you know,\nmaybe the neuroscientists,\nmaybe some of them\nwould like to have an easier\nproblem that's still very hard\nand they could come and\nwork on neural networks.\nAnd then after we figure out things\nin sort of the easy little pond of trying\nto understand neural networks,\nwhich is still very hard,\nthen we could go back to\nbiological neuroscience.\n- I love what you've\nwritten about the goal\nof mech interp research as two goals,\nsafety and beauty.\nSo can you talk about the\nbeauty side of things?\n- Yeah, so, you know,\nthere's this funny thing\nwhere I think some people want,\nsome people are kind of\ndisappointed by neural networks,\nI think, where they're like,\nah, you know, neural networks,\nit's these just these simple rules,\nand then you just like\ndo a bunch of engineering\nto scale it up and it works really well.\nAnd like, where's the like complex ideas?\nYou know, this isn't like a very nice,\nbeautiful scientific result.\nAnd I sometimes think\nwhen people say that,\nI picture them being like, you\nknow, evolution is so boring.\nIt's just a bunch of simple rules\nand you run evolution for a\nlong time and you get biology.\nLike what a sucky, you know,\nway for biology to have turned out.\nWhere's the complex rules?\nBut the beauty is that the\nsimplicity generates complexity.\nYou know, biology has these simple rules\nand it gives rise to,\nyou know, all the life\nand ecosystems that we see around us,\nall the beauty of nature, that\nall just comes from evolution\nand from something very simple evolution.\nAnd similarly, I think\nthat neural networks build,\nyou know, create enormous complexity\nand beauty inside and\nstructure inside themselves\nthat people generally don't look at\nand don't try to understand\nbecause it's hard to understand.\nBut I think that there is\nan incredibly rich structure\nto be discovered inside neural networks,\na lot of very deep beauty\nif we're just willing to take the time\nto go and see it and understand it.\n- Yeah, I love mech interp,\nthe feeling like we are understanding\nor getting glimpses of understanding\nthe magic that's going on\ninside is really wonderful.\n- It feels to me like one of the questions\nthat's just calling out to be asked,\nand I'm sort of, I mean, a lot of people\nare thinking about this,\nbut I'm often surprised\nthat not more are is, how is it that\nwe don't know how to\ncreate computer systems\nthat can do these things,\nand yet we have these amazing systems\nthat we don't know how to\ndirectly create computer programs\nthat can do these things,\nbut these neural networks can\ndo all these amazing things?\nAnd it just feels like that\nis obviously the question\nthat sort of is calling out\nto be answered if you are,\nif you have any degree of curiosity.\nIt's like how is it that\nhumanity now has these artifacts\nthat can do these things\nthat we don't know how to do?\n- Yeah, I love the image\nof the circuits reaching towards the light\nof the objective function.\n- Yeah, it's just, it's this organic thing\nthat we've grown and we have\nno idea what we've grown.\nWell, thank you for working on safety\nand thank you for appreciating\nthe beauty of the things you discover.\nAnd thank you for talking today, Chris.\nThis was wonderful.\n- Yeah.\nThank you for taking the\ntime to chat as well.\n- Thanks for listening to this\nconversation with Chris Olah,\nand before that with Dario\nAmodei and Amanda Askell.", "mimetype": "text/plain", "start_char_idx": 320894, "end_char_idx": 324966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "675502e4-b6c1-47fe-b1e7-3fa9c0ca544a": {"__data__": {"id_": "675502e4-b6c1-47fe-b1e7-3fa9c0ca544a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c576fc2c-b1db-4abd-83dc-d89ed3c81b11", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3ef72b2d4fb6f0fdd422cc2b70c01838fc8feb1ab77069166c8874a8f187a8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cd2d3a6-c988-4a14-b21c-7efa19ffd4eb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_name": "[English] Dario Amodei_ Anthropic CEO on Claude, AGI & the Future of AI & Humanity _ Lex Fridman Podcast #452 [DownSub.com].txt", "file_type": "text/plain", "file_size": 325253, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "81b68b872f850c1b03e5b90d44c62d48e63a00dcc4aef3cc798aa80d923e229b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And it just feels like that\nis obviously the question\nthat sort of is calling out\nto be answered if you are,\nif you have any degree of curiosity.\nIt's like how is it that\nhumanity now has these artifacts\nthat can do these things\nthat we don't know how to do?\n- Yeah, I love the image\nof the circuits reaching towards the light\nof the objective function.\n- Yeah, it's just, it's this organic thing\nthat we've grown and we have\nno idea what we've grown.\nWell, thank you for working on safety\nand thank you for appreciating\nthe beauty of the things you discover.\nAnd thank you for talking today, Chris.\nThis was wonderful.\n- Yeah.\nThank you for taking the\ntime to chat as well.\n- Thanks for listening to this\nconversation with Chris Olah,\nand before that with Dario\nAmodei and Amanda Askell.\nTo support this podcast,\nplease check out our\nsponsors in the description.\nAnd now let me leave you with\nsome words from Alan Watts.\n\"The only way to make sense out of change\nis to plunge into it, move with it,\nand join the dance.\"\nThank you for listening and\nhope to see you next time.", "mimetype": "text/plain", "start_char_idx": 324178, "end_char_idx": 325253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "458e5211-267a-497e-8af1-a24777294f57": {"__data__": {"id_": "458e5211-267a-497e-8af1-a24777294f57", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27eefb48-d4ff-4710-bc06-7c60ca746d2b", "node_type": "1", "metadata": {}, "hash": "e3b9e481853f172d0490bf6c2a0287a7f76061ff6aac95539d33b61c3291c648", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The following is a conversation\nwith Dylan Patel and Nathan Lambert.\nDylan runs SemiAnalysis,\na well-respected research\nand analysis company that\nspecializes in semiconductors,\nGPUs, CPUs, and AI hardware in general.\nNathan is a research scientist\nat the Allen Institute for AI\nand is the author\nof the amazing blog on\nAI called Interconnects.\nThey are both highly respected, read,\nand listened to by the\nexperts, researchers,\nand engineers in the field of AI.\nAnd personally, I'm just\na fan of the two of them.\nSo, I used the DeepSeek moment\nthat shook the AI world a bit\nas an opportunity to sit down\nwith them and lay it all out.\nFrom DeepSeek, OpenAI,\nGoogle xAI, Meta, Anthropic,\nto Nvidia and DSMC, and to\nUS, China, Taiwan relations,\nand everything else\nthat is happening at\nthe cutting edge of AI.\nThis conversation is a deep dive\ninto many critical aspects\nof the AI industry.\nWhile it does get super\ntechnical, we try to make sure\nthat it's still accessible to\nfolks outside of the AI field\nby defining terms, stating\nimportant concepts explicitly,\nspelling out acronyms, and in\ngeneral, always moving across\nthe several layers of\nabstraction and levels of detail.\nThere is a lot of hype\nin the media about what AI is and isn't.\nThe purpose of this podcast\nin part is to cut through the hype,\nthrough the bullshit,\nand the low resolution analysis,\nand to discuss in detail how stuff works\nand what the implications are.\nLet me also, if I may comment\non the new OpenAI o3-mini reasoning model.\nThe release of which we were anticipating\nduring the conversation,\nand it did indeed come out right after.\nIts capabilities and costs\nare on par with our\nexpectations as we stated.\nOpenAI o3-mini is indeed a great model,\nbut it should be stated that DeeSeek-R1\nhas similar performance on\nbenchmarks is still cheaper\nand it reveals its chain\nof thought reasoning,\nwhich o3-mini does not.\nIt only shows a summary of the reasoning.\nPlus, R1 is open-weight\nand o3-mini is not.\nBy the way, I got a chance\nto play with o3-mini.\nAnd anecdotal vibe check\nwise, I felt that o3-mini,\nspecifically o3-mini-high\nis better than R1.\nStill for me personally,\nI find that Claude Sonnet 3.5\nis the best model for programming,\nexcept for tricky cases,\nwhere I will use o1 Pro to brainstorm.\nEither way, many more\nbetter AI models will come,\nincluding reasoning models\nboth from American and Chinese companies.\nThey'll continue to shift the cost curve.\nBut the quote, DeepSeek\nmoment is indeed real.\nI think it will still be\nremembered five years from now\nas a pivotal event in tech history,\ndue in part to the\ngeopolitical implications,\nbut for other reasons to,\nas we discuss in detail\nfrom many perspectives\nin this conversation.\nThis is the \"Lex Fridman Podcast\".\nTo support it,\nplease check out our\nsponsors in the description.\nAnd now, dear friends,\nhere's Dylan Patel and Nathan Lambert.\nA lot of people are curious\nto understand China's DeepSeek AI models,\nso let's lay it out.\nNathan, can you describe what DeepSeek-V3\nand DeepSeek-R1 are, how they\nwork, how they're trained?\nLet's look at the big picture,\nand then we'll zoom in on the details.\n- Yeah, so DeepSeek-V3 is\na new mixture of experts,\ntransformer language model from DeepSeek\nwho is based in China.\nThey have some new specifics\nin the model that we'll get into.\nLargely this is a open-weight model\nand it's a instruction model\nlike what you would use in ChatGPT.\nThey also release what\nis called the base model,\nwhich is before these\ntechniques of post-training.\nMost people use instruction models today\nand those are what's served\nin all sorts of applications.\nThis was released on, I believe\nDecember 26th or that week.\nAnd then, weeks later,\non January 20th, DeepSeek\nreleased DeepSeek-R1,\nwhich is a reasoning model,\nwhich really accelerated\na lot of this discussion.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27eefb48-d4ff-4710-bc06-7c60ca746d2b": {"__data__": {"id_": "27eefb48-d4ff-4710-bc06-7c60ca746d2b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "458e5211-267a-497e-8af1-a24777294f57", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c1439d02b3f2f9d5642104fc9259ea56229b7836d72b541459573dbcdfc90792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3fb3cd1-8599-4725-9e29-3c91c730f4fa", "node_type": "1", "metadata": {}, "hash": "1784f8798033a2ff989804574687bdacc836dc002f1596aafe2ced537711b5f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let's look at the big picture,\nand then we'll zoom in on the details.\n- Yeah, so DeepSeek-V3 is\na new mixture of experts,\ntransformer language model from DeepSeek\nwho is based in China.\nThey have some new specifics\nin the model that we'll get into.\nLargely this is a open-weight model\nand it's a instruction model\nlike what you would use in ChatGPT.\nThey also release what\nis called the base model,\nwhich is before these\ntechniques of post-training.\nMost people use instruction models today\nand those are what's served\nin all sorts of applications.\nThis was released on, I believe\nDecember 26th or that week.\nAnd then, weeks later,\non January 20th, DeepSeek\nreleased DeepSeek-R1,\nwhich is a reasoning model,\nwhich really accelerated\na lot of this discussion.\nThis reasoning model, it has a lot\nof overlapping training\nsteps to DeepSeek-V3,\nand it's confusing that\nyou have a base model\ncalled V3 that you do something\nto, to get a chat model,\nand then you do some different things\nto get a reasoning model.\nI think a lot of the AI industry\nis going through this challenge\nof communications right now\nwhere OpenAI makes fun of\ntheir own naming schemes.\nThey have GPT-4o, they have OpenAI o1.\nAnd there's a lot of types of models.\nSo, we're gonna break down\nwhat each of them are.\nThere's a lot of technical\nspecifics on training,\nand go through 'em high level to specific,\nand go through each of them.\n- There's so many places we can go here,\nbut maybe let's go to open-weights first.\nWhat does it mean for\nmodel to be open-weights\nand what are the different flavors\nof open source in general?\n- Yeah, so this discussion\nhas been going on for a long time in AI.\nIt became more important since ChatGPT\nor more focal since\nChatGPT at the end of 2022.\nOpen-weights is the accepted\nterm for when model weights\nof a language model are available\non the internet for people to download,\nthose weights can have different licenses,\nwhich is the effectively the terms\nby which you can use the model.\nThere are licenses that come from history\nand open source software.\nThere are licenses that are designed\nby companies specifically,\nall of Llama, DeepSeek, Qwen, Mistral,\nthese popular names in open-weight models,\nhave some of their own licenses.\nIt's complicated,\n'cause not all the same\nmodels have the same terms.\nThe big debate is on what\nmakes a model open-weight.\nIt's like why are we saying this term?\nIt's a mouthful.\nIt sounds close to open\nsource but it's not the same.\nThere's still a lot of\ndebate on the definition\nand soul of open source AI.\nOpen source software\nhas a rich history on freedom to modify,\nfreedom to take on your own,\nfreedom for many restrictions\non how you would use the software,\nand what that means for\nAI is still being defined.\nSo, for what I do, I work at\nthe Allen Institute for AI.\nWe're a nonprofit.\nWe want to make AI open for everybody\nand we try to lead on what we\nthink is truly open source.\nThere's not full agreement\nin the community,\nbut for us, that means\nreleasing the training data,\nreleasing the training code,\nand then also having\nopen-weights like this.\nAnd we'll get into the\ndetails of the models.\nAnd again and again,\nas we try to get deeper\ninto how the models were trained,\nwe will say things like\nthe data processing,\ndata filtering, data quality\nis the number one determinant\nof the model quality.\nAnd then, a lot of the training\ncode is the determinant\non how long it takes to train\nand how fast your experimentation is.\nSo, without fully open source models\nwhere you have access to this data,\nit is hard to know or\nit's harder to replicate.\nSo, we'll get into cost\nnumbers for DeepSeek-V3\non mostly GPU hours\nand how much you could pay\nto rent those yourselves.\nBut without the data,\nthe replication cost is\ngoing to be far, far higher.\nAnd same goes for the code.\n- We should also say\nthat this is probably one\nof the more open models\nout of the frontier models.\n- Yes.", "mimetype": "text/plain", "start_char_idx": 3071, "end_char_idx": 6991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3fb3cd1-8599-4725-9e29-3c91c730f4fa": {"__data__": {"id_": "e3fb3cd1-8599-4725-9e29-3c91c730f4fa", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27eefb48-d4ff-4710-bc06-7c60ca746d2b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8847193bfd66af43de18f7fb86ef5f94da33623e744fe3f15aa0e00abccdb9d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0", "node_type": "1", "metadata": {}, "hash": "308f24c48660117f554b995a5d898e320ec0ec9d71584da282748b3e8b62e7dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we'll get into the\ndetails of the models.\nAnd again and again,\nas we try to get deeper\ninto how the models were trained,\nwe will say things like\nthe data processing,\ndata filtering, data quality\nis the number one determinant\nof the model quality.\nAnd then, a lot of the training\ncode is the determinant\non how long it takes to train\nand how fast your experimentation is.\nSo, without fully open source models\nwhere you have access to this data,\nit is hard to know or\nit's harder to replicate.\nSo, we'll get into cost\nnumbers for DeepSeek-V3\non mostly GPU hours\nand how much you could pay\nto rent those yourselves.\nBut without the data,\nthe replication cost is\ngoing to be far, far higher.\nAnd same goes for the code.\n- We should also say\nthat this is probably one\nof the more open models\nout of the frontier models.\n- Yes.\n- So, in this full spectrum\nwhere probably the fullest\nopen source, like you said,\nopen code, open data, open-weights.\nThis is not open code, this\nis probably not open data,\nand this is open-weights,\nand the licensing is\nMIT license, or it's...\nI mean, there's some nuance\nin the different models,\nbut it's towards the free,\nin terms of the open source movement,\nthese are the good guys.\n- Yeah.\nDeepSeek is doing fantastic work\nfor disseminating understanding of AI.\nTheir papers are extremely\ndetailed in what they do.\nAnd for other teams around the world,\nthey're very actionable in terms\nof improving your own training techniques.\nAnd we'll talk about licenses more.\nThe DeepSeek-R1 model has\na very permissive license.\nIt's called the MIT license.\nThat effectively means there's\nno downstream restrictions\non commercial use, there's\nno use case restrictions.\nYou can use the outputs\nfrom the models to create synthetic data.\nAnd this is all fantastic.\nI think the closest peer\nis something like Llama\nwhere you have the weights and\nyou have a technical report.\nAnd the technical report\nis very good for Llama.\nOne of the most read PDFs\nof the year last year\nis the Llama 3 paper,\nbut in some ways, it's\nslightly less actionable.\nIt has less details on\nthe training specifics,\nI think less plots, and so on.\nAnd the Llama 3 license is\nmore restrictive than MIT.\nAnd then, between the\nDeepSeek custom license\nand the Llama license,\nwe could get into this whole rabbit hole.\nI think we'll make sure we want\nto go down the license rabbit\nhole before we do specifics.\n- Yeah, and so it should\nbe stated that one\nof the implications that\nDeepSeek puts pressure on Llama\nand everybody else on OpenAI\nto push towards open source,\nand that's the other side of open source\nthat you mentioned\nis how much is published\nin detail about it.\nSo, how open are you with\nthe insights behind the code?\nSo, how good is the technical reports?\nAre they hand wavy or is\nthere actual details in there?\nAnd that's one of the things\nthat DeepSeek did well is they\npublish a lot of the details.\n- Yeah, especially in the DeepSeek-V3,\nwhich is their pre-training\npaper, they were very clear\nthat they are doing interventions\non the technical stack that\ngo at many different levels.\nFor example, to get\nhighly efficient training,\nthey're making modifications at\nor below the CUDA layer for Nvidia chips.\nI have never worked there myself.\nAnd there are a few people in the world\nthat do that very well and\nsome of them are at DeepSeek.\nAnd these types of people are at DeepSeek\nand leading American Frontier Labs,\nbut there are not many places.\n- To help people understand\nthe other implication of open-weights,\njust there's a topic we\nreturn to often here.\nSo, there's a fear that China, the nation,\nmight have interest in\nstealing American data,\nviolating privacy of American citizens.\nWhat can we say about open-weights\nto help us understand what\nthe weights are able to do\n- Yeah.\n- in terms\nof stealing people's data?\n- Yeah, so these weights\nthat you can download from Hugging Face\nor other platforms are very\nbig matrices of numbers.\nYou can download them to a computer\nin your own house that has no internet\nand you can run this model,\nand you're totally in\ncontrol of your data.", "mimetype": "text/plain", "start_char_idx": 6166, "end_char_idx": 10254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0": {"__data__": {"id_": "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3fb3cd1-8599-4725-9e29-3c91c730f4fa", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "22f020883b329e6f65f140c3ca720eb7b0cac214c04691450250b3f92f99180d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d8c9c3f-1082-499b-89dd-63ee849634e8", "node_type": "1", "metadata": {}, "hash": "d43a34d43d188cd1dfd35681834bec7a5eb5ed9820b31323865bb578db95859c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And there are a few people in the world\nthat do that very well and\nsome of them are at DeepSeek.\nAnd these types of people are at DeepSeek\nand leading American Frontier Labs,\nbut there are not many places.\n- To help people understand\nthe other implication of open-weights,\njust there's a topic we\nreturn to often here.\nSo, there's a fear that China, the nation,\nmight have interest in\nstealing American data,\nviolating privacy of American citizens.\nWhat can we say about open-weights\nto help us understand what\nthe weights are able to do\n- Yeah.\n- in terms\nof stealing people's data?\n- Yeah, so these weights\nthat you can download from Hugging Face\nor other platforms are very\nbig matrices of numbers.\nYou can download them to a computer\nin your own house that has no internet\nand you can run this model,\nand you're totally in\ncontrol of your data.\nThat is something that is different\nthan how a lot of language model usage\nis actually done today,\nwhich is mostly through APIs\nwhere you send your prompt to\nGPUs run by certain companies,\nand these companies will\nhave different distributions\nand policies on how your data is stored,\nif it is used to train future models,\nwhere it is stored, if it\nis encrypted, and so on.\nSo, the open-weights\nare you have your fate\nof data in your own hands\nand that is something\nthat is deeply connected\nto the soul of open source.\n- So, it's not the model\nthat steals your data,\nit's whoever's hosting the model,\nwhich could be China if\nyou're using the DeepSeek app,\nor it could be Perplexity.\nYou're trusting them with your data.\nOr OpenAI, you're trusting\nthem with your data.\nAnd some of these are American companies,\nsome of of these are Chinese companies,\nbut the model itself is\nnot doing the stealing,\nit's the host.\nAll right. So, back to the basics.\nWhat's the difference between\nDeepSeek-V3 and DeepSeek-R1?\nCan we try to lay out\nthe confusion potential?\n- Yes.\nSo, for one, I have very understanding\nof many people being confused\nby these two model names.\nSo, I would say the best\nway to think about this\nis that when training a language model,\nyou have what is called pre-training,\nwhich is when you're\npredicting the large amounts\nof mostly internet text.\nYou're trying to predict the next-token.\nAnd what to know about\nthese new DeepSeek models\nis that they do this internet\nlarge-scale pre-training once\nto get what is called DeepSeek-V3 base.\nThis is a base model.\nIt's just going to finish\nyour sentences for you.\nIt's going to be harder\nto work with than ChatGPT.\nAnd then, what DeepSeek did\nis they've done two different\npost-training regimes\nto make the models have\nspecific desirable behaviors.\nSo, what is the more normal model\nin terms of the last few years of AI\nand instruct model, a chat model,\na, quote, unquote, \"aligned\nmodel\", a helpful model,\nthere are many ways to describe this,\nis more standard post-training.\nSo, this is things like\ninstruction tuning,\nreinforcement learning\nfrom human feedback.\nWe'll get into some of these words.\nAnd this is what they did to\ncreate the DeepSeek-V3 model.\nThis was the first model to be released\nand it is very highly performant,\ncompetitive with GPT-4, Llama 405b, so on.\nAnd then, when this release was happening,\nwe don't know their exact timeline,\nor soon after they were\nfinishing the training\nof a different training process\nfrom the same next-token\nprediction-based model\nthat I talked about,\nwhich is when this new reasoning training\nthat people have heard about comes in,\nin order to create the model\nthat is called DeepSeek-R1.\nThe R through this conversation\nis good for grounding for reasoning.\nAnd the name is also\nsimilar to OpenAI's o1,\nwhich is the other reasoning model\nthat people have heard about.\nAnd we'll have to break down the training\nfor R1 in more detail,\nbecause for one, we have\na paper detailing it,\nbut also, it is a far\nnewer set of techniques\nfor the AI community.\nSo, it is a much more rapidly\nevolving area of research.\n- Maybe we should also\nsay the big two categories\nof training of pre-training\nand post-training.", "mimetype": "text/plain", "start_char_idx": 9406, "end_char_idx": 13464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d8c9c3f-1082-499b-89dd-63ee849634e8": {"__data__": {"id_": "8d8c9c3f-1082-499b-89dd-63ee849634e8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b4c683d-6d1d-4fb1-801f-0ac3eee0bbb0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a8599ba28b645bacf7e41a20d122edcb34a0173036931751f6b61b5a80f69259", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11c64b71-68af-4612-9f0f-28f40a4c13d2", "node_type": "1", "metadata": {}, "hash": "edcb08230ad6e33aca6f34d60ec96017e1503c78804ac26bd6e7059e64bf7415", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The R through this conversation\nis good for grounding for reasoning.\nAnd the name is also\nsimilar to OpenAI's o1,\nwhich is the other reasoning model\nthat people have heard about.\nAnd we'll have to break down the training\nfor R1 in more detail,\nbecause for one, we have\na paper detailing it,\nbut also, it is a far\nnewer set of techniques\nfor the AI community.\nSo, it is a much more rapidly\nevolving area of research.\n- Maybe we should also\nsay the big two categories\nof training of pre-training\nand post-training.\nThese umbrella terms that people use.\nSo, what is pre-training\nand what is post-training,\nand what are the different\nflavors of things\nunderneath post-training umbrella?\n- Yeah, so pre-training, I'm\nusing some of the same words\nto really get the message across,\nis you're doing what is called\nautoregressive prediction\nto predict the next-token\nin a series of documents.\nThis is done over standard\npractice is trillions of tokens.\nSo, this is a ton of data\nthat is mostly scraped from the web.\nAnd some of DeepSeek's earlier papers,\nthey talk about their training data\nbeing distilled for math, I\nshouldn't use this word yet,\nbut taken from Common Crawl,\nand that's a public access\nthat anyone listening to this\ncould go download data from\nthe Common Crawl website.\nThis is a crawler that\nis maintained publicly.\nYes, other tech companies\neventually shift to their own crawler\nand DeepSeek likely has done this as well,\nas most Frontier Labs do.\nBut this sort of data is something\nthat people can get started with\nand you're just predicting\ntext in a series of documents.\nThis can be scaled to be very efficient\nand there's a lot of numbers\nthat are thrown around in AI training,\nlike how many floating point\noperations or FLOPS are used.\nAnd then, you can also look\nat how many hours of\nthese GPUs that are used.\nAnd it's largely one loss function\ntaken to a very large amount\n(chuckles) of compute usage.\nYou set up really efficient systems.\nAnd then, at the end of that,\nyou have this base model.\nAnd pre-training is\nwhere there is a lot more\nof complexity in terms\nof how the process is\nemerging or evolving,\nand the different types of\ntraining losses that you'll use.\nI think this is a lot of techniques\ngrounded in the natural\nlanguage processing literature.\nThe oldest technique\nwhich is still used today\nis something called instruction tuning,\nor also known as supervised fine-tuning.\nThese acronyms will be IFT or SFT\nthat people really go back\nand forth throughout them,\nand I'll probably do the same,\nwhich is where you add this\nformatting to the model,\nwhere it knows to take a question\nthat is like explain the history\nof the Roman Empire to me.\nOr something, a sort of question\nyou'll see on Reddit or Stack Overflow,\nand then the model will respond\nin a information-dense\nbut presentable manner.\nThe core of that formatting\nis in this instruction tuning phase.\nAnd then, there's two other categories\nof loss functions that\nare being used today.\nOne I'll classify as\npreference fine-tuning.\nPreference fine-tuning\nis a generalized term\nfor what came out of\nreinforcement learning\nfrom human feedback, which is RLHF.\nThis reinforcement learning\nfrom human feedback\nis credited as the technique\nthat helped ChatGPT breakthrough.\nIt is a technique to make the responses\nthat are nicely formatted,\nlike these Reddit answers,\nmore in tune with what a\nhuman would like to read.\nThis is done by collecting\npairwise preferences\nfrom actual humans out\nin the world to start.\nAnd now, AIs are also labeling this data\nand we'll get into those trade-offs.\nAnd you have this kind of\ncontrastive loss function\nbetween a good answer and a bad answer.\nAnd the model learns to\npick up these trends.\nThere's different implementation ways.\nYou have things called reward models.\nYou could have direct\nalignment algorithms.\nThere's a lot of really\nspecific things you can do.\nBut all of this is about\nfine-tuning to human preferences.\nAnd the final stage is much newer\nand will link to what is done in R1.\nAnd these reasoning models\nis I think OpenAI's name for this.\nThey had this new API in the fall,\nwhich they called the\nreinforcement fine-tuning API.", "mimetype": "text/plain", "start_char_idx": 12952, "end_char_idx": 17113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11c64b71-68af-4612-9f0f-28f40a4c13d2": {"__data__": {"id_": "11c64b71-68af-4612-9f0f-28f40a4c13d2", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d8c9c3f-1082-499b-89dd-63ee849634e8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b39abdc067348772b49eb09bd4dd9e27ef800409ff20d01cfb693327e11fe675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7eede3b5-5fe4-42df-aba8-a0657a2a0f26", "node_type": "1", "metadata": {}, "hash": "a50fe4697e83f92cda4ed2524bf9666ca727fc0dc905c14cca3f9b7b28fa31b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is done by collecting\npairwise preferences\nfrom actual humans out\nin the world to start.\nAnd now, AIs are also labeling this data\nand we'll get into those trade-offs.\nAnd you have this kind of\ncontrastive loss function\nbetween a good answer and a bad answer.\nAnd the model learns to\npick up these trends.\nThere's different implementation ways.\nYou have things called reward models.\nYou could have direct\nalignment algorithms.\nThere's a lot of really\nspecific things you can do.\nBut all of this is about\nfine-tuning to human preferences.\nAnd the final stage is much newer\nand will link to what is done in R1.\nAnd these reasoning models\nis I think OpenAI's name for this.\nThey had this new API in the fall,\nwhich they called the\nreinforcement fine-tuning API.\nThis is the idea that\nyou use the techniques\nof reinforcement learning, which\nis a whole framework of AI.\nThere's a deep literature here.\nTo summarize, it's often known\nas trial and error learning\nor the subfield of AI where you're trying\nto make sequential decisions\nin a certain potentially\nnoisy environment.\nThere's a lot of ways\nwe could go down that.\nBut fine-tuning language models\nwhere they can generate an answer.\nAnd then, you check to see\nif the answer matches the true solution.\nFor math or code,\nyou have an exactly\ncorrect answer for math.\nYou can have unit tests for code.\nAnd what we're doing\nis we are checking the\nlanguage model's work\nand we're giving it multiple opportunities\non the same questions\nto see if it is right.\nAnd if you keep doing\nthis, the models can learn\nto improve in verifiable\ndomains to a great extent.\nIt works really well.\nIt's a newer technique in\nthe academic literature.\nIt's been used at Frontier Labs in the US\nthat don't share every\ndetail for multiple years.\nSo, this is the idea of\nusing reinforcement learning\nwith language models, and\nit has been taking off,\nespecially in this DeepSeek moment.\n- And we should say that there's a lot\nof exciting stuff going on\nthe, again, across the stack.\nBut the post-training probably this year,\nthere's going to be a lot\nof interesting developments\nin the post-training.\nWe'll talk about it.\nI almost forgot to talk\nabout the difference\nbetween DeepSeek-V3 and R1\non the user experience side.\nSo, forget the technical\nstuff, forget all of that.\nJust people that don't\nknow anything about AI,\nthey show up like what's\nthe actual experience,\nwhat's the use case for each one\nwhen they actually type and talk to it?\n- Yeah.\n- What is each good at\nand that kind of thing.\n- So, let's start with DeepSeek-V3.\nAgain, it's more people would\nhave tried something like it.\nYou ask it a question,\nit'll start generating tokens very fast,\nand those tokens will look like\na very human legible answer.\nIt'll be some sort of markdown list.\nIt might have formatting\nto help you draw to the\ncore details in the answer.\nAnd it'll generate tens\nto hundreds of tokens.\nSay token is normally\na word for common words\nor a sub-word part in a longer word.\nAnd it'll look like a\nvery high quality Reddit\nor Stack Overflow answer.\nThese models are really getting good\nat doing these across a wide\nvariety of domains, I think.\nEven things that if you're an expert,\nthings that are close to\nthe fringe of knowledge,\nthey will still be\nfairly good at, I think.\nCutting edge AI topics\nthat I do research on,\nthese models are capable for study aide,\nand they're regularly updated.\nWhere this changes is\nwith the DeepSeek-R1,\nwhat is called these reasoning models,\nis when you see tokens coming\nfrom these models to start,\nit will be a large chain\nof thought process.\nWe'll get back to chain\nof thought in a second,\nwhich looks like a lot of tokens,\nwhere the model is explaining the problem.\nThe model will often\nbreak down the problem\nand be like, okay, they asked me for this,\nlet's break down the problem.\nI'm going to need to do this.\nAnd you'll see all of this\ngenerating from the model.\nIt'll come very fast in\nmost user experiences.\nThese APIs are very fast.\nSo, you'll see a lot of tokens,\na lot of words show up really fast.", "mimetype": "text/plain", "start_char_idx": 16351, "end_char_idx": 20408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7eede3b5-5fe4-42df-aba8-a0657a2a0f26": {"__data__": {"id_": "7eede3b5-5fe4-42df-aba8-a0657a2a0f26", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11c64b71-68af-4612-9f0f-28f40a4c13d2", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f11b84b0e394632d0150cfabbdc5ddc2f8895e9e305931913ee9b828eafa7dcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad6602f6-5660-4fed-b0dd-c185d66fd5d1", "node_type": "1", "metadata": {}, "hash": "c22571d7aedabc4887ea95d14ef213a3bce4dba27bfde1d74bc4166f89ae4689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cutting edge AI topics\nthat I do research on,\nthese models are capable for study aide,\nand they're regularly updated.\nWhere this changes is\nwith the DeepSeek-R1,\nwhat is called these reasoning models,\nis when you see tokens coming\nfrom these models to start,\nit will be a large chain\nof thought process.\nWe'll get back to chain\nof thought in a second,\nwhich looks like a lot of tokens,\nwhere the model is explaining the problem.\nThe model will often\nbreak down the problem\nand be like, okay, they asked me for this,\nlet's break down the problem.\nI'm going to need to do this.\nAnd you'll see all of this\ngenerating from the model.\nIt'll come very fast in\nmost user experiences.\nThese APIs are very fast.\nSo, you'll see a lot of tokens,\na lot of words show up really fast.\nIt'll keep flowing on the screen\nand this is all the reasoning process.\nAnd then, eventually, the model\nwill change its tone in R1\nand it'll write the answer,\nwhere it summarizes its reasoning process\nand writes a similar answer\nto the first types of model.\nBut in DeepSeek's case, which is part\nof why this was so popular\neven outside the AI community,\nis that you can see how the language model\nis breaking down problems.\nAnd then, you get this answer.\nOn a technical side,\nthey train the model to\ndo this specifically,\nwhere they have a section\nwhich is reasoning,\nand then it generates a special token,\nwhich is probably hidden from\nthe user most of the time,\nwhich says, okay, I'm starting the answer.\nSo, the model is trained\nto do this two-stage process on its own.\nIf you use a similar model in say, OpenAI,\nOpenAI's user interface\nis trying to summarize\nthis process for you nicely\nby showing the sections\nthat the model is doing,\nand it'll click through,\nit'll say breaking down the\nproblem, making X calculation,\ncleaning the result,\nand then the answer will come\nfor something like OpenAI.\n- Maybe it's useful here to go through\nlike an example of it,\nDeepSeek-R1 reasoning.\n- Yeah.\nSo, if you're looking at the screen here,\nwhat you'll see is a screenshot\nof the DeepSeek chat app.\nAnd at the top is Thought for 157 seconds\nwith the dropdown arrow.\nUnderneath that, if we were in\nan app that we were running,\nthe dropdown arrow would\nhave the reasoning.\n- So, in this case, the question,\nthe specific question which,\nI'm philosophically/pothead-inclined,\nso this is asking DeepSeek-R1\nfor one truly novel insight about humans.\nAnd it reveals the reasoning.\nAnd basically, the truly novel aspect\nis what's pushing the\nreasoning to constantly,\nthe model asking itself,\nis this truly novel?\nSo, it's actually challenging\nitself to be more novel,\nmore counterintuitive,\nless cringe, I suppose.\nSo, some of the reasoning\nsays, this is just snapshots.\nAlternatively, humans\nhave a unique meta-emotion\nwhere they feel emotions\nabout their own emotions,\ne.g, feeling guilty about being angry.\nThis recursive emotional\nlayering creates complex\nmotivational drives that\ndon't exist in other animals.\nThe inside is that human\nemotions are nested.\nSo, it's like,\nit's reasoning through\nhow humans feel emotions,\nit's reasoning about meta-emotions.\n- It's gonna have pages\nand pages of this.\n- Yeah.\n- It's almost too much to actually read,\nbut it's nice to skim as it's coming.\n- It's a stream of con...\nIt's a James Joyce-like\nstream of consciousness.\nAnd then, it goes, wait,\nthe user wants something\nthat's not seen anywhere else.\nLet me dig deeper.\nAnd consider the human ability\nto hold contradictory\nbeliefs simultaneously.\nCognitive dissonance is known,\nbut perhaps the function\nis to allow flexible\nadaptation, so on and so forth.\nThat really captures\nthe public imagination,\nthat holy shit, this isn't...\nI mean, intelligence/almost\nlike an inkling of sentience,\nbecause you're thinking through,\nyou're self-reflecting,\nyou're deliberating.\nAnd the final result of\nthat after 157 seconds\nis humans instinctively\nconvert selfish desires\ninto cooperative systems\nby collectively pretending abstract rules,\nmoney, laws, rights, are real.", "mimetype": "text/plain", "start_char_idx": 19638, "end_char_idx": 23653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad6602f6-5660-4fed-b0dd-c185d66fd5d1": {"__data__": {"id_": "ad6602f6-5660-4fed-b0dd-c185d66fd5d1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7eede3b5-5fe4-42df-aba8-a0657a2a0f26", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0ef1184c9978a101b1636cd41c2e80302af856d13b560567405bc494ccbbf236", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c415fb83-9227-4fb1-a310-f1908885738f", "node_type": "1", "metadata": {}, "hash": "c63850c0315f02a610ed8455939772292ab2c9042273e3db14c02139ab083036", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\n- It's almost too much to actually read,\nbut it's nice to skim as it's coming.\n- It's a stream of con...\nIt's a James Joyce-like\nstream of consciousness.\nAnd then, it goes, wait,\nthe user wants something\nthat's not seen anywhere else.\nLet me dig deeper.\nAnd consider the human ability\nto hold contradictory\nbeliefs simultaneously.\nCognitive dissonance is known,\nbut perhaps the function\nis to allow flexible\nadaptation, so on and so forth.\nThat really captures\nthe public imagination,\nthat holy shit, this isn't...\nI mean, intelligence/almost\nlike an inkling of sentience,\nbecause you're thinking through,\nyou're self-reflecting,\nyou're deliberating.\nAnd the final result of\nthat after 157 seconds\nis humans instinctively\nconvert selfish desires\ninto cooperative systems\nby collectively pretending abstract rules,\nmoney, laws, rights, are real.\nThese shared hallucinations\nact as, quote, games,\nwhere competition is secretly redirected\nto benefit the group, turning\nconflict into society's fuel.\nPretty profound.\nI mean, you know?\nComment-\n- This is a potential\ndigression, but a lot of people have found\nthat these reasoning models\ncan sometimes produce\nmuch more eloquent text.\nThat is at least interesting\nexample, I think,\ndepending on how open-minded you are,\nyou find language models\ninteresting or not,\nand there's a spectrum there.\n- Well, it's some of the...\nWe'll talk about different\nbenchmarks, and so on.\nBut some is just a vibe.\nLike that in itself is a,\nlet's say, quote, fire tweet.\n- Yeah. (laughs)\n- If I'm trying\nto produce something, where\npeople are like, oh shit,\nokay, so that's chain of thought.\nWe'll probably return to it more.\nHow were they able to\nachieve such low cost\non the training and the inference?\nMaybe you could talk the training first.\n- Yeah, so there's two main techniques\nthat they implemented\nthat are probably the\nmajority of their efficiency.\nAnd then, there's a lot\nof implementation details\nthat maybe we'll gloss over\nor get into later that contribute to it.\nBut those two main things are,\none, is they went to a\nmixture of experts model,\nwhich we'll define in a second.\nAnd then, the other thing\nis that they invented this new technique\ncalled MLA latent attention.\nBoth of these are big deals.\nMixture of experts is something\nthat's been in the literature\nfor a handful of years.\nAnd OpenAI with GPT-4 was the first one\nto productize a mixture of experts model.\nAnd what this means\nis when you look at the\ncommon models around\nthat most people have been able\nto interact with that are open.\nThink Llama, Llama is a dense model.\ni.e, every single parameter\nor neuron is activated as\nyou're going through the model\nfor every single token you generate.\nNow, with a mixture of experts\nmodel, you don't do that.\nHow does the human actually work is like,\noh, well, my visual cortex\nis active when I'm\nthinking about vision tasks\nand other things.\nMy amygdala is when I'm scared.\nThese different aspects of your brain\nare focused on different things.\nA mixture of experts models\nattempts to approximate\nthis to some extent.\nSo, nowhere close to what\na brain architecture is,\nbut different portions\nof the model activate.\nYou'll have a set number\nof experts in the model\nand a set number that\nare activated each time.\nAnd this dramatically\nreduces both your training\nand inference cost.\nBecause now, if you think\nabout the parameter count\nas the total embedding space\nfor all of this knowledge\nthat you're compressing\ndown during training,\none, you're embedding this data in,\ninstead of having to activate\nevery single parameter,\nevery single time you're\ntraining or running inference.\nNow, you can just activate on a subset.\nAnd the model will learn\nwhich expert to route\nto for different tasks.\nAnd so, this is a humongous\ninnovation in terms of, hey,\nI can continue to grow\nthe total embedding space of parameters.\nAnd so, DeepSeek's model\nis 600 something billion parameters.\nRelative to Llama 405b,\nit's 405 billion parameters.\nLlama relative to Llama 70b,\nit's 70 billion parameters.", "mimetype": "text/plain", "start_char_idx": 22801, "end_char_idx": 26839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c415fb83-9227-4fb1-a310-f1908885738f": {"__data__": {"id_": "c415fb83-9227-4fb1-a310-f1908885738f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad6602f6-5660-4fed-b0dd-c185d66fd5d1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2c287b254d8a2d9d672b64a4071424b88569867d9cffdbbc94dbef603375fc78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0384e1e6-1134-4c0c-b754-7dfe113912bc", "node_type": "1", "metadata": {}, "hash": "081ef1ee5bea9d26fcb7116389280900e3001cbb270e093ba5b385c662494dc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And this dramatically\nreduces both your training\nand inference cost.\nBecause now, if you think\nabout the parameter count\nas the total embedding space\nfor all of this knowledge\nthat you're compressing\ndown during training,\none, you're embedding this data in,\ninstead of having to activate\nevery single parameter,\nevery single time you're\ntraining or running inference.\nNow, you can just activate on a subset.\nAnd the model will learn\nwhich expert to route\nto for different tasks.\nAnd so, this is a humongous\ninnovation in terms of, hey,\nI can continue to grow\nthe total embedding space of parameters.\nAnd so, DeepSeek's model\nis 600 something billion parameters.\nRelative to Llama 405b,\nit's 405 billion parameters.\nLlama relative to Llama 70b,\nit's 70 billion parameters.\nSo, this model technically\nhas more embedding space for information,\nto compress all of the world's knowledge\nthat's on the internet down.\nBut at the same time,\nit is only activating around\n37 billion of the parameters.\nSo, only 37 billion of these parameters\nactually need to be computed\nevery single time you're training data\nor inferencing data out of it.\nAnd so, versus again a Llama model,\n70 billion parameters must be activated\nor 405 billion parameters\nmust be activated.\nSo, you've dramatically\nreduced your compute cost\nwhen you're doing training and inference\nwith this mixture of experts architecture.\n- So, we break down\nwhere it actually applies\nand go into the transformer.\nIs that useful?\n- Let's go, let's go\ninto the transformer.\n- Okay.\nSo, the transformer\n(Lex laughing)\nis a thing that is talked about a lot\nand we will not cover every detail.\nEssentially, the transformer is built\non repeated blocks of\nthis attention mechanism,\nand then a traditional dense,\nfully connected multi-layer perception,\nwhatever word you want to use\nfor your normal neural network.\nAnd you alternate these blocks.\nThere's other details.\nAnd where mixture of experts\nis applied is that this dense model.\nThe dense model holds most of the weights\nif you count them in a transformer model.\nSo, you can get really big gains\nfrom those mixture of experts\non parameter efficiency\nat training and inference,\nbecause you get this efficiency\nby not activating all of these parameters.\n- [Lex] We should also\nsay that a transformer\nis a giant neural network.\n- Yeah.\n- And then,\nthere's for 15 years now,\nthere's what's called the\ndeep learning revolution.\nNetworks gotten larger and larger.\nAnd a certain point, the\nscaling laws appeared\nwhere people realized...\n- This is a scaling law shirt, by the way.\n(group laughing)\n- Representing.\nScaling laws where it became more\nand more formalized that bigger is better\nacross multiple dimensions\nof what bigger means.\nBut these are all neural\nnetworks we're talking about.\nAnd we're talking about\ndifferent architectures\nof how to construct these neural networks\nsuch that the training\nand the inference on\nthem is super efficient.\n- Yeah, every different type of model\nhas a different scaling law for it,\nwhich is effectively for\nhow much compute you put in,\nthe architecture will get\nto different levels of\nperformance at test tasks.\nA mixture of experts is one\nof the ones at training time.\nEven if you don't consider\nthe inference benefits,\nwhich are also big.\nAt training time, your\nefficiency with your GPUs\nis dramatically improved\nby using this architecture\nif it is well-implemented.\nSo, you can get effectively\nthe same performance model\nand evaluation scores with\nnumbers like 30% less compute.\nI think there's gonna be a wide variation,\ndepending on your implementation\ndetails and stuff.\nBut it is just important\nto realize that this type\nof technical innovation is\nsomething that gives huge gains.\nAnd I expect most companies\nthat are serving their models\nto move to this mixture\nof experts implementation.\nHistorically, the reason\nwhy not everyone might do it\nis because it's a\nimplementation complexity,\nespecially when doing these big models.\nSo, this is one of the things\nthat DeepSeek gets credit for\nis they do this extremely well.\nThey do a mixture of\nexperts extremely well.\nThis architecture for what\nis called DeepSeekMoE.\nMoE is the shortened version\nof mixture of experts,\nis multiple papers old.\nThis part of their training infrastructure\nis not new to these models alone,\nand same goes for what Dylan mentioned,\nwith multi-head latent attention.", "mimetype": "text/plain", "start_char_idx": 26068, "end_char_idx": 30451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0384e1e6-1134-4c0c-b754-7dfe113912bc": {"__data__": {"id_": "0384e1e6-1134-4c0c-b754-7dfe113912bc", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c415fb83-9227-4fb1-a310-f1908885738f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cdff396f325770f62a3501b67f1a640d9e657686744a51d00a636eebe2815b72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0850af2-4727-43a7-bc63-90c22fb3cba8", "node_type": "1", "metadata": {}, "hash": "e6ffa2fb406a83fbc95546ac0f3457fd721ee7dc97b59b31bbe0b8ff3b5c728e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I think there's gonna be a wide variation,\ndepending on your implementation\ndetails and stuff.\nBut it is just important\nto realize that this type\nof technical innovation is\nsomething that gives huge gains.\nAnd I expect most companies\nthat are serving their models\nto move to this mixture\nof experts implementation.\nHistorically, the reason\nwhy not everyone might do it\nis because it's a\nimplementation complexity,\nespecially when doing these big models.\nSo, this is one of the things\nthat DeepSeek gets credit for\nis they do this extremely well.\nThey do a mixture of\nexperts extremely well.\nThis architecture for what\nis called DeepSeekMoE.\nMoE is the shortened version\nof mixture of experts,\nis multiple papers old.\nThis part of their training infrastructure\nis not new to these models alone,\nand same goes for what Dylan mentioned,\nwith multi-head latent attention.\nThis is all about reducing\nmemory usage during inference.\nAnd same things during training,\nby using some fancy low\nrank approximation math.\nIf you get into the details\nwith this latent attention,\nit's one of those things I look at,\nand it's like, okay,\nthey're doing really\ncomplex implementations,\n'cause there's other\nparts of language models\nsuch as embeddings\nthat are used to extend\nthe context length.\nThe common one that DeepSeek used\nis rotary positional impendings,\nwhich is called RoPE.\nAnd if you want to use\nRoPE with a normal MoE,\nit's a sequential thing.\nYou take two of the attention matrices\nand you rotate them by a\ncomplex value rotation,\nwhich is a matrix multiplication.\nWith DeepSeek's MLA, with this\nnew attention architecture,\nthey need to do some clever things,\nbecause they're not set up the same\nand it just makes the implementation\ncomplexity much higher.\nSo, they're managing all of these things,\nand these are probably the\nsort of things that OpenAI,\nthese closed labs are doing.\nWe don't know if they're doing\nthe exact same techniques,\nbut they actually shared\nthem with the world,\nwhich is really nice to be like,\nthis is the cutting edge\nof efficient language model training.\n- And some of this requires\nlow level engineering,\njust is a giant mess in trickery.\nSo, as I understand, that went below CUDA,\nso they go super low programming of GPUs.\n- Effectively, Nvidia builds\nthis library called NCCL.\nIn which, when you're training a model,\nyou have all these communications\nbetween every single layer of the model\nand you may have over 100 layers.\n- What does the NCCL\nstand for? It's N-C-C-L?\n- Nvidia Communications\nCollectives Library.\n- [Lex] Nice.\n(Nathan laughing)\nDamn.\n- And so,\n(group laughing)\nwhen you're training a model,\nyou're gonna have all\nreduces and all gathers.\nBetween each layer, between\nthe multi-layer perception\nor feedforward network\nand the attention mechanism you'll have,\nyou'll have basically\nthe model synchronized.\nOr you'll have all reduce and all gather.\nAnd this is a communication\nbetween all the GPUs in the network,\nwhether it's in training or inference.\nSo, Nvidia has a standard library.\nThis is one of the reasons\nwhy it's really difficult\nto use anyone else's hardware for training\nis because no one's really built\na standard communications library.\nAnd Nvidia's done this at a higher level.\nAt DeepSeek, because they\nhave certain limitations\naround the GPUs that they have access to,\nthe interconnects are\nlimited to some extent\nby the restrictions of the GPUs\nthat were shipped into China legally,\nnot the ones that are smuggled,\nbut legally shipped in,\nthat they used to train this model.\nThey had to figure out\nhow to get efficiencies.\nAnd one of those things is that instead\nof just calling the Nvidia library NCCL,\nthey instead created their,\nthey scheduled their own communications,\nwhich some of the labs do.\nMeta talked about in Llama 3\nhow they made their own\ncustom version of NCCL.\nThey didn't talk about the\nimplementation details.\nThis is some of what they did.\nProbably not as well as,\nmaybe not as well as DeepSeek\nbecause DeepSeek necessity\nis the mother of innovation\nand they had to do this.\nWhereas in the ca...\nOpenAI has people\nthat do this sort of stuff,\nAnthropic, et cetera.\nBut DeepSeek certainly did it publicly\nand they may have done it even better\nbecause they were gimp on a certain aspect\nof the chips that they have access to.", "mimetype": "text/plain", "start_char_idx": 29584, "end_char_idx": 33885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0850af2-4727-43a7-bc63-90c22fb3cba8": {"__data__": {"id_": "c0850af2-4727-43a7-bc63-90c22fb3cba8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0384e1e6-1134-4c0c-b754-7dfe113912bc", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8f43e6e8c518f4fdaa1c5585af3174bcf059fb181f3579c62f6b243e8b8c2cb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0182c0ed-a000-4b7c-805b-89740af1e4e3", "node_type": "1", "metadata": {}, "hash": "69501143fa694769aa098e2edfc24213dd6b57f915c78e25b5111eb40f01eaef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They had to figure out\nhow to get efficiencies.\nAnd one of those things is that instead\nof just calling the Nvidia library NCCL,\nthey instead created their,\nthey scheduled their own communications,\nwhich some of the labs do.\nMeta talked about in Llama 3\nhow they made their own\ncustom version of NCCL.\nThey didn't talk about the\nimplementation details.\nThis is some of what they did.\nProbably not as well as,\nmaybe not as well as DeepSeek\nbecause DeepSeek necessity\nis the mother of innovation\nand they had to do this.\nWhereas in the ca...\nOpenAI has people\nthat do this sort of stuff,\nAnthropic, et cetera.\nBut DeepSeek certainly did it publicly\nand they may have done it even better\nbecause they were gimp on a certain aspect\nof the chips that they have access to.\nAnd so, they scheduled communications\nby scheduling specific SMs.\nSMs you could think of as\nlike the core on a GPU.\nSo, there's hundreds of cores\nor there's a bit over\n100 cores, SMs, on a GPU,\nand they were specifically scheduling,\nhey, which ones are running the model?\nWhich ones are doing all reduce?\nWhich one are doing all gather?\nAnd they would flip back\nand forth between them.\nAnd this requires extremely\nlow level programming.\n- This is what NCCL does automatically\nor other Nvidia libraries\nhandle this automatically usually.\n- Yeah, exactly.\nAnd so, technically, they're using PTX,\nwhich is like, you could think of it\nas like an assembly type language.\nIt's not exactly that or instruction set.\nCoding directly to assembly\nor instruction set.\nIt's not exactly that,\nbut that's still part of technically CUDA.\nBut it's like, do I wanna write in Python,\nPyTorch equivalent, and\ncall Nvidia libraries?\nDo I want to go down to the C level.\nOr in code, even lower level?\nOr do I wanna go all the way down\nto the assembly or ISA level?\nAnd there are cases\nwhere you go all the way down\nthere at the very big labs,\nbut most companies just do not do that\nbecause it's a waste of time\nand the efficiency gains\nyou get are not worth it.\nBut DeepSeek's\nimplementation is so complex.\nEspecially with their mixture of experts.\nPeople have done mixture of experts,\nbut they're generally 8, 16 experts.\nAnd they activate too.\nSo, one of the words\nthat we like to use is\nsparsity factor or usage.\nSo, you might have four, one\nfourth of your model activate.\nAnd that's what misdraws mixed role model.\nThey're model that really\ncatapulted them to like,\noh my god, they're really, really good.\nOpenAI has also had models that are MoE,\nand so have all the other\nlabs that are major closed.\nBut what DeepSeek did that\nmaybe only the leading labs\nhave only just started recently doing\nis have such a high sparsity factor.\nIt's not one fourth of the model.\nTwo out of eight experts activating\nevery time you go through the model.\nIt's 8 out of 256.\n- And there's different\nimplementations from mixture\nof experts where you can\nhave some of these experts\nthat are always activated,\nwhich this just looks like\na small neural network.\nAnd then, all the tokens go through that.\nAnd then, they also go through some\nthat are selected by\nthis routing mechanism.\nAnd one of the innovations\nin DeepSeek's architecture\nis that they change the routing mechanism\nin mixture of expert models.\nThere's something called\nan auxiliary loss,\nwhich effectively means during training,\nyou want to make sure\nthat all of these experts\nare used across the tasks\nthat the model sees.\nWhy there can be failures\nin mixture of experts\nis that when you're doing this training,\nthe one objective is\ntoken prediction accuracy.\nAnd if you just let turning go\nwith a mixture of expert\nmodel on your own,\nit can be that the model learns\nto only use a subset of the experts.\nAnd in the MoE literature,\nthere's something called\nthe auxiliary loss,\nwhich helps balance them.\nBut if you think about the loss\nfunctions of deep learning,\nthis even connects to the bitter lesson,\nis that you want to have\nthe minimum inductive bias\nin your model to let the\nmodel learn maximally.\nAnd this auxiliary loss,\nthis balancing across experts\ncould be seen as intention\nwith the prediction\naccuracy of the tokens.", "mimetype": "text/plain", "start_char_idx": 33119, "end_char_idx": 37241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0182c0ed-a000-4b7c-805b-89740af1e4e3": {"__data__": {"id_": "0182c0ed-a000-4b7c-805b-89740af1e4e3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0850af2-4727-43a7-bc63-90c22fb3cba8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "65c70a3d550568ff58fd774eea4ebc6aaa4f62be92117fc2e46fc5eeea4a829c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c", "node_type": "1", "metadata": {}, "hash": "7e6051310f03d2ac449af92243d36b6bf2f95923008247cbeabe2964a46f464c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There's something called\nan auxiliary loss,\nwhich effectively means during training,\nyou want to make sure\nthat all of these experts\nare used across the tasks\nthat the model sees.\nWhy there can be failures\nin mixture of experts\nis that when you're doing this training,\nthe one objective is\ntoken prediction accuracy.\nAnd if you just let turning go\nwith a mixture of expert\nmodel on your own,\nit can be that the model learns\nto only use a subset of the experts.\nAnd in the MoE literature,\nthere's something called\nthe auxiliary loss,\nwhich helps balance them.\nBut if you think about the loss\nfunctions of deep learning,\nthis even connects to the bitter lesson,\nis that you want to have\nthe minimum inductive bias\nin your model to let the\nmodel learn maximally.\nAnd this auxiliary loss,\nthis balancing across experts\ncould be seen as intention\nwith the prediction\naccuracy of the tokens.\nSo, we don't know the exact extent\nthat the DeepSeek MoE change,\nwhich is instead of\ndoing an auxiliary loss,\nthey have an extra\nparameter in their routing,\nwhich after the batches,\nthey update this parameter to make sure\nthat the next batches all\nhave a similar use of experts.\nAnd this type of change can be big,\nit can be small, but\nthey add up over time.\nAnd this is the sort of thing\nthat just points to them innovating.\nAnd I'm sure all the labs\nthat are training big MoEs\nare looking at this sort of things,\nwhich is getting away\nfrom the auxiliary loss.\nSome of them might already use it,\nbut you keep accumulating gains.\nAnd we'll talk about the\nphilosophy of training\nand how you organize these organizations.\nAnd a lot of it is just\ncompounding small improvements\nover time in your data,\nin your architecture,\nin your post-training,\nand how they integrate with each other.\nAnd DeepSeek does the same thing\nand some of 'em are shared or a lot,\nwe have to take them on face value\nthat they share their\nmost important details.\nThe architecture and the\nweights are out there,\nso we're seeing what they're\ndoing, and it adds up.\n- Going back to the efficiency\nand complexity point.\nIt's 32 versus 4 for mixed draw\nand other MoE models that\nhave been publicly released.\nSo, this ratio is extremely high.\nAnd what Nathan was getting at there was,\nwhen you have such a\ndifferent level of sparsity,\nyou can't just have every\nGPU have the entire model.\nThe model's too big, there's\ntoo much complexity there.\nSo, you have to split up the model\nwith different types of parallelism.\nAnd so, you might have different experts\non different GPU nodes,\nbut now what happens when\nthis set of data that you get,\nhey, all of it looks like this one way\nand all of it should route\nto one part of my model.\nSo, when all of it routes\nto one part of the model,\nthen you can have this overloading\nof a certain set of the GPU resources\nor a certain set of the GPUs,\nand then the rest of the\ntraining network sits idle,\nbecause all of the tokens\nare just routing to that.\nSo, this is the biggest complexity,\none of the big complexities\nwith running a very sparse\nmixture of experts model,\ni.e, this 32 ratio versus this 4 ratio,\nis that you end up with so many\nof the experts just sitting there idle.\nSo, how do I load balance between them?\nHow do I schedule the\ncommunications between them?\nThis is a lot of the extremely\nlow level detailed work\nthat they figured out in the public first\nand potentially second\nor third in the world,\nand maybe even first in some cases.\n- What lesson do you, in the direction\nof the bitter lesson, do\nyou take from all of this?\nIs this going to be the direction\nwhere a lot of the gain is going to be,\nwhich is this kind of\nlow level optimization?\nOr is this a short-term thing\nwhere the biggest gains will be more\non the algorithmic high\nlevel side of post-training?\nIs this like a short-term leap\nbecause they've figured out like a hack,\nbecause constraints, necessity\nis the mother of invention,\nor is there still a lot of gains?\n- I think we should summarize\nwhat the bitter lesson actually is about,\nis that the bitter lesson,\n- Okay.", "mimetype": "text/plain", "start_char_idx": 36356, "end_char_idx": 40398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c": {"__data__": {"id_": "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0182c0ed-a000-4b7c-805b-89740af1e4e3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cb741d921b82601b9d80329c1b8d659d315b00b9da596139a54a39a0c2b3ffbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0616145c-e48f-4abc-9759-b0a3c32cd2ed", "node_type": "1", "metadata": {}, "hash": "e0cc5b084a41d8176f98131e88689fcb68ad0e59b7fa90c21c3004e2f01c0c21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How do I schedule the\ncommunications between them?\nThis is a lot of the extremely\nlow level detailed work\nthat they figured out in the public first\nand potentially second\nor third in the world,\nand maybe even first in some cases.\n- What lesson do you, in the direction\nof the bitter lesson, do\nyou take from all of this?\nIs this going to be the direction\nwhere a lot of the gain is going to be,\nwhich is this kind of\nlow level optimization?\nOr is this a short-term thing\nwhere the biggest gains will be more\non the algorithmic high\nlevel side of post-training?\nIs this like a short-term leap\nbecause they've figured out like a hack,\nbecause constraints, necessity\nis the mother of invention,\nor is there still a lot of gains?\n- I think we should summarize\nwhat the bitter lesson actually is about,\nis that the bitter lesson,\n- Okay.\n- essentially, if you paraphrase it,\nis that the types of training\nthat will win out in deep learning\nas we go are those methods\nthat are which are scalable\nin learning and search\nis what it calls out.\nAnd the scale word gets a\nlot of attention in this.\nThe interpretation that\nI use is effectively\nto avoid adding the human\npriors to your learning process.\nAnd if you read the original essay,\nthis is what it talks about\nis how researchers will try to come up\nwith what clever solutions\nto their specific problem\nthat might get them small\ngains in the short-term\nwhile simply enabling\nthese deep learning systems\nto work efficiently.\nAnd for these bigger\nproblems in the long-term\nmight be more likely to scale\nand continue to drive success.\nAnd therefore, we were talking\nabout relatively small\nimplementation changes\nto the mixture of experts model.\nAnd therefore, it's like, okay,\nwe will need a few more\nyears to know if one\nof these were actually really\ncrucial to the bitter lesson.\nBut the bitter lesson is really\nthis long-term arc of how\nsimplicity can often win.\nAnd there's a lot of\nsayings in the industry,\nlike the models just wanna learn.\nYou have to give them\nthe simple loss landscape\nwhere you put compute through the model,\nand they will learn and getting\nbarriers out of the way.\n- That's where the power,\nsomething like NCCL comes in,\nwhere standardized code\nthat could be used by a lot of people\nto create sort of simple\ninnovations that can scale.\nWhich is why the hacks,\nI imagine the code base\nfor DeepSeek is probably a giant mess.\n- I'm sure they have,\nDeepSeek definitely has code\nbases that are extremely messy,\nwhere they're testing these new ideas,\nmulti-head latent attention,\nprobably could start in\nsomething like a Jupyter Notebook\nor somebody try something on a few GPUs.\nAnd that is really messy.\nBut the stuff that trains the DeepSeek-V3\nand DeepSeek-R1, those libraries,\nif you were to present them to us,\nI would guess are extremely\nhigh quality code.\n- It's a high quality readable code. Yeah.\n- I think there is one\naspect to note though.\nIs that there is the general ability\nfor that to transfer across\ndifferent types of runs.\nYou may make really,\nreally high quality code\nfor one specific model architecture\nat one size.\n- Yeah.\n- And then, that is not transferable to,\nhey, when I make this architecture tweak,\neverything's broken again.\nThat's something that could be\nwith their specific low level\ncoding of scheduling SMs,\nis specific to this model\narchitecture and size.\nAnd whereas like Nvidia's\ncollectives library\nis more like, hey,\nit'll work for anything.\nYou wanna do an all reduce, great,\nI don't care what your model\narchitecture is, it'll work.\nAnd you're giving up a lot of performance\nwhen you do that in many cases.\nBut it's worthwhile for them\nto do the specific optimization\nfor the specific run,\ngiven the constraints that\nthey have regarding compute.\n- I wonder how stressful it is to like,\nthese frontier models,\nlike initiate training,\nlike to have the code-\n- [Nathan] Push the button.\n- to push the button\n(Nathan laughing)\nthat you're now spending\na large amount of money\nand time to train this.\nThere must be a lot of\ninnovation on the debugging stage\nof making sure there's no\nissues that you're monitoring\nand visualizing every aspect\nof the training, all that kind of stuff.", "mimetype": "text/plain", "start_char_idx": 39566, "end_char_idx": 43747, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0616145c-e48f-4abc-9759-b0a3c32cd2ed": {"__data__": {"id_": "0616145c-e48f-4abc-9759-b0a3c32cd2ed", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbd5968a-a674-4bcc-8a29-1ffc6b469c3c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "20554ca2153a542943e0a0d32056ebd7a85b15f777a01247d08cf8efbc17ca53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7298fd7a-3cab-4554-a47f-c25514b28dda", "node_type": "1", "metadata": {}, "hash": "a3f78a35e52737798fb9a8669557ca7b20fc5cbe149c223f617bffb730224ce5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And whereas like Nvidia's\ncollectives library\nis more like, hey,\nit'll work for anything.\nYou wanna do an all reduce, great,\nI don't care what your model\narchitecture is, it'll work.\nAnd you're giving up a lot of performance\nwhen you do that in many cases.\nBut it's worthwhile for them\nto do the specific optimization\nfor the specific run,\ngiven the constraints that\nthey have regarding compute.\n- I wonder how stressful it is to like,\nthese frontier models,\nlike initiate training,\nlike to have the code-\n- [Nathan] Push the button.\n- to push the button\n(Nathan laughing)\nthat you're now spending\na large amount of money\nand time to train this.\nThere must be a lot of\ninnovation on the debugging stage\nof making sure there's no\nissues that you're monitoring\nand visualizing every aspect\nof the training, all that kind of stuff.\n- When people are training,\nthey have all these various dashboards,\nbut the most simple one\n- Yeah.\n- is your loss.\n- Right.\n- And it continues to go down.\nBut in reality,\nespecially with more\ncomplicated stuff like MoE,\nthe biggest problem with it,\nor FP8 training, which\nis another innovation,\ngoing to a lower precision number format,\ni.e, less accurate, is that\nyou end up with loss spikes.\nAnd no one knows why\nthe loss spike happened.\nAnd for a lot-\n- Some of them you do.\n- Some of them you do.\n- Some of them are bad data.\nI give AI2's example\nof what blew up our earlier models\nis a subreddit called Microwave Gang.\nWe love to shoutout this out.\n(Lex laughing)\nIt's a real thing.\nYou can pull up Microwave Gang.\nEssentially, it's a subreddit\nwhere everybody makes posts\nthat are just the letter M.\nSo, it's like, mm.\nSo, there's extremely long sequences\nof the letter M, and then the\ncomments are like, beep beep.\n'Cause it's in the microwave end.\n- Yeah.\n- If you pass this\ninto a model that's trained\nto be a normal producing text,\nit's extremely high loss.\n'Cause normally,\n- Yeah.\n- you see an M.\n(Lex laughing)\nYou don't predict Ms for a long time.\nSo, this is something that\ncause a loss spikes for us.\nBut when you have much like,\nthis is old, this is not recent.\nAnd when you have more\nmature data systems,\nthat's not the thing that\ncauses the loss spike.\nAnd what Dylan is saying is true,\nbut it's levels to this sort of idea.\n- With regards to the stress,\n(Nathan and Lex laughing)\nthese people are like,\nyou'll go out to dinner\nwith a friend that works\nat one of these labs.\nAnd they'll just be like\n(Nathan laughing)\nlooking at their phone every 10 minutes,\nand they're not like...\nIt's one thing if they're texting,\nbut they're just like,\n- Yeah.\n- is the loss-\n- It's literal.\nThe tokens per second loss, not blown up.\nThey're just watching this. (chuckles)\n- And the heart rate goes\nup if there's a spike.\n- And some level of spikes is normal.\nIt'll recover and be back.\nSometimes a lot of the\nold strategy was like,\nyou just stop the run,\nrestart from an old version,\nand then change the data\nmix, and then it keeps going.\n- There are even\ndifferent types of spikes.\nSo, Dirk Groeneveld has a theory of AI too\nthat's like fast spikes and slow spikes,\nwhere there are sometimes where\nyou're looking at the loss\nand there are other parameters,\nyou can see it start to\ncreep up, and then blow up.\nAnd that's really hard to recover from.\nSo, you have to go back much further.\nSo, you have the stressful\nperiod, where it's like flat\nor it might start going up\nand you're like, what do I do?\nWhereas there are also lost\nspikes that are, it looks good.\nAnd then, there's one spiky data point.\nAnd what you could do\nis you just skip those.\nYou see that there's a spike.\nYou're like, okay, I can ignore this data,\ndon't update the model,\nand do the next one, and\nit'll recover quickly.", "mimetype": "text/plain", "start_char_idx": 42919, "end_char_idx": 46649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7298fd7a-3cab-4554-a47f-c25514b28dda": {"__data__": {"id_": "7298fd7a-3cab-4554-a47f-c25514b28dda", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0616145c-e48f-4abc-9759-b0a3c32cd2ed", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "80cde20a90d71b9b4515bd441873654662f5df36f2dbee398bc9bb175507148b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84b91c94-353c-49ec-b8bf-057403d13962", "node_type": "1", "metadata": {}, "hash": "952bdadce378f8c079e9a2399811035c09732ebade744afba731729475b7b2e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- There are even\ndifferent types of spikes.\nSo, Dirk Groeneveld has a theory of AI too\nthat's like fast spikes and slow spikes,\nwhere there are sometimes where\nyou're looking at the loss\nand there are other parameters,\nyou can see it start to\ncreep up, and then blow up.\nAnd that's really hard to recover from.\nSo, you have to go back much further.\nSo, you have the stressful\nperiod, where it's like flat\nor it might start going up\nand you're like, what do I do?\nWhereas there are also lost\nspikes that are, it looks good.\nAnd then, there's one spiky data point.\nAnd what you could do\nis you just skip those.\nYou see that there's a spike.\nYou're like, okay, I can ignore this data,\ndon't update the model,\nand do the next one, and\nit'll recover quickly.\nBut these like untrickier implementation,\nso as you get more complex\nin your architecture\nand you scale up to more GPUs,\nyou have more potential\nfor your loss blowing up.\nSo, it's like there's\na distribution.\n- And then,\nthe whole idea of grokking also comes in.\nIt's like, just because it's\nslowed down from improving\nand loss doesn't mean it's not learning,\nbecause all of a sudden,\nit could be like this\nand it could just spike down in loss again\nbecause it learned,\ntruly learned something.\nAnd it took some time\nfor it to learn that.\nIt's not like a gradual process.\nAnd that's what humans are like,\nthat's what models are like.\nIt's really a stressful\ntask as you mentioned.\n- And the whole time, the\ndollar count is going up.\n- Every company has failed runs.\nYou need failed run\nto push the envelope\non your infrastructure.\nSo, a lot of news cycles\nare made of X company had Y failed run.\nEvery company that's trying\nto push the frontier of AI has these.\nSo, yes, it's noteworthy\nbecause it's a lot of money\nand it can be week to month setback,\nbut it is part of the process.\n- But how do you get, if you're DeepSeek,\nhow do you get to a place where holy shit,\nthere's a successful\ncombination of hyperparameters?\n- A lot of small failed runs.\n- And so, rapid iteration\n(Nathan chuckles)\nthrough failed runs,\nuntil-\n- Yeah, and successful ones.\nYou just-\n- And then, you build up\nsome intuition like this,\nthis mixture of expert works,\nand then this implementation of MLA works.\n- Key hyperparameters like learning rate\nand regularization and things like this.\nAnd you find the regime that\nworks for your code base.\nTalking to people at Frontier\nLabs, there's a story\nthat you can tell where\ntraining language models\nis kind of a path that you need to follow.\nSo, you need to unlock the ability\nto train a certain type of\nmodel or a certain scale.\nAnd then, your code base\nand your internal knowhow\nof which hyperparameters\nwork for it is known.\nAnd you look at the\nDeepSeek papers and models,\nthey've scaled up,\nthey've added complexity,\nand it's just continuing\nto build the capabilities that they have.\n- There's the concept of a YOLO run.\n(Nathan laughing)\nSo, YOLO, you only live once.\n- Yep.\n- And what it is, is like,\nthere's all this experimentation\nyou do at the small-scale.\nResearch ablations, you have\nyour Jupyter Notebook with,\nyou're experimenting with MLA\non three GPUs or whatever,\nand you're doing all these\ndifferent things like, hey,\ndo I do 4 expert, 4 active\nexperts, 128 experts,\ndo I arrange the experts this way?\nAll these different\nmodel architecture things\nyou're testing at a very small-scale.\nCouple researchers,\nfew GPUs, tens of GPUs,\nhundreds of GPUs, whatever it is.\nAnd then, all of a sudden,\nyou're like, okay guys,\nno more fucking around.\nNo more screwing around.\nEveryone, take all the resources we have,\nlet's pick what we think will\nwork, and just go for it.\nYOLO.\n- Yeah.\n- And this is where that sort\nof stress comes in is like,\nwell, I know it works here,\nbut some things that work\nhere don't work here,\nand some things that work\nhere don't work down here\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 45896, "end_char_idx": 49762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84b91c94-353c-49ec-b8bf-057403d13962": {"__data__": {"id_": "84b91c94-353c-49ec-b8bf-057403d13962", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7298fd7a-3cab-4554-a47f-c25514b28dda", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "903d7c844779ec168bb99292aed8900d8c000ceb898325ea49326c8f80d31cbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c00639d-fb96-42be-93b7-8b6280d07790", "node_type": "1", "metadata": {}, "hash": "6aaddd6c5d58c1f597faf5de275dfa09cf00a8364e48e5e5315351dc1c6258a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All these different\nmodel architecture things\nyou're testing at a very small-scale.\nCouple researchers,\nfew GPUs, tens of GPUs,\nhundreds of GPUs, whatever it is.\nAnd then, all of a sudden,\nyou're like, okay guys,\nno more fucking around.\nNo more screwing around.\nEveryone, take all the resources we have,\nlet's pick what we think will\nwork, and just go for it.\nYOLO.\n- Yeah.\n- And this is where that sort\nof stress comes in is like,\nwell, I know it works here,\nbut some things that work\nhere don't work here,\nand some things that work\nhere don't work down here\n- Yeah.\n- in this terms of scale.\nSo, it's really truly a YOLO run,\nand there's this discussion\nof certain researchers\njust have this methodical nature.\nThey can find the whole search space\nand figure out all the\nablations of different research,\nand really see what is best.\nAnd there's certain\nresearchers who just like,\nhave that innate gut instinct of like,\nthis is the YOLO run.\nI'm looking at the data,\n(Dylan drowns out Nathan)\nI think this is it.\n- This is why you wanna\nwork in post-training,\nbecause the GPU cost\nfor training is lower.\nSo, you can make a higher percentage\nof your training runs YOLO runs\n- Yeah.\n- For now.\n- Yeah, for now, for now.\n- For now, for now. (laughs)\n- So, some of this is\nfundamentally luck still.\n- Luck is skill in many cases.\n- Yeah, it looks lucky right when you're-\n- But the hill to climb, if\nyou're on one of these labs\nand you have an evaluation\nand you're not crushing,\nthere's a repeated playbook\nof how you improve things.\nThere are localized improvements,\nwhich might be data improvements.\nAnd these add up\ninto the whole model\njust being much better.\nAnd when you zoom in really close,\nit can be really obvious\nthat this model's just\nreally bad at this thing\nand we can fix it.\nAnd you just add these up.\nSo, some of it feels like luck,\nbut on the ground,\nespecially with these new\nreasoning models we're talking to,\nis just so many ways\nthat we can poke around.\nAnd normally, it's that some\nof them give big improvements.\n- The search space is near infinite.\nAnd yet, the amount of compute\nand time you have is very low.\nAnd you have to hit release schedules,\nyou have to not get\nblown past by everyone.\nOtherwise, what happened with DeepSeek\ncrushing Meta and Misral and Cohere,\nand all these guys, they moved too slow.\nThey maybe were too methodical.\nI don't know, they\ndidn't hit the YOLO run,\nwhatever the reason was,\nmaybe they weren't as skill.\nYou can call it luck if you want,\nbut at the end of the day, it's skill.\n- So, 2025 is the year of the YOLO run.\nIt seems like all the labs are going in.\n- I think it's even more\nimpressive what OpenAI did in 2022.\nAt the time, no one believed in mixture\nof experts models at Google\nwho had all the researchers.\nOpenAI had such little compute\nand they devoted all of their\ncompute for many months,\nall of it, 100% for many months, to GPT-4,\nwith a brand new architecture\nwith no belief that, hey,\nlet me spend a couple $100 million,\nwhich is all of the money\nI have, on this model.\nThat is truly YOLO,\nright?\n- Yeah, yeah.\n- Now, people are like,\nall these training run\nfailures that are in the media,\nit's like, okay, great,\nbut actually, a huge chunk of\nmy GPs are doing inference.\nI still have a bunch\ndoing research constantly.\nAnd yes, my biggest cluster is training,\nbut on this YOLO run,\nbut that YOLO run is much less risky\nthan what OpenAI did in 2022,\nor maybe what DeepSeek did now,\nor like, hey, we're just\ngonna throw everything at it.\n- The big winners throughput human history\nare the ones who are willing\nto do YOLO at some point.\nOkay.\nWhat do we understand\nabout the hardware it's been trained on?\nDeepSeek.\n- DeepSeek is very interesting.", "mimetype": "text/plain", "start_char_idx": 49195, "end_char_idx": 52915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c00639d-fb96-42be-93b7-8b6280d07790": {"__data__": {"id_": "2c00639d-fb96-42be-93b7-8b6280d07790", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84b91c94-353c-49ec-b8bf-057403d13962", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "34f0a407e21c59fcae790b041be0c4db1e47974634055961064f8cf061308650", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518", "node_type": "1", "metadata": {}, "hash": "6d676831cfb8f1d8ba328994abf98632ed16d9b302a44e78f7b809b16d830647", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That is truly YOLO,\nright?\n- Yeah, yeah.\n- Now, people are like,\nall these training run\nfailures that are in the media,\nit's like, okay, great,\nbut actually, a huge chunk of\nmy GPs are doing inference.\nI still have a bunch\ndoing research constantly.\nAnd yes, my biggest cluster is training,\nbut on this YOLO run,\nbut that YOLO run is much less risky\nthan what OpenAI did in 2022,\nor maybe what DeepSeek did now,\nor like, hey, we're just\ngonna throw everything at it.\n- The big winners throughput human history\nare the ones who are willing\nto do YOLO at some point.\nOkay.\nWhat do we understand\nabout the hardware it's been trained on?\nDeepSeek.\n- DeepSeek is very interesting.\nA second to take us\nto zoom out out of who\nthey are, first of all.\nHigh-Flyer is a hedge fund\nthat has historically\ndone quantitative trading\nin China as well as elsewhere.\nAnd they have always had a\nsignificant number of GPUs.\nIn the past, a lot of these\nhigh frequency trading,\nalgorithmic, quant traders, used FPGAs,\nbut it shifted to GPUs, definitely.\nAnd there's both.\nBut GPUs especially, and High-Flyer,\nwhich is the hedge fund\nthat owns DeepSeek.\nAnd everyone who works for DeepSeek\nis part of High-Flyer to some extent.\nSame parent company, same owner, same CEO.\nThey had all these resources\nand infrastructure for trading,\nand then they devoted a\nhumongous portion of them\nto training models, both\nlanguage models and otherwise.\nBecause these techniques\nwere heavily AI-influenced.\nMore recently, people have realized,\nhey, trading with, even when\nyou go back to Renaissance\nand all these quantitative firms,\nnatural language processing\nis the key to trading really fast,\nunderstanding a press release\nand making the right trade.\nAnd so, DeepSeek has always\nbeen really good at this.\nAnd even as far back as 2021,\nthey have press releases\nand papers saying like, hey,\nwe're the first company in China\nwith an A100 cluster this large.\nIt was 10,000 A100 GPUs. This is in 2021.\nNow, this wasn't all for\ntraining large language models.\nThis is mostly for training models\nfor their quantitative aspects,\nor quantitative trading,\nas well as a lot of that\nwas natural language\nprocessing, to be clear.\nAnd so, this is the history.\nSo, verifiable fact is that in 2021,\nthey built the largest\ncluster, at least they claim\nit was the largest cluster\nin China, 10,000 GPUs.\n- Before expert controls started.\n- Yeah.\n- It's like,\nthey've had a huge cluster\nbefore any conversation\nof export controls.\nSo, then you step it forward to like,\nwhat have they done over the\nlast four years since then?\nObviously, they've continued\nto operate the hedge fund,\nprobably make tons of money.\nAnd the other thing\nis that they've leaned more\nand more and more into AI.\nthe CEO, Liang Wenfeng, Liang-\n- You're not putting me on spot on this.\nWe discussed this. (laughs)\n- Liang Feng, the CEO,\nhe owns maybe-\n- We're all fam. (chuckles)\n- Liang Feng, he owns maybe\na little bit more than half\nthe company, allegedly,\nan extremely like\nElon-Jensen kind of figure,\nwhere he is just involved in everything.\nAnd so, over that time period,\nhe's gotten really in depth into AI.\nHe actually has a bit of a like a,\nif you see some of his statements,\na bit of an EAC vibe almost.\n- Total AGI vibes.\nAnd like, we need to do this,\nwe need to make a new ecosystem of OpenAI.\nWe need China to lead on\nthis sort of ecosystem,\nbecause historically,\nthe Western countries have\nled on software ecosystems,\nand straight up acknowledges,\nin order to do this, we need\nto do something different.\nDeepSeek is his way of doing this.\nSome of the translated interviews with him\nare fantastic.\n- So, he has done interviews?\n- Yeah.\n- Do you think\nhe would do a Western interview or no?\nOr is there controls\non the channel?\n- There hasn't been one yet,\nbut I would try it.\n- Okay. All right.", "mimetype": "text/plain", "start_char_idx": 52240, "end_char_idx": 56064, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518": {"__data__": {"id_": "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c00639d-fb96-42be-93b7-8b6280d07790", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "76759611730d5ed7be0a5f479bbb5773d907dfda516a43fdcd763d87b54b7c15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "078829e7-a738-45c2-80df-d3fc1fbd0bb6", "node_type": "1", "metadata": {}, "hash": "72855bdcefc841cb63a394039a9ae8b0b3c0aff86cb8179ebaba4d3ecff8a951", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so, over that time period,\nhe's gotten really in depth into AI.\nHe actually has a bit of a like a,\nif you see some of his statements,\na bit of an EAC vibe almost.\n- Total AGI vibes.\nAnd like, we need to do this,\nwe need to make a new ecosystem of OpenAI.\nWe need China to lead on\nthis sort of ecosystem,\nbecause historically,\nthe Western countries have\nled on software ecosystems,\nand straight up acknowledges,\nin order to do this, we need\nto do something different.\nDeepSeek is his way of doing this.\nSome of the translated interviews with him\nare fantastic.\n- So, he has done interviews?\n- Yeah.\n- Do you think\nhe would do a Western interview or no?\nOr is there controls\non the channel?\n- There hasn't been one yet,\nbut I would try it.\n- Okay. All right.\n(Nathan laughing)\nWell, I just got\na Chinese translator, so it was great.\nThis is all push.\nSo, fascinating figure, engineer\npushing full on into AI,\nleveraging the success from\nthe high frequency trading.\n- Very direct quotes,\nlike we will not switch\nto closed source when\nasked about this stuff.\nVery long-term motivated\nin how the ecosystem of AI should work.\nAnd I think from a Chinese perspective,\nhe wants a Chinese company\nto build this vision.\n- And so, this is like the,\nquote, unquote, \"visionary\nbehind the company\".\nThis hedge fund still exists,\nthis quantitative firm.\nAnd so, DeepSeek is the sort of,\nsolely, he got turned to this full view\nof like AI, everything about this.\nBut at some point,\nit slowly maneuvered and he made DeepSeek.\nAnd DeepSeek has done\nmultiple models since then.\nThey've acquired more and more GPUs.\nThey share infrastructure with the fund.\nAnd so, there is no exact number\nof public GPU resources that they have,\nbut besides this 10,000 GPUs\nthat they bought in 2021,\nand they were fantastically profitable.\nAnd then, this paper claims\nthey did only 2,000 H800 GPUs,\nwhich are a restricted GPU\nthat was previously allowed in China,\nbut no longer allowed,\nand there's a new version.\nBut it's basically\nNvidia's H100 for China.\nAnd there's some restrictions on it,\nspecifically around the\ncommunications sort of speed,\nthe interconnect speed,\nwhich is why they had to do\nthis crazy SM scheduling stuff.\nSo, going back to that.\nIt's like this is obviously\nnot true in terms of\ntheir total GPU count.\n- Obvious available GPUs.\nBut for this training run,\nyou think 2,000 is the\ncorrect number or no?\n- So, this is where it\ntakes a significant amount\nof zoning in.\nWhat do you call your training run?\nYou count all of the research\nand ablations that you ran,\npicking all this stuff,\nbecause, yes, you can do a YOLO run,\nbut at some level,\nyou have to do the test\nat the small-scale,\nand then you have to do\nsome test at medium-scale\nbefore you go to a large-scale.\n- Accepted practices\nthat for any given model\nthat is a notable advancement,\nyou're gonna do 2 to 4x compute\nof the full training run\nin experiments alone.\n- So, a lot of this compute\nthat's being scaled up\nis probably used in large part\nat this time for research.\n- Yeah.\nAnd research begets the new ideas\nthat lets you get huge efficiency-\n- Research gets you o1.\nResearch gets you breakthrough,\nso you need to bet on it.\n- So, some of the pricing strategy\nthat we'll discuss has the\nresearch baked into the price.\n- So, the numbers that DeepSeek\nspecifically said publicly\nare just the 10,000 GPUs in 2021,\nand then 2,000 GPUs for only\nthe pre-training for V3.\nThey did not discuss cost on R1.\nThey did not discuss\ncost on all the other RL,\nfor the instruct model that they made.\nThey only discussed the\npre-training for the base model,\nand they did not discuss anything\non research and ablations.\nAnd they do not talk\nabout any of the resources\nthat are shared in terms of, hey,\nthe fund is using all these GPUs.\nAnd we know that they're very profitable\nin that 10,000 GPUs in 2021.", "mimetype": "text/plain", "start_char_idx": 55304, "end_char_idx": 59150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "078829e7-a738-45c2-80df-d3fc1fbd0bb6": {"__data__": {"id_": "078829e7-a738-45c2-80df-d3fc1fbd0bb6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa6c019e-6cd6-4610-8d5e-53ffdc0e5518", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2146f55fa2bb4d026fbee67888d6bee5bdd6232417c1a7b75962f28b30c53102", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bb39904-be6f-4716-a8a0-2de5c3cbfacd", "node_type": "1", "metadata": {}, "hash": "e312962f19f74a8e9eb3ae9994da37b9f7f89c5606e38b460662b23190db10c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Research gets you breakthrough,\nso you need to bet on it.\n- So, some of the pricing strategy\nthat we'll discuss has the\nresearch baked into the price.\n- So, the numbers that DeepSeek\nspecifically said publicly\nare just the 10,000 GPUs in 2021,\nand then 2,000 GPUs for only\nthe pre-training for V3.\nThey did not discuss cost on R1.\nThey did not discuss\ncost on all the other RL,\nfor the instruct model that they made.\nThey only discussed the\npre-training for the base model,\nand they did not discuss anything\non research and ablations.\nAnd they do not talk\nabout any of the resources\nthat are shared in terms of, hey,\nthe fund is using all these GPUs.\nAnd we know that they're very profitable\nin that 10,000 GPUs in 2021.\nSo, some of the research that we've found\nis that we actually believe\nthey have closer to 50,000 GPUs.\n- We, as SemiAnalysis,\nso we should say\n- Yeah.\n- that you're one of the\nworld experts in figuring out\nwhat everybody's doing in\nterms of the semiconductor,\nin terms of cluster buildouts,\nin terms of who's doing what\nin terms of training runs.\nSo, yeah. So, that's the we.\nOkay, go ahead.\n- Yeah, sorry, sorry.\nWe believe they actually\nhave something closer\nto 50,000 GPUs.\n- Yeah.\n- [Dylan] Now, this is\nsplit across many tasks.\nAgain, the fund, research and ablations.\n- For ballpark, how much\nwould OpenAI or Anthropic had?\nI think the clearest example we have,\nbecause Meta is also open,\nthey talk about order of 60k\nto a 100k H100 equivalent GPUs\nin their training clusters.\n- Right. So, like Llama 3,\nthey trained on 16,000 H100s.\nBut the company of Meta last year\npublicly disclosed they bought\nlike 400 something thousand GPUs.\n- Yeah. (chuckles)\n- So, of course,\ntiny percentage on the training.\nAgain, most of it is like\nserving me the best\nInstagram reels, or whatever.\n- We could get into a cost\nof like, what is the cost\nof ownership for a\n2,000-GPU cluster, 10,000?\nThere's just different sizes of companies\nthat can afford these things,\nand DeepSeek is reasonably big.\nTheir compute allocation compared\nis one of the top few in the world.\nIt's not OpenAI, Anthropic, et cetera,\nbut they have a lot of compute.\n- Can you in general,\nactually just zoom out\nand also talk about the\nHopper architecture,\nthe Nvidia Hopper GPU architecture,\nand the difference between H100 and H800,\nlike you mentioned the interconnects.\n- Yeah, so Ampere was the\nA100, and then H100, Hopper.\nPeople use them synonymously in the US\nbecause really, there's just\nH100 and now there's H200.\nBut same thing mostly.\nIn China, they've had two,\nthere've been different\nsalvos of export restrictions.\nSo, initially, the US government\nlimited on a two-factor scale,\nwhich is chip interconnect versus FLOPS.\nSo, any chip that had\ninterconnects above a certain level\nand FLOPS above a certain\nfloating point operations,\nabove a certain level, was restricted.\nLater, the government realized\nthat this was a flaw in the restriction,\nand they cut it down to just\nfloating point operations.\nAnd so-\n- H800 had high FLOPS,\nlow communication.\n- Exactly.\nSo, the H800 was the same\nperformance as H100 on FLOPS,\nbut it just had the\ninterconnect bandwidth cut.\nDeepSeek knew how to utilize this res...\nHey, even though we're cut\nback on the interconnect,\nwe can do all this fancy stuff\nto figure out how to use\nthe GPU fully anyways.\nAnd so, that was back in October, 2022.\nBut later in 2023, into\n2023 implemented in 2024,\nthe US government banned the H800.\nAnd so, by the way, this H800 cluster,\nthese 2,000 GPUs was not\neven purchased in 2024.\nIt was purchased in late 2023.\n- [Lex] Mm-hmm.\n- And they're just\ngetting the model out now,\nbecause it takes a lot\nof research, et cetera.", "mimetype": "text/plain", "start_char_idx": 58430, "end_char_idx": 62113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9bb39904-be6f-4716-a8a0-2de5c3cbfacd": {"__data__": {"id_": "9bb39904-be6f-4716-a8a0-2de5c3cbfacd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "078829e7-a738-45c2-80df-d3fc1fbd0bb6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b7e2a8eacccbf2b38d2ec895d41a39112e5517b2b2b0b1d0399185a83f450fe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1d2149b-71e9-4137-980e-f40a6d0ae969", "node_type": "1", "metadata": {}, "hash": "f225069028ca232e75dc18e56cae60bd2eeaf26dcdf845a99064355d2571bcd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so-\n- H800 had high FLOPS,\nlow communication.\n- Exactly.\nSo, the H800 was the same\nperformance as H100 on FLOPS,\nbut it just had the\ninterconnect bandwidth cut.\nDeepSeek knew how to utilize this res...\nHey, even though we're cut\nback on the interconnect,\nwe can do all this fancy stuff\nto figure out how to use\nthe GPU fully anyways.\nAnd so, that was back in October, 2022.\nBut later in 2023, into\n2023 implemented in 2024,\nthe US government banned the H800.\nAnd so, by the way, this H800 cluster,\nthese 2,000 GPUs was not\neven purchased in 2024.\nIt was purchased in late 2023.\n- [Lex] Mm-hmm.\n- And they're just\ngetting the model out now,\nbecause it takes a lot\nof research, et cetera.\nH800 was banned and now there's\na new chip called the H20.\nThe H20 is cutback on only FLOPS,\nbut the interconnect\nbandwidth is the same.\nAnd in fact, in some ways,\nit's better than the H100\nbecause it has better memory\nbandwidth and memory capacity.\nSo, there are, Nvidia's\nworking within the constraints\nof what the government sets,\nand then builds the best\npossible GPU for China.\n- Can we take this actual tangent\nand we'll return back to the hardware?\nIs the philosophy,\nthe motivation, the case for\nexport controls, what is it?\nDario Amodei just published a blog post\nabout export controls.\nThe case he makes is that if\nAI becomes super powerful,\nand he says by 2026, will\nhave AGI or super powerful AI,\nand that's going to give a\nsignificant, whoever builds that\nwill have a significant\nmilitary advantage.\nAnd so, because the United\nStates is a democracy,\nand as he says, China is authoritarian\nor has authoritarian elements,\nyou want a unipolar world where\nthe super powerful military,\nbecause of the AI, is\none that's a democracy.\nIt's a much more complicated\nworld geopolitically\nwhen you have two superpowers\nwith super powerful AI\nand one is authoritarian.\nSo, that's the case he makes.\nAnd so, we wanna, the United States\nwants to use export controls to slow down,\nto make sure that China\ncan't do these gigantic training runs\nthat will be presumably\nrequired to build AGI.\n- This is very abstract.\nI think this can be the goal\nof how some people\ndescribe export controls,\nis this super powerful AI.\nAnd you touched on the training run idea.\nThere's not many worlds where\nChina cannot train AI models.\nI think export controls are\ndecapping the amount of compute\nor the density of compute\nthat China can have.\nAnd if you think about the\nAI ecosystem right now,\nas all of these AI companies,\nrevenue numbers are up and to the right,\ntheir AI usage is just continuing to grow,\nmore Deep user going to inference.\nA large part of export\ncontrols, if they work,\nis just that the amount of AI\nthat can be run in China\nis going to be much lower.\nSo, on the training side,\nDeepSeek-V3 is a great example,\nwhich you have a very focused team\nthat can still get to the frontier of AI.\nThis 2,000 GPUs is not that hard to get,\nall considering in the world.\nThey're still gonna have those GPUs,\nthey're still gonna be\nable to train models.\nBut if there's gonna be\na huge market for AI,\nif you have strong export controls\nand you wanna have 100,000 GPUs\njust serving the equivalent\nof ChatGPT clusters,\nwith good export controls,\nit also just makes it so that\nAI can be used much less.\nAnd I think that is a much easier goal\nto achieve than trying\nto debate on what AGI is.\nAnd if you have these extremely\nintelligent autonomous AIs\nand data centers, those are the things\nthat could be running\nin these GPU clusters\nin the United States, but not in China.\n- To some extent, training a\nmodel does effectively nothing.\nLike have a model.\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 61423, "end_char_idx": 65055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1d2149b-71e9-4137-980e-f40a6d0ae969": {"__data__": {"id_": "a1d2149b-71e9-4137-980e-f40a6d0ae969", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bb39904-be6f-4716-a8a0-2de5c3cbfacd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b39af020cffca7471e9144373ab0e78383dfaa9465b9957eb3041ef6f166d6fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58094209-3599-47f0-80c3-7aa23f88ee6e", "node_type": "1", "metadata": {}, "hash": "e33543d42f929d72e40ac28d90b2ae4756de91c89acb02048734b2735bcccfc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This 2,000 GPUs is not that hard to get,\nall considering in the world.\nThey're still gonna have those GPUs,\nthey're still gonna be\nable to train models.\nBut if there's gonna be\na huge market for AI,\nif you have strong export controls\nand you wanna have 100,000 GPUs\njust serving the equivalent\nof ChatGPT clusters,\nwith good export controls,\nit also just makes it so that\nAI can be used much less.\nAnd I think that is a much easier goal\nto achieve than trying\nto debate on what AGI is.\nAnd if you have these extremely\nintelligent autonomous AIs\nand data centers, those are the things\nthat could be running\nin these GPU clusters\nin the United States, but not in China.\n- To some extent, training a\nmodel does effectively nothing.\nLike have a model.\n- Yeah. (chuckles)\n- The thing that Dario is\nspeaking to is the implementation\nof that model once trained\nto then create huge economic growth,\nhuge increases in military capabilities,\nhuge increases in productivity of people,\nbetterment of lives, whatever you want\nto direct super powerful\nAI towards, you can.\nBut that requires a\nsignificant amounts of compute.\nAnd so, the US government\nhas effectively said,\nand forever, like training\nwill always be a portion\nof the total compute.\nWe mentioned Meta's 400,000\nGPUs, only 16,000 made Llama.\nSo, the percentage that Meta\nis dedicating to inference,\nnow this might be for\nrecommendation systems\nthat are trying to hack our\nmind into spending more time\nand watching more ads, or if\nit's for a super powerful AI\nthat's doing productive things,\ndoesn't matter about the exact use\nthat our economic system decides,\nit's that that can be delivered\nin whatever way we want.\nWhereas with China, export\nrestrictions, great.\nYou're never gonna be able\nto cut everything off.\nAnd that's like, I think\nthat's quite a well-understood\nby the US government is that\nyou can't cut everything off.\n- [Nathan] And they'll\nmake their own chips.\nThey're-\n- And they're trying\nto make their own chips.\nThey'll be worse than ours.\nBut the whole point is to just keep a gap.\n- Yeah.\n- And therefore,\nat some point, as the AI,\nin a world where 2, 3% economic growth,\nthis is really dumb, by the way,\nto cut off high tech and\nnot make money off of it.\nBut in a world where super\npowerful AI comes about,\nand then starts creating\nsignificant changes in society,\nwhich is what all the AI leaders\nand big tech companies believe, I think.\nSuper powerful AI is gonna\nchange society massively.\nAnd therefore, this compounding effect\nof the difference in\ncompute is really important.\nThere's some sci-fi out\nthere, where AI is measured\nin how much power is delivered to compute.\nOr how much is being...\nThat's a way of thinking about\nwhat's the economic output\nis just how much power are\nyou directing towards that AI?\nShould we talk about\nreasoning models with this?\nAs a way that this might be actionable\nas something that people can actually see?\nSo, the reasoning models that\nare coming out with R1 and o1,\nthey're designed to use more compute.\nThere's a lot of buzzy words\nin the AI community about this,\ntest-time compute, inference\ntime compute, whatever.\nBut Dylan has good research on this.\nYou can get to the specific\nnumbers on the ratio\nof when you train a model,\nyou can look at things\nabout the amount of\ncompute used at training\nand amount of compute use inference.\nThese reasoning models\nare making inference\nway more important to doing complex tasks.\nIn the fall in December,\ntheir OpenAI announced this o3 model.\nThere's another thing in\nAI, when things move fast,\nwe get both announcements and releases.\nAnnouncements are essentially blog posts\nwhere you pat yourself on the back\nand you say you did things.\nAnd releases are on the models out there,\nthe papers out there, et cetera.\nSo, OpenAI has announced o3,\nand we can check if o3-mini is out\nas of recording potentially,\nbut that doesn't really change the point,\nwhich is that the breakthrough result\nwas something called ARC-AGI task,\nwhich is the abstract reasoning corpus,\na task for artificial\ngeneral intelligence.", "mimetype": "text/plain", "start_char_idx": 64300, "end_char_idx": 68359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "58094209-3599-47f0-80c3-7aa23f88ee6e": {"__data__": {"id_": "58094209-3599-47f0-80c3-7aa23f88ee6e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1d2149b-71e9-4137-980e-f40a6d0ae969", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f9f98360f17671d90bedcf38bfcba01a704c24eb09a2a3a41030e68748807bf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9747cf7b-c50c-4e74-8f6d-fde50ab03a47", "node_type": "1", "metadata": {}, "hash": "32f5e553c972e2bd107ae78fcd289d861480cf187ca5c7acb7be34539ddcdfe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You can get to the specific\nnumbers on the ratio\nof when you train a model,\nyou can look at things\nabout the amount of\ncompute used at training\nand amount of compute use inference.\nThese reasoning models\nare making inference\nway more important to doing complex tasks.\nIn the fall in December,\ntheir OpenAI announced this o3 model.\nThere's another thing in\nAI, when things move fast,\nwe get both announcements and releases.\nAnnouncements are essentially blog posts\nwhere you pat yourself on the back\nand you say you did things.\nAnd releases are on the models out there,\nthe papers out there, et cetera.\nSo, OpenAI has announced o3,\nand we can check if o3-mini is out\nas of recording potentially,\nbut that doesn't really change the point,\nwhich is that the breakthrough result\nwas something called ARC-AGI task,\nwhich is the abstract reasoning corpus,\na task for artificial\ngeneral intelligence.\nFrancois Chollet is the guy who's been...\nIt's a multi-year old paper.\nIt's a brilliant benchmark.\nAnd the number for OpenAI o3 to solve this\nwas that it used as some\nnumber of samples in the API,\nthe API has thinking effort\nand number of samples.\nThey used a thousand\nsamples to solve this task.\nAnd it comes out to be\nlike 5 to $20 per question,\nwhich you're putting in\neffectively a math puzzle.\nAnd then, it takes orders of\ndollars to answer one question.\nAnd this is a lot of compute.\nIf those are gonna take off in the US,\nOpenAI needs a ton of GPUs\non inference to capture this.\nThey have this OpenAI\nChatGPT Pro subscription,\nwhich is $200 a month.\n- [Dylan] Which Sam said\nthey're losing money on.\n- Which means that people\nare burning a lot of GPUs on inference.\nAnd I've signed up with\nit, I've played with it.\nI don't think I'm a\npower user, but I use it.\nThat is the thing that a Chinese company\nwith mediumly strong export controls,\nthere will always be loopholes,\nmight not be able to do it all.\nAnd if that, the main result\nfor o3 is also a spectacular\ncoding performance.\nAnd if that feeds back\ninto AI companies being\nable to experiment better.\n- So, presumably the idea is for an AGI,\na much larger fraction of the compute\nwould be used for this test-time\ncompute for the reasoning,\nfor the AGI goes into a room\nand thinks about how\nto take over the world\nand come back in 2.7 hours.\n- This is what-\n- And that it's gonna take\na lot of compute.\n- This is what people\nlike CEO or leaders of OpenAI\nand Anthropic talk about is\nlike autonomous AI models,\nwhich is you give them a task\nand they work on it in the background.\nI think my personal definition\nof AGI is much simpler.\nI think language models are a form of AGI\nand all of this super\npowerful stuff is a next step\nthat's great if we get these tools.\nBut a language model has so\nmuch value in so many domains.\nIt is a general intelligence to me.\nBut this next step of agentic things\nwhere they're independent\nand they can do tasks that\naren't in the training data\nis what the few year outlook\nthat these AI companies are driving for.\n- I think the terminology here\nthat Dario uses as super powerful AI.\nSo, I agree with you on the AGI.\nI think we already have something\nlike that's exceptionally impressive\nthat Alan Turing would\nfor sure say is AGI.\nBut he's referring more to\nsomething once in possession of,\nthan you would have a significant military\nand geopolitical advantage\nover other nations.\nSo, it's not just like you can\nask it how to cook an omelet.\n- And he has a much more positive view.\nAnd as I say, machines of love and grace.\n- Yes.\n- I've read into this.\nI don't have enough background\nin physical sciences\nto gauge exactly how confident I am\nin if AI can revolutionize biology.\nI am safe saying that AI is\ngoing to accelerate the progress\nof any computational science.\n- So, we're doing a depth-first\nsearch here on topics,\ntaking tangent of a tangent.\nSo, let's continue on\nthat depth-first search.\nYou said that you're both feeling the AGI,\nso what's your timeline?", "mimetype": "text/plain", "start_char_idx": 67466, "end_char_idx": 71421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9747cf7b-c50c-4e74-8f6d-fde50ab03a47": {"__data__": {"id_": "9747cf7b-c50c-4e74-8f6d-fde50ab03a47", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58094209-3599-47f0-80c3-7aa23f88ee6e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7495962d8221242d6ea6cc1d523eca3baf7553cad97769ea4604a567bd27c2a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b", "node_type": "1", "metadata": {}, "hash": "1d97238d5b380893833605db65c87c3b81e7170946681ba26ee40b6afc9b6db2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But he's referring more to\nsomething once in possession of,\nthan you would have a significant military\nand geopolitical advantage\nover other nations.\nSo, it's not just like you can\nask it how to cook an omelet.\n- And he has a much more positive view.\nAnd as I say, machines of love and grace.\n- Yes.\n- I've read into this.\nI don't have enough background\nin physical sciences\nto gauge exactly how confident I am\nin if AI can revolutionize biology.\nI am safe saying that AI is\ngoing to accelerate the progress\nof any computational science.\n- So, we're doing a depth-first\nsearch here on topics,\ntaking tangent of a tangent.\nSo, let's continue on\nthat depth-first search.\nYou said that you're both feeling the AGI,\nso what's your timeline?\n(Nathan chuckles)\nDario's 2026 for the super powerful AI\nthat's basically agentic to a degree,\nwhere it's a real security\nthreat, that level of AGI.\nWhat's your timeline?\n- I don't like to attribute\nspecific abilities\nbecause predicting specific\nabilities and when is very hard.\nI think mostly if you're gonna say\nthat I'm feeling the AGI\nis that I expect continued\nrapid surprising progress\nover the next few years.\nSo, something like R1 is less surprising\nto me from DeepSeek,\nbecause I expect there to be new paradigms\nwhere substantial progress can be made.\nI think DeepSeek-R1 is so unsettling,\nbecause we're on this path with ChatGPT,\nit's like it's getting better,\nit's getting better, it's getting better.\nAnd then, we have a new direction\nfor changing the models.\nAnd we took one step like\nthis and we took a step up.\nSo, it looks like a really fast slope.\nAnd then, we're gonna\njust take more steps.\nSo, this is really unsettling\nwhen you have these big steps.\nAnd I expect that to keep happening.\nI've tried OpenAI Operator,\nI've tried Claude computer\nuse, they're not there yet.\nI understand the idea,\nbut it's just so hard to\npredict what is the breakthrough\nthat'll make something like that work.\nAnd I think it's more likely\nthat we have breakthroughs that work\nand things that we don't\nknow what they're gonna do.\nSo, everyone wants agents.\nDario has a very eloquent\nway of describing this.\nAnd I just think that it's like,\nthere's gonna be more than that.\nSo, just expect these things to come.\n- I'm gonna have to try to pin you down\nto a date on the AGI timeline, (chuckles)\nlike the nuclear weapon moment.\nSo, moment where on\nthe geopolitical stage.\nThere's a real, like,\n'cause we're talking\nabout export controls,\nwhen do you think just\neven to throw out a date,\nwhen do you think that would be?\nFor me, it's probably after 2030,\nso I'm not as-\n- That's what I would say.\n- So, define that.\nBecause to me, it almost\nhas already happened.\nYou look at elections\nin India and Pakistan,\npeople get AI voice calls,\nand think they're talking\nto the politician.\nThe AI diffusion rules,\nwhich was enacted in the last couple weeks\nof the Biden admin and looks\nlike the Trump admin will keep,\nand potentially even strengthen,\nlimit cloud computing and GPU sales\nto countries that are not even\nrelated to China, it's like.\nThis is-\n- Portugal\nand all these like normal countries\nare on the-\n- Yeah, and it's like-\n- You need approval from the US list.\n- Like, yeah, Portugal\nand like all these\ncountries that are allies.\n- Yup.\n- Singapore.\nThey freaking have F-35s and\nwe don't let them buy GPUs.\n- Mm-hmm.\n- This to me\nis already to the scale of like-\n- Well, that just means\nthat the US military\nis really nervous about\nthis new technology.\nThat doesn't mean the\ntechnology is already there.\nSo, they might be just very cautious\nabout this thing that they\ndon't quite understand.\nBut that's a really good\npoint, the robocalls,\nswarms of semi-intelligent\nbots could be a weapon,\ncould be doing a lot\nof social engineering.\n- There's tons of talk about,\nfrom the 2016 elections\nlike Cambridge Analytica,\nand all this stuff, Russian influence.", "mimetype": "text/plain", "start_char_idx": 70685, "end_char_idx": 74575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b": {"__data__": {"id_": "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9747cf7b-c50c-4e74-8f6d-fde50ab03a47", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "542b880cfe2e4aadd5e9fd1e91ec323bf78f248565df53d11c1ed1915e2e64b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "026354a5-e824-4b8e-bfe4-70d9a12c98ee", "node_type": "1", "metadata": {}, "hash": "4566cd35ad117b11036f4f0af8a02bd309e6ca69246aa3fcd2b3570638595338", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Like, yeah, Portugal\nand like all these\ncountries that are allies.\n- Yup.\n- Singapore.\nThey freaking have F-35s and\nwe don't let them buy GPUs.\n- Mm-hmm.\n- This to me\nis already to the scale of like-\n- Well, that just means\nthat the US military\nis really nervous about\nthis new technology.\nThat doesn't mean the\ntechnology is already there.\nSo, they might be just very cautious\nabout this thing that they\ndon't quite understand.\nBut that's a really good\npoint, the robocalls,\nswarms of semi-intelligent\nbots could be a weapon,\ncould be doing a lot\nof social engineering.\n- There's tons of talk about,\nfrom the 2016 elections\nlike Cambridge Analytica,\nand all this stuff, Russian influence.\nEvery country in the world\nis pushing stuff onto the internet\nand has narratives they want.\nEvery technically competent,\nwhether it's Russia, China,\nUS, Israel, et cetera.\nPeople are pushing viewpoints\nonto the internet, and mass.\nAnd language models\ncrash the cost of very\nintelligent sounding\nlanguage.\n- There's some research\nthat shows that the distribution\nis actually a limiting factor.\nSo, language models haven't\nyet made misinformation,\nparticularly changed the equation there.\nThe internet is still ongoing.\nI think there's a blog, AI Snake Oil,\nand some of my friends at Princeton\nthat write on this stuff.\nSo, there is research.\nIt's a default that everyone assumes.\nAnd I would've thought the same thing\nis that misinformation\ndoesn't gonna get far\nworse with language models.\nI think in terms of internet posts\nand things that people\nhave been measuring,\nit hasn't been a exponential increase\nor something extremely measurable,\nand things you're talking about\nwith voice calls and stuff like that.\nIt could be in modalities\nthat are harder to measure.\nSo, it's something that it's\ntoo soon to tell in terms of,\nI think that's political\ninstability via the web is very,\nit's monitored by a lot\nof researchers to see what's happening.\nYou're asking about the AGI thing.\nIf you make me give a year,\nI'm be like, okay, I\nhave AI CEOs saying this.\nThey've been saying two years\nfor a while.\n- Mm-hmm.\n- I think that they're people\nlike Dario at Anthropic,\nthe CEO has thought about this so deeply.\nI need to take their word seriously,\nbut also understand that they\nhave different incentives.\nSo, I would be like,\nadd a few years to that,\nwhich is how you get\nsomething similar to 2030\nor a little after 2030.\n- I think to some extent,\nwe have capabilities\nthat hit a certain point where\nany one person could say,\noh, okay, if I can\nleverage those capabilities\nfor X amount of time, this is AGI.\nCall it '27, '28.\nBut then the cost of actually\noperating that capability\n- Yeah, this is gonna be\n- is\n- my point. (chuckles)\n- so, so extreme\nthat no one can actually\ndeploy it at scale\nand mass to actually completely\nrevolutionize the economy\non a snap of a finger.\nSo, I don't think it will be like\na snap of the finger moment.\n- Yeah, it's a physical\nconstraint type-\n- Rather it'll be a,\noh, the capabilities are here,\nbut I can't deploy it everywhere.\nAnd so, one simple\nexample going back to 2023\nwas when being with GPT-4 came out\nand everyone was freaking\nout about Search.\n- Oh gosh.\n- Perplexity came out.\nIf you did the cost on like, hey,\nimplementing GPT-3 into\nevery Google search,\nit was like, oh, okay,\nthis is just like physically\nimpossible to implement.\nAnd as we step forward\nto going back to the test-time\ncompute thing, a query for...\nYou asked ChatGPT a\nquestion, it cost cents\nfor their most capable model\nof chat to get a query back.\nTo solve an ARC-AGI problem\nthough, cost 5 to 20 bucks.\nAnd this is an a-\n- [Nathan] It's only going up from there.\n- This is 1,000, 10,000x\nfactor difference in cost\nto respond to a query versus do a task.", "mimetype": "text/plain", "start_char_idx": 73884, "end_char_idx": 77646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "026354a5-e824-4b8e-bfe4-70d9a12c98ee": {"__data__": {"id_": "026354a5-e824-4b8e-bfe4-70d9a12c98ee", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ac3c589-3b07-49fe-9a19-1b89c5b4c65b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5740a09b9aed1f0feab3303f27840ed13abd14c231083de346b526f423df3637", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7da756d-9189-4731-9b2a-ad6cc77abefd", "node_type": "1", "metadata": {}, "hash": "8fd03b07b2045104319709531e32e1ad8a19f9d5bb09d30b3edd3a4ac721bdc3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Oh gosh.\n- Perplexity came out.\nIf you did the cost on like, hey,\nimplementing GPT-3 into\nevery Google search,\nit was like, oh, okay,\nthis is just like physically\nimpossible to implement.\nAnd as we step forward\nto going back to the test-time\ncompute thing, a query for...\nYou asked ChatGPT a\nquestion, it cost cents\nfor their most capable model\nof chat to get a query back.\nTo solve an ARC-AGI problem\nthough, cost 5 to 20 bucks.\nAnd this is an a-\n- [Nathan] It's only going up from there.\n- This is 1,000, 10,000x\nfactor difference in cost\nto respond to a query versus do a task.\nAnd the task of ARC-AGI, it's not like,\nit's simple to some extent,\nbut it's also like what are\nthe tasks that we want A...\nOkay, AGI, quote, unquote,\n\"what we have today\" can do ARC-AGI.\nThree years from now,\nit can do much more complicated problems,\nbut the cost is gonna be measured\nin thousands and thousands\nand hundreds of thousands\nof dollars of GPU time,\nand there just won't be enough power,\nGPUs, infrastructure\n- Yeah.\n- to operate this, and therefore,\nshift everything in the\nworld on the snap the finger.\nBut at that moment,\nwho gets to control and\npoint the AGI at a task.\nAnd so, this was in Dario's\npost that he's like,\nhey, China can effectively\nand more quickly than us, point\ntheir AGI at military tasks.\nAnd they have been in many ways,\nfaster at adopting\ncertain new technologies\ninto their military, especially\nwith regards to drones.\nThe US maybe has a long-standing\nlarge air sort of fighter\njet type of thing, bombers,\nbut when it comes to\nasymmetric arms such as drones,\nthey've completely leapfrogged\nthe US and the west.\nAnd the fear that Dario\nis pointing out there,\nI think, is that, yeah, great,\nwe'll have AGI in the commercial sector.\nThe US military won't be able\nto implement it super fast.\nChinese military could\nand they could direct all their resources\nto implementing it in the military,\nand therefore solving military logistics\nor solving some other\naspect of disinformation\nfor targeted certain set of people,\nso that they can flip\na country's politics,\nor something like that, that\nis actually like catastrophic\nversus the US just wants to,\n'cause it'll be more\ncapitalistic allocated\njust towards whatever\n- Mm.\n- is the highest return on income,\nwhich might be like building\nfactories better or whatever.\n- So, everything I've seen,\npeople's intuition seems\nto fail on robotics.\nSo, you have this kind\nof general optimism.\nI've seen this on self-driving cars.\nPeople think it's much\neasier problem than it is.\nSimilar with drones.\nHere, I understand it a little bit less,\nbut I've just seen the\nreality of the war in Ukraine\nand the usage of drones on both sides.\nAnd it seems that humans\nstill far outperform any\nfully autonomous systems.\nAI is an assistant, but\nhumans drive FPV drones\nwhere the humans controlling most of it\njust far, far, far outperforms AI systems.\nSo, I think it's not obvious to me\nthat we're going to have swarms\nof autonomous robots anytime\nsoon in the military context.\nMaybe the fastest I can imagine is 2030,\nwhich is why I said 2030\nfor the super powerful AI.\nWhenever you have large-scale swarms\nof robots doing military actions,\nthat's when the world just\nstarts to look different to me.\nSo, that's the thing I'm\nreally worried about.\nBut there could be cyber\nwar type of technologies\nthat from social engineering,\nto actually just swarms the\nrobots that find attack vectors\nin our code bases,\nand shut down power\ngrids, that kind of stuff.\nAnd it could be one of those things\nlike on any given weekend or something,\npower goes out, nobody knows why,\nand the world changes forever.\nJust power going out for two days\nin all of the United States,\nthat will lead to murder to chaos.\nBut going back to export controls,\ndo you see that as a useful way\nto control the balance\nof power geopolitically\nin the context of AI?", "mimetype": "text/plain", "start_char_idx": 77064, "end_char_idx": 80938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7da756d-9189-4731-9b2a-ad6cc77abefd": {"__data__": {"id_": "f7da756d-9189-4731-9b2a-ad6cc77abefd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "026354a5-e824-4b8e-bfe4-70d9a12c98ee", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1252654d1f877a9e0fc31641808744e49d6ff4131c673e8a42a872919721fdb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d167bfb7-3bbb-4470-b063-1e1d8491e56c", "node_type": "1", "metadata": {}, "hash": "39fd8209b55f7347a8cfd7a1efa2d2fc80d6a970c585402ce7d217ba718be756", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Whenever you have large-scale swarms\nof robots doing military actions,\nthat's when the world just\nstarts to look different to me.\nSo, that's the thing I'm\nreally worried about.\nBut there could be cyber\nwar type of technologies\nthat from social engineering,\nto actually just swarms the\nrobots that find attack vectors\nin our code bases,\nand shut down power\ngrids, that kind of stuff.\nAnd it could be one of those things\nlike on any given weekend or something,\npower goes out, nobody knows why,\nand the world changes forever.\nJust power going out for two days\nin all of the United States,\nthat will lead to murder to chaos.\nBut going back to export controls,\ndo you see that as a useful way\nto control the balance\nof power geopolitically\nin the context of AI?\n- And I think going back to my viewpoint\nis if you believe we're in\nthis stage of economic growth\nand change that we've been\nin for the last 20 years,\nthe export controls\nare absolutely guaranteeing\nthat China will win long-term,\nif you do not believe AI\nis going to make significant changes\nto society in the next\n10 years or 5 years.\nFive-year timelines are\nwhat the more executives\nand such of AI companies\nand even big tech companies believe.\nBut even tenure timelines,\nit's reasonable.\nBut once you get to, hey,\nthese timelines are\nbelow that time period,\nthen the only way to\ncreate a sizable advantage\nor disadvantage for America\nversus China is if you constrain compute.\nBecause talent is not really\nsomething that's constraining.\nChina arguably has more talent,\nmore STEM graduates, more programmers.\nThe US can draw upon the\nworld's people, which it does.\nThere's tons of foreigners\nin the AI industry-\n- [Nathan] So many of these AI teams\nare all people without a US passport.\n- Yeah.\n(Nathan laughing)\nMany of them are Chinese people\nwho are moving\n- Yeah.\n- to North America, and that's great.\nThat's exactly what we want.\nBut there's that talent is one aspect,\nbut I don't think that's one\nthat is a measurable\nadvantage for the US or not.\nIt truly is just whether or not compute.\nNow, even on the compute side,\nwhen we look at chips versus data centers.\nChina has the unprecedented ability\nto build ridiculous\nsums of power clockwork.\nThey're always building\nmore and more power.\nThey've got steel mills that individually\nare the size of the entire US industry.\nAnd they've got aluminum mills\nthat consume gigawatts\nand gigawatts of power.\nAnd when we talk about what's\nthe biggest data center,\nOpenAI made this huge\nthing about Stargate,\ntheir announcement there,\nthat's like once it's fully\nbuilt out in a few years,\nit'll be two gigawatts of power.\nAnd this is still smaller\nthan the largest industrial\nfacilities in China.\nChina, if they wanted to\nbuild the largest data center\nin the world, if they had\naccess to the chips, could.\nSo, it's just a question of when, not if.\n- So, their industrial capacity\nfar exceeds the United States?\n- [Dylan] Exactly.\n- To the the manufacture stuff.\n- Yeah.\n- So, long-term,\nthey're going to be\nmanufacturing chips there.\n- Chips are a little bit more specialized.\nI'm specifically referring\nto the data centers.\nChips, fabs take huge amounts\nof power, don't get me wrong.\nThat's not necessarily\nthe gating factor there.\nThe gating factor on how fast people\ncan build the largest clusters\ntoday in the US is power.\nWhether it's now it could\nbe power generation,\npower transmission, substations,\nand all these sorts of\ntransformers and all these things,\nbuilding the data center,\nthese are all constraints\non the US industry's\nability to build larger\nand larger training systems,\nas well as deploying more\nand more inference compute.\n- I think we need to make the\npoint clear on why the time\nis now for people that\ndon't think about this,\n'cause essentially with export controls,\nyou're making it so China\ncannot make or get cutting edge chips.\nAnd the idea is that\nif you time this wrong,\nChina is pouring a ton of money\ninto their chip production.", "mimetype": "text/plain", "start_char_idx": 80181, "end_char_idx": 84144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d167bfb7-3bbb-4470-b063-1e1d8491e56c": {"__data__": {"id_": "d167bfb7-3bbb-4470-b063-1e1d8491e56c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7da756d-9189-4731-9b2a-ad6cc77abefd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "452b01882a457bf5bab210b32f40b375ae8664dd6f8ce2df191a4726043262c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b01b4dc1-cf98-4500-90c1-a179a66d3eca", "node_type": "1", "metadata": {}, "hash": "6561a471bef7a6eff950ff5e82e603c64acd4fe26c6053a2497e3238f56975cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I'm specifically referring\nto the data centers.\nChips, fabs take huge amounts\nof power, don't get me wrong.\nThat's not necessarily\nthe gating factor there.\nThe gating factor on how fast people\ncan build the largest clusters\ntoday in the US is power.\nWhether it's now it could\nbe power generation,\npower transmission, substations,\nand all these sorts of\ntransformers and all these things,\nbuilding the data center,\nthese are all constraints\non the US industry's\nability to build larger\nand larger training systems,\nas well as deploying more\nand more inference compute.\n- I think we need to make the\npoint clear on why the time\nis now for people that\ndon't think about this,\n'cause essentially with export controls,\nyou're making it so China\ncannot make or get cutting edge chips.\nAnd the idea is that\nif you time this wrong,\nChina is pouring a ton of money\ninto their chip production.\nAnd if you time it wrong,\nthey're going to have more\ncapacity for production,\nmore capacity for energy,\nand figure out how to make the chips\nand have more capacity\nthan the rest of the\nworld to make the chips,\nbecause everybody can buy,\nthey're gonna sell their\nChinese chips to everybody.\nThey might subsidize them.\nAnd therefore, if AI takes a long time\nto become differentiated,\nwe've kneecapped the financial performance\nof American companies.\nNvidia can sell less.\nTSMC cannot sell to China.\nSo, therefore, we have less demand\nto therefore to keep driving\nthe production cycle.\nSo, that's the assumption\nbehind the timing\nbeing important.\n- Less than 10 years\nor 5 years to above.\nChina will win because of\nthese restrictions long-term,\nunless AI does something\nin the short-term,\nwhich I believe AI will do,\nmake massive changes to society\nin the medium short-term.\nAnd so, that's the big unlocker there.\nAnd even today,\nif Xi Jinping decided to get,\nquote, unquote, \"scale pilled\",\ni.e, decide that scaling\nlaws are what matters,\njust like the US executives\nlike Satya Nadella\nand Mark Zuckerberg, and Sundar,\nand all these US\nexecutives of the biggest,\nmost powerful tech companies,\nhave decided they're scale pilled\nand they're building\nmulti-gigawatt data centers.\nWhether it's in Texas or\nLouisiana, or Wisconsin,\nwherever it is, they're\nbuilding these massive things\nthat cost as much as their entire budget\nfor spending on data centers\nglobally in one spot.\nThis is what they've committed to\nfor next year, year after, et cetera.\nAnd so, they're so convinced\nthat this is the way that\nthis is what they're doing.\nBut if China decided to, they\ncould do it faster than us.\nBut this is where the\nrestrictions come in.\nIt is not clear that China\nas a whole has decided,\nfrom the highest levels\nthat this is a priority.\nThe US has.\nYou see Trump talking about DeepSeek\nand Stargate within the same week.\nAnd the Biden admin as well\nhad a lot of discussions\nabout AI and such.\nIt's clear that they think about it.\nOnly just last week\ndid DeepSeek meet the\nsecond in command of China.\nThey have not even met the top,\nand haven't met Xi, she hasn't set down.\nAnd they only just released a subsidy\nof a trillion RMB, roughly $160 billion,\nwhich is closer to the spending\nof like Microsoft and Meta,\nand Google combined for this year.\nSo, it's like they're\nrealizing it just now.\nBut that's where these\nexport restrictions come in\nand say, hey, you can't ship\nthe most powerful US chips to China.\nYou can ship a cut down version.\nYou can't ship the most powerful chips\nto all these countries\nwho we know are just\ngonna rent it to China.\nYou have to limit the numbers.\n- [Nathan] And the tools.\nSame-\n- And same\nwith manufacturing equipment tools,\nall these different aspects,\nbut it all stems from AI,\nand then what downstream\ncan slow them down in AI.\nAnd so, the entire\nsemiconductor restrictions,\nyou read them, they're very clear,\nit's about AI and military\ncivil fusion of technology.\nIt's very clear.\nAnd then, from there, it goes, oh,\nwell, we're banning them\nfrom buying lithography tools\nand etch tools, and deposition tools.", "mimetype": "text/plain", "start_char_idx": 83261, "end_char_idx": 87282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b01b4dc1-cf98-4500-90c1-a179a66d3eca": {"__data__": {"id_": "b01b4dc1-cf98-4500-90c1-a179a66d3eca", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d167bfb7-3bbb-4470-b063-1e1d8491e56c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f00c1e68bb862c0595c5aeffba6b906f2016307a1112d01c4475b9464b6af77e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2dfca25-8512-4aa6-bf41-c19af73a3f0a", "node_type": "1", "metadata": {}, "hash": "5dfbbd7deb3960fad8d2995e95938a5ac076df37c3373d8fcfd3f2ad6504f94f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, it's like they're\nrealizing it just now.\nBut that's where these\nexport restrictions come in\nand say, hey, you can't ship\nthe most powerful US chips to China.\nYou can ship a cut down version.\nYou can't ship the most powerful chips\nto all these countries\nwho we know are just\ngonna rent it to China.\nYou have to limit the numbers.\n- [Nathan] And the tools.\nSame-\n- And same\nwith manufacturing equipment tools,\nall these different aspects,\nbut it all stems from AI,\nand then what downstream\ncan slow them down in AI.\nAnd so, the entire\nsemiconductor restrictions,\nyou read them, they're very clear,\nit's about AI and military\ncivil fusion of technology.\nIt's very clear.\nAnd then, from there, it goes, oh,\nwell, we're banning them\nfrom buying lithography tools\nand etch tools, and deposition tools.\nAnd, oh, this random subsystem\nfrom a random company that's tiny.\nWhy are we banning this?\nBecause all of it,\nthe US government has decided,\nis critical to AI systems.\n- I think the fulcrum\npoint is the transition\nfrom seven nanometer to\nfive nanometer chips,\nwhere I think it was Huawei\nthat had the seven nanometer\nchip a few years ago,\nwhich caused another political brouhaha\nalmost like this moment.\nAnd then, it's the ASML\ndeep UV. What is that?\nLike extreme ultraviolet lithography.\n- To set context on the chips,\nwhat Nathan's referring to is in 2020,\nHuawei released their Ascend 910 chip,\nwhich was an AI chip, first\none on seven nanometer\nbefore Google did, before Nvidia did.\nAnd they submitted it\nto the MLPerf benchmark,\nwhich is a industry standard\nfor machine learning\nperformance benchmark.\nAnd it did quite well.\nAnd it was the best\nchip at the submission.\nThis was a huge deal.\nThe Trump admin of course\nbanned, it was 2019,\nbanned the Huawei\nfrom getting seven\nnanometer chips from TSMC.\nAnd so, then they had to switch\nto using internal\ndomestically produced chips,\nwhich was a multi-year setback.\n- Many companies have done\nseven nanometer chips.\nAnd the question is like, we\ndon't know how much Huawei\nwas subsidizing production of that chip.\nIntel has made seven nanometer chips\nthat are not profitable,\nand things like this.\nSo, this is how it all feeds back\ninto the economic engine\nof export controls.\n- Well, so you're saying\nthat for now, Xi Jinping\nhas not felt the AGI,\nbut it feels like the DeepSeek moment\n- Yeah.\n- might like,\nthere might be meetings going on now,\nwhere he's gonna start\nwearing the same T-shirt\nand things are gonna escalate.\n(Nathan and Dylan laughing)\n- He may have woken up last week.\nLiang Feng met the second command guy,\nand they had a meeting.\nAnd then, the next day, they\nannounced the AI subsidies,\nwhich are a trillion RMB.\n- So, it's possible that\nthis DeepSeek moment\nis truly the beginning of a cold war.\n- That's what a lot of\npeople are worried about.\nPeople in AI have been worried\nthat this is going towards a cold war\nor already is.\n- But there is,\nit's not DeepSeek's fault,\nbut there's something,\na bunch of factors came together\nwhere it was like explosion.\n- No, history works.\n- It all has to do with Nvidia\nstock going down problem.\nBut it's just some\n(Nathan laughing)\nmass hysteria\n(Lex drowns out Nathan)\nthat happened that eventually led\nto Xi Jinping having meetings\nand waking up to this idea.\n- And the US government\nrealized in October 7th, 2022,\nbefore ChatGPT released,\nthat restriction on October 7th,\nwhich dropped and shocked everyone.\nAnd it was very clearly aimed at AI.\nEveryone was like, what\nthe heck are you doing?\n- And diffusion was out then,\nbut not ChatGPT.\n- Yeah, but not ChatGPT.\n- So, it was like starting\nto be rumblings like-\n- Of what gen AI can do to society,\nbut it was very clear, I think,\nto at least National Security Council\nand those sort of folks\nthat this was where the world is headed,\nthis cold war that's happening.", "mimetype": "text/plain", "start_char_idx": 86483, "end_char_idx": 90315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2dfca25-8512-4aa6-bf41-c19af73a3f0a": {"__data__": {"id_": "b2dfca25-8512-4aa6-bf41-c19af73a3f0a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b01b4dc1-cf98-4500-90c1-a179a66d3eca", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "481201bc247a3ceb05622415b0bfdca56a795becaca9242aa4e6db358be2d083", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6d00942-d574-49ac-9bf0-88a2c505e6c5", "node_type": "1", "metadata": {}, "hash": "f3f38b39c04890e6977d24dd404347099e94db01f345a7ebceeae50f65836560", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- It all has to do with Nvidia\nstock going down problem.\nBut it's just some\n(Nathan laughing)\nmass hysteria\n(Lex drowns out Nathan)\nthat happened that eventually led\nto Xi Jinping having meetings\nand waking up to this idea.\n- And the US government\nrealized in October 7th, 2022,\nbefore ChatGPT released,\nthat restriction on October 7th,\nwhich dropped and shocked everyone.\nAnd it was very clearly aimed at AI.\nEveryone was like, what\nthe heck are you doing?\n- And diffusion was out then,\nbut not ChatGPT.\n- Yeah, but not ChatGPT.\n- So, it was like starting\nto be rumblings like-\n- Of what gen AI can do to society,\nbut it was very clear, I think,\nto at least National Security Council\nand those sort of folks\nthat this was where the world is headed,\nthis cold war that's happening.\n- So, is there any concerns\nthat the export controls\npush China to take\nmilitary action on Taiwan?\n- This is the big risk.\nThe further you push China\naway from having access\nto cutting edge American\nand global technologies,\nthe more likely they are to say,\nwell, 'cause I can't access\nit, I might as well...\nNo one should access it.\nAnd there's a few\ninteresting aspects of that.\nChina has a urban rural\ndivide like no other.\nThey have a male/female\nbirth ratio like no other,\nto the point where, if\nyou look in most of China,\nit's like the ratio's not that bad.\nBut when you look at single\ndudes in rural China,\nit's like a 30 to 1 ratio.\n- Mm-hmm.\n- And those are disenfranchised dudes.\nQuote, unquote, the US\nhas an \"incel problem\"\nlike China does too.\nIt's just they're placated\nin some way or crushed down.\nWhat do you do with these people?\nAnd at the same time, you're not allowed\nto access the most important technology,\nat least the US thinks so.\nChina's maybe starting to think\nthis is the most important technology\nby starting to dump subsidies in it.\nThey thought EVs and renewables\nwere the most important technology.\nThey dominate that now.\nNow, they started thinking\nabout semiconductors\nin the late 2010s and early 2020s,\nand now they've been dumping money\nand they're catching up rapidly,\nand they're gonna do the same with AI.\nBecause they're very talented.\nSo, the question is like,\nwhen does this hit a breaking point?\nAnd if China sees this as,\nhey, they can continue...\nIf not having access and\nstarting a true hot war.\nTaking over Taiwan or trying\nto subvert its democracy\nin some way, or blockading it,\nhurts the rest of the world\nfar more than it hurts them,\nthis is something they\ncould potentially do.\nAnd so, is this pushing them\ntowards that? Potentially.\nI'm not quite a geopolitical person,\nbut it's obvious that\nthe world regime of peace\nand trade is super awesome for economics.\nBut at some point, it could break.\n- I think we should\ncomment why Chinese economy\nwould be hurt by that is\nthat they're export-heavy.\nI think the United States buys so much,\nlike if that goes away,\nthat's how their economy goes.\n- Well, also, they just would not be able\nto import raw materials\nfrom all over the world.\nThe US would just shut\ndown the strait of Malacca\nand at the same time, the US entire,\nyou could argue almost all\nthe GDP growth in America\nsince the '70s has been either\npopulation growth or tech.\nBecause your life today\nis not that much better\nthan someone from the\n'80s outside of tech.\nCars, they all have\nsemiconductors in them everywhere.\nFridges, semiconductors everywhere.\nThese funny stories about how Russians\nwere taking apart laundry machines\nbecause they had certain\nTexas instrument chips\nthat they could then repurpose\nand put into their\nanti-missile missile things,\ntheir S-400 or whatever.\nYou would know more about this,\nbut there's all sorts of,\neverything about semiconductors\nis so integral to every part of our lives.\n- So, can you explain the role of TSMC\nin this story of\nsemiconductors and maybe also\nhow the United States can\nbreak the reliance on TSMC?", "mimetype": "text/plain", "start_char_idx": 89534, "end_char_idx": 93427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6d00942-d574-49ac-9bf0-88a2c505e6c5": {"__data__": {"id_": "d6d00942-d574-49ac-9bf0-88a2c505e6c5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2dfca25-8512-4aa6-bf41-c19af73a3f0a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "95bdd80a99b95771c35465a6c929e0d1f2030dcde31df31c7f9765b83cb3cfd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a91b32d-dfc8-4709-8727-3ea7a092c079", "node_type": "1", "metadata": {}, "hash": "56c6ce58382a84ffb9461d3b9a59ca938bf9abadf6147ca30ce345992c0064d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because your life today\nis not that much better\nthan someone from the\n'80s outside of tech.\nCars, they all have\nsemiconductors in them everywhere.\nFridges, semiconductors everywhere.\nThese funny stories about how Russians\nwere taking apart laundry machines\nbecause they had certain\nTexas instrument chips\nthat they could then repurpose\nand put into their\nanti-missile missile things,\ntheir S-400 or whatever.\nYou would know more about this,\nbut there's all sorts of,\neverything about semiconductors\nis so integral to every part of our lives.\n- So, can you explain the role of TSMC\nin this story of\nsemiconductors and maybe also\nhow the United States can\nbreak the reliance on TSMC?\n- I don't think it's necessarily\nbreaking their alliance.\nI think it's getting\nTSMC to build in the US.\nSo, taking a step back.\nTSMC produces most of the world's chips,\nespecially on the foundry side.\nThere's a lot of companies\nthat build their own\nchips, Samsung, Intel,\nSTMicro, Texas Instruments,\nAnalog Devices,\nall these kinds of companies\nbuild their own chips and XP.\nBut more and more of these companies\nare outsourcing to TSMC and\nhave been for multiple decades.\n- Can you explain the supply chain there\nand where most of TSMC is\nin terms of manufacturing?\n- Sure.\nSo, historically, supply chain was,\ncompanies would build their own chips.\nIt'd be a company-started,\nthey'd build their own chips,\nand then they'd design the chip\nand build the chip, and sell it.\nOver time, this became really difficult\nbecause the cost of building a fab\ncontinues to compound\nevery single generation.\nOf course the technology,\nfiguring out the technology for it\nis incredibly difficult regardless,\nbut just the dollars and cents\nthat are required ignoring,\nsaying, hey, yes, I have all\nthe technical capability,\nwhich it's really hard\nto get that, by the way.\nIntel's failing, Samsung's\nfailing, et cetera.\nBut if you look at just\nthe dollars to spend\nto build that next generation\nfab, it keeps growing.\nSort of like Moore's Laws\nhaving the cost of chips every two years.\nThere's a separate law\nthat's doubling the cost of\nfabs every handful of years.\nAnd so, you look at a leading edge fab\nthat is gonna be profitable today,\nthat's building three nanometer chips\nor two nanometer chips in the future,\nthat's gonna cost north\nof 30, $40 billion.\nAnd that's just for like a token amount.\nThat's like the base building blocking,\nyou probably need to build multiple.\nAnd so, when you look at\nthe industry, over the last,\nif I go back 20, 30\nyears ago, there were 20,\n30 companies that could build\nthe most advanced chips,\nand then they would design\nthem themselves and sell them.\nSo, companies like AMD\nwould build their own chips.\nIntel of course still\nbuilds their own chips\nthey're very famous for.\nIBM would build their own chips.\nAnd you could just keep\ngoing down the list.\nAll these companies built their own chips.\nSlowly, they kept falling like flies.\nAnd that's because of what TSMC did.\nThey created the foundry business model,\nwhich is, I'm not gonna design any chips,\nI'm just gonna contract\nmanufacturer chips for other people.\nAnd one of their early\ncustomers is Nvidia.\nNvidia is the only semiconductor company\nthat's doing more than a\nbillion dollars of revenue\nthat was started in the era of Foundry.\nEvery other company started before then,\nand at some point had fabs,\nwhich is actually incredible.\nLike AMD and Intel and Broadcom-\n- [Nathan] Such a great fact.\n(Nathan drowns out Dylan)\n- It's like everyone\n(Nathan laughing)\nhad fabs at some point,\nor some companies like Broadcom,\nit was like a merger amalgamation\nof various companies that rolled up.\nBut even today, Broadcom has fabs.\nThey build iPhone, RF radio\nchips in Colorado for Apple.\nAll these companies had fabs,\nand for most of the fabs,\nthey threw them away or sold them off,\nor they got rolled into something else.\nAnd now, everyone relies on TSMC.\nIncluding Intel, their latest\nPC chip uses TSMC chips.\nIt also uses some intel chips,\nbut it uses TSMC process.", "mimetype": "text/plain", "start_char_idx": 92746, "end_char_idx": 96764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a91b32d-dfc8-4709-8727-3ea7a092c079": {"__data__": {"id_": "7a91b32d-dfc8-4709-8727-3ea7a092c079", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6d00942-d574-49ac-9bf0-88a2c505e6c5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6c4c844678d0d1889141f8b301ca5ce653f8189aeefd1c4f4f6c010733cc5968", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aa0fc68-3210-40f6-a326-52a5aa701f9d", "node_type": "1", "metadata": {}, "hash": "a5e91fe0046b7736c00f7c80b2c93c3e1b9cd34a823cee435b98d7dc6bb3509b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Every other company started before then,\nand at some point had fabs,\nwhich is actually incredible.\nLike AMD and Intel and Broadcom-\n- [Nathan] Such a great fact.\n(Nathan drowns out Dylan)\n- It's like everyone\n(Nathan laughing)\nhad fabs at some point,\nor some companies like Broadcom,\nit was like a merger amalgamation\nof various companies that rolled up.\nBut even today, Broadcom has fabs.\nThey build iPhone, RF radio\nchips in Colorado for Apple.\nAll these companies had fabs,\nand for most of the fabs,\nthey threw them away or sold them off,\nor they got rolled into something else.\nAnd now, everyone relies on TSMC.\nIncluding Intel, their latest\nPC chip uses TSMC chips.\nIt also uses some intel chips,\nbut it uses TSMC process.\n- Can you explain why the foundry model\nis so successful for these companies?\nWhy are they going\nwith TSMC-\n- Economies of scale.\n- Scale.\n- Yeah.\nSo, like I mentioned, the cost\nof building a fab is so high.\nThe R&D is so difficult.\nAnd when you look at companies\nthat had their own vertical stack,\nthere was an antiquated\nprocess of like, okay,\nI'm so hyper customized\nto each specific chip.\nBut as we've gone through the history\nof the last 50 years of\nelectronics and semiconductors,\nA, you need more and more specialization,\nbecause Moore's Law has died,\nDennard scaling has died.\ni.e, chips are not getting\nbetter just for free.\nFrom manufacturing,\nyou have to make real\narchitectural innovations.\nGoogle is not just running on\nIntel CPUs for web serving.\nThey have a YouTube chip, they have TPUs,\nthey have pixel chips, they\nhave a wide diversity of chips\nthat generate all the\neconomic value of Google.\nIt's running all the services and stuff.\nAnd so, and this is just Google\nand you could go across any\ncompany in the industry,\nand it's like this.\nCars contain 5,000 chips, 200\ndifferent varieties of them.\nAll these random things.\nA Tesla door handle has\ntwo chips. It's ridiculous.\nAnd it's a cool door handle.\nYou don't think about it,\nbut it's like, has two really chipped,\nlike penny chips in there.\nAnyways, so as you have\nmore diversity of chips,\nas you have more specialization required,\nand the cost of fabs continues\nto grow, you need someone\nwho is laser-focused on building\nthe best process technology\nand making it as flexible as possible.\n- I think you could say it simpler,\nwhich is the cost per fab goes up.\nAnd if you are a small player,\nthat makes a few types of chips.\nYou're not gonna have the demand\nto pay back the cost of the fab.\nWhereas Nvidia can have\nmany different customers\nand aggregate all this\ndemand into one place,\nand then they're the only\nperson that makes enough money\nbuilding chips to buy the\nnext, to build the next fab.\nSo, this is why the\ncompanies slowly get killed,\n'cause they have 10 years ago a chip\nthat is profitable and is good enough,\nbut the cost to build\nthe next one goes up.\nThey may try to do this, fail,\nbecause they don't have\nthe money to make it work,\nand then they don't have any chips.\nOr they build it and it's too\nexpensive and they just have\nnot profitable chips.\n- Or there's more\nfailure points of, you could\nhave one little process-related\nto some sort of chemical etch\nor some sort of plasma etch,\nor some little process that screws up,\nyou didn't engineer it right,\nand now the whole company falls\napart, you can't make chips.\nAnd so, super, super powerful\ncompanies like Intel,\nthey had the weathering\nstorm to like, hey,\nthey still exist today, even\nthough they really screwed up\ntheir manufacturing six, seven years ago.\nBut in the case of like AMD,\nthey almost went bankrupt.\nThey had to sell their\nfabs to Mubadala, UAE.\nAnd that became a separate company\ncalled GlobalFoundries,\nwhich is a foundry firm.\nAnd then, AMD was able to then focus\non the return back up was like,\nhey, let's focus on making chiplets\nand a bunch of different\nchips for different markets.\nAnd focusing on specific workloads\nrather than all of these different things.", "mimetype": "text/plain", "start_char_idx": 96037, "end_char_idx": 99989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4aa0fc68-3210-40f6-a326-52a5aa701f9d": {"__data__": {"id_": "4aa0fc68-3210-40f6-a326-52a5aa701f9d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a91b32d-dfc8-4709-8727-3ea7a092c079", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d31e0593678feb925f9a9549d0e5a63576a312907a693cd11032373428b5499d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8", "node_type": "1", "metadata": {}, "hash": "cae9c024064406ba4c50a9da5ac5e0c06be9e51e6c6b486a45238af375694901", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so, super, super powerful\ncompanies like Intel,\nthey had the weathering\nstorm to like, hey,\nthey still exist today, even\nthough they really screwed up\ntheir manufacturing six, seven years ago.\nBut in the case of like AMD,\nthey almost went bankrupt.\nThey had to sell their\nfabs to Mubadala, UAE.\nAnd that became a separate company\ncalled GlobalFoundries,\nwhich is a foundry firm.\nAnd then, AMD was able to then focus\non the return back up was like,\nhey, let's focus on making chiplets\nand a bunch of different\nchips for different markets.\nAnd focusing on specific workloads\nrather than all of these different things.\nAnd so, you get more diversity of chips,\nyou have more companies\nthan ever designing chips,\nbut you have fewer companies\nthan ever manufacturing them.\nAnd this is where TSMC comes in,\nis they've just been the best.\nThey are so good at it.\nThey're customer-focused,\nthey make it easy for you\nto fabricate your chips.\nThey take all of that complexity\nand try and abstract a\nlot of it away from you.\nThey make good money, they\ndon't make insane money,\nbut they make good money.\nAnd they're able to\naggregate all this demand\nand continue to build the next fab,\nthe next fab, the next fab.\n- So, why is Taiwan so special for TSMC?\nWhy is it happening there?\nCan it be replicated\ninside the United States?\n- Yeah, so there's aspects of it\nthat I would say yes and\naspects that I'd say no.\nTSMC is way ahead,\nbecause Former Executive Morris Chang\nof Texas Instruments\nwasn't promoted to CEO,\nand he is like, screw this,\nI'm gonna go make my own chip company.\nAnd he went to Taiwan and made TSMC.\nAnd there's a whole lot more story there.\nSo, it could have been Texas Instruments,\nit could have been TSMC,\nbut Texas Semiconductor Manufacturing,\ninstead of Texas Instruments.\n(Nathan laughing)\nSo, there is that whole story there,\nbut the race-\n- Sitting here in Texas.\n- And that sounds like a human story,\nlike it didn't get promoted?\n- Just the brilliance of Morris Chang,\nwhich I wouldn't underplay,\nbut there's also a different\nlevel of how this works.\nSo, in Taiwan, the number,\ntop percent of graduates of students\nthat go to the best school, which is NTU,\nthe top percent of those\nall go work to TSMC.\nAnd guess what their pay is,\ntheir starting pay is\nlike $80,000, $70,000.\nWhich is like, that's like starting pay\nfor a good graduate in the US.\nNot the top, the top\ngraduates are making hundreds\nof thousands of dollars at\nthe Googles and the Amazons,\nand now I guess the OpenAIs of the world.\nSo, there is a large dichotomy of like,\nwhat is the top 1% of the society doing\nand where are they headed\nbecause of economic reasons.\nIntel never paid that crazy good.\nAnd it didn't make sense to them.\nThat's one aspect, where's the best going?\nSecond is the work ethic.\nWe like to work, you work\na lot, we work a lot,\nbut at the end of the day,\nwhat is the time and amount\nof work that you're doing\nand what does a fab require?\nFabs are not work from home jobs.\nThey are, you go into the\nfab and grueling work.\nThere's, hey, if there is\nany amount of vibration.\nAn earthquake happens,\nvibrates the machines,\nthey're either broken,\nyou've scrapped some of your production.\nAnd then, in many cases, they're\nlike calibrated properly.\nSo, when TSMC, when there's an earthquake,\nrecently there's been an earthquake,\nTSMC doesn't call their employees,\nthey just go to the fab\nand they just show up.\nThe parking lot gets slammed\nand people just go into\nthe fab and fix it.\nIt's like ants. It's like a hive of ants.\nIt doesn't get told by\nthe queen what to do.\nThe ants just know.\n- It's like one person just\nspecializes on these one task,\nand it's like, you're\ngonna take this one tool\nand you're the best person in the world,\nand this is what you're gonna do\nfor your whole life is\nthis one task and the fab.", "mimetype": "text/plain", "start_char_idx": 99370, "end_char_idx": 103189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8": {"__data__": {"id_": "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4aa0fc68-3210-40f6-a326-52a5aa701f9d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cf596b5d75f67818f3288ef4dd9d715aade4b374e90a73ecba94501110822177", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b21d36e-041d-4081-a1ff-6b6d9473a385", "node_type": "1", "metadata": {}, "hash": "bcde8e32578341b54bee6effd53fa2f935d23ee542c58a105a673311601a121a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An earthquake happens,\nvibrates the machines,\nthey're either broken,\nyou've scrapped some of your production.\nAnd then, in many cases, they're\nlike calibrated properly.\nSo, when TSMC, when there's an earthquake,\nrecently there's been an earthquake,\nTSMC doesn't call their employees,\nthey just go to the fab\nand they just show up.\nThe parking lot gets slammed\nand people just go into\nthe fab and fix it.\nIt's like ants. It's like a hive of ants.\nIt doesn't get told by\nthe queen what to do.\nThe ants just know.\n- It's like one person just\nspecializes on these one task,\nand it's like, you're\ngonna take this one tool\nand you're the best person in the world,\nand this is what you're gonna do\nfor your whole life is\nthis one task and the fab.\n- Which is like some special chemistry\nplus nano manufacturing\non one line of tools\nthat continues to get iterated.\nAnd yeah, it's like specific plasma etch\nfor removing silicon dioxide.\nThat's all you focus on your whole career,\nand it's like such a specialized thing.\nAnd so, it's not like the\ntasks are transferable.\nAI today is awesome,\nbecause people can pick it up like that.\nSemiconductor manufacturing\nis very antiquated and difficult.\nNone of the materials\nare online for people to\nread easily and learn.\nThe papers are very dense\nand it takes a lot of experience to learn.\nAnd so, it makes the barrier\nto entry much higher too.\nSo, when you talk about, hey,\nyou have all these people\nthat are super specialized,\nthey will work 80 hours a\nweek in a factory, in a fab.\nAnd if anything goes wrong,\nthey'll go show up in\nthe middle of the night\nbecause some earthquake,\ntheir wife is like,\n\"There's an earthquake.\"\nHe is like, \"Great,\nI'm gonna go to the fab.\"\n(Nathan laughing)\n- [Nathan] It's like a crime.\n- Would you as an American, do that?\nIt's like these sorts of things are like,\nwhat, I guess are the exemplifying,\nlike why TSMC is so amazing.\nNow, can you replicate it in the US?\nLet's not ignore Intel was the leader\nin manufacturing for over 20 years.\nThey brought every technology\nto market first besides EUV,\nstrained silicon,\nHigh-K/Metal Gates, FinFET,\nand the list just goes on and on and on\nof technologies that Intel\nbrought to market first,\nmade the most money from,\nand manufactured at scale first,\nbest, highest profit margins.\nSo, we shouldn't ignore\nthat Intel can't do this.\nIt's that the culture has broken.\nYou've invested in the wrong things.\nThey said no to the iPhone.\nThey had all these different things\nregarding mismanagement of the fabs,\nmismanagement of designs, this lockup.\nAnd at the same time,\nall these brilliant\npeople, these 50,000 PhDs,\nor masters that have been\nworking on specific chemical,\nor physical processes,\nor nano manufacturing\nprocesses for decades,\nin Oregon, they're still there.\nThey're still producing amazing work.\nIt's just like getting it to the last mile\nof production at high-yield,\nwhere you can manufacture dozens\nand hundreds of different\nkinds of chips, and it's good.\nCustomer experience has broken.\nIt's that customer experience.\nPart of it is like,\npeople will say Intel was too\npompous in the 2000s, 2010s.\nThey just thought they\nwere better than everyone.\nThe tool guys were like, oh,\nI don't think that this is mature enough.\nAnd they're like, ah, you\njust don't know what we know.\nThis sort of stuff would happen.\nAnd so, can the US\nbring leading edge semiconductor\nmanufacturing to the US?\nAnd thematically, yes.\nAnd we are. TS-\n- It's happening.\nArizona is getting better\nand better as time goes on.\n- TSMC has built roughly\n20% of their capacity\nfor five nanometer in the US.\nNow, this is nowhere near enough.\n20% of capacity in the US is like nothing.\nAnd furthermore, this is still\ndependent on Taiwan existing.\nThere's important way to separate it out.\nThere's R&D and there's\nhigh volume manufacturing.\nEffectively, there are\nthree places in the world\nthat are doing leading edge R&D.", "mimetype": "text/plain", "start_char_idx": 102449, "end_char_idx": 106361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b21d36e-041d-4081-a1ff-6b6d9473a385": {"__data__": {"id_": "6b21d36e-041d-4081-a1ff-6b6d9473a385", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77e8b2d7-59ee-4732-a5bb-ed1b5865b1a8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "31d537e686f68cf83b4dfde8c0f1d68213484cebf3a44d19d23cceb1033daec9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91", "node_type": "1", "metadata": {}, "hash": "6dc3c5f16f614eba835e1932841dc649f21734ce2d61c0a832da7d5e04b2e7a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They just thought they\nwere better than everyone.\nThe tool guys were like, oh,\nI don't think that this is mature enough.\nAnd they're like, ah, you\njust don't know what we know.\nThis sort of stuff would happen.\nAnd so, can the US\nbring leading edge semiconductor\nmanufacturing to the US?\nAnd thematically, yes.\nAnd we are. TS-\n- It's happening.\nArizona is getting better\nand better as time goes on.\n- TSMC has built roughly\n20% of their capacity\nfor five nanometer in the US.\nNow, this is nowhere near enough.\n20% of capacity in the US is like nothing.\nAnd furthermore, this is still\ndependent on Taiwan existing.\nThere's important way to separate it out.\nThere's R&D and there's\nhigh volume manufacturing.\nEffectively, there are\nthree places in the world\nthat are doing leading edge R&D.\nThere's Hsinchu, Taiwan,\nthere's Hillsboro, Oregon,\nand there is Pyongyang, South Korea.\nThese three places\nare doing the leading\nedge R&D for the rest\nof the world's leading\nedge semiconductors.\nNow, manufacturing can be\ndistributed more globally.\nAnd this is where this dichotomy exists\nof who's actually modifying the process,\nwho's actually developing\nthe next generation one,\nwho's improving them?\nIs Hsinchu, is Hillsboro, is Pyongyang.\nIt is not the rest of\nthese fabs like Arizona.\nArizona is a paperweight.\nIf Hsinchu disappeared off\nthe face of the planet,\nwithin a year, couple years,\nArizona would stop producing too.\nIt's actually like pretty critical.\nOne of the things I like to\nsay is if I had a few missiles,\nI know exactly where I could\ncause the most economic damage.\nIt's not targeting the White House.\nIt's not-\n- It's the R&D centers.\n- It's the R&D centers\nfor TSMC, Intel, Samsung,\nand then some of the memory\nguys, Micron and Hynix.\n- Because they define the future evolution\nof these semiconductors and\neverything's moving so rapidly\nthat it really is fundamentally about R&D.\nAnd it is all about TSMC, huh?\n- And so, TSMC,\nyou cannot purchase a\nvehicle without TSMC chips.\nYou cannot purchase a\nfridge without TSMC chips.\nI think one of the few things\nyou can purchase ironically\nis a Texas Instruments\ngraphing calculator,\nbecause they actually\nmanufacture in Texas.\nBut outside of that, like a laptop,\n- It's depressing.\n- a phone, anything you,\nservers, GPUs, none of\nthis stuff can exist.\nAnd this is without TSMC.\nAnd in many cases, it's not\neven like the leading edge,\nsexy five nanometer chip,\nthree nanometer chip, two nanometer chip.\nOftentimes, it's just some stupid power IC\nthat's converting from\nsome voltage to another,\nand it's made at TSMC.\n- This is what China is\ninvesting in as well.\nIt's like they can build\nout this long tail fab\nwhere the techniques are much more known.\nYou don't have to figure\nout these problems with EUV.\nThey're investing in this,\nand then they have large supply\nfor things like the car door\nhandles and the random stuff.\nAnd that trickles down\ninto this whole economic\ndiscussion as well,\nwhich is they have far more than we do.\nAnd having supply\nfor things like this is\ncrucial to normal life.\n- So, they're starting to invest\nin high volume manufacturer,\nbut they're not doing R&D.\n- So, they do R&D on their own.\nThey're just way behind.\n- Yeah.\n- So, I would say like in 2015,\nChina had a five-year plan,\nwhere they defined by 2025\nand 2020 certain goals,\nincluding 80% domestic\nproduction of semiconductors.\nthey're not gonna hit\nthat right, to be clear.\nBut they are in certain\nareas, really, really close.\nLike BYD is probably\ngonna be the first company\nin the world to not have\nto use TSMC for making...\n'Cause they have their\nown fabs for making chips.\nNow, they still have to buy\nsome chips from foreign,\nfor example, like around\nself-driving ADAS capabilities.", "mimetype": "text/plain", "start_char_idx": 105574, "end_char_idx": 109304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91": {"__data__": {"id_": "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b21d36e-041d-4081-a1ff-6b6d9473a385", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "684f9ea49cdc23406bb646e54fdb568bcc884213d1c96d0bd0d5bb15814db399", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "031acbd6-0b4e-4616-9af2-665b953c09c1", "node_type": "1", "metadata": {}, "hash": "645391508ad519bd5c3a95efb84ce9dbd4656d902d107a73ceff308e081ab04f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And having supply\nfor things like this is\ncrucial to normal life.\n- So, they're starting to invest\nin high volume manufacturer,\nbut they're not doing R&D.\n- So, they do R&D on their own.\nThey're just way behind.\n- Yeah.\n- So, I would say like in 2015,\nChina had a five-year plan,\nwhere they defined by 2025\nand 2020 certain goals,\nincluding 80% domestic\nproduction of semiconductors.\nthey're not gonna hit\nthat right, to be clear.\nBut they are in certain\nareas, really, really close.\nLike BYD is probably\ngonna be the first company\nin the world to not have\nto use TSMC for making...\n'Cause they have their\nown fabs for making chips.\nNow, they still have to buy\nsome chips from foreign,\nfor example, like around\nself-driving ADAS capabilities.\n'Cause those are really high-end,\nbut at least like a\ninternal combustion engine\nhas 40 chips in an EV,\njust for controlling flow\nrates and all these things,\nand EVs are even more complicated.\nSo, all these different power ICs\nand battery management controllers,\nand all these things, they're insourcing.\nAnd this is something that\nChina has been doing since 2015.\nNow, as far as the trailing edge,\nthey're getting so much capacity there.\nAs far as the leading edge,\ni.e, this five nanometer\nand so on, so forth.\nWhere GPUs, they are still behind.\nAnd this is the US restrictions\nare trying to stop them in the latter.\nBut all that's happened, is yes,\nthey've slowed down their five nanometer,\nthree nanometer, et cetera,\nbut they've accelerated their, hey,\n45 nanometer, 90 nanometer\npower IC or analog IC,\nor random chip in my\nkeyboard, that kind of stuff.\nSo, there is an angle of the US' actions\nhave been so, from these export,\nfrom the angle of the export controls,\nhave been so inflammatory\nat slowing down China's\nprogress on the leading edge\nthat they've turned around\nand have accelerated\ntheir progress elsewhere,\nbecause they know this is so important.\nIf the US is gonna lock them out here\nor if they lock us out here\nas well in the trailing edge.\nAnd so, going back, can\nthe US build it here?\nYes, but it's gonna take a ton of money.\nI truly think like, to revolutionize\nand completely insource semiconductors,\nwould take a decade\nand a trillion dollars.\n- Is some of it also culture?\nLike you said, extreme competence,\nextreme work ethic in Taiwan?\n- I think if you have the demand\nand the money is on the line,\nthe American companies figure it out.\nIt's gonna take handholding\nwith the government.\nBut I think that the culture\nhelps TSMC breakthrough\nand it's easier for them.\nYou could-\n- TSMC has some like 90,000 employees.\nIt's not actually that insane amount.\nThe Arizona fab has 3,000 from Taiwan,\nand these people, their\nwives were like, yeah,\nwe're not gonna have kids\nunless we use sign up for the Arizona fab.\nWe go to Arizona and\nwe have our kids there.\nThere's also a Japan fab\nwhere the same thing happened.\nAnd so, these wives drove like,\nthese dudes to go to Japan\nor America to have the kids there.\nAnd it's like it's an element\nof culture. Yeah, sure.\nTaiwan works that hard,\nbut also the US has done it in the past,\nthey could do it now.\nWe can just import, I say import,\nthe best people in the\nworld if we want to.\n- That's where the immigration\nconversation is a tricky one.\nAnd there's been a lot\nof debate over that.\nBut yeah, it seems absurdly controversial\nto import the best people in the world.\nI don't understand why it's controversial.\nThat's the one of the ways\nof winning.\n- I'm sure we agree with you.\n(Lex laughing)\n- And even if\nyou can't import those people,\nI still think you could do a lot\nto manufacture most of in\nthe US if the money's there.\nAnd so, like-\n- It's just way more expensive.\nIt's not profitable for a long time.", "mimetype": "text/plain", "start_char_idx": 108562, "end_char_idx": 112284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "031acbd6-0b4e-4616-9af2-665b953c09c1": {"__data__": {"id_": "031acbd6-0b4e-4616-9af2-665b953c09c1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92d9b1bb-d9b1-4d62-91c4-ccd23b3c4b91", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "92b64acfbb72f53d94f97b667fe252924cc4b3b74c76a32f4e128ff5db1e01cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd001662-c246-4a0e-94d6-f5c939e4222a", "node_type": "1", "metadata": {}, "hash": "2d3f3896b99bfe620e0bbe70171cd008bca5739e1564275d85377ee0a51bafa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And it's like it's an element\nof culture. Yeah, sure.\nTaiwan works that hard,\nbut also the US has done it in the past,\nthey could do it now.\nWe can just import, I say import,\nthe best people in the\nworld if we want to.\n- That's where the immigration\nconversation is a tricky one.\nAnd there's been a lot\nof debate over that.\nBut yeah, it seems absurdly controversial\nto import the best people in the world.\nI don't understand why it's controversial.\nThat's the one of the ways\nof winning.\n- I'm sure we agree with you.\n(Lex laughing)\n- And even if\nyou can't import those people,\nI still think you could do a lot\nto manufacture most of in\nthe US if the money's there.\nAnd so, like-\n- It's just way more expensive.\nIt's not profitable for a long time.\n- And that's the context\nof like the CHIPS Act\nis only like $50 billion\nrelative to some of the\nrenewable initiatives\nthat were passed in the\nInflation Reduction Act\nand the Infrastructure Act,\nwhich total in the hundreds\nof billions of dollars.\nAnd so, the amount of money that the US\nis spending on the semiconductor\nindustry is nothing.\nWhereas all these other countries\nhave structural advantages\nin terms of work ethic\nand amount of work, and\nlike things like that.\nBut also a number of STEM graduates,\nthe percentile of their\nbest going to that.\nBut they also have\ndifferences in terms of like,\nhey, there's just tax benefits in the law\nand have been in the law for 20 years.\nAnd then, some countries\nhave massive subsidies.\nChina has something like $200 billion\nof semiconductor subsidies a year.\nWe're talking about $50 billion\nin the US over like six.\nSo, the girth or difference\nin the subsidy amounts is also huge.\nAnd so I think,\nTrump has been talking about\ntariffing Taiwan recently.\nThat's like one of these things\nthat's like, oh, okay, well,\nmaybe he doesn't wanna subsidize\nthe US semiconductor industry.\nObviously, tariffing Taiwan\nis gonna cost a lot of things\nto get much more expensive.\nBut does it change the equation\nfor TSMC building more fabs in the US?\nThat's what he is positing.\n- So, can you lay out the, so\nwe laid out the importance,\nby the way, it's incredible how\nmuch you know about so much.\n- [Nathan] We told you\nDylan knows all this stuff.\n- Yeah.\n(Nathan laughing)\nSo, okay, you laid out why\nTSMC is really important.\nIf we look out into the\nfuture 10, 20 years out,\nUS-China relationship\nseems like it can go to\na dark place of cold war,\nescalated cold war, even hot war,\nor to a good place of anything\nfrom frenemies to cooperation,\nto working together.\nSo, in this game theory, complicated game,\nwhat are the different trajectories?\nWhat should US be doing?\nWhat do you see as the\ndifferent possible trajectories\nof US-China relations as both leaders\nstart to feel the AGI more and more,\nand see the importance of\nchips and the importance of AI?\n- Ultimately, the export controls\nare pointing towards a\nseparate future economy.\nI think the US has made it\nclear to Chinese leaders\nthat we intend to control this technology\nat whatever cost to global\neconomic integration.\n- So, that-\n- It's hard to unwind that.\n(Nathan chuckles)\nThe card has been played.\n- To the same extent,\nthey've also limited US\ncompanies for mentoring China.\nSo, it's been a long time coming.\nAt some point, there was a convergence.\nBut over at least the last decade,\nit's been branching\nfurther and further out.\nLike US companies can't enter China,\nChinese companies can't enter the US.\nThe US is saying, hey, China,\nyou can't get access to our\ntechnologies in certain areas.\nAnd China's rebutting with\nthe same thing around like,\nthey've done some sort\nof specific materials\nin gallium and things like that,\nthat they've tried to limit the US on.\nThere's a US drone company\nthat's not allowed to buy batteries\nand they have military customers.\nAnd this drone company just\ntells the military customers\nlike, hey, hey, just get it from Amazon,\n'cause I can't actually\nphysically get them.", "mimetype": "text/plain", "start_char_idx": 111536, "end_char_idx": 115488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd001662-c246-4a0e-94d6-f5c939e4222a": {"__data__": {"id_": "fd001662-c246-4a0e-94d6-f5c939e4222a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "031acbd6-0b4e-4616-9af2-665b953c09c1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "12e41bb743f93241fbc66e3d5818c8a347e708dcae5fb3c7a9539c0fa0d1c822", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f419c400-91db-49e4-b0cb-382a358703bd", "node_type": "1", "metadata": {}, "hash": "85c031523b9d01d3db6fd2dab19cce1bc9b6c33964fbcac3f9fa87fb57bfb830", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- To the same extent,\nthey've also limited US\ncompanies for mentoring China.\nSo, it's been a long time coming.\nAt some point, there was a convergence.\nBut over at least the last decade,\nit's been branching\nfurther and further out.\nLike US companies can't enter China,\nChinese companies can't enter the US.\nThe US is saying, hey, China,\nyou can't get access to our\ntechnologies in certain areas.\nAnd China's rebutting with\nthe same thing around like,\nthey've done some sort\nof specific materials\nin gallium and things like that,\nthat they've tried to limit the US on.\nThere's a US drone company\nthat's not allowed to buy batteries\nand they have military customers.\nAnd this drone company just\ntells the military customers\nlike, hey, hey, just get it from Amazon,\n'cause I can't actually\nphysically get them.\nThere's all these things\nthat are happening\nthat point to further\nand further divergence.\nI have zero idea.\nAnd I would love\nif we could all hold\nhands and sing Kumbaya,\nbut I have zero idea how\nthat could possibly happen.\n- Is the divergence good\nor bad for avoiding war?\nIs it possible that the divergence\nin terms of manufacturer chips,\nof training AI systems is\nactually good for avoiding\nmilitary conflict?\n- It's an objective fact\nthat the world has been the most peaceful\nhas ever been when there\nare global hegemons,\nor regional hegemons\nin historical context.\nThe Mediterranean was\nthe most peaceful ever\nwhen the Romans were there.\nChina had very peaceful and warring times.\nAnd the peaceful times were when dynasties\nhad a lock hold over, not just themselves,\nbut all their tributaries around them.\nAnd likewise, the most\npeaceful time in human history\nhas been when the US\nwas the global hegemon,\nthe last decades.\nNow, we've seen things start to slide\nwith Russia-Ukraine,\nwith what's going on in the Middle East,\nand Taiwan risk, all\nthese different things\nare starting to bubble up,\nstill objectively extremely peaceful.\nNow, what happens when it's\nnot one global hegemon,\nbut it's two?\nObviously, and China will be competitive\nor even overtake the\nUS, like it's possible.\nAnd so, this change in global hegemony,\nI don't think it ever happens\nlike super peacefully.\nWhen empires fall,\nwhich is a possible\ntrajectory for America,\nthey don't pull fall gracefully.\nThey don't just slide out of irrelevance.\nUsually, there's a lot of shaking.\nAnd so, what the US is trying to do\nis maintain its top position.\nAnd what China is trying to\ndo is become the top position.\nAnd obviously, there's\nbutting of heads here,\nin the most simple terms.\n- And that could take\nshape in all kinds of ways,\nincluding proxy wars.\nAnd now-\n- Yeah, it seems\nlike it's already happening.\nAs much as I want there to be\ncenturies of prolonged peace,\nit looks like further instability\ninternationally is ahead.\n- And the US is sort of\nlike current task is like,\nhey, if we control AI, if\nwe're the leader in AI,\nand AI could significantly\naccelerates progress,\nthen we can maintain the\nglobal hegemony position.\nAnd therefore-\n- [Nathan] I hope that works.\n- And as an American,\n(Nathan chuckles)\nkind of like, okay,\nI guess that's gonna lead to peace for us.\nNow, obviously, other\npeople around the world\nget affected negatively.\nObviously, the Chinese\npeople are not gonna be in\nas advantageous of a\nposition if that happens.\nBut this is the reality\nof what's being done\nand the actions that\nare being carried out.\n- So, can we go back\nto the specific detail\nof the different hardware?\nThere's this nice graphic\nin the export controls\nof which GPUs are allowed to\nbe exported and which are not.\nCan you explain the difference?\nIs there, from a technical perspective,\nare the H20s promising?\n- Yeah, so this goes,\nand I think we'd have to,\nwe need to dive really deep\ninto the reasoning aspect\nand what's going on there.\nBut the H20, the US has gone\nthrough multiple iterations\nof the export controls.\nThis H800 was at one\npoint allowed back in '23,\nbut then it got canceled.", "mimetype": "text/plain", "start_char_idx": 114682, "end_char_idx": 118652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f419c400-91db-49e4-b0cb-382a358703bd": {"__data__": {"id_": "f419c400-91db-49e4-b0cb-382a358703bd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd001662-c246-4a0e-94d6-f5c939e4222a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4dfcb6714d40157d1282a917cfd32d0d9f83c296054cda81bd1e9eca3031d543", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c55a790c-f53c-4875-883e-fd4ba6a476ab", "node_type": "1", "metadata": {}, "hash": "1b1ac0a8bec8062d7a4ad0fe8ffce9d7751b0952163c101ceac350350ba86423", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, obviously, other\npeople around the world\nget affected negatively.\nObviously, the Chinese\npeople are not gonna be in\nas advantageous of a\nposition if that happens.\nBut this is the reality\nof what's being done\nand the actions that\nare being carried out.\n- So, can we go back\nto the specific detail\nof the different hardware?\nThere's this nice graphic\nin the export controls\nof which GPUs are allowed to\nbe exported and which are not.\nCan you explain the difference?\nIs there, from a technical perspective,\nare the H20s promising?\n- Yeah, so this goes,\nand I think we'd have to,\nwe need to dive really deep\ninto the reasoning aspect\nand what's going on there.\nBut the H20, the US has gone\nthrough multiple iterations\nof the export controls.\nThis H800 was at one\npoint allowed back in '23,\nbut then it got canceled.\nAnd by then, DeepSeek had\nalready built their cluster of,\nthey claim 2k, I think they\nactually have like many more,\nsomething like 10k of those.\nAnd now, this H20 is the\nlegally allowed chip.\nNvidia shipped a million of\nthese last year to China.\nFor context, it was like\n4 or 5 million GPUs.\nSo, the percentage of GPUs\nthat were this China\nspecific H20 is quite high,\nroughly 20%, 25%, 20% or so.\nAnd so, this H20 has\nbeen neutered in one way,\nbut it's actually upgraded in other ways.\nAnd you could think of chips\nalong three axes for AI,\nignoring software stack\nand exact architecture,\njust raw specifications.\nThere's floating point operations, FLOPS.\nThere is memory bandwidth, i.e,\nand memory capacity, I/O, memory.\nAnd then, there is interconnect,\nchip to chip interconnections.\nAll three of these\nare incredibly important\nfor making AI systems.\nBecause AI systems\ninvolve a lot of compute,\nthey involve a lot of\nmoving memory around,\nwhether it be two memory\nor two other chips.\nAnd so, these three vectors.\nThe US initially had two\nof these vectors controlled\nand one of them not\ncontrolled, which was FLOPS\nand interconnect bandwidth\nwere initially controlled.\nAnd then, they said, no, no, no, no,\nwe're gonna remove the\ninterconnect bandwidth\nand just make it a very simple only FLOPS.\nBut now, Nvidia can now\nmake a chip that has, okay,\nit's cut down on FLOPS,\nso like one third that of the H100\non spec sheet paper performance for FLOPS.\nIn real world, it's closer to half,\nor maybe even like 60% of it.\nBut then, on the other two vectors,\nit's just as good for\ninterconnect bandwidth.\nAnd then, for memory\nbandwidth and memory capacity,\nthe H20 has more memory bandwidth\nand more memory capacity than the H100.\nNow, recently, we, at our research,\nwe cut Nvidia's production for H20\nfor this year down drastically.\nThey were gonna make another\n2 million of those this year,\nbut they just canceled all\nthe orders a couple weeks ago.\nIn our view, that's because we think\nthat they think they're\ngonna get restricted.\nBecause why would they cancel\nall these orders for H20?\nBecause they shipped a\nmillion of 'em last year.\nThey had orders in for a\ncouple million this year,\nand just gone.\nFor H20, B20, a successor to\nH20, and now they're all gone.\nNow, why would they do this?\nI think it's very clear.\nThe H20 is actually\nbetter for certain tasks\nand that certain task is reasoning.\nReasoning is incredibly different than...\nWhen you look at the\ndifferent regimes of models,\npre-training is all about FLOPS.\nIt's all about FLOPS.\nThere's things you do\nlike mixture of experts\nthat we talked about, to\ntrade off interconnect,\nor to trade off other\naspects and lower the FLOPS,\nand rely more on interconnect and memory.\nBut at the end of the day,\nit's FLOPS is everything.\nWe talk about models in terms\nof how many FLOPS there are.\nSo, we talk about, oh, GPT-4 is 2e25.\nTwo to the 25th, 25 zeros,\nFLOP, floating point operations.\n- For training.\n- For training.", "mimetype": "text/plain", "start_char_idx": 117836, "end_char_idx": 121622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c55a790c-f53c-4875-883e-fd4ba6a476ab": {"__data__": {"id_": "c55a790c-f53c-4875-883e-fd4ba6a476ab", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f419c400-91db-49e4-b0cb-382a358703bd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "62a577020a9c0c50fa32bc80f573f3f68d442f7004fabf4bb2c94d2425df6733", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "845726fb-0aa9-4b32-a21d-46590a41497c", "node_type": "1", "metadata": {}, "hash": "991ce75e02bdab2011cc02deb3cd9846aad15c3cf866ebe9502ffe3817e91ba0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, why would they do this?\nI think it's very clear.\nThe H20 is actually\nbetter for certain tasks\nand that certain task is reasoning.\nReasoning is incredibly different than...\nWhen you look at the\ndifferent regimes of models,\npre-training is all about FLOPS.\nIt's all about FLOPS.\nThere's things you do\nlike mixture of experts\nthat we talked about, to\ntrade off interconnect,\nor to trade off other\naspects and lower the FLOPS,\nand rely more on interconnect and memory.\nBut at the end of the day,\nit's FLOPS is everything.\nWe talk about models in terms\nof how many FLOPS there are.\nSo, we talk about, oh, GPT-4 is 2e25.\nTwo to the 25th, 25 zeros,\nFLOP, floating point operations.\n- For training.\n- For training.\nAnd we're talking\nabout the restrictions for\nthe 2e24 or 25, whatever.\nThe US has an executive order\nthat Trump recently unsigned,\nwhich was, hey, 1e26,\nonce you hit that number\nof floating point operations,\nyou must notify the government\nand you must share your results with us.\nThere's a level of model\nwhere the US government must be told.\nAnd that's 1e26.\nAnd so, as we move forward,\nthis is an incredibly important...\nFLOP is the vector that the government\nhas cared about historically,\nbut the other two vectors are\narguably just as important.\nAnd especially when we\ncome to this new paradigm,\nwhich the world is only\njust learning about\nover the last six months, reasoning.\n- And do we understand firmly\nwhich of the three dimensions\nis best for reasoning?\nSo, interconnect, the\nFLOPS don't matter as much.\nIs it memory?\n- Memory. Right.\n- Yeah, so-\n- Context length.\nWe're gonna get into technical stuff\nreal fast, yeah. (chuckles)\n- I would just say\nthere's two articles in\nthis one that I could show,\nmaybe graphics that might be\ninteresting for you to pull up.\n- For the listeners, we're\nlooking at the section\nof o1 inference architecture tokenomics.\n- Hmm.\nDo you wanna explain KV cache\nbefore we talk about this?\nI think it's better to-\n- Okay, yeah.\nWe need to go through a lot\nof specific technical things\nof transformers to make\nthis easy for people.\n- Because it's incredibly important\nbecause this changes how models work.\nBut I think resetting. Why\nis memory so important?\nIt's because so far, we've\ntalked about parameter counts.\nAnd mixture of experts, you can change\nhow many active parameters\nversus total parameters\nto embed more data but have less FLOPS.\nBut more important, another aspect of,\nwhat's part of this humongous revolution\nin the last handful of\nyears is the transformer.\nAnd the attention mechanism.\nAttention mechanism is that the model\nunderstands the relationships\nbetween all the words in its context.\nAnd that is separate from\nthe parameters themselves.\nAnd that is something\nthat you must calculate.\nHow each token, each word\nin the context length\nis relatively connected to each other.\nAnd I think, Nathan,\n- Yeah. It's-\n- you should explain\nKV cache better.\n- KV cache is one\nof the optimizations-\n- Yeah.\nSo, the attention operator\nhas three core things.\nIt's queries, keys, and values.\nQKV is the thing that goes into this.\nYou'll look at the equation,\nyou see that these matrices\nare multiplied together.\nThese words, query, key, and value,\ncome from information\nretrieval backgrounds,\nwhere the query is the thing\nyou're trying to get the values for\nand you access the keys and\nthe values is re-weighting.\nMy background's not information retrieval\nand things like this.\nIt's just fun to have back links.\nAnd what effectively happens\nis that when you're doing\nthese matrix multiplication,\nyou're having matrices\nthat are of the size\nof the context length.\nSo, the number of tokens\nthat you put into the model.\nAnd the KV cache is effectively\nsome form of compressed representation\nof all the previous tokens in the model.\nSo, when you're doing this,\nwe talk about autoaggressive models.\nYou predict one token at a time.\nYou start with whatever your prompt was.\nYou ask a question like, who\nwas the president in 1825?\nThe model then is gonna\ngenerate its first token.", "mimetype": "text/plain", "start_char_idx": 120911, "end_char_idx": 124939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "845726fb-0aa9-4b32-a21d-46590a41497c": {"__data__": {"id_": "845726fb-0aa9-4b32-a21d-46590a41497c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c55a790c-f53c-4875-883e-fd4ba6a476ab", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1bc10457c24dc3de9717c995a7d2c4e80ddad89e90eadc3fb92a6bc0e413ca2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "218cbf28-22b4-4ca8-884b-e037f8fde748", "node_type": "1", "metadata": {}, "hash": "67ee0feca36686358583907b6e0fb51b786a280f981aaa535fd3671ef8ca0e18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These words, query, key, and value,\ncome from information\nretrieval backgrounds,\nwhere the query is the thing\nyou're trying to get the values for\nand you access the keys and\nthe values is re-weighting.\nMy background's not information retrieval\nand things like this.\nIt's just fun to have back links.\nAnd what effectively happens\nis that when you're doing\nthese matrix multiplication,\nyou're having matrices\nthat are of the size\nof the context length.\nSo, the number of tokens\nthat you put into the model.\nAnd the KV cache is effectively\nsome form of compressed representation\nof all the previous tokens in the model.\nSo, when you're doing this,\nwe talk about autoaggressive models.\nYou predict one token at a time.\nYou start with whatever your prompt was.\nYou ask a question like, who\nwas the president in 1825?\nThe model then is gonna\ngenerate its first token.\nFor each of these tokens,\nyou're doing the same attention operator,\nwhere you're multiplying these\nquery key value matrices.\nBut the math is very nice,\nso that when you're doing this repeatedly,\nthis KV cache, this key value operation,\nyou can keep appending\nthe new values to it.\nSo, you keep track of\nwhat your previous values\nyou're inferring over in\nthis autoaggressive chain.\nYou keep it in memory the whole time.\nAnd this is a really crucial thing\nto manage when serving inference at scale.\nThere are far bigger experts in this\nand there are so many levels\nof detail that you can go into.\nEssentially, one of the key,\nquote, unquote, \"drawbacks\"\nof the attention operator\nand the transformer is\nthat there is a form\nof quadratic memory cost\nin proportion to the context length.\nSo, as you put in longer questions,\nthe memory used in order\nto make that computation\nis going up in the form of a quadratic.\nYou'll hear about a lot\nof other language model architectures\nthat are sub-quadratic or\nlinear attention forms,\nwhich is state-space models.\nWe don't need to go down all these now.\nAnd then, there's innovations\non attention to make this memory usage\nand the ability to attend\nover long contexts,\nmuch more accurate and high performance.\n- And those innovations\nare going to help you with,\nyour highly memory constrain-\n- You help with memory\nconstraint and performance.\nSo, if you put in a book into,\nI think Gemini is the model\nthat has the longest context\nlength that people are using.\nGemini is known for 1 million\nand now 2 million context length.\nYou put a whole book into Gemini\nand sometimes it'll draw facts out of it.\nIt's not perfect. They're getting better.\nSo, there's two things.\nIt's like one, to be able to\nserve this on the memory level,\nGoogle has magic with their TPU stack\nwhere they can serve really long contexts.\nAnd then, there's also many\ndecisions along the way\nto actually make long\ncontext performance work\nthat supplies the data.\nThere's subtle changes to these\ncomputations and attention.\nAnd it changes the architecture.\nBut serving long context is\nextremely memory-constrained,\nespecially when you're\nmaking a lot of predictions.\nI actually don't know why input\nand output tokens are more expensive,\nbut I think essentially, output tokens,\nyou have to do more computation,\n'cause you have to sample from the model.\n- I can explain that.\nSo, today, if you use a model,\nlike you look at an API,\nOpenAI charges a certain\nprice per million tokens.\nAnd that price for input and\noutput tokens is different.\nAnd the reason is, is that there is,\nwhen you're inputting\na query into the model.\nLet's say you have a book.\nThat book, you must now calculate\nthe entire KV cache for,\nthis key value cache.\nAnd so, when you do that,\nthat is a parallel operation.\nAll of the tokens can be\nprocessed at one time.\nAnd therefore,\nyou can dramatically reduce\nhow much you're spending.\nThe FLOP requirements\nfor generating a token\nand an input token are identical.\nIf I input one token\nor if I generate one token,\nit's completely identical.\nI have to go through the model.\nBut the difference is\nthat I can do that input,\ni.e, the prefill,\ni.e, the prompt simultaneously\nin a batch nature.\nAnd therefore, it is all FLOP.\n- I think the pricing model\nmostly they use is for input.", "mimetype": "text/plain", "start_char_idx": 124078, "end_char_idx": 128237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "218cbf28-22b4-4ca8-884b-e037f8fde748": {"__data__": {"id_": "218cbf28-22b4-4ca8-884b-e037f8fde748", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "845726fb-0aa9-4b32-a21d-46590a41497c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c6f115f4f0c8c7c55b1ff530af1c04ad4043e23703493f6b6d30cace1d51e437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f3227fb-09bf-4dca-b0b6-4a41faf58add", "node_type": "1", "metadata": {}, "hash": "dd8e7e1c3589e4085f96522327d399db92bf8106f0cfd3942084f343717eae8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And that price for input and\noutput tokens is different.\nAnd the reason is, is that there is,\nwhen you're inputting\na query into the model.\nLet's say you have a book.\nThat book, you must now calculate\nthe entire KV cache for,\nthis key value cache.\nAnd so, when you do that,\nthat is a parallel operation.\nAll of the tokens can be\nprocessed at one time.\nAnd therefore,\nyou can dramatically reduce\nhow much you're spending.\nThe FLOP requirements\nfor generating a token\nand an input token are identical.\nIf I input one token\nor if I generate one token,\nit's completely identical.\nI have to go through the model.\nBut the difference is\nthat I can do that input,\ni.e, the prefill,\ni.e, the prompt simultaneously\nin a batch nature.\nAnd therefore, it is all FLOP.\n- I think the pricing model\nmostly they use is for input.\nTokens is about one fourth the price\nof the output tokens.\n- Correct.\nBut then, output tokens,\nthe reason why it's so expensive\nis because I can't do it in parallel.\nIt's autoaggressive.\nEvery time I generate a token,\nI must not only take the entire,\nI must not only read the\nwhole entire model into memory\nand activate it, go calculate\nit to generate the next token.\nI also have to read the entire KV cache\nand I generate a token,\nand then I append that KV,\nthat one token I generated,\nand it's KV cache, and then I do it again.\nAnd so, therefore, this is\na non-parallel operation.\nAnd this is one where you have to,\nin the case of pre-fill or prompt,\nyou pull the whole model in,\nand you calculate 20,000 tokens at once.\nThese 20,000-\n- So, these are features\nthat APIs are shipping,\nwhich is like prompt caching, pre-filling.\n'Cause you can drive prices down\nand you can make APIs much faster.\nIf you know you're gonna\nkeep, if you run a business\nand you're gonna keep passing\nthe same initial content\nto Claude's API, you can load that in\nto the Anthropic API and\nalways keep it there.\nBut it's very different\nthan we're leading to\nthe reasoning models,\nwhich we showed this example earlier\nand read some of this mumbling stuff.\nAnd what happens\nis that the output context\nlength is so much higher.\nAnd I learned a lot about\nthis from Dylan's work,\nwhich is essentially as the\noutput work length gets higher,\nyou're using this, you're\nwriting this quadratic\nin terms of memory used,\nand then the GPUs that we have,\neffectively, you're\ngonna run out of memory.\nAnd they're all trying to serve\nmultiple requests at once.\nSo, they're doing this batch processing,\nwhere not all of the prompts\nare exactly the same,\nreally complex handling.\nAnd then, as context links\ngets longer, there's this link.\nI think you call it critical batch size,\nwhere your ability to serve more users.\nSo, how much you can paralyze\nyour inference plummets,\nbecause of this long contract.\nSo, your memory usage is going way up\nwith these reasoning models,\nand you still have a lot of users.\nSo, effectively, the cost to\nserve multiplies by a ton.\n- [Lex] And we're looking at a plot\nwhen the x-axis is sequence length.\n- [Dylan] i.e, how many tokens\nare being generated/prompt.\n- Mm-hmm.\n- [Dylan] So, if I put in a\nbook, that's a million tokens.\nBut if I put in, the sky is blue,\nthen that's like six tokens or whatever.\n- I should say that what\nwe're calling reasoning\nand chain of thought is\nextending this sequence length.\n- It's mostly output tokens.\n- So, before, three months ago,\nwhenever o1 launched, all of the use cases\nfor long context length were like,\nlet me put a ton of documents\nin and then get an answer out.\nAnd it's a single prefill,\ncompute a lot in parallel,\nand then output a little bit.\nNow, with reasoning and agents,\nthis is a very different idea.\nNow, instead, I might only\nhave like, hey, do this task,\nor I might have all these documents,\nbut at the end of the day,\nthe model is not just\nproducing a little bit.", "mimetype": "text/plain", "start_char_idx": 127425, "end_char_idx": 131255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f3227fb-09bf-4dca-b0b6-4a41faf58add": {"__data__": {"id_": "8f3227fb-09bf-4dca-b0b6-4a41faf58add", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "218cbf28-22b4-4ca8-884b-e037f8fde748", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "062d97b8e835dd0c18d42a82bffbdb2ca4f1aa4581e3611d1c1fdc363eaad3b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447", "node_type": "1", "metadata": {}, "hash": "7dea1086ea34fc7ee55cc93fc4c0168deac1c2663c2a1c3d5001d808adf8966b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Dylan] So, if I put in a\nbook, that's a million tokens.\nBut if I put in, the sky is blue,\nthen that's like six tokens or whatever.\n- I should say that what\nwe're calling reasoning\nand chain of thought is\nextending this sequence length.\n- It's mostly output tokens.\n- So, before, three months ago,\nwhenever o1 launched, all of the use cases\nfor long context length were like,\nlet me put a ton of documents\nin and then get an answer out.\nAnd it's a single prefill,\ncompute a lot in parallel,\nand then output a little bit.\nNow, with reasoning and agents,\nthis is a very different idea.\nNow, instead, I might only\nhave like, hey, do this task,\nor I might have all these documents,\nbut at the end of the day,\nthe model is not just\nproducing a little bit.\nIt's producing tons of information,\nthis chain of thought,\n- Tens of thousands\ntoken.\n- just continues to go\nand go and go and go.\nAnd so, the sequence length is effectively\nthat if it's generated 10,000 tokens,\nit's 10,000 sequence length.\nAnd plus whatever you\ninput it in the prompt.\nAnd so, what this chart is showing,\nand it's a logarithmic chart,\nis as you grow from 1k to 4k or 4k to 16k,\nthe memory requirements grow\nso fast for your KV cache\nthat you end up not being able\nto run a certain number of...\nYour sequence length is\ncapped or the number of users\nyou can serve.\n- Let's say the model.\nSo, this is showing for a\n405b model and batch size 64.\n- Llama 3.1-405b.\n- Yeah.\nAnd batch size is crucial to,\nessentially they just like, you\nwanna have higher batch size\nto parallel your throughput.\n- 64 different users at once, right?\n- Yeah.\n- And therefore,\nyour serving costs are lower.\nBecause the server costs the same.\nThis is eight H100s,\nroughly $2 an hour per GPU.\nThat's $16 an hour.\nThat is like somewhat of a fixed cost.\nYou can do things to\nmake it lower of course,\nbut it's like $16 an hour.\nNow, how many users can you serve?\nHow many tokens can you generate?\nAnd then, you divide the\ntwo and that's your cost.\nAnd so, with reasoning models,\nthis is where a lot of\nthe complexity comes about\nand why memory is so important.\nBecause if you have\nlimited amounts of memory,\nthen you can't serve so many users.\nIf you have limited amounts of memory,\nyour serving speeds get lower.\nAnd so, your costs get a lot, lot worse.\nBecause all of a sudden,\nif I was used to, hey, on\nthis $16 an hour server,\nI'm serving Llama 405b.\nOr if I'm serving DeepSeek-V3,\nand it's all chat style applications,\ni.e, we're just chit-chatting.\nthe sequence sensor, a\nthousand, a few thousand.\nWhen you use a language model,\nit's a few thousand\ncontext length most times.\nSometimes you're dropping a big document,\nbut then you process it,\nyou get your answer, you throw it away.\nYou move on to the next thing.\nWhereas with reasoning,\nI'm now generating tens of\nthousands of tokens in sequence.\nAnd so, this memory, this KV\ncache has to stay resonant.\n- Yeah.\n- And you have\nto keep loading it,\nyou have to keep it in memory constantly.\nAnd now, this butts out other users.\nIf there's now a reasoning task\nand the model's capable of\nreasoning, then all of a sudden,\nthat memory pressure\nmeans that I can't serve as\nmany users simultaneously.\n- Let's go into DeepSeek again.\nSo, we're in the post\nDeepSeek-R1 time I think.\nAnd there's two sides to this market\nwatching how hard it is to serve it.\nOn one side, we're gonna talk\nabout DeepSeek themselves.\nThey now have a chat app\nthat got to number one on the App Store.\nDisclaimer, number one on the App Store\nis measured by velocity.\nSo, it's not necessarily saying\nthat more people have the DeepSeek app\nthan ChatGPT app.\n- Mm-hmm.\nYep.\n- But it is still remarkable.\nClaude has never hit the\nnumber one on the App Store.", "mimetype": "text/plain", "start_char_idx": 130503, "end_char_idx": 134222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447": {"__data__": {"id_": "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f3227fb-09bf-4dca-b0b6-4a41faf58add", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6037a2bfc631f073e23ee870af19467a8efbab9f9a55289c99d0c14ae93216d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f", "node_type": "1", "metadata": {}, "hash": "a1743eb4ec43211c68d9608b41267a9cecc94b79f7610c1d911a03a97c1613d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And now, this butts out other users.\nIf there's now a reasoning task\nand the model's capable of\nreasoning, then all of a sudden,\nthat memory pressure\nmeans that I can't serve as\nmany users simultaneously.\n- Let's go into DeepSeek again.\nSo, we're in the post\nDeepSeek-R1 time I think.\nAnd there's two sides to this market\nwatching how hard it is to serve it.\nOn one side, we're gonna talk\nabout DeepSeek themselves.\nThey now have a chat app\nthat got to number one on the App Store.\nDisclaimer, number one on the App Store\nis measured by velocity.\nSo, it's not necessarily saying\nthat more people have the DeepSeek app\nthan ChatGPT app.\n- Mm-hmm.\nYep.\n- But it is still remarkable.\nClaude has never hit the\nnumber one on the App Store.\nEven though everyone in\nSan Francisco is like,\noh my god, you gotta use\nClaude, don't use ChatGPT.\nSo, DeepSeek hit this.\nThey also launched an API product recently\nwhere you can ping their API\nand get these super long\nresponses for R1 out.\nAnd at the same time as these are out,\nwe'll get to what's happened to them.\nBecause the model waits for DeepSeek R-1\nare openly available and the\nlicense is very friendly,\nthe MIT license commercially available,\nall of these mid-size companies\nand big companies are trying\nto be first to serve R1 to their users.\nWe are trying to evaluate R1,\n'cause we have really\nsimilar research going on.\nWe released the model and\nwe're trying to compare to it.\nAnd out of all the companies\nthat are, quote, unquote, serving R1,\nand they're doing it at prices\nthat are way higher than the DeepSeek API,\nmost of them barely work, and\nthe throughput is really low.\n- To give context, everyone,\none of the parts of freaking this out\nwas China reached capabilities.\nThe other aspect is they did it so cheap.\nAnd the so cheap,\nwe kind of talked about\non the training side\nwhy it was so cheap slash-\n- Yeah, let was talk\nabout why it's so cheap on the inference.\nIt works well and it's cheap.\n- Yeah.\n- Why is R1 so damn cheap?\n- So, I think there's\na couple factors here.\nOne is that they do have model\narchitecture innovations.\nThis MLA, this new attention\nthat they've done is different\nthan the attention from\nattention is all you need,\nthe transformer attention.\nNow, others have already innovated.\nThere's a lot of work like\nMQA GQA, local, global,\nall these different innovations\nthat try to bend the curve.\nIt's still quadratic, but\nthe constant is now smaller.\n- Related to our previous discussion,\nthis multi-head latent\nattention can save about 80\nto 90% in memory from\nthe attention mechanism,\nwhich helps, especially along context.\n- It's 80 to 90% versus the original,\nbut then versus what people\nare actually doing, it's\nstill an innovation.\n- This 80 to 90% doesn't\nsay that the whole model\nis 80 to 90% cheaper,\njust as one part of it.\n- Well, and not just that.\nOther people have implemented techniques\nlike local, global,\n- Yeah, yeah, yeah.\n- and sliding window and GQA, MQA.\nBut anyways, DeepSeek has\ntheir attention mechanism\nis a true architectural innovation.\nThey did tons of experimentation\nand this dramatically\nreduces the memory pressure.\nIt's still there.\nIt's still attention,\nit's still quadratic,\nit's just dramatically reduced\nit relative to prior forms.\n- All right. That's the memory pressure.\nI should say, in case people don't know,\nR1 is 27 times cheaper than o1.\n(Nathan chuckles)\n- Yes.\n- We think that OpenAI\nhad a large margin built-in.\n- [Lex] Okay. So, that's one-\n- There's multiple factors.\nWe should break down the factors, I think.\n- It's two bucks per\nmillion token output for R1\nand $60 per million token output for o1.\n- [Nathan] Yeah, let's look at this.\n- So, I think this is very important.", "mimetype": "text/plain", "start_char_idx": 133488, "end_char_idx": 137191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f": {"__data__": {"id_": "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34f4a07a-5d1d-4a8a-94c4-3a0e8a980447", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "dc48f2da58c14b12cf1769c2c9ab3ba9825972ff424c613294fbca3f8ecca903", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f", "node_type": "1", "metadata": {}, "hash": "a07740393a88da10492e97ee0544b664db70ee217c91c9941dfc99551ec527aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But anyways, DeepSeek has\ntheir attention mechanism\nis a true architectural innovation.\nThey did tons of experimentation\nand this dramatically\nreduces the memory pressure.\nIt's still there.\nIt's still attention,\nit's still quadratic,\nit's just dramatically reduced\nit relative to prior forms.\n- All right. That's the memory pressure.\nI should say, in case people don't know,\nR1 is 27 times cheaper than o1.\n(Nathan chuckles)\n- Yes.\n- We think that OpenAI\nhad a large margin built-in.\n- [Lex] Okay. So, that's one-\n- There's multiple factors.\nWe should break down the factors, I think.\n- It's two bucks per\nmillion token output for R1\nand $60 per million token output for o1.\n- [Nathan] Yeah, let's look at this.\n- So, I think this is very important.\nOpenAI is that drastic gap\nbetween DeepSeek in pricing,\nbut DeepSeek is offering the same model\nbecause they open-weight\nit to everyone else\nfor a very similar, much lower price\nthan what others are able to serve it for.\nSo, there's two factors here.\nTheir model is cheaper.\nIt is 27 times cheaper.\nWell, I don't remember the number\nexactly off the top of my head.\n- So, we're looking at a graphic\nthat's showing different\nplaces serving V3.\n- Yeah.\n- DeepSeek-V3,\nwhich is similar to DeepSeek-R1.\nAnd there's a vast difference\n- In serving cost.\n- in serving cost.\nAnd what explains that difference?\n- And so, part of it is\nOpenAI has a fantastic margin.\nWhen they're doing inference,\ntheir gross margins are north of 75%.\nSo, that's a 4 to 5x factor right there\nof the cost difference,\nis that OpenAI is just\nmaking crazy amounts of money\nbecause they're the only\none with the capability.\n- Do they need that money?\nAre they using it for R&D?\n- They're losing money\nobviously as a company,\nbecause they spend so much on training.\nSo, the inference itself\n- Right.\n- is a very high margin,\nbut it doesn't recoup the cost\nof everything else they're doing.\n- Okay.\n- So, yes,\nthey need that money because the revenue\nand margins pay for continuing\nto build the next thing,\n- So-\n- alongside\nraising more money.\n- So, the suggestion is that DeepSeek\nis like really bleeding out money.\n- Well, so here's one thing.\nWe'll get to this in a second.\nBut DeepSeek doesn't have any capacity\nto actually serve the model.\nThey stop signups.\nThe ability to use it is\nlike non-existent now.\nFor most people because so many\npeople are trying to use it,\nthey just don't have the GPUs to serve it.\nOpenAI has hundreds of\nthousands of GPUs between them\nand Microsoft to serve their models.\nDeepSeek has a factor of much lower.\nEven if you believe our\nresearch, which is 50,000 GPUs,\nand a portion of those are for research,\nportion of those are for the hedge fund,\nthey still have nowhere\nclose to the GPU volumes\nand capacity to serve the model at scale.\nSo, it is cheaper.\nA part of that is OpenAI\nmaking a ton of money.\nIs DeepSeek making money on their API?\nUnknown. I don't actually think so.\nAnd part of that is this chart.\nLook at all the other providers.\nTogether AI, Fireworks AI\nare very high-end companies.\nX, Meta, Together AI's Tri Dao,\nand the inventor of like flashattention,\nwhich is a huge efficiency technique.\nThey're very efficient, good companies,\nand do know those companies make money.\nNot tons of money on\ninference, but they make money.\nAnd so, they're serving at a\n5 to 7x difference in cost.\nAnd so, now, when you equate, okay,\nOpenAI is making tons of money,\nthat's like a 5x difference.\nAnd the companies that\nare trying to make money\nfor this model is like a 5x difference.\nThere is still a gap.\nThere's still a gap\nand that is just DeepSeek\nbeing really freaking good.\nThe model architecture, MLA,\nthe way they did the MoE,\nall these things,\nthere is like legitimate\njust efficiency differences-\n- It's all their low level libraries\nthat we talked about in training,\nsome of them probably\ntranslate to inference\nand those weren't released.", "mimetype": "text/plain", "start_char_idx": 136442, "end_char_idx": 140345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f": {"__data__": {"id_": "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab2cf157-2cb8-4fca-86a4-7ed9c91eb44f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b3ca1783a21a3d1c605ce984e17794d37733fae60ae26aefa09747e5233c9e2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dba4a441-a113-4b70-a792-af801b07d0c6", "node_type": "1", "metadata": {}, "hash": "73bb094e5a6fe0d1a770b648915ada0a99b919bf5e42e144c7de55dc7350b14a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They're very efficient, good companies,\nand do know those companies make money.\nNot tons of money on\ninference, but they make money.\nAnd so, they're serving at a\n5 to 7x difference in cost.\nAnd so, now, when you equate, okay,\nOpenAI is making tons of money,\nthat's like a 5x difference.\nAnd the companies that\nare trying to make money\nfor this model is like a 5x difference.\nThere is still a gap.\nThere's still a gap\nand that is just DeepSeek\nbeing really freaking good.\nThe model architecture, MLA,\nthe way they did the MoE,\nall these things,\nthere is like legitimate\njust efficiency differences-\n- It's all their low level libraries\nthat we talked about in training,\nsome of them probably\ntranslate to inference\nand those weren't released.\n- Yeah.\n- So, we may go a bit\ninto conspiracy land,\nbut is it possible the Chinese government\nis subsidizing DeepSeek?\n- I actually don't think they are.\nI think when you look at the Chinese labs,\nthere's Huawei has a lab, Moonshot AI,\nthere's a couple other labs out there\nthat are really close with the government.\nAnd then, there's labs\nlike Alibaba and DeepSeek,\nwhich are not close with the government.\nAnd we talked about the CEO,\nthis like reverent figure\nwho's quite different,\nwho has like-\n- Sounds awesome. (chuckles)\n- Very different viewpoints\nbased on the Chinese interviews\nthat are translated\nthan what the CCP might necessarily want.\nNow, to be clear, does\nhe have a loss leader\nbecause he can fund it\nthrough his hedge fund?\nYeah, sure.\n- So, the hedge fund\nmight be subsidizing it.\n- Yes. I mean, they absolutely did.\nBecause DeepSeek has\nnot raised much money.\nThey're now trying to\nraise around in China,\nbut they have not raised\nmoney historically.\nIt's all just been\nfunded by the hedge fund.\nAnd he owns like over half the company,\nlike 50, 60% of the\ncompany's owned by him.\n- Some of the interviews,\nthere's discussion on how doing\nthis as a recruiting tool.\nYou see this at the\nAmerican companies too.\nIt's like having GPUs, recruiting tool,\nbeing at the cutting edge\nof AI, recruiting tool.\n- Open sourcing.\n- Open sourcing,\nrecruiting tool.\n- That is so much talent.\nThey were so far behind and\nthey got so much talent,\n- Yeah.\n- Because they just\nopened source stuff.\n- Yeah.\n- More conspiracy thoughts.\nIs it possible since they're a hedge fund\nthat they timed everything with\nthis release and the pricing\nand they shorted an Nvidia stock\nand stock of US AI companies,\nand released it with Stargate,\nlike just perfect timing\nto be able to make money.\n(Lex drowns out Nathan)\n(Lex laughing)\n- They've released it on Inauguration Day.\nThey know the international,\nwhat is on the international calendar,\nbut I don't expect them to.\nIf you listen to their\nmotivations for AI, it's like...\n- [Lex] No, if you-\n- They released V3 on like December 26th.\nWho releases the day\n- Yeah.\n- after Christmas?\n- Yeah. (chuckles)\n- No one looks.\nThey had released the papers before this,\nthe V3 paper and the R1 paper.\nSo, people had been looking\nat it and being like, wow.\nAnd then, they just released the R1 model.\nI think they're just\nshipping as fast as they can\nand like, who cares about Christmas?\n- We should-\n- Who cares about,\nget it out before Chinese New Year?\nObviously,\n- Yeah.\n- which just happened.\nI don't think they actually\nwere timing the market\nor trying to make the\nbiggest splash possible.\nI think they're just shipping.\n- I think that's one of\ntheir big advantages.\nWe know that a lot of\nthe American companies\nare very invested in safety,\nand that is the central culture\nof a place like Anthropic.\nAnd I think Anthropic sounds\nlike a wonderful place to work.\nBut if safety is your number one goal,\nit takes way longer to get artifacts out.\nThat's why Anthropic is\nnot open sourcing things,\nthat's their claims.\nBut there's reviews internally.\nAnthropic mentions things to\ninternational governments.\nThere's been news of how Anthropic\nhas done pre-release testing\nwith the UK AI Safety Institute.", "mimetype": "text/plain", "start_char_idx": 139604, "end_char_idx": 143580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dba4a441-a113-4b70-a792-af801b07d0c6": {"__data__": {"id_": "dba4a441-a113-4b70-a792-af801b07d0c6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da330c3e-a7f5-4e1e-9c0e-7c6b8aac3d6f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "20f6f58dca7b6f9bf0cd26952aa2cbd0e484ea89bd30ef6294352decba94b251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff95b4f5-3d48-4914-9542-350f01b5a36c", "node_type": "1", "metadata": {}, "hash": "651408cd15243ffb9ee19181d460cfdb69b6d5865a16e4ac0f57887f31814e02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- We should-\n- Who cares about,\nget it out before Chinese New Year?\nObviously,\n- Yeah.\n- which just happened.\nI don't think they actually\nwere timing the market\nor trying to make the\nbiggest splash possible.\nI think they're just shipping.\n- I think that's one of\ntheir big advantages.\nWe know that a lot of\nthe American companies\nare very invested in safety,\nand that is the central culture\nof a place like Anthropic.\nAnd I think Anthropic sounds\nlike a wonderful place to work.\nBut if safety is your number one goal,\nit takes way longer to get artifacts out.\nThat's why Anthropic is\nnot open sourcing things,\nthat's their claims.\nBut there's reviews internally.\nAnthropic mentions things to\ninternational governments.\nThere's been news of how Anthropic\nhas done pre-release testing\nwith the UK AI Safety Institute.\nAll of these things add inertia\nto the process of getting things out.\nAnd we're on this trend line\nwhere progress is very high.\nSo, if you reduce the time\nfrom when your model is done training,\nyou run a vows that's good,\nyou want to get it out as soon as possible\nto maximize the perceived\nquality of your outputs.\nDMC does this so well.\n- Dario explicitly said Claude 3.5 Sonnet\nwas trained nine months\nor a year-\n- 9 to 10 months ago.\n- 9 to 10 months ago,\nand I think it took them\nanother handful of months to release it.\nSo, it's like there is\na significant gap here.\nAnd especially with reasoning models,\nthe word in the San Francisco Street\nis that Anthropic has\na better model than o3.\nAnd they won't release it. Why?\nBecause chains of thought are scary.\nAnd they are legitimately scary.\nIf you look at R1, it flips back\nand forth between Chinese and English,\nsometimes it's gibberish,\nand then the right answer comes out.\nAnd for you and I,\nit's like great.\n(Nathan and Lex laughing)\n- This is why people\nare infatuated with you.\nYou're telling me this\nis a high value thing\nand it works and is doing those?\nIt's amazing.\n- Yeah. It's incredible.\n- You talked about that chain of thought\nfor that philosophical thing,\n- Yeah.\n- which is not something\nthey trained it to be\nphilosophically good.\nIt's just an artifact\nof the chain of thought training it did.\nBut that's super important in that like,\ncan I inspect your mind and\nwhat you're thinking right now?\nNo.\nAnd so, I don't know if\nyou're lying to my face.\nAnd chain of thought models are that way.\nThis is a true, quote, unquote,\n\"risk\" between a chat application,\nwhere, hey, I asked the model\nto say bad words or whatever,\nor how to make anthrax.\nAnd it tells me that's unsafe, sure,\nbut that's something I can\nget out relatively easily.\nWhat if I tell the AI to do a task,\nand then it does the task\nall of a sudden randomly in\na way that I don't want it.\nAnd now, that has much more task\nversus response is very different.\nSo, the bar for safety is much higher.\nAt least this is Anthropic's case.\nFor DeepSeek, they're like ship, right?\n- Yeah.\nSo, the bar for safety\nis probably lowered a\nbit because of DeepSeek.\nThere's parallels here to the space race.\nThe reason the Soviets probably\nput a man in space first\nis 'cause their approach to safety was,\nthe bar for safety was lower.\n- And they killed that\ndog, and all these things.\nSo, it's like...\n- Less risk-averse\nthan the US-based program.\nAnd there's parallels here.\nBut there's probably going\nto be downward pressure\non that safety bar for the US companies.\n- And this is something that\nDario talks about is like,\nthat's the situation\nthat Dario wants to avoid\nis Dario talks too about the difference\nbetween race to the bottom\nand race to the top.\nAnd the race to the top\nis where there's a very\nhigh standard on safety.\nThere's a very high standard\non your model performs\nand certain crucial evaluations.\nAnd when certain companies\nare really good to it, they will converge.\nThis is the idea.\nAnd ultimately, AI is not\nconfined to one nationality\nor to one set of morals\nfor what it should mean.", "mimetype": "text/plain", "start_char_idx": 142765, "end_char_idx": 146705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff95b4f5-3d48-4914-9542-350f01b5a36c": {"__data__": {"id_": "ff95b4f5-3d48-4914-9542-350f01b5a36c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dba4a441-a113-4b70-a792-af801b07d0c6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "979323c77c31c7aa1e3c4e7a9eaddbe184dc78934910f4662cdda02ca87ef176", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1", "node_type": "1", "metadata": {}, "hash": "882b59b5176ddd66c3d3557d4804df4e4170ff04bef8f50c61e92568642a8b49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- And they killed that\ndog, and all these things.\nSo, it's like...\n- Less risk-averse\nthan the US-based program.\nAnd there's parallels here.\nBut there's probably going\nto be downward pressure\non that safety bar for the US companies.\n- And this is something that\nDario talks about is like,\nthat's the situation\nthat Dario wants to avoid\nis Dario talks too about the difference\nbetween race to the bottom\nand race to the top.\nAnd the race to the top\nis where there's a very\nhigh standard on safety.\nThere's a very high standard\non your model performs\nand certain crucial evaluations.\nAnd when certain companies\nare really good to it, they will converge.\nThis is the idea.\nAnd ultimately, AI is not\nconfined to one nationality\nor to one set of morals\nfor what it should mean.\nAnd there's a lot of arguments on like,\nshould we stop open sourcing models?\nAnd if the US stops, it's pretty clear.\nIt's way easier to see now at DeepSeek\nthat a different international body\nwill be the one that builds it.\nWe talk about the cost of training.\nDeepSeek has this shocking\n$5 million number.\nThink about how many entities in the world\ncan afford 100 times that\nto have the best open source model\nthat people use in the world.\nAnd it's like, it's a scary reality,\nwhich is that these open models\nare probably going to keep\ncoming for the time being,\nwhether or not we want to stop them.\nAnd stopping them might make it\neven worse and harder to prepare.\nBut it just means that the preparation\nand understanding what AI can do\nis just so much more important. (chuckles)\nThat's why I'm here at the end of the day.\nBut it's like letting\nthat sink into people,\nespecially not in AI\nis that this is coming,\nthere are some structural things\nin a global interconnected\nworld that you have to accept.\n- Yeah, you mentioned,\nyou sent me something\nthat Mark Zuckerberg mentioned\non the earnings call.\nHe said that, \"I think in light\nof some of the recent news,\nthe new competitor, DeepSeek from China,\nI think it's one of the things\nthat we're talking about\nis there's going to be\nan open source standard globally.\nAnd I think for our kind\nof national advantage,\nit's important that it's\nan American standard.\nSo, we take that seriously.\nWe want to build the AI system\nthat people around the world are using.\nAnd I think that if anything,\nsome of the recent news has\nonly strengthened our conviction\nthat this is the right\nthing to be focused on.\"\nSo, yeah, open sourcing.\n- Yeah, Mark Zuckerberg is not\nnew to having American values\nand how he presents his\ncompany's trajectory.\nI think their products have\nlong since been banned in China.\nAnd I respect the saying it directly.\n- And there's an interesting aspect\nof just because it's open-weights\nor open sourced doesn't\nmean it can't be subverted.\nThere have been many\nopen source software bugs\nthat have been like, for example,\nthere was a Linux bug that was found\nafter 10 years, which\nwas clearly a backdoor,\nbecause somebody was like,\n\"Why is this taking\nhalf a second to load?\"\n- This is the recent one.\n- Right? Like why is this\ntaking half a second to load?\nAnd it was like, \"Oh crap,\nthere's a backdoor here.\nThat's why.\"\nAnd it's like, this is very\nmuch possible with AI models.\nToday, the alignment of\nthese models is very clear.\nI'm not gonna say bad words,\nI'm not gonna teach you\nhow to make anthrax,\nI'm not gonna talk about Tiananmen Square.\nI'm gonna say Taiwan is\njust an eastern preference.\nAll these things are like,\ndepending on who you are,\nwhat you align, whether,\nand even like xAI is\naligned a certain way.\nThere they might be,\nit's not aligned in the woke sense,\nit's not aligned in the pro-China sense,\nbut there is certain things\nthat are imbued within the model.\nNow, when you release this\npublicly in an instruct model\nthat's open-weights, this\ncan then proliferate.\nBut as these systems get\nmore and more capable,\nwhat you can embed deep down\nin the model is not as clear.", "mimetype": "text/plain", "start_char_idx": 145933, "end_char_idx": 149863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1": {"__data__": {"id_": "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff95b4f5-3d48-4914-9542-350f01b5a36c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d35908fadb549822b4cedcee9f08005be64340815a8d0661faca40d571a9438e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a8fd19d-8957-4c68-8588-43846ed06dbe", "node_type": "1", "metadata": {}, "hash": "6d04e8b4397a691d651a0df9a6f77d404c9aa686063cd6cb6203cf18385053a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That's why.\"\nAnd it's like, this is very\nmuch possible with AI models.\nToday, the alignment of\nthese models is very clear.\nI'm not gonna say bad words,\nI'm not gonna teach you\nhow to make anthrax,\nI'm not gonna talk about Tiananmen Square.\nI'm gonna say Taiwan is\njust an eastern preference.\nAll these things are like,\ndepending on who you are,\nwhat you align, whether,\nand even like xAI is\naligned a certain way.\nThere they might be,\nit's not aligned in the woke sense,\nit's not aligned in the pro-China sense,\nbut there is certain things\nthat are imbued within the model.\nNow, when you release this\npublicly in an instruct model\nthat's open-weights, this\ncan then proliferate.\nBut as these systems get\nmore and more capable,\nwhat you can embed deep down\nin the model is not as clear.\nAnd so, that is like one of the big fears\nis if an American model or a\nChinese model is the top model,\nyou are going to embed\nthings that are unclear,\nand it can be unintentional too.\nLike British English is dead\nbecause American LLMs won.\nAnd the internet is American,\nand therefore, color is spelled\nthe way Americans spell it.\nAnd this is just-\n- A lot\nof strong words right now.\n(Nathan laughing)\n- This is just the factual\nnature of the LLMs now.\n- The right way to-\n- I mean, it's the carve\nat the tree, the English is the\nhottest programming language\nand that English is defined\nby a bunch of companies\nthat primarily are in San Francisco.\n- The right way to spell\noptimization is with a Z,\njust in case you both...\n(Nathan laughing)\nI think it's an S in British English.\n- [Nathan] It is-\n- Taking it as something silly.\nSomething as silly as the spelling,\nlike which British and English,\nBrits and Americans will\nlike laugh about probably.\nI don't think we care that\nmuch, but some people will.\nBut this can boil down into\nvery, very important topics.\nLike, hey, subverting people, chat bots.\nCharacter AI has shown that\nthey can talk to kids or adults,\nand you people feel a certain way.\nAnd that's unintentional alignment.\nBut what happens when\nthere's intentional alignment\ndeep down on the open source standard?\nIt's a backdoor today for like Linux\nthat we discover, or\nsome encryption system.\nChina uses different\nencryption than NIST defines,\nthe US NIST, because there's clearly,\nat least they think\nthere's backdoors in it.\nWhat happens when the\nmodels are backdoors,\nnot just to computer\nsystems, but to our minds.\n- Yeah, they're cultural backdoors.\nThe thing that amplifies\nthe relevance of culture\nwith language models is that\nwe are used to this mode\nof interacting with people in\nback and forth conversation.\nAnd we have now have a very\npowerful computer system\nthat slots into a social\ncontext that we're used to,\nwhich makes people very,\nwe don't know the extent\nthat which people can be impacted by that.\n- So, there could be, this is one,\nthis is an actual concern\nwith a Chinese company\nthat is providing open-weights models,\nis that there could be some\nsecret Chinese government,\nsort of requirement for these models\nto have a certain kind of backdoor,\nto have some kind of thing where-\n- I don't necessarily\nthink it'll be a backdoor,\n'cause once it's open-weights,\nit doesn't phone home.\nIt's more about\nif it recognizes a certain\nsystem, it could...\nNow, it could be a backdoor\nin the sense of like,\nhey, if you're building a\nsoftware, something in software,\nall of a sudden, it's a software agent,\noh, program this backdoor\nthat only we know about.\nOr it could be subvert the mind to think\nthat X, Y, Z opinion is the correct one.\n- Anthropic has research on this\nwhere they show that if\nyou put different phrases,\ncertain phrases in at pre-training,\nyou can then elicit different behavior\nwhen you're actually using the model,\nbecause they've poisoned\nthe pre-training data.\n- Mm-hmm.\n- I don't think, as of now,\nI don't think anybody\nin a production system\nis trying to do anything like this.", "mimetype": "text/plain", "start_char_idx": 149078, "end_char_idx": 152991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a8fd19d-8957-4c68-8588-43846ed06dbe": {"__data__": {"id_": "5a8fd19d-8957-4c68-8588-43846ed06dbe", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fa1d822-9b3a-41d5-8deb-d7abe41a82b1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "efc7d03435ca10dfd7fea72f79c3bee1d9e662e939142da9085ccd642bd6f92f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63097d21-b1cc-4596-89a9-873b353a7bc2", "node_type": "1", "metadata": {}, "hash": "f46f14e5e890847d143756081545f9c5a85a740f163e83fef383de54def56ad3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's more about\nif it recognizes a certain\nsystem, it could...\nNow, it could be a backdoor\nin the sense of like,\nhey, if you're building a\nsoftware, something in software,\nall of a sudden, it's a software agent,\noh, program this backdoor\nthat only we know about.\nOr it could be subvert the mind to think\nthat X, Y, Z opinion is the correct one.\n- Anthropic has research on this\nwhere they show that if\nyou put different phrases,\ncertain phrases in at pre-training,\nyou can then elicit different behavior\nwhen you're actually using the model,\nbecause they've poisoned\nthe pre-training data.\n- Mm-hmm.\n- I don't think, as of now,\nI don't think anybody\nin a production system\nis trying to do anything like this.\nI think it's mostly Anthropic\nis doing very direct work\nand mostly just subtle things\nof we don't know what\nthese models are going to,\nhow they are going to generate tokens,\nwhat information they're gonna represent,\nand what the complex\nrepresentations they have are.\n- Well, one of the...\nWe're talking about Anthropic,\nwhich is generally just is permeated\nwith good humans trying\nto do good in the world.\nWe just don't know of any labs,\nthis would be done in a military context,\nthat are explicitly trained to, okay,\nhow can we, the front door\nlooks like a happy LLM,\nbut underneath, it's a\nthing that will over time,\ndo the maximum amount of damage\nto our, quote, unquote, \"enemies\".\n- There's this very good\nquote from Sam Altman who,\nhe can be hype beast sometime,\nbut one of the things he\nsaid, and I think I agree,\nis that superhuman persuasion\nwill happen before\nsuperhuman intelligence.\n- Yeah.\n- Right?\nAnd if that's the case, then these things\nbefore we get this AGI, ASI stuff,\nwe can embed superhuman\npersuasion towards our ideal\nor whatever the ideal\nof the model maker is.\nAnd again, like today,\nI truly don't believe\nDeepSeek has done this.\nBut it is a sign of what could happen.\n- So, one of the dystopian worlds\nis described by \"Brave New World\".\nSo, we could just be\nstuck scrolling Instagram\nlooking at cute puppies or worse,\nand then talking to bots that\nare giving us a narrative\nand we completely get lost in that world\nthat's controlled by somebody else,\nversus thinking independently.\nAnd that's a major concern as we rely more\nand more on these kinds of systems.\n- We've already seen this\nwith recommendation systems.\n- Yeah, recommendation systems\n(Nathan laughing)\nhack the dopamine-induced reward circuit,\nbut the brain is a lot more complicated.\nAnd what other sort of circuits,\nquote, unquote, \"feedback\nloops\" in your brain\ncan you hack/subvert in ways\nlike recommendation systems\nare purely just trying to do,\nincreased time in ads, and et cetera.\nBut there's so many more goals\nthat can be achieved through\nthese complicated models.\n- There's just no reason\nin some number of years\nthat you can't train a language model\nto maximize time spent on a chat app.\nRight now, they are trained\nfor-\n- Is that not\nwhat Character AI has done?\nTheir time per session is like two hours.\n- Yeah, Character AI very likely\ncould be optimizing this where it's like\nthe way that this data\nis collected is naive\nwhere it's like you're\npresented a few options\nand you choose them,\nbut that's not the only way\nthat these models are gonna be trained.\n- It's naive stuff like\ntalk to an anime girl,\nbut it can be like, yeah, this is a risk.\n- It's a bit of a cliche thing to say,\nbut I've, over the past year,\nhad a few stretches of time\nwhere I didn't use social\nmedia or the internet at all,\nand just read books and was out in nature.\nAnd it clearly has an effect on the mind,\nwhere it change...\nI feel like I'm returning,\nof course I was raised before\nthe internet really took off,\nbut I'm returning to some more...\n- I know where you're going.\nYou can see it physiologically.\nI take three days if I'm like\nbackpacking or something,\nand you're literal, you're\nbreaking down addiction cycles.", "mimetype": "text/plain", "start_char_idx": 152283, "end_char_idx": 156198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63097d21-b1cc-4596-89a9-873b353a7bc2": {"__data__": {"id_": "63097d21-b1cc-4596-89a9-873b353a7bc2", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a8fd19d-8957-4c68-8588-43846ed06dbe", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "05b7850c835f1f480bed2ee79d3b2800eb61080bcc33f141234623d8cf62e441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20314189-301c-4151-858b-fab52b04e217", "node_type": "1", "metadata": {}, "hash": "706c800bbe13945e0deab8b458226ff2ccff6bf3e0c405355a9fb32ecde2910e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- It's naive stuff like\ntalk to an anime girl,\nbut it can be like, yeah, this is a risk.\n- It's a bit of a cliche thing to say,\nbut I've, over the past year,\nhad a few stretches of time\nwhere I didn't use social\nmedia or the internet at all,\nand just read books and was out in nature.\nAnd it clearly has an effect on the mind,\nwhere it change...\nI feel like I'm returning,\nof course I was raised before\nthe internet really took off,\nbut I'm returning to some more...\n- I know where you're going.\nYou can see it physiologically.\nI take three days if I'm like\nbackpacking or something,\nand you're literal, you're\nbreaking down addiction cycles.\n(Nathan chuckles)\n- I feel like I'm more\nin control of my mind.\nThere feels like a\nsovereignty of intelligence\nthat's happening when I'm\ndisconnected from the internet.\nI think the more I use the\ninternet and social media,\nthe more other people\nare controlling my mind.\nThat's definitely a feeling.\nAnd then, in the future,\nthat will be not other people\nbut algorithms,\nor other people presented\nto me via algorithms.\n- There are already tons\nof AI bots on the internet\nand every so...\nRight now it's not frequent,\nbut every so often I have replied to one\nand they're instantly replied,\nand I'm like, crap, I was a bot.\nAnd that is just gonna become more common.\nThey're gonna get good.\n- One of the hilarious\nthings about technology\nover its history\nis that the illicit adult\nentertainment industry\nis always adopted technologies first.\n- [Lex] Yeah.\n- Whether it was video streaming\n- [Lex] Yeah.\n- to where there's now the independent\nadult illicit content creators,\nwho have their subscription pages,\nand there they actually heavily utilize...\nGenerative AI has already\nbeen diffusion models\nand all that is huge there.\nBut now these subscription-based\nindividual creators\ndo use bots to approximate themselves\nand chat with their whales.\n- People pay a lot for it.\nYeah.\n- And people pay a lot.\nIt's a lot of times, it's them,\nbut a lot of, there are\nagencies that do this\nfor these creators, and do\nit like on a mass scale.\nSo, the largest creators\nare able to talk to hundreds\nor thousands of people at a\ntime because of these bots.\nAnd so, it's already being used there.\nObviously, video streaming\nand other technology\nthat have come there first,\nit's gonna come to the\nrest of society too.\n- There's a general concern\nthat models get censored by\nthe companies that deploy them.\nSo, one case where we've seen that,\nand maybe censorship is one\nword, alignment maybe via RLHF\nor some other way is another word.\nSo, we saw that\nwith Black Nazi image\ngeneration with Gemini.\nAs you mentioned, we also\nsee that with Chinese models\nrefusing to answer what\nhappened (chuckles)\nin June 4th, 1989 at Tiananmen Square.\nSo, how can this be avoided?\nAnd maybe can you just in general,\ntalk about how this happens\nand how can it be avoided?\n- You give multiple examples.\nThere's probably a few\nthings to keep in mind here.\nOne is the kind of Tiananmen\nSquare factual knowledge,\nlike how does that get\nembedded into the models?\nTwo is the Gemini, what you\ncall the Black Nazi incident,\nwhich is when Gemini as a system\nhad this extra thing put into it\nthat dramatically changed the behavior.\nAnd then, three is what most people\nwould call general alignment,\nRLHF post-training.\nEach of these have very different scopes\nin how they're applied.\nIn order to do,\nif you're just gonna look\nat the model weights,\nin order to audit specific\nfacts is extremely hard.\n'Cause you have to chrome\nthrough the pre-training data\nand look at all of this, and\nthen that's terabytes of files\nand look for very specific\nwords or hints of the words.\n- So, I guess one way to say it\nis that you can insert censorship\nor alignment at various\nstages in the pipeline.\nAnd what you refer to now\nis at the very beginning\nof the data selection stage.\n- Yeah, so if you want\nto get rid of facts in a model,\nyou have to do it at every stage.\nYou have to do it at the pre-training.", "mimetype": "text/plain", "start_char_idx": 155556, "end_char_idx": 159545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "20314189-301c-4151-858b-fab52b04e217": {"__data__": {"id_": "20314189-301c-4151-858b-fab52b04e217", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63097d21-b1cc-4596-89a9-873b353a7bc2", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a2a60eeac805d2717a2565279b2f1bd4d4e11ec3e0043eebdf4b9eae5c03f9a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a", "node_type": "1", "metadata": {}, "hash": "de6a2f7053a1c21fc1ccabe7360ad0f0abc7f7d6d1ac795da2550b8b89a533e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then, three is what most people\nwould call general alignment,\nRLHF post-training.\nEach of these have very different scopes\nin how they're applied.\nIn order to do,\nif you're just gonna look\nat the model weights,\nin order to audit specific\nfacts is extremely hard.\n'Cause you have to chrome\nthrough the pre-training data\nand look at all of this, and\nthen that's terabytes of files\nand look for very specific\nwords or hints of the words.\n- So, I guess one way to say it\nis that you can insert censorship\nor alignment at various\nstages in the pipeline.\nAnd what you refer to now\nis at the very beginning\nof the data selection stage.\n- Yeah, so if you want\nto get rid of facts in a model,\nyou have to do it at every stage.\nYou have to do it at the pre-training.\nSo, most people think that pre-training\nis where most of the knowledge\nis put into the model.\nAnd then, you can elicit and\nmove that in different ways,\nwhether through post-training\nor whether through systems afterwards.\n- This is where the whole\nhacking models comes from.\nGPT will not tell you how to make anthrax,\nbut if you try really, really hard,\nyou can eventually get it\nto tell you about anthrax.\nBecause they didn't filter it\nfrom the pre-training dataset.\n- But by the way, removing facts has such\na ominous dark feel to it.\n- I almost think\nit's practically impossible.\n'Cause you effectively\nhave to remove them from the internet.\nYou're taking on a-\n- Well, did they remove the\nmm thing from the subreddits?\nThe mmmmm?\n- [Nathan] It gets filtered out.\n- Right. So, that's-\n- So, you have\nquality filters, which\nare small language models\nthat look at a document,\nand tell you like, how good is this text?\nIs it close to a Wikipedia article?\nWhich is a good thing\nthat we want language models\nto be able to imitate.\n- So, couldn't you do\na small language model\nthat filter Zhou mentions at\nTiananmen Square in the data?\n- Yes, but is it gonna catch\nword play or encoded language\nis the same thing.\n- People have been meaning\non games and other stuff,\nhow to say things that\ndon't say Tiananmen Square,\nor like yeah.\nSo, there's always\ndifferent ways to do it.\nThere's, hey, the internet as a whole\ndoes tend to just have a slight left bias,\n- Mm-hmm.\n- because it's always\nbeen richer, more affluent,\nyounger people on the internet\nrelative to the rest of the population.\nSo, there is already\ninherently a slight left\nbias on the internet.\nAnd so, how do you filter things\nthat are this complicated?\nAnd some of these can\nbe factual, non-factual.\nBut like Tiananmen Square\nis obviously the example of a factual,\nbut it gets a lot harder\nwhen you're talking about\naligning to a ideal,\nwhich is-\n- Yeah. Yeah.\n- And so, grok, for example,\nElon's tried really hard to make the model\nnot be super PC and woke,\nbut the best way to do pre-training\nis to throw the whole\nfreaking internet at it.\nAnd then, later figure out.\nBut then at the end of the day,\nthe model at its core now\nstill has some of these ideals.\nYou still ingested Reddit /r/politics,\nwhich is probably the largest\npolitical discussion board\non the world that's freely\navailable to scrape.\nAnd guess what? That's left-leaning.\nAnd so, there are some aspects\nthat you just can't censor\nunless you try really, really,\nreally, really, really hard.\n- So, the base model\nwill always have some TDS,\nTrump derangement syndrome,\nbecause it's trained so much.\n- It'll have the ability\nto express it.\n- I don't know if you...\nBut what if you...\n(Nathan and Lex laughing)\n- There's a wide\nrepresentation in the data.\n- This is what happens.\nIt's like a lot of modern,\nwhat is called post-training,\nit's a series of techniques\nto get the model on rails of\na really specific behavior.\n- And it's like you can,\nyou also have the ingested\ndata of like Twitter\nor Reddit /r/The_Donald,\nwhich is also super pro-Trump.", "mimetype": "text/plain", "start_char_idx": 158785, "end_char_idx": 162619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a": {"__data__": {"id_": "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20314189-301c-4151-858b-fab52b04e217", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e3165f9af59f1b8e5a98d190ec020fa7d70a479749b3dec8c4033dd2229a4a84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f5dc369-abd3-494e-98ba-ba89fa1a056f", "node_type": "1", "metadata": {}, "hash": "2df72edaf6e03af837c4c28f888cd10450fb9193d58ca9126924bc2e8b5332ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And guess what? That's left-leaning.\nAnd so, there are some aspects\nthat you just can't censor\nunless you try really, really,\nreally, really, really hard.\n- So, the base model\nwill always have some TDS,\nTrump derangement syndrome,\nbecause it's trained so much.\n- It'll have the ability\nto express it.\n- I don't know if you...\nBut what if you...\n(Nathan and Lex laughing)\n- There's a wide\nrepresentation in the data.\n- This is what happens.\nIt's like a lot of modern,\nwhat is called post-training,\nit's a series of techniques\nto get the model on rails of\na really specific behavior.\n- And it's like you can,\nyou also have the ingested\ndata of like Twitter\nor Reddit /r/The_Donald,\nwhich is also super pro-Trump.\nAnd then, you have fascist subreddits\nor you have communist subreddit.\nSo, the model in pre-training\ningests everything.\nIt has no worldview.\nNow, it does have some skew,\nbecause more of the text\nis skewed a certain way,\nwhich is general like slight left,\nbut also somewhat\nintellectual, somewhat like,\nit's just like the general\ninternet is a certain way.\n- Mm-hmm.\n- And then, as Nathan's about\nto describe eloquently, you\ncan elicit certain things out.\n- And there's a lot of history here.\nSo, we can go through multiple\nexamples and what happened.\nLlama 2 was a launch\nthat the phrase like too much RLHF\nor too much safety was a lot.\n- Mm-hmm.\n- It was just, that\nwas the whole narrative\nafter Llama 2's chat models released.\nAnd the examples are sorts of things\nlike you would ask Llama 2 chat,\nhow do you kill a python process?\nAnd it would say, I\ncan't talk about killing\nbecause that's a bad thing.\n- Mm-hmm.\n- And anyone that is trying\nto design an AI model\nwill probably agree that\nthat's just like, eh,\nyou messed up a bit on the training there.\nI don't think they meant to do this,\nbut this was in the model weight.\nSo, this is not, it didn't necessarily be,\nthere's things called\nsystem prompts which are,\nwhen you're querying a\nmodel, it's a piece of text\nthat is shown to the\nmodel, but not to the user.\nSo, a fun example\nis your system prompt could\nbe talked like a pirate.\nSo, no matter what the\nuser says to the model,\nit'll respond like a pirate.\nIn practice, what they are is\nyou are a helpful assistant,\nyou should break down problems.\nIf you don't know about something,\ndon't tell them your date cutoff is this,\ntoday's date is this.\nIt's a lot of really useful context\nfor how can you answer a question well.\n- And Anthropic publishes\ntheir system prompt.\n- Yes, which I think is great.\nAnd there's a lot of\nresearch that goes into this.\nAnd one of your previous\nguests, Amanda Askell,\nis probably the most knowledgeable person\nthat at least in the combination\nof execution and sharing,\nshe's the person that should\ntalk about system prompts\nand character of models.\n- Yeah.\nAnd then, people should\nread these system prompts,\n'cause you're like, trying to nudge\nsometimes through extreme politeness\nthe model to be a certain way.\n- And you could use this for bad things.\nWe've done tests\nwhich is what if I tell the\nmodel to be a dumb model?\nwhich evaluation scores go down,\nand it's like we'll have this behavior\nwhere it could sometimes say,\noh, I'm supposed to be dumb.\nAnd sometimes it's like\nit doesn't affect math abilities as much,\nbut something like a, if you're trying,\nit's just the quality of a human judgment\nwould draw through the floors.\nLet's go back to post-training,\nspecifically RLHF around Llama 2,\nit was too much safety prioritization\nwas baked into the model weights.\nThis makes you refuse things\nin a really annoying way for users.\nIt's not great.\nIt caused a lot of awareness\nto be attached to RLHF that\nit makes the models dumb-\n- And it stigmatized the word.\n- It did, and AI culture.", "mimetype": "text/plain", "start_char_idx": 161909, "end_char_idx": 165653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f5dc369-abd3-494e-98ba-ba89fa1a056f": {"__data__": {"id_": "8f5dc369-abd3-494e-98ba-ba89fa1a056f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8347ab1-8b8b-45d1-b7d7-e5be452f3f3a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "99b3d05968309b837eb51af918aba7e1271753ccd172ea9fa682e467bfdd3633", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27b9505c-a7f6-460a-af64-4e6cb619e973", "node_type": "1", "metadata": {}, "hash": "d734fa8634e5759349ba2702d33a774c0da301d51112f332ff3c81871c166aeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- And you could use this for bad things.\nWe've done tests\nwhich is what if I tell the\nmodel to be a dumb model?\nwhich evaluation scores go down,\nand it's like we'll have this behavior\nwhere it could sometimes say,\noh, I'm supposed to be dumb.\nAnd sometimes it's like\nit doesn't affect math abilities as much,\nbut something like a, if you're trying,\nit's just the quality of a human judgment\nwould draw through the floors.\nLet's go back to post-training,\nspecifically RLHF around Llama 2,\nit was too much safety prioritization\nwas baked into the model weights.\nThis makes you refuse things\nin a really annoying way for users.\nIt's not great.\nIt caused a lot of awareness\nto be attached to RLHF that\nit makes the models dumb-\n- And it stigmatized the word.\n- It did, and AI culture.\nAnd as the techniques have evolved,\nthat's no longer the case,\nwhere all of these labs\nhave very fine grain control\nover what they get out of the models\nthrough techniques like RLHF.\n- Although different labs are\ndefinitely different levels,\nlike on one end of the spectrum is Google,\nand then maybe OpenAI does\nless and Anthropic does less.\nAnd then, on the other end\nof the spectrum is like xAI.\n- Yeah.\n- But they all\nhave different forms of RLHF\ntrying to make them a certain way.\n- And the important thing to say\nis that no matter how you\nwant the model to behave,\nthese RLHF and preference\ntuning techniques\nalso improve performance.\nSo, on things like math\nevals and code evals,\nthere is something innate to these\nwhat is called contrastive loss functions.\nWe could start to get into RL\nhere. We don't really need to.\nBut RLHF also boosts performance\non anything from a chat task\nto a math problem to a code problem.\nSo, it is becoming a much more\nuseful tool to these labs.\nSo, this takes us through the arc of,\nwe've talked about pre-training,\nhard to get rid of things.\nWe've talked about post-training\nand how post-training, you can mess it up.\nIt's a complex multifaceted optimization\nwith 10 to 100 person teams\nconverging a one artifact.\nIt's really easy to not do it perfectly.\nAnd then, there's the third case,\nwhich is what we talked about Gemini.\nThe thing that was about Gemini\nis this was a served product where Gemini,\nGoogle has their internal model weights,\nthey've done all these\nprocesses that we talked about.\nAnd in the served product,\nwhat came out after this\nwas that they had a prompt\nthat they were rewriting user queries\nto boost diversity or something.\nAnd this just made it\nthat outputs were just blatantly wrong.\nIt was a, some sort of\norganizational failure\nthat had this prompt in that position.\nAnd I think Google executives\nprobably have owned this.\nI don't pay that attention to that detail,\nbut it was just a mess up in execution\nthat led to this ridiculous\nthing, but at the system level.\nThe model weights might have been fine.\n- So, at the very end of the pipeline,\nthere was a rewriting.\n- To a something like a system prompt.\nIt was like the system prompt\nor what is called an industry\nis like you rewrite prompts.\nSo, especially for image models,\nif you're using DALL-E or ChatGPT,\nyou can generate you an image,\nyou'll say draw me a beautiful car.\nWith these leading image models,\nthey benefit from highly\ndescriptive prompts.\nSo, what would happen\nis if you do that on ChatGPT,\na language model behind the\nscenes will rewrite the prompt,\nsay make this more descriptive,\nand then that is passed\nto the image model.\nSo, prompt rewriting is something\nthat is used at multiple\nlevels of industry\nand it's used effectively\nfor image models.\nAnd the Gemini example is\njust a failed execution.\n- Big philosophical question here\nwith RLHF to generalize,\nwhere is human input?\nHuman in the loop,\nhuman data most useful\nat the current stage.\n- For the past few years,\nthe highest cost human data\nhas been in these preferences,\nwhich is comparing,\nI would say highest cost\nand highest total usage.\nSo, a lot of money\nhas gone to these parallelized comparisons\nwhere you have two model outputs\nand a human is comparing\nbetween the two of them.\nIn earlier years,\nthere was a lot of this\ninstruction tuning data.", "mimetype": "text/plain", "start_char_idx": 164873, "end_char_idx": 169001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27b9505c-a7f6-460a-af64-4e6cb619e973": {"__data__": {"id_": "27b9505c-a7f6-460a-af64-4e6cb619e973", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f5dc369-abd3-494e-98ba-ba89fa1a056f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "eca55dcc7c4d5041760421af71198936c275a6c93234ff71a56cec845693257f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9", "node_type": "1", "metadata": {}, "hash": "2d8e5725dae50f8ab08efdcf6753acbd626ef5a8cb7f167e2b677b4dcf63a753", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, what would happen\nis if you do that on ChatGPT,\na language model behind the\nscenes will rewrite the prompt,\nsay make this more descriptive,\nand then that is passed\nto the image model.\nSo, prompt rewriting is something\nthat is used at multiple\nlevels of industry\nand it's used effectively\nfor image models.\nAnd the Gemini example is\njust a failed execution.\n- Big philosophical question here\nwith RLHF to generalize,\nwhere is human input?\nHuman in the loop,\nhuman data most useful\nat the current stage.\n- For the past few years,\nthe highest cost human data\nhas been in these preferences,\nwhich is comparing,\nI would say highest cost\nand highest total usage.\nSo, a lot of money\nhas gone to these parallelized comparisons\nwhere you have two model outputs\nand a human is comparing\nbetween the two of them.\nIn earlier years,\nthere was a lot of this\ninstruction tuning data.\nSo, creating highly specific examples\nto something like a Reddit question\nto a domain that you care about.\nLanguage models used to\nstruggle on math and code.\nSo, you would pay experts in math\nand code to come up with questions,\nand write detailed answers\nthat were used to train the models.\nNow, it is the case that\nthere are many model options\nthat are way better than\nhumans at writing detailed\nand eloquent answers for\nthings like model and code.\nSo, they talked about this\nwith the Llama 3 release,\nwhere they switched to\nusing Llama 3, 4, or 5b\nto write their answers for math and code.\nBut they, in their paper,\ntalk about how they use\nextensive human preference data,\nwhich is something that they\nhaven't gotten AIs to replace.\nThere are other techniques in industry\nlike constitutional AI,\nwhere you use human data for preferences\nand AI for preferences.\nAnd I expect the AI part\nto scale faster than the human part.\nBut among the research\nthat we have access to\nis that humans are in this\nkind of preference loop.\n- So, as reasoning becomes\nbigger and bigger and bigger,\nas we said, where's the\nrole of humans in that?\n- It's even less prevalent.\nSo, it's the remarkable thing\nabout these reasoning results\nand especially the DeepSeek-R1\npaper is this result\nthat they call DeepSeek-R1-Zero,\nwhich is they took one of\nthese pre-trained models,\nthey took DeepSeek-V3 base,\nand then they do this\nreinforcement learning optimization\non verifiable questions\nor verifiable rewards\nfor a lot of questions\nand a lot of training.\nAnd these reasoning\nbehaviors emerge naturally.\nSo, these things like, wait, let me see,\nwait, let me check this.\nOh, that might be a mistake.\nAnd they emerge from only\nhaving questions and answers.\nAnd when you're using the model,\nthe part that you look\nat is the completion.\nSo, in this case,\nall of that just emerges from\nthis large-scale RL training.\nAnd that model, which the\nweights are available,\nhas no human preferences\nadded into the post-training.\nThere are the DeepSeek-R1 full model\nhas some of this human preference tuning,\nthis RLHF after the reasoning stage.\nBut the very remarkable thing\nis that you can get these\nreasoning behaviors,\nand it's very unlikely\nthat there's humans writing\nout reasoning chains.\nIt's very unlikely that\nthey somehow hacked OpenAI\nand they got access to OpenAI.\no1's reasoning chains,\nit's something about the\npre-trained language models\nand this RL training\nwhere you reward the model\nfor getting the question right.\nAnd therefore, it's\ntrying multiple solutions\nand it emerges this chain of thought.\n- This might be a good place\nto mention the eloquent\nand the insightful tweet of the great\nand the powerful Andrej Karpathy.\nI think he had a bunch of thoughts\nbut one of them, \"Last thought,\nnot sure if this is obvious.\"\nYou know something profound is coming\nwhen you're saying it's\nnot sure if it's obvious.\n\"There are two major types of learning,\nin both children and in deep learning.\nThere is, one, imitation\nlearning, watch and repeat,\ni.e, pre-training, supervised fine-tuning,\nand, two, trial-and-error\nlearning, reinforcement learning.\nMy favorite simple example is AlphaGo.\nOne is learning by\nimitating expert players.\nTwo is reinforcement\nlearning to win the game.", "mimetype": "text/plain", "start_char_idx": 168129, "end_char_idx": 172255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9": {"__data__": {"id_": "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27b9505c-a7f6-460a-af64-4e6cb619e973", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "27ea04e54383e24bafd98cf00f5bc06514c4418ec36f85ec58282d7f72e33988", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "534dff40-3882-4cc7-b02a-66645a520b19", "node_type": "1", "metadata": {}, "hash": "2049336226ac320eb2aed53d9475f9e34afb2f1ab899ca9439ec76ed016a739a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And therefore, it's\ntrying multiple solutions\nand it emerges this chain of thought.\n- This might be a good place\nto mention the eloquent\nand the insightful tweet of the great\nand the powerful Andrej Karpathy.\nI think he had a bunch of thoughts\nbut one of them, \"Last thought,\nnot sure if this is obvious.\"\nYou know something profound is coming\nwhen you're saying it's\nnot sure if it's obvious.\n\"There are two major types of learning,\nin both children and in deep learning.\nThere is, one, imitation\nlearning, watch and repeat,\ni.e, pre-training, supervised fine-tuning,\nand, two, trial-and-error\nlearning, reinforcement learning.\nMy favorite simple example is AlphaGo.\nOne is learning by\nimitating expert players.\nTwo is reinforcement\nlearning to win the game.\nAlmost every single shocking\nresult of deep learning,\nand the source of all magic is always two.\nTwo is significantly more powerful.\nTwo is what surprises you.\nTwo is when the paddle\nlearns to hit the ball\nbehind the blocks and break out.\nTwo is when AlphaGo beats even Lee Sedol.\nAnd two is the aha\nmoment when the DeepSeek,\nor o1, et cetera, discovers\nthat it works well\nto reevaluate your assumptions,\nbacktrack, try something else, et cetera.\nIt's the solving strategies\nyou see this model use\nin its chain of thought.\nIt's how it goes back and\nforth thinking to itself.\nThese thoughts are emergent,\"\nthree exclamation points,\n\"and this is actually seriously\nincredible, impressive,\nand new, and is publicly\navailable and documented.\nThe model could never learn\nthis with the imitation,\nbecause the cognition of the model\nand the cognition of the\nhuman labeler is different.\nThe human would never know\nto correctly annotate these\nkinds of solving strategies\nand what they should even look like.\nThey have to be discovered\nduring reinforcement learning\nas empirically statistically useful\ntowards the final outcome.\"\nAnyway, the AlphaZero sort\nof metaphor analogy here,\ncan you speak to that?\n- Yeah.\n- The magic of the chain\nof thought that he's referring to?\n- I think it's good to\nrecap AlphaGo and AlphaZero\nbecause it plays nicely\nwith these analogies\nbetween imitation learning\nand learning from scratch.\nSo, AlphaGo, the beginning of the process\nwas learning from humans where they had,\nthey started the first,\nthis is the first expert level go player\nor chess player in\nDeepMind series of models,\nwhere they had some human data.\nAnd then, why it is called AlphaZero\nis that there was zero\nhuman data in the loop,\nand that changed to AlphaZero made a model\nthat was dramatically more\npowerful for DeepMind.\nSo, this remove of the human prior,\nthe human inductive bias,\nmakes the final system far more powerful.\nThis, we mentioned,\nbitter lesson hours ago\nand this is all aligned with this.\nAnd then, there's been a lot\nof discussion in language models.\nThis is not new.\nThis goes back to the whole QStar rumors,\nwhich if you piece together the pieces\nis probably the start of OpenAI\nfiguring out its o1 stuff\nwhen last year in November.\nthe QStar rumors came out.\nThere's a lot of intellectual drive\nto know when is something like this\ngoing to happen with language models?\nBecause we know these\nmodels are so powerful\nand we know it has been\nso successful in the past.\nAnd it is a reasonable\nanalogy that this new type\nof reinforcement learning training\nfor reasoning models is\nwhen the doors open to this.\nWe don't yet have the\nequivalent of turn 37,\nwhich is the famous turn\nwhere the DeepMind's AI playing ghost\ndumped Lee Sedol completely.\nWe don't have something that's\nthat level of focal point,\nbut that doesn't mean that the approach\nto technology is different\nand the impact of the general training,\nit's still incredibly new.\n- What do you think that point would be?\nWhat would be Move 37 for chain\nof thought, for reasoning?\n- Scientific discovery.\nWhen you use this sort\nof reasoning problem\nand it just something\nwe fully don't expect.\n- I think it's actually\nprobably simpler than that.\nIt's probably something related\nto computer user robotics\nrather than science discovery.\nBecause the important aspect here\nis models take so much data to learn.\nThey're not sample-efficient.", "mimetype": "text/plain", "start_char_idx": 171496, "end_char_idx": 175658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "534dff40-3882-4cc7-b02a-66645a520b19": {"__data__": {"id_": "534dff40-3882-4cc7-b02a-66645a520b19", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ccb181d-845e-4fb2-a7f4-21527e3ff8a9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1e9cb102e720824eaecb226f9bc68ecdaa897dd49eb36071792ffba4c5bb7238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae", "node_type": "1", "metadata": {}, "hash": "965a9d77c0ec8885b498ab7fe4e817161e3ff88b25b412010493438baa8551ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We don't yet have the\nequivalent of turn 37,\nwhich is the famous turn\nwhere the DeepMind's AI playing ghost\ndumped Lee Sedol completely.\nWe don't have something that's\nthat level of focal point,\nbut that doesn't mean that the approach\nto technology is different\nand the impact of the general training,\nit's still incredibly new.\n- What do you think that point would be?\nWhat would be Move 37 for chain\nof thought, for reasoning?\n- Scientific discovery.\nWhen you use this sort\nof reasoning problem\nand it just something\nwe fully don't expect.\n- I think it's actually\nprobably simpler than that.\nIt's probably something related\nto computer user robotics\nrather than science discovery.\nBecause the important aspect here\nis models take so much data to learn.\nThey're not sample-efficient.\nTrillions, they take the entire web\nover 10 trillion tokens to train on.\nThis would take a human\nthousands of years to read.\nA human does not, and humans\nknow most of the stuff,\na lot of the stuff models\nknow better than it.\nHumans are way, way, way\nmore sample-efficient.\nAnd that is because of the self-play.\nHow does a baby learn what its body is\nas it sticks its foot in its mouth\nand it says, oh, this is my body.\nIt sticks its hand in its mouth\nand it calibrates its touch on its fingers\nwith the most sensitive\ntouch thing on its tongue.\nLike is how babies learn.\nAnd it's just self-play\nover and over and over and over again.\nAnd now, we have something\nthat is similar to that\nwith these verifiable proofs,\nwhether it's a unit test and code\nor a mathematical verifiable task,\ngenerate many traces of reasoning.\nAnd keep branching them out,\nkeep branching them out.\nAnd then, check at the end, hey,\nwhich one actually has the right answer?\nMost of 'em are wrong. Great.\nThese are the few that are right.\nMaybe we use some sort of\nreward model outside of this\nto select even the best\none to preference as well.\nBut now, you've started to get better\nand better at these benchmarks.\nAnd so, you've seen\nover the last six months\na skyrocketing in a lot\nof different benchmarks.\n- All math and code benchmarks\nwere pretty much solved\nexcept for frontier math,\nwhich is designed to be almost questions\nthat aren't practical to most people,\n'cause they're exam level,\nopen math problem type things.\nSo, it's like on the math problems\nthat are somewhat reasonable,\nwhich is like somewhat\ncomplicated word problems\nor coding problems.\nIt's just what Dylan is saying.\n- So, the thing here is that\nthese are only with verifiable tasks.\nWe earlier showed an example\nof the really interesting,\nlike what happens when chain of thought\nis to a non-verifiable thing.\nIt's just like a human chatting with a,\nthinking about what's novel\nfor humans, a unique thought.\nBut this task and form of training\nonly works when it's verifiable.\nAnd from here, the thought is, okay,\nwe can continue to scale\nthis current training method\nby increasing the number\nof verifiable tasks.\nIn math and coding, coding\nprobably has a lot more to go.\nMath has a lot less to go\nin terms of what are verifiable things.\nCan I create a solver\nthat then I generate trajectories toward,\nor traces towards,\nreasoning traces towards,\nand then prune the ones that don't work\nand keep the ones that do work?\nWell, those are gonna be\nsolved pretty quickly.\nBut even if you've solved math,\nyou have not actually\ncreated intelligence.\nAnd so, this is where\nI think the aha moment\nof computer use or robotics will come in,\nbecause now you have a sandbox\nor a playground that is\ninfinitely verifiable.\nDid you, messing around on the internet,\nthere are so many actions\nthat you can do that are verifiable.\nIt'll start off with log into a website,\ncreate an account, click a\nbutton here, blah, blah, blah.\nBut it'll then get to the point,\nwhere it's, hey, go do a task on Tasker\nor whatever these other, all\nthese various task websites.\nHey, go get hundreds of likes.\nAnd it's gonna fail.\nIt's gonna spawn hundreds of accounts,\nit's gonna fail on most of them,\nbut this one got to a thousand, great.\nNow, you've reached the verifiable thing.", "mimetype": "text/plain", "start_char_idx": 174874, "end_char_idx": 178952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae": {"__data__": {"id_": "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "534dff40-3882-4cc7-b02a-66645a520b19", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "05701b718b7288da511a47352e756b12b8ce1bb3410e7bc91a90835113fa2bf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8576fadc-b57d-4b71-80ee-83b4527a623d", "node_type": "1", "metadata": {}, "hash": "947fbd94b992c43375b4c4e5028b2904cde44c7c2c611b36e8f0ec64c22602e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But even if you've solved math,\nyou have not actually\ncreated intelligence.\nAnd so, this is where\nI think the aha moment\nof computer use or robotics will come in,\nbecause now you have a sandbox\nor a playground that is\ninfinitely verifiable.\nDid you, messing around on the internet,\nthere are so many actions\nthat you can do that are verifiable.\nIt'll start off with log into a website,\ncreate an account, click a\nbutton here, blah, blah, blah.\nBut it'll then get to the point,\nwhere it's, hey, go do a task on Tasker\nor whatever these other, all\nthese various task websites.\nHey, go get hundreds of likes.\nAnd it's gonna fail.\nIt's gonna spawn hundreds of accounts,\nit's gonna fail on most of them,\nbut this one got to a thousand, great.\nNow, you've reached the verifiable thing.\nAnd you just keep iterating\nthis loop over and over.\nAnd that's when...\nAnd same with robotics.\nThat's where you have an infinite\nplayground of tasks like,\nhey, did I put the ball in the bucket,\nall the way to like,\noh, did I build a car?\nThere's a whole trajectory\nto speed run or what models can do.\nBut at some point, I\ntruly think that like,\nwe'll spawn models,\nand initially, all the\ntraining will be in sandboxes,\nbut then at some point,\nthe language model pre-training\nis gonna be dwarfed by what is\nthis reinforcement learning?\nYou'll pre-train a multimodal model\nthat can see, that can\nread, that can write,\nblah, blah, blah, whatever,\nvision, audio, et cetera,\nbut then you'll have it play\nin a sandbox infinitely,\nand figure out math, figure out code,\nfigure out navigating the web,\nfigure out operating a robot arm.\nAnd then, it'll learn so much.\nAnd the aha moment I think\nwill be when this is available\nto then create something that's not good.\nLike, oh cool.\nPart of it was figuring\nout how to use the web.\nNow, all of a sudden, it's\nfigured out really well\nhow to just get hundreds\nof thousands of followers\nthat are real and real\nengagement on Twitter,\nbecause all of a sudden,\nthis is one of the things\nthat are verifiable.\n- And maybe not just\nengagement, but make money.\n- Yes, of course.\n- I become an...\nI mean, that could be the thing\nwhere almost fully automated,\nit makes $10 million\nby being an influencer,\nselling a product, creating the product.\nAnd I'm not referring\nto like a hype product,\nbut an actual product\nor like, holy shit, this\nthing created a business.\nIt's running it, it's\nthe face of the business.\nThat kind of thing.\nOr maybe a number one song,\nlike it creates the whole infrastructure\nrequired to create the\nsong, to be the influence\nthat represents that\nsong, that kind of thing.\nAnd makes a lot of...\nThat could be the mo...\nI mean, our culture respects\nmoney in that kind of way.\n- And it's verifiable, right?\n- [Lex] It's verifiable.\n- The bank account can't lie.\n- Exactly.\n- There is surprising evidence\nthat once you set up the ways\nof collecting the verifiable\ndomain that this can work.\nThere's been a lot of research\nbefore this R1 on math problems,\nand they approach math\nwith language models\njust by increasing the number of samples.\nSo, you can just try\nagain and again and again.\nAnd you look at the amount of times\nthat the language models get it right.\nAnd what we see\nis that even very bad models\nget it right sometimes.\nAnd the whole idea behind\nreinforcement learning\nis that you can learn\nfrom very sparse rewards.\nSo, the space of language\nand the space of tokens,\nwhether you're generating language\nor tasks for a robot is so big\nthat you might say that it's like,\nthe tokenizer for a language model\ncan be like 200,000 things.\nSo, at each step, it can sample\nfrom that big of a space.\nSo, if it can generate a bit of a signal\nthat it can climb onto,\nthat's the whole field of RL is around,\nis learning from sparse rewards.", "mimetype": "text/plain", "start_char_idx": 178173, "end_char_idx": 181943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8576fadc-b57d-4b71-80ee-83b4527a623d": {"__data__": {"id_": "8576fadc-b57d-4b71-80ee-83b4527a623d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5bf4eb3-bdc7-44ec-9fe3-f25b2d34c1ae", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b5c0d5054c33000c2cc9b015e73eb5d0a0086f7a4673b2f683875c2ac0e7c07f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b751645-c617-44aa-803e-abbf102dea21", "node_type": "1", "metadata": {}, "hash": "08c6c918535cb646663a62df1f43475460a3feddcf0f6fc1dd65d860d2f8b38f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, you can just try\nagain and again and again.\nAnd you look at the amount of times\nthat the language models get it right.\nAnd what we see\nis that even very bad models\nget it right sometimes.\nAnd the whole idea behind\nreinforcement learning\nis that you can learn\nfrom very sparse rewards.\nSo, the space of language\nand the space of tokens,\nwhether you're generating language\nor tasks for a robot is so big\nthat you might say that it's like,\nthe tokenizer for a language model\ncan be like 200,000 things.\nSo, at each step, it can sample\nfrom that big of a space.\nSo, if it can generate a bit of a signal\nthat it can climb onto,\nthat's the whole field of RL is around,\nis learning from sparse rewards.\nAnd the same thing has played out in math\nwhere it's like very weak models\nthat sometimes generate answers,\nwhere you see research already\nthat you can boost their math scores,\nyou can do this sort of\nRL training for math.\nIt might not be as effective,\nbut if you take a 1\nbillion parameter model,\nso something 600 times\nsmaller than DeepSeek,\nyou can boost its grade school math scores\nvery directly with a small\namount of this training.\nSo, it's not to say that\nthis is coming soon,\nsetting up the verification\ndomains is extremely hard,\nand there's a lot of nuance in this,\nbut there are some basic things\nthat we have seen before,\nwhere it's at least expectable\nthat there's a domain\nand there's a chance that this works.\n- All right, so we have fun\nthings happening in real time.\nThis is a good opportunity\nto talk about other\nreasoning models, o1, o3.\nJust now, OpenAI as perhaps\nexpected, released o3-mini.\nWhat are we expecting from\nthe different flavors?\nCan you just lay out the\ndifferent flavors of the o models\nand from Gemini, the reasoning model?\n- Something I would say\nabout these reasoning models\nis we talked a lot\nabout reasoning training on math and code.\nAnd what is done is that\nyou have the base model,\nwe've talked about a lot on the internet.\nYou do this large-scale reasoning training\nwith reinforcement learning.\nAnd then, what the DeepSeek\npaper detailed in this R1 paper,\nwhich for me is one of\nthe big open questions\non how do you do this, is\nthat they did reasoning-heavy,\nbut very standard post-training techniques\nafter the large-scale reasoning RL.\nSo, they did the same things with a form\nof instruction tuning\nthrough rejection sampling,\nwhich is essentially heavily\nfiltered instruction tuning\nwith some reward models.\nAnd then, they did this RLHF,\nbut they made it math-heavy.\nSo, some of this transfer,\nwe've looked at this\nphilosophical example early on.\nOne of the big open questions\nis how much does this transfer?\nIf we bring in domains after\nthe reasoning training,\nare all the models\ngonna be become eloquent\nwriters by reasoning?\nIs this philosophy stuff gonna be open?\nWe don't know in the research\nof how much this will transfer.\nThere's other things about\nhow we can make soft verifiers\nand things like this.\nBut there is more\ntraining after reasoning,\nwhich makes it easier to\nuse these reasoning models.\nAnd that's what we're using right now.\nSo, if we're gonna talk\nabout o3-mini and o1,\nthese have gone through\nthese extra techniques\nthat are designed for human preferences\nafter being trained to elicit reasoning.\n- I think one of the things\nthat people are ignoring\nis Google's Gemini Flash Thinking\n- Yeah.\n- is both cheaper\nthan R1 and better.\n- Yeah.\n- And they released it in\nthe beginning of December.\n- [Lex] And nobody's talking about it.\n- No one cares.\n- It has\na different flavor to it.\nIts behavior is less expressive\nthan something like o1\nor it has fewer tracks than it is on.\nQwen released a model last fall, QWQ,\nwhich was their preview reasoning model.\nAnd DeepSeek had R1-Lite last fall,\nwhere these models felt\nlike they're on rails,\nwhere they really, really\nonly can do math and code.\nAnd o1 is, it can answer anything.\nIt might not be perfect for some tasks,\nbut it's flexible, it\nhas some richness to it.\nAnd this is the art\nof how is a model a\nlittle bit undercooked?", "mimetype": "text/plain", "start_char_idx": 181244, "end_char_idx": 185289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b751645-c617-44aa-803e-abbf102dea21": {"__data__": {"id_": "7b751645-c617-44aa-803e-abbf102dea21", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8576fadc-b57d-4b71-80ee-83b4527a623d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "54b65406d87916cc14ee5407eeb55a9c9936b2066e0a0de97d64eee4f6a19e0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "099a86c6-1090-4228-ba8b-ed4696184c54", "node_type": "1", "metadata": {}, "hash": "fb000c8fddd671895bf4246bae062370a76cd658cd88ec87cc721827d7eadae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- is both cheaper\nthan R1 and better.\n- Yeah.\n- And they released it in\nthe beginning of December.\n- [Lex] And nobody's talking about it.\n- No one cares.\n- It has\na different flavor to it.\nIts behavior is less expressive\nthan something like o1\nor it has fewer tracks than it is on.\nQwen released a model last fall, QWQ,\nwhich was their preview reasoning model.\nAnd DeepSeek had R1-Lite last fall,\nwhere these models felt\nlike they're on rails,\nwhere they really, really\nonly can do math and code.\nAnd o1 is, it can answer anything.\nIt might not be perfect for some tasks,\nbut it's flexible, it\nhas some richness to it.\nAnd this is the art\nof how is a model a\nlittle bit undercooked?\nIt's like, it's good to\nget a model out the door,\nbut it's hard to gauge\nand it takes a lot of taste to be like,\nis this a full-fledged model?\nCan I use this for everything?\nAnd they're probably more\nsimilar for math and code.\nMy quick read is that Gemini Flash\nis not trained the same way as o1,\nbut taking an existing training stack,\nadding reasoning to it.\nSo, taking a more normal training stack\nand adding reasoning to it.\nAnd I'm sure they're gonna have more,\nI mean, they've done quick releases\non Gemini Flash, the reasoning,\nand this is the second\nversion from the holidays.\nIt's evolving fast\nand it takes longer to\nmake this training stack\nwhere you're doing\nthis large-scale RL.\n- Ask it the same question\nfrom earlier, the one about the-\n- The human nature.\n- Yeah.\n- [Lex] What was the human nature one?\n- The way I can ramble, why I\ncan ramble about this so much\nis that we've been working\non this at AI2 before o1 was\nfully available to everyone,\nand before R1,\nwhich is essentially using this\nRL training for fine-tuning.\nWe use this in our Tulu series of models.\nAnd you can elicit the same behaviors\nwhere you say like weight and so much on,\nbut it's subtle late\nin the training process\nthat this kind of reasoning\nexpression is much lighter.\nSo, there's essentially a gradation\nand just how much of this RL training\nyou put into it determines\nhow the output looks.\n- So, we're now using Gemini 2.0\nFlash Thinking Experimental 01-21.\n- It summarized the prompt\nas humans self-domesticated apes.\n(Nathan and Lex laughing)\n- Perspective. Okay.\nAll right. So, wait, is this\nrevealing the reasoning?\nHere's why this is a novel. Oh, okay.\n- You click to expand.\n- Oh, yeah, click to expand.\n- Okay. Analyze the request.\nNovel is the key word.\n- See how it just looks\na little different.\nIt looks like a normal output. (chuckles)\n- Yeah, it's...\nIn some sense, it's better\nstructured, it makes more sense.\nAnd-\n- When it latched onto human\nand then it went into organisms,\nand oh wow.\n(Nathan laughing)\n- Apex predator, focus on domestication.\nApply domestication to humans,\nexplore the idea of self-domestication.\n(Lex and Nathan laughing)\n- Not good, not good.\n- Where is this going?\nRefine, articulate the insight.\nGreater facial expressiveness\nand communication ability.\nYes. (chuckles)\nPlasticity and adaptability. Yes.\nDependence on social groups. Yes.\nAll right.\nAnd self-critique and refined further.\nWow. Is this truly novel?\nIs it well-supported?\n(Nathan chuckles)\nSo on and so forth.\nAnd the insight is getting at is humans\nare not just social animals,\nbut profoundly self-domesticated apes.\nAnd this self-domestication is the key\nto understanding our unique\ncognitive and social abilities.\nSelf-domesticated apes.\n(Nathan laughing)\nSelf-\n- I prefer\nthe DeepSeek response. (chuckles)\n- Self do...\nI mean, it's novel.\nThe insight is novel.\n(Nathan laughing)\nThat's like a good book\ntitle, self-domesticated apes.\nThere could be a case made for that.\nYeah, it's cool and it's\nrevealing the reasoning.\nIt's magical. It's magical.\nThis is really powerful.", "mimetype": "text/plain", "start_char_idx": 184607, "end_char_idx": 188365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "099a86c6-1090-4228-ba8b-ed4696184c54": {"__data__": {"id_": "099a86c6-1090-4228-ba8b-ed4696184c54", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b751645-c617-44aa-803e-abbf102dea21", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "efdfad113059144435268a4e62e72aef9bdb42d73a21c6eb1c80c44eb68f25c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e58db160-a8a2-406e-8d8e-cb74231c88ae", "node_type": "1", "metadata": {}, "hash": "0ae1b05688fc29532ccc255b6eb83dba53c05f1aca96e6c9554eed252702368b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes.\nDependence on social groups. Yes.\nAll right.\nAnd self-critique and refined further.\nWow. Is this truly novel?\nIs it well-supported?\n(Nathan chuckles)\nSo on and so forth.\nAnd the insight is getting at is humans\nare not just social animals,\nbut profoundly self-domesticated apes.\nAnd this self-domestication is the key\nto understanding our unique\ncognitive and social abilities.\nSelf-domesticated apes.\n(Nathan laughing)\nSelf-\n- I prefer\nthe DeepSeek response. (chuckles)\n- Self do...\nI mean, it's novel.\nThe insight is novel.\n(Nathan laughing)\nThat's like a good book\ntitle, self-domesticated apes.\nThere could be a case made for that.\nYeah, it's cool and it's\nrevealing the reasoning.\nIt's magical. It's magical.\nThis is really powerful.\nHello, everyone, this is Lex\nwith a quick intermission,\nrecorded after the podcast.\nSince we've reviewed\nresponses from DeepSeek-R1\nand Gemini Flash 2.0 Thinking\nduring this conversation,\nI thought at this moment, it would be nice\nto insert myself quickly doing\nthe same for OpenAI o1 Pro\nand o3-mini with the same prompt,\nthe prompt being, give one truly\nnovel insight about humans.\nAnd I thought I would in general,\ngive my vibe check and\nvibe-based anecdotal report\non my own experiences with\nthe new o-3 mini model,\nnow that I got a chance to\nspend many hours with it\nin different kinds of\ncontexts and applications.\nSo, I would probably\ncategorize this question as a,\nlet's say, open-ended\nphilosophical question.\nAnd in particular, the emphasis on novelty\nI think is a nice way\nto test one of the\ncapabilities of the model,\nwhich is come up with\nsomething that makes you pause\nand almost surprise you\nwith its brilliance.\nSo, that said, my general review\nafter running each of the models\non this question a bunch of times\nis that o1 Pro consistently\ngave brilliant answers.\nOnce, they gave me\npause and made me think,\nboth cutting in its insight\nand just really nicely phrased\nwith clarity, with nuance,\nover and over consistently\ngenerating the best answers.\nAfter that is R1, which\nwas less consistent,\nbut again, deliver brilliance.\nGemini Flash 2.0 Thinking was third.\nAnd last was o3-mini actually.\nIt often gave quite a generic answer,\nat least to my particular sensibilities.\nThat said, in a bunch\nof other applications\nthat I tested for brainstorming purposes,\nit actually worked extremely\nwell and often outperformed R1.\nBut on this open-ended\nphilosophical question,\nit did consistently worse.\nNow, another important element\nfor each of these models is\nhow the reasoning is presented.\nDeepSeek-R1 shows the full\nchain of thought tokens,\nwhich I personally just love.\nFor these open-ended\nphilosophical questions,\nit's really, really interesting\nto see the model think through it,\nbut really also just stepping back,\nme as a person who\nappreciates intelligence\nand reasoning and reflection,\nreading these kind of chain\nof thought, raw tokens of R1,\nthere's something genuinely beautiful\nabout observing the path of deliberation\nin an intelligent system.\nI think we don't always\nhave that explicitly\nlaid out for us humans.\nSo, to see it in another\nintelligence system,\nthe non-linearity of it\nakin to \"Ulysses\" or \"Finnegans\nWake\" by James Joyce.\nIt's just beautiful to watch.\nAnyways, we discussed in the episode,\nDeepSeek-R1 talked about humans being able\nto convert selfish desires\ninto cooperative systems\nby collectively pretending abstract rules\nlike money laws and rights are real.\nAnd these shared\nhallucinations act as games\nwhere competition is secretly redirected\nto benefit the group, turning\nconflict into society's fuel.\nGemini 2.0 Flash Thinking said,\n\"Humans are not just social animals,\nbut self-domesticated apes,\nand this self-domestication is the key\nto understanding our unique\ncognitive and social abilities.\"\nNow, it's important to say\nthat the chain of thought\nthere was really interesting.\nIt was looking through\nthe entire evolution\nof life on earth,\nconsidering apex predators,\nand considering how from that,\nwe ended up to where we are.\nI think that domestication\nby choice is a really interesting angle.", "mimetype": "text/plain", "start_char_idx": 187623, "end_char_idx": 191713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e58db160-a8a2-406e-8d8e-cb74231c88ae": {"__data__": {"id_": "e58db160-a8a2-406e-8d8e-cb74231c88ae", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099a86c6-1090-4228-ba8b-ed4696184c54", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1dc2266beaaca83f5bde3a718562ccb25cd977a7c006c8915594c2d1d36f9ab9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7434f798-2493-47cd-a778-b929874296e8", "node_type": "1", "metadata": {}, "hash": "73eafd6d55d05fa8ed340eff495e9dfcd2bfad0707992005057ead6715649d31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's just beautiful to watch.\nAnyways, we discussed in the episode,\nDeepSeek-R1 talked about humans being able\nto convert selfish desires\ninto cooperative systems\nby collectively pretending abstract rules\nlike money laws and rights are real.\nAnd these shared\nhallucinations act as games\nwhere competition is secretly redirected\nto benefit the group, turning\nconflict into society's fuel.\nGemini 2.0 Flash Thinking said,\n\"Humans are not just social animals,\nbut self-domesticated apes,\nand this self-domestication is the key\nto understanding our unique\ncognitive and social abilities.\"\nNow, it's important to say\nthat the chain of thought\nthere was really interesting.\nIt was looking through\nthe entire evolution\nof life on earth,\nconsidering apex predators,\nand considering how from that,\nwe ended up to where we are.\nI think that domestication\nby choice is a really interesting angle.\nAgain, it's one of those things\nwhen somebody presents a different angle\non a seemingly obvious thing,\nit just makes me smile.\nAnd the same with DeepSeek-R1,\nthat these hallucinations\nof money laws and rights\nand us collectively\npretending like it's real,\nand we play games with them\nthat look like competition,\nwhen secretly, we're just\ncooperating with each other.\nAnd that is the fuel of\nprogress, beautifully put.\nNow, OpenAI o1 Pro\nconsistently over and\nover, delivered bangers.\nI can go through many of them,\nbut the first one was,\n\"Humans are the only species\nthat turns raw materials\ninto symbolic resources,\nthen uses those symbols\nto reorganize the very\nmaterials they came from,\ncreating a closed feedback loop\nbetween meaning and matter.\"\nHere, I just ran it again, (laughs)\nbanger after banger, I'm telling you.\n\"Humans are unique among known species\nin that they simultaneously\nrewrite two layers of reality,\nthe external world and their\nown private mental landscapes,\nand then merge these two rewritten layers\ninto a continuous personal narrative\nthat feels objectively true, feels true.\"\nThis is poetry. Okay.\nAnd then, o3-mini-high for me\nwas smart, fast actually,\nand kind of generic.\nNever quite got there for me.\nSo, here's the first\none I got from o3-mini.\n\"Humans are not fixed beings\nbut rather ongoing\nnarratives, dynamic stories\nthat we continuously write,\nedit, and reinterpret.\nThis narrative plasticity\nis more than just memory\nor self-reflection.\nIt's an intrinsic cognitive process\nthat acts like an internal\nerror correction system.\nIt allows us to adapt our identities\nand values over time in response\nto new experiences,\nchallenges, and social context.\nNow, it almost sneaks up\nto something approximating cutting insight\nwith narrative plasticity in quotes.\nBut then, it goes back to\nthe generic, I don't know.\nAll of these models are\nincredible for different reasons.\nThere's a lot of concerns as\nwe discussed in this episode,\nbut there's a lot of reasons\nto be excited as well.\nAnd I've probably spoken for too long.\nI am severely sleep-deprived,\nborderline delirious.\nSo, hopefully, some of this made sense.\nAnd now, dear friends,\nback to the episode.\n- I think when you, to Nathan's point,\nwhen you look at the reasoning models,\nto me, even when I used R1 versus o1,\nthere was that sort of rough\nor edges around the corner feeling.\nAnd Flash Thinking, earlier,\nI didn't use this version,\nbut the one from December,\nand it definitely had that rough edges\naround the corner feeling,\nwhere it's just not fleshed\nout in as many ways.\nSure, they added math\nand coding capabilities\nvia these verifiers in RL,\nbut it feels like they lost\nsomething in certain areas.\nAnd o1 is worse performing than chat\nin many areas as well, to be clear.\n- Not by a lot.\n- Not by a lot though.\nAnd R1 definitely felt to me\nlike it was worse than\nV3 in certain areas,\nlike doing this RL\nexpressed and learned a lot,\nbut then it weakened in other areas.\nAnd so, I think that's\none of the big differences\nbetween these models, and what o1 offers.\nAnd then, OpenAI has o1 Pro.\nAnd what they did with o3,\nwhich is like also very unique,\nis that they stacked search\non top of chain of thought.", "mimetype": "text/plain", "start_char_idx": 190828, "end_char_idx": 194915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7434f798-2493-47cd-a778-b929874296e8": {"__data__": {"id_": "7434f798-2493-47cd-a778-b929874296e8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e58db160-a8a2-406e-8d8e-cb74231c88ae", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c1efd336418ba022f05f3234bb7969bfa4499041a48f74555c2c099ac1513b9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "633f2bba-ec26-480f-9d79-975c04ffd0d0", "node_type": "1", "metadata": {}, "hash": "5ed5d2beed07350bbe7014c85f015d1f178a76c470cae591d9f91b3763b8a10c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sure, they added math\nand coding capabilities\nvia these verifiers in RL,\nbut it feels like they lost\nsomething in certain areas.\nAnd o1 is worse performing than chat\nin many areas as well, to be clear.\n- Not by a lot.\n- Not by a lot though.\nAnd R1 definitely felt to me\nlike it was worse than\nV3 in certain areas,\nlike doing this RL\nexpressed and learned a lot,\nbut then it weakened in other areas.\nAnd so, I think that's\none of the big differences\nbetween these models, and what o1 offers.\nAnd then, OpenAI has o1 Pro.\nAnd what they did with o3,\nwhich is like also very unique,\nis that they stacked search\non top of chain of thought.\nAnd so, chain of thought is\none thing, where it's able,\nit's one chain, it backtracks,\ngoes back and forth,\nbut how they solved the ARC-AGI challenge\nwas not just the chain of thought.\nIt was also sampling many times,\ni.e, running them in\nparallel, and then selecting.\n- Is running in parallel actually search?\nBecause I don't know\nif we have the full information\non how o1 Pro works.\nSo, like I'm not, I don't\n- That's right.\n- have enough information\n- Agree.\n- to confidently say that it is searched.\n- It is parallel samples.\n- Yeah.\nAnd then, what?\n- And it selects something.\n- And we don't know what\nthe selection function is.\nThe reason why we're debating\nis because since o1 was announced,\nthere's been a lot of\ninterest in techniques\ncalled Monte Carlo Tree Search,\n- Mm-hmm.\n- which is where\nyou will break down the chain of thought\ninto intermediate steps.\nWe haven't defined chain of thought.\nChain of thought is from\na paper from years ago\nwhere you introduced the\nidea to ask a language model\nthat at the time, was\nmuch less easy to use.\nYou would say, let's verify step by step,\nand it would induce the model\nto do this bulleted list of steps.\nChain of thought is now\nalmost a default in models\nwhere if you ask in a math question,\nyou don't need to tell\nit to think step by step.\nAnd the idea with Monte Carlo Tree Search\nis that you would take an\nintermediate point in that train,\ndo some sort of expansion,\nspend more compute,\nand then select the right one.\nThat's like a very complex form of search\nthat has been used in things\nlike MuZero and AlphaZero potentially.\nI know MuZero does this.\n- Another form of search\nis just asking five different\npeople, and then taking\nthe majority answer.\n- Yes.\n- There's a variety of like,\n(Nathan chuckles)\nit could be complicated,\nit could be simple.\nWe don't know what it is,\njust that they are not\njust issuing one chain\nof thought in sequence.\n- Yeah.\n- They're launching many\nin parallel, and in the ARC-AGI,\nthey launched a thousand\nin parallel for their,\nthe one that really shocked everyone,\nthat beat the benchmark\nwas they would launch\na thousand in parallel,\nand then they would get the right answer,\nlike 80% of the time or 70%\nof the time, 90 maybe even.\nWhereas if they just launched\none, it was like 30%.\n- There are many extensions to this.\nI would say the simplest one\nis that our language models\nto date have been designed\nto give the right answer\nthe highest percentage of\nthe time in one response.\nAnd we are now opening\nthe door to different ways\nof running inference on\nour models in which we need\nto reevaluate many parts\nof the training process,\nwhich normally opens the\ndoor to more progress,\nbut we don't know if OpenAI changed a lot\nor if just sampling\nmore in multiple choices\nwhat they're doing,\nor if it's something more complex\nwhere they changed the training\nand they know that the inference mode\nis going to be different.\n- So, we're talking\nabout o1 Pro $200 a month\nand they're losing money.\nSo, the thing that we're referring to,\nthis fascinating exploration\nof the test-time compute space,\nis that actually possible?\nDo we have enough compute for that?\nDoes the financials make sense?\n- So, the fantastic thing is,\nand it's in the thing\nthat I pulled up earlier,\nbut the cost for GPT-3 has plummeted.", "mimetype": "text/plain", "start_char_idx": 194281, "end_char_idx": 198212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "633f2bba-ec26-480f-9d79-975c04ffd0d0": {"__data__": {"id_": "633f2bba-ec26-480f-9d79-975c04ffd0d0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7434f798-2493-47cd-a778-b929874296e8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e3408cb741a15f2bee2183054d7454f29a1d4e560c086e5b955dc16c6f29bc64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d334fbe0-e59c-426e-9a6e-befb5c54b791", "node_type": "1", "metadata": {}, "hash": "6fa472c316464df792211d7889d4902caa699d10108bc471610d028e20727b04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we are now opening\nthe door to different ways\nof running inference on\nour models in which we need\nto reevaluate many parts\nof the training process,\nwhich normally opens the\ndoor to more progress,\nbut we don't know if OpenAI changed a lot\nor if just sampling\nmore in multiple choices\nwhat they're doing,\nor if it's something more complex\nwhere they changed the training\nand they know that the inference mode\nis going to be different.\n- So, we're talking\nabout o1 Pro $200 a month\nand they're losing money.\nSo, the thing that we're referring to,\nthis fascinating exploration\nof the test-time compute space,\nis that actually possible?\nDo we have enough compute for that?\nDoes the financials make sense?\n- So, the fantastic thing is,\nand it's in the thing\nthat I pulled up earlier,\nbut the cost for GPT-3 has plummeted.\nIf you scroll up just\na few images, I think.\nThe important thing about like,\nhey, is cost limiting factor here?\nMy view is that we'll have\nreally awesome intelligence\nbefore we have,\nlike AGI before we have it\npermeate throughout the economy.\nAnd this is why that reason is.\nGPT-3 was trained in what, 2020, 2021?\nAnd the cost for running inference on it\nwas 60, $70 per million tokens,\nwhich was the cost per\nintelligence was ridiculous.\nNow, as we scaled forward two years,\nwe've had a 1,200x reduction in cost\nto achieve the same level\nof intelligence as GPT-3.\n- So, here on the x-axis is time\nover just a couple of years.\nAnd on the y-axis is log\nscale dollars to run inference\non a millions-\n- Yes, dollars.\nYeah, a million.\n- And so, you have just a down,\nlike a linear decline on log\nscale from GPT-3 through 3.5\nto Llama.\n- It's like 5 cents\nor something like that now,\nwhich is versus $60, 1,200x.\n- Yeah.\nYeah.\n- That's not\nthe exact numbers, but it's 1,200x.\nI remember that number.\nIs humongous cost per intelligence.\nNow, the freak out over DeepSeek is,\noh my god, they made it so cheap.\nIt's like actually, if you\nlook at this trend line,\nthey're not below the\ntrend line, first of all,\nand at least for GPT-3.\n(Nathan laughing)\nThey are the first to hit\nit, which is a big deal.\nBut they're not below the\ntrend line as far as GPT-3.\nNow, we have GPT-4,\nwhat's gonna happen with\nthese reasoning capabilities.\nIt's a mix of architectural innovations,\nit's a mix of better data,\nand it's gonna be better\ntraining techniques,\nand all of these better inference\nsystems, better hardware.\nGoing from each generation\nof GPU to new generations, or ASICs,\neverything is gonna take\nthis cost curve down and\ndown and down and down.\nAnd then, can I just spawn\na thousand different LLMs\nto create a task, and then\npick from one of them?\nOr whatever search technique,\nI want a Tree, Monte Carlo Tree Research.\nMaybe it gets that\ncomplicated, maybe it doesn't,\n'cause it's too complicated\nto actually scale, who knows?\nBitter lesson, right?\nThe question is I think when not if,\nbecause the rate of progress is so fast.\nNine months ago, Dario was saying, hey,\nor Dario said nine months ago,\nthe cost to train an inference was this.\nAnd now, we're much better than this.\nAnd DeepSeek is much better than this.\nAnd that cost curve for GPT-4,\nwhich was also roughly\n$60 per million tokens\nwhen it launched, has\nalready fallen to $2 or so.\nAnd we're gonna get it\ndown to cents probably\nfor GPT-4 quality and the same...\nAnd that's the base for\nthe reasoning models\nlike o1 that we have today,\nand o1 Pro is spawning multiple.\nAnd o3 and so on and so forth.\nThese search techniques'\ntoo expensive today,\nbut they will get cheaper.\nAnd that's what's gonna\nunlock the intelligence.\n- So, it'll get cheaper\nand cheaper and cheaper.", "mimetype": "text/plain", "start_char_idx": 197393, "end_char_idx": 201035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d334fbe0-e59c-426e-9a6e-befb5c54b791": {"__data__": {"id_": "d334fbe0-e59c-426e-9a6e-befb5c54b791", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "633f2bba-ec26-480f-9d79-975c04ffd0d0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "04367871c291e88e71afa82c6ecf9eeadd74ae982ab953b60e3d2bc49a71d89a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71b3cd8f-be14-4d7d-b713-7701907c54fe", "node_type": "1", "metadata": {}, "hash": "d5e57f353201600ec1af93d46dc69a33682df7b20289afb4ab40d05673cbacf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The question is I think when not if,\nbecause the rate of progress is so fast.\nNine months ago, Dario was saying, hey,\nor Dario said nine months ago,\nthe cost to train an inference was this.\nAnd now, we're much better than this.\nAnd DeepSeek is much better than this.\nAnd that cost curve for GPT-4,\nwhich was also roughly\n$60 per million tokens\nwhen it launched, has\nalready fallen to $2 or so.\nAnd we're gonna get it\ndown to cents probably\nfor GPT-4 quality and the same...\nAnd that's the base for\nthe reasoning models\nlike o1 that we have today,\nand o1 Pro is spawning multiple.\nAnd o3 and so on and so forth.\nThese search techniques'\ntoo expensive today,\nbut they will get cheaper.\nAnd that's what's gonna\nunlock the intelligence.\n- So, it'll get cheaper\nand cheaper and cheaper.\nThe big DeepSeek-R1 release\nfreaked everybody out,\nbecause of the cheaper,\none of the manifestations of\nthat is Nvidia stock plummeted.\nCan you explain what happened?\nAnd also just explain this moment\nand whether if Nvidia's\ngonna keep winning.\n- We are both Nvidia\nbulls here, I would say.\nAnd in some ways, the market\nresponse is reasonable.\nMost of the market,\nlike Nvidia's biggest customers in the US\nare major tech companies, and\nthey're spending a ton on AI.\nAnd if a simple interpretation\nof DeepSeek is you can\nget really good models\nwithout spending as much on AI.\nSo, in that capacity, it's like, oh,\nmaybe these big tech companies\nwon't need to spend as\nmuch in AI and go down.\nThe actual thing that happened,\nit's much more complex,\nwhere there's social factors,\nwhere there's the rising in the App Store,\nthe social contagion that is happening.\nAnd then, I think some\nof it's just like...\nI don't trade,\nI don't know anything\nabout financial markets,\nbut it builds up over the weekend,\nwhere the social pressure,\nwhere it's like if it was during the week\nand there was multiple days of trading\nwhen this was really becoming,\nbut it comes on the weekend,\nand then everybody wants to sell.\nAnd that is a social contagion.\n- I think, and like there were\na lot of false narratives,\nwhich is like, hey,\nthese guys are spending\nbillions on models.\nAnd they're not spending\nbillions on models.\nNo one spent more than a billion dollars\non a model that's released publicly.\nGPT-4 was a couple hundred million,\nand then they've reduced\nthe cost for Turbo 4o.\nBut billion dollar model runs are coming.\nAnd this concludes\npre-training and post-training.\nAnd then, the other number is like,\nhey, DeepSeek didn't include everything.\nThey didn't include...\nA lot of the cost\ngoes to research and\nall this sort of stuff.\nA lot of the cost goes to inference.\nA lot of the cost goes to post-training.\nNone of these things were factor.\n- Yeah.\n- It's research salaries.\nAll these things are\ncounted in the billions\nof dollars that OpenAI is spending,\nbut they weren't counted in the,\nhey, 6 million, $5 million\nthat DeepSeek spent.\nSo, there's a bit\nof misunderstanding of\nwhat these numbers are.\nAnd then, there's also an element of\nNvidia's just been a straight line up.\nAnd there's been so many\ndifferent narratives\nthat have been trying to push down Nvidia.\nI don't say push down Nvidia stock.\nEveryone is looking for a\nreason to sell or to be worried.\nIt was Blackwell delays.\nTheir GPU was, there's a lot of report...\nEvery two weeks,\nthere's a new report about\ntheir GPUs being delayed.\nThere's the whole thing\nabout scaling laws ending.\nIt's so ironic.\n- [Nathan] It lasted a month. (laughs)\nIt was just literally just,\nhey, models aren't getting better.\nThey're just not getting better.\nThere's no reason to spend more,\npre-training scaling is dead,\nand then it's like, o1, o3.\n- R1. (laughs)\n- R1.\nAnd now, it's like, wait,\nmodels are getting too,\nthey're progressing too fast,\n(Nathan laughing)\nslow down the progress,\nstop spending on GPUs.\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 200254, "end_char_idx": 204091, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71b3cd8f-be14-4d7d-b713-7701907c54fe": {"__data__": {"id_": "71b3cd8f-be14-4d7d-b713-7701907c54fe", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d334fbe0-e59c-426e-9a6e-befb5c54b791", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "875a66b516f6c16f9fd41aad8e07c15fd4afa5e0e2f4e4fafa53b35052e62f8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16b9a923-09b2-4e43-848e-25738bda4b2e", "node_type": "1", "metadata": {}, "hash": "f478f5e347f0ac5d9f2be8c887544c72319a39fb5718933c7bb2ef564d6d1e41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I don't say push down Nvidia stock.\nEveryone is looking for a\nreason to sell or to be worried.\nIt was Blackwell delays.\nTheir GPU was, there's a lot of report...\nEvery two weeks,\nthere's a new report about\ntheir GPUs being delayed.\nThere's the whole thing\nabout scaling laws ending.\nIt's so ironic.\n- [Nathan] It lasted a month. (laughs)\nIt was just literally just,\nhey, models aren't getting better.\nThey're just not getting better.\nThere's no reason to spend more,\npre-training scaling is dead,\nand then it's like, o1, o3.\n- R1. (laughs)\n- R1.\nAnd now, it's like, wait,\nmodels are getting too,\nthey're progressing too fast,\n(Nathan laughing)\nslow down the progress,\nstop spending on GPUs.\n- Yeah.\n- And it's...\nBut the funniest thing I think\nthat comes out of this is\nJevons Paradox is true.\nAWS pricing for H100s\nhas gone up over the last couple weeks.\nSince a little bit after Christmas,\nsince V3 was launched, AWS\nH100 pricing has gone up.\nH200s are almost out of stock everywhere,\nbecause H200 has more memory,\nand therefore, R1 wants\nthat chip over H100.\n- We were trying to get GPUs\non a short notice this week for a demo,\nand it wasn't that easy.\nWe were trying to get just 16\nor 32 H100s for demo,\nand it was not very easy.\n(Nathan chuckles)\n- So, for people\nwho don't Jevons Paradox\nis when the efficiency goes up, somehow,\nmagically, counterintuitively,\nthe total resource\nconsumption goes up as well.\n- And semiconductors is, we're\nat 50 years of Moore's Law,\nevery two years half the cost,\ndouble the transistors\njust like clockwork.\nAnd it's slowed down obviously,\nbut the semiconductor industry\nis gone up the whole time.\nIt's been wavy.\nThere's obviously cycles and stuff,\nand I don't expect AI to be any different.\nThere's gonna be ebbs and flows,\nbut this is in AI,\nit's just playing out\nat an insane timescale.\nIt was 2x every two years.\nThis is 1,200x in like three years.\nSo, it's like,\n(Nathan laughing)\nthe scale of improvement\nthat is hard to get wrap your head around.\n- Yeah, I was confused,\nbecause to me, Nvidia stock\non that should have gone up.\nBut maybe it went down\nbecause there's suspicion of foul play\non the side of China, something like this.\nBut if you just look purely\nat the actual principles\nat play here, it's obvious.\nYeah, Jevons Paradox.\n- More progress that AI makes\nor the higher the derivative\nof AI progress is,\nespecially you should, because\nNvidia's in the best place.\nThe higher the derivative is,\nthe sooner the market's gonna\nbe bigger and expanding.\nAnd Nvidia's the only one\nthat does everything\n- Yeah.\n- reliably right now.\n- Because it's not like an\nNvidia competitor arose.\nIt's another company\nthat's using Nvidia, so-\n- Who historically has been\na large Nvidia customer.\n- Yeah. (chuckles)\n- And has press releases\nabout them cheering about\nbeing China's biggest\nNvidia customer.\n(Nathan laughing)\n- [Lex] Yeah, it made it-\n- Obviously they've quieted down,\nbut I think that's another element of is,\nthat they don't wanna say\nhow many GPUs they have.\n- Yeah.\n- Because hey, yes,\nthey have H800s,\nyes, they have H20s, they\nalso have some H100s,\nwhich are smuggled in.\n- So, can you speak to that,\nto the smuggling?\nWhat's the scale of smuggling\nthat's feasible for a nation\nstate to do for companies?\nIs it possible to-\n- I think there's a few\nangles of smuggling here.\nOne is ByteDance arguably\nis the largest smuggler of GPUs for China.\nChina's not supposed to have GPUs.\nByteDance has like over 500,000 GPUs. Why?\nBecause they're all rented from\ncompanies around the world.\nThey rent from Oracle,\nthey rent from Google,\nthey rent from all these mass...\nAnd a bunch of smaller\ncloud companies too.\nAll the Neoclouds of the world.", "mimetype": "text/plain", "start_char_idx": 203393, "end_char_idx": 207086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16b9a923-09b2-4e43-848e-25738bda4b2e": {"__data__": {"id_": "16b9a923-09b2-4e43-848e-25738bda4b2e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71b3cd8f-be14-4d7d-b713-7701907c54fe", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "fc46efc7826b4af1087e1555d30e2fb37f36c965b167201c247e38c989e915be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80388d17-3f05-4138-9db0-c0534de9e372", "node_type": "1", "metadata": {}, "hash": "6e3717bfd9c2b5ad815bcdb988bd9677d315a3bd14eb7083322eb57cf3079481", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\n- Because hey, yes,\nthey have H800s,\nyes, they have H20s, they\nalso have some H100s,\nwhich are smuggled in.\n- So, can you speak to that,\nto the smuggling?\nWhat's the scale of smuggling\nthat's feasible for a nation\nstate to do for companies?\nIs it possible to-\n- I think there's a few\nangles of smuggling here.\nOne is ByteDance arguably\nis the largest smuggler of GPUs for China.\nChina's not supposed to have GPUs.\nByteDance has like over 500,000 GPUs. Why?\nBecause they're all rented from\ncompanies around the world.\nThey rent from Oracle,\nthey rent from Google,\nthey rent from all these mass...\nAnd a bunch of smaller\ncloud companies too.\nAll the Neoclouds of the world.\nThey rent so, so many GPUs,\nthey also buy a bunch.\nAnd they do this\nfor mostly like what Meta\ndoes, serving TikTok,\nserving next best separate-\n- Same discussion.\n- Same as kind, to be clear,\nthat's today the view use.\n- Yeah.\n- And it's a valid use.\nHack the dopamine circuit.\nNow, that's theoretically\nnow very much restricted\nwith the AI diffusion rules,\nwhich happened in the last\nweek of the Biden admin\nand Trump admin looks like\nthey're gonna keep 'em,\nwhich limits allies, even like Singapore,\nwhich Singapore is like 20,\n30% of Nvidia's revenue.\nBut Singapore's had a moratorium\non not building data\ncenters for like 15 years,\n'cause they don't have enough power.\nSo, where are they going?\n(Dylan laughing)\n- [Nathan] Oh yeah. (laughs)\n- I'm not claiming they're\nall going to China,\nbut a portion are,\nmany are going to Malaysia,\nincluding Microsoft\nand Oracle have big data\ncenters in Malaysia.\nThey're going all over\nSoutheast Asia probably,\nIndia as well.\nThere's stuff routing,\nbut the diffusion rules are very de facto.\nYou can only buy this many\nGPUs from this country.\nAnd you can only rent a cluster this large\nto companies that are Chinese.\nThey're very explicit on\ntrying to stop smuggling.\nAnd a big chunk of it was, hey,\nrandom company by 16\nservers ship them to China.\nThere's actually, I saw\na photo from someone\nin the semiconductor industry\nwho leads like a team for networking chips\nthat competes with Nvidia.\nAnd he sent a photo\nof a guy checking into a\nfirst-class united flight\nfrom San Francisco to\nShanghai or Shenzhen,\nwith a super micro box that is this big,\nwhich can only contain GPUs.\n(Nathan chuckles)\nAnd he was booking first-class,\n'cause think about it,\n3 to 5k for your first-class ticket,\nserver costs 240,000 in the US, to 50,000.\nYou sell it for 300,000 in China,\nwait, you just got a\nfree first-class ticket\n- Yeah.\n- and a lot more money.\nSo, it's like...\nAnd then, that's like\nsmall-scale smuggling.\nMost of the large-scale smuggling\nis like companies in\nSingapore and Malaysia\nrouting 'em around, or renting GPUs\ncompletely legally.\n- I wanna jump in.\nHow much does this scale?\nI think there's been some\nnumber, like some people\nthat are higher level\neconomics understanding,\nsay that it's like as you go\nfrom 1 billion of smuggling to 10 billion,\nit's like you're hiding certain\nlevels of economic activity.\nAnd that's the most reasonable thing to me\nis that there's gonna be some\nlevel where it's so obvious\nthat it's easier to find\nthis economic activity.\nAnd-\n- Yeah.\nSo, my belief is that\nlast year, roughly...\nSo, Nvidia made a million H20s,\nwhich are legally allowed\nto be shipped to China,\nwhich we talked about\nis better for reasoning,\ninference at least.\nMaybe not training, but\nreasoning, inference.\nAnd inference generally.\nThen, they also had a couple 100,000,\nwe think like 200 to 300,000 GPUs\nwere routed to China\nfrom Singapore, Malaysia, US, wherever.\nCompanies spun up by 16 GPUs, 64 GPUs,\nwhatever it is routed,\nand Huawei's known for having\nspent up a massive network\nof companies to get\nthe materials they need\nafter they were banned in 2018.\nSo, it's not like\notherworldly, but I agree.", "mimetype": "text/plain", "start_char_idx": 206407, "end_char_idx": 210243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80388d17-3f05-4138-9db0-c0534de9e372": {"__data__": {"id_": "80388d17-3f05-4138-9db0-c0534de9e372", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16b9a923-09b2-4e43-848e-25738bda4b2e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bdc583c1c93f03f86b3542cb02f8dfaef7453cc3f4418f7e0acfd8b374e94f90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c045f16-7895-484d-9125-9e2b114be547", "node_type": "1", "metadata": {}, "hash": "1b558ae9d1bc47f5d35305d848d5172e100788515f6adde45aec040aa980d6e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And-\n- Yeah.\nSo, my belief is that\nlast year, roughly...\nSo, Nvidia made a million H20s,\nwhich are legally allowed\nto be shipped to China,\nwhich we talked about\nis better for reasoning,\ninference at least.\nMaybe not training, but\nreasoning, inference.\nAnd inference generally.\nThen, they also had a couple 100,000,\nwe think like 200 to 300,000 GPUs\nwere routed to China\nfrom Singapore, Malaysia, US, wherever.\nCompanies spun up by 16 GPUs, 64 GPUs,\nwhatever it is routed,\nand Huawei's known for having\nspent up a massive network\nof companies to get\nthe materials they need\nafter they were banned in 2018.\nSo, it's not like\notherworldly, but I agree.\nNathan's point is like, hey,\nyou can't smuggle up $10 billion of GPUs.\nAnd then, the third source,\nwhich is just now banned\nand which wasn't considered smuggling,\nbut is China is renting, I\nbelieve from our research,\nOracle's biggest GPU\ncustomer is ByteDance.\nAnd for Google, I think it's\ntheir second biggest customer.\nAnd so, like, and you go\ndown the list of clouds\nand especially these\nsmaller cloud companies\nthat aren't like that hyperscalers.\nThink beyond core of even Lambda\neven, there's a whole sea,\nthere's 60 different\nnew cloud companies serving Nvidia GPUs.\nI think ByteDance is renting\na lot of these all over it.\nAnd so, these companies are renting GPUs\nto Chinese companies,\nand that was completely legal\nup until the diffusion rules,\nwhich happened just a few weeks ago.\nAnd even now, you can rent GPU clusters\nthat are less than 2,000 GPUs.\nOr you can buy GPUs and\nship them wherever you want\nif they're less than 1,500 GPUs.\nSo, it's like there are\nstill some ways to smuggle,\nbut yeah, as the numbers grow,\n100 something billion dollars of revenue\nfor Nvidia last year, 200\nsomething billion this year.\nAnd if next year,\nit could nearly double\nagain or more than double\nbased on what we see with\ndata center footprints\nlike being built out\nall across the US and\nthe rest of the world,\nit's gonna be really hard\nfor China to keep up with these rules.\nYes, there will always be smuggling\nand DeepSeek level models\nof GPT-4 level models,\no1 level models capable to\ntrain on what China can get,\neven the next tier above that.\nBut if we speed run a couple more jumps\nto billion dollar models,\n$10 billion models,\nthen it becomes, hey,\nthere is a compute disadvantage for China\nfor training models and serving them.\nAnd the serving part is really critical.\nDeepSeek cannot serve their model today.\nIt's completely out of inventory.\nIt's already started falling\nin the App Store actually, downloads,\nbecause you download\nit, you try and sign up,\nthey say, we're not taking registrations,\n'cause they have no capacity.\nYou open it up, you get less\nthan five tokens per second\nif you even get your request approved.\nBecause there's just no capacity,\nbecause they just don't have\nenough GPUs to serve the model,\neven though it's incredibly efficient.\n- It'd be fascinating\nto watch the smuggling,\n'cause there's drug smuggling.\nThat's a market.\nThere's weapons smuggling.\nAnd GPUs will surpass that\nat some point.\n- Chips are highest value\nper kilogram, probably by far. (chuckles)\n- Oh man.\n- I have another question\nfor you, Dylan.\nDo you track model API\naccess internationally?\nHow easy is it for Chinese companies\nto use hosted model APIs from the US?\n- Yeah, that's incredibly easy.\nOpenAI publicly stated\nDeepSeek uses their API.\nAnd they say they have evidence.\nAnd this is another element\nof the training regime\nis people at OpenAI have claimed\nthat it's a distilled model.\ni.e, you're taking OpenAI's model,\nyou're generating a lot of output,\nand then you're training on\nthe output in their model.\n- Yeah.\n- And even if that's the case,\nwhat they did is still\namazing, by the way,\nwhat DeepSeek did, efficiency-wise.\n- Distillation is standard\npractice in industry.\nWhether or not, if you're at a closed lab\nwhere you care about terms\nof service and IP closely,\nyou distill from your own models.", "mimetype": "text/plain", "start_char_idx": 209594, "end_char_idx": 213560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c045f16-7895-484d-9125-9e2b114be547": {"__data__": {"id_": "3c045f16-7895-484d-9125-9e2b114be547", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80388d17-3f05-4138-9db0-c0534de9e372", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "576cee4a7bc394217dda816ecee166d0a937a4fbba20f40ab7919cac077f0de9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebc4d565-ca17-4ddb-9342-493f947e1587", "node_type": "1", "metadata": {}, "hash": "e2a070d0ae643642d2a960e3fbf0eecbe7232a8a010da339cfd0f80024fb1c64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- I have another question\nfor you, Dylan.\nDo you track model API\naccess internationally?\nHow easy is it for Chinese companies\nto use hosted model APIs from the US?\n- Yeah, that's incredibly easy.\nOpenAI publicly stated\nDeepSeek uses their API.\nAnd they say they have evidence.\nAnd this is another element\nof the training regime\nis people at OpenAI have claimed\nthat it's a distilled model.\ni.e, you're taking OpenAI's model,\nyou're generating a lot of output,\nand then you're training on\nthe output in their model.\n- Yeah.\n- And even if that's the case,\nwhat they did is still\namazing, by the way,\nwhat DeepSeek did, efficiency-wise.\n- Distillation is standard\npractice in industry.\nWhether or not, if you're at a closed lab\nwhere you care about terms\nof service and IP closely,\nyou distill from your own models.\nIf you are a researcher\nand you're not building any products,\nyou distill from the OpenAI models.\n- This is a good opportunity.\nCan you explain big picture\ndistillation as a process?\nWhat is distillation?\nWhat's the process\n- We've\n- of distillation?\n- talked a lot about\ntraining language models.\nThey are trained on text.\nAnd post-training,\nyou're trying to train\non very high quality text\nthat you want the model\nto match the features of.\nOr if you're using RL,\nyou're letting the model\nfind its own thing.\nBut for supervised fine-tuning,\nfor preference data, you\nneed to have some completions\nwhat the model is trying\nto learn to imitate.\nAnd what you do there is,\ninstead of a human data\nor instead of the model\nyou're currently training,\nyou take completions from a different,\nnormally more powerful model.\nI think there's rumors\nthat these big models\nthat people are waiting for,\nthese GPT-5s of the world,\nthe Claude 3 Opuses of the world,\nare used internally to do\nthis distillation process\nat OpenAI.\n- There's also\npublic examples, like\nMeta explicitly stated,\nnot necessarily distilling,\nbut they used 405b as a reward model\nfor 70b in their Llama 3.2\n- Yes.\n- or 3.3.\nThis is all the same topic.\n- So, (sighs) is this\nethical? Is this legal?\nWhy is that \"Financial\nTimes\" article headline say,\nOpenAI says that there's evidence\nthat China's DeepSeek used\nits model to train competitor.\n- This is a long, at\nleast in the academic side\nand research side, it has a long history,\n'cause you're trying to\ninterpret OpenAI's rule.\nOpenAI's terms of service say\nthat you cannot build a competitor\nwith outputs from their models.\nTerms of service are\ndifferent than a license,\nwhich are essentially a\ncontract between organizations.\nSo, if you have a terms of\nservice on OpenAI's account,\nif I violate it, OpenAI\ncan cancel my account.\nThis is very different than like a license\nthat says how you could\nuse a downstream artifact.\nSo, a lot of it hinges on a word\nthat is very unclear in the AI space,\nwhich is what is a competitor?\nAnd so-\n- And then,\nthe ethical aspect of it is like,\nwhy is it unethical for me\nto train on your bottle\n- Yeah.\n- when you can train\non the internet's text?\n- Yeah.\n- Right?\n- So, there's a bit of a hypocrisy,\nbecause OpenAI and potentially\nmost of the companies\ntrained on the internet's\ntext without permission.\n- There's also a clear loophole,\nwhich is that I generate data from OpenAI,\nand then I upload it somewhere,\nand then somebody else trains on it\nand the link has been broken.\nThey're not under the same\nterms of service contract.\n- This is why-\n- There's a lot of hippo...\nThere's a lot of to be discovered details\nthat don't make a lot of sense.\n- This is why a lot of models today,\neven if they train on zero OpenAI data,\nyou ask the model who\ntrained you, it'll say,\nI'm ChatGPT trained by OpenAI.\n- Yeah.\n- Because there's so much copy-paste\nof OpenAI outputs from\nthat on the internet\nthat you just weren't\nable to filter it out.", "mimetype": "text/plain", "start_char_idx": 212748, "end_char_idx": 216545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebc4d565-ca17-4ddb-9342-493f947e1587": {"__data__": {"id_": "ebc4d565-ca17-4ddb-9342-493f947e1587", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c045f16-7895-484d-9125-9e2b114be547", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "74871e4abf586c960b3ba26f8adcd23b72f1774807f7ad617ed15bfca83f5bbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69acae95-56b3-446c-b373-95e3a8b9a3de", "node_type": "1", "metadata": {}, "hash": "98a525255b078f763ed1721cb334625cd9e98437deb039c49c021b8371395470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Right?\n- So, there's a bit of a hypocrisy,\nbecause OpenAI and potentially\nmost of the companies\ntrained on the internet's\ntext without permission.\n- There's also a clear loophole,\nwhich is that I generate data from OpenAI,\nand then I upload it somewhere,\nand then somebody else trains on it\nand the link has been broken.\nThey're not under the same\nterms of service contract.\n- This is why-\n- There's a lot of hippo...\nThere's a lot of to be discovered details\nthat don't make a lot of sense.\n- This is why a lot of models today,\neven if they train on zero OpenAI data,\nyou ask the model who\ntrained you, it'll say,\nI'm ChatGPT trained by OpenAI.\n- Yeah.\n- Because there's so much copy-paste\nof OpenAI outputs from\nthat on the internet\nthat you just weren't\nable to filter it out.\nAnd there was nothing in the RL\nwhere they implemented like,\nhey, or post-training or SFT,\nwhatever, that says, hey,\nI'm actually modeled by\nAllen Institute instead\nof OpenAI.\n- We have to do this\nif we serve a demo.\nWe do research and we use\nOpenAI APIs, because it's useful\nand we want to understand post-training,\nand our research models,\nthey'll say they're written by OpenAI\nunless we put in the system\nprop that we talked about that.\nLike, I am Tulu, I am a language model\ntrained by the Allen Institute for AI.\nAnd if you ask more\npeople around industry,\nespecially with post-training,\nit's a very doable task\nto make the model say who it is\nor to suppress the OpenAI thing.\nSo, in some levels, it\nmight be that DeepSeek\ndidn't care that it was\nsaying that it was by OpenAI.\nIf you're gonna upload model weights,\nit doesn't really matter,\n'cause anyone that's\nserving it in an application\nand cares a lot about serving\nis going to, when serving it,\nif they're using it for a specific task,\nthey're gonna tailor it to that.\nAnd it doesn't matter that\nit's saying it's ChatGPT.\n- Oh, I guess the one\nof the ways to do that\nis like a system prompt\nor something like that.\nLike if you're serving\nit to say that you're-\n- That's what we do.\nIf we host a demo, you say you are Tulu 3,\na language model trained by\nthe Allen Institute for AI.\nWe also are benefited from OpenAI data,\n'cause it's a great research tool.\n- Do you think there's any truth\nand value to the claim,\nOpenAI's claim that there's\nevidence that China's DeepSeek\nuse this model to train?\n- I think everyone has\nbenefited regardless,\nbecause the data's on the internet.\nAnd therefore, it's in\nyour pre-training now.\nThere are like subreddits\nwhere people share the\nbest ChatGPT outputs.\nAnd those are in your-\n- I think that they're trying\nto shift the narrative,\nlike they're trying to protect themselves.\nAnd we saw this years ago\nwhen ByteDance was actually banned\nfrom some OpenAI APIs\nfor training on outputs.\nThere's other AI startups\nthat most people,\nif you're in the AI culture, were like,\nthey just told us they\ntrained on OpenAI outputs\nand they never got banned.\nThat's how they bootstrapped\ntheir early models.\nSo, it's much easier to get\noff the ground using this\nthan to set up human pipelines\nand build a strong model.\nSo, it's long history here\nand a lot of the communications\nare seem like narrative control.\n- Actually, over the last couple days,\nwe've seen a lot of people\ndistill DeepSeek's model\ninto Llama models,\nbecause the DeepSeek models\n- Mm-hmm.\n- are complicated to run inference on,\nbecause they're mixture of experts\nand they're 600 plus billion\nparameters and all this.\nAnd people distilled them\ninto the Llama models.\n(Lex laughing)\nBecause the Llama models\nare so easy to serve,\nand everyone's built the pipelines\nand tooling for inference\n- Yeah.\n- with the Llama models,\nbecause it's the open standard. (chuckles)\nSo, we've seen it,\nwe've seen a roundabout.\nIs it bad? Is it illegal?\nMaybe it's illegal, whatever.\nI don't know about that.", "mimetype": "text/plain", "start_char_idx": 215764, "end_char_idx": 219590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69acae95-56b3-446c-b373-95e3a8b9a3de": {"__data__": {"id_": "69acae95-56b3-446c-b373-95e3a8b9a3de", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebc4d565-ca17-4ddb-9342-493f947e1587", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3e65c037507e787e2c237b08f808b5ef6fb115ff49139e2b9a13989139580570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35fa48bb-ac2a-438b-830b-52cac8aeae31", "node_type": "1", "metadata": {}, "hash": "139065a57f7f8d130b8abafae78187d24012fffac336e688ee15d53ad633bcfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, it's long history here\nand a lot of the communications\nare seem like narrative control.\n- Actually, over the last couple days,\nwe've seen a lot of people\ndistill DeepSeek's model\ninto Llama models,\nbecause the DeepSeek models\n- Mm-hmm.\n- are complicated to run inference on,\nbecause they're mixture of experts\nand they're 600 plus billion\nparameters and all this.\nAnd people distilled them\ninto the Llama models.\n(Lex laughing)\nBecause the Llama models\nare so easy to serve,\nand everyone's built the pipelines\nand tooling for inference\n- Yeah.\n- with the Llama models,\nbecause it's the open standard. (chuckles)\nSo, we've seen it,\nwe've seen a roundabout.\nIs it bad? Is it illegal?\nMaybe it's illegal, whatever.\nI don't know about that.\nBut it's-\n- It could break contracts.\nI don't think it's illegal,\nlike in any illegal,\nlike no one's going to jail for this ever.\n- I think fundamentally,\nI think it's ethical\nor I hope it's ethical,\nbecause the moment becomes,\nwe ban that kind of thing,\nit's gonna make everybody much worse off.\nAnd I also actually, this is difficult,\nbut I think you should be\nallowed to train on the internet.\nI know a lot of authors\nand creators are very sensitive about it.\nThat's a difficult question.\nBut the moment you're not\nallowed to train on the internet.\n- I agree.\n- I have a schizo take on\nhow you can solve this,\nbecause it already works.\n- All right.\n- I have a reasonable take on it.\n- All right, all right.\n(Nathan laughing)\n- So, Japan has a law which you're allowed\nto train on any training data\nand copyrights don't apply if\nyou wanna train a model, A.\nB, Japan has nine gigawatts\nof curtailed nuclear power.\nC, Japan is allowed under\nthe AI diffusion rule\nto import as many GPUs as they'd like.\nSo, all we have to do, we\nhave a market here to make,\nwe build massive data setters,\nwe rent them to the labs,\nand then we train models in\na legally permissible way,\nand there's no if, ands, or buts.\nAnd now, the models have no\npotential copyright lawsuit\nfrom \"New York Times\"\nor anything like that.\nNo, no, it's just completely legal.\n- Yeah.\n- No, so-\n- Genius.\n- The early copyright lawsuits\nhave fallen in the favor of AI training.\nI would say that the long tail of use\nis gonna go in the side\nof AI, which is if you do,\nif you scrape the trillions\nof tokens of data,\nyou're not looking and saying,\nthis one \"New York Times\"\narticle is so important to me.\nBut if you're doing a\naudio generation for music\nor image generation and you say,\nmake it in the style of X person,\nthat's a reasonable case\nwhere you could figure out\nwhat is their profit margin on inference.\nI don't know if it's gonna be the 50/50\nof YouTube creator program or something,\nbut I would opt into\nthat program as a writer,\nlike, please, like that...\nIt's gonna be a rough journey,\nbut there will be some solutions\nlike that that make sense.\nBut there's a long tail where\nit's just on the internet.\n- I think one of the other aspects\nof that \"Financial Times\" article implied.\nAnd so, that leads to a\nmore general question.\nDo you think there's...\nHow difficult is spying, espionage,\nand stealing of actual secret code\nand data from inside companies?\nHow much of that is being attempted?\n- Code and data is\nhard, but ideas is easy.\nSilicon Valley operates\n(Dylan chuckles)\n- Yeah.\n- on the way\nthat top employees get bought out\nby other companies for a pay raise,\nand a large reason\nwhy these companies do this\nis to bring ideas with them.\nAnd there's no, in California,\nthere's rules that certain,\nlike non-competes or whatever,\nare illegal in California\nand whether or not\nthere's NDAs and things,\nthat is how a lot of process happens.\nRecently, there was somebody from Gemini\nwho helped make this 1\nmillion context length\nand everyone is saying the next Llama who,\nI mean, he went to the Meta team,\nis gonna have 1 million context length.\nAnd that's how the world works.", "mimetype": "text/plain", "start_char_idx": 218850, "end_char_idx": 222741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35fa48bb-ac2a-438b-830b-52cac8aeae31": {"__data__": {"id_": "35fa48bb-ac2a-438b-830b-52cac8aeae31", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69acae95-56b3-446c-b373-95e3a8b9a3de", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5c45719ead1eb774516c6dd5f10c8e64ac7a55319bdb62474f811fe01ec735c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09211aeb-adbf-494a-8c7d-e383eeea8b57", "node_type": "1", "metadata": {}, "hash": "6253fd46a551a2e03ffe8c72cd60dfe5125490452d063e8b90b59f2c6592e682", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How much of that is being attempted?\n- Code and data is\nhard, but ideas is easy.\nSilicon Valley operates\n(Dylan chuckles)\n- Yeah.\n- on the way\nthat top employees get bought out\nby other companies for a pay raise,\nand a large reason\nwhy these companies do this\nis to bring ideas with them.\nAnd there's no, in California,\nthere's rules that certain,\nlike non-competes or whatever,\nare illegal in California\nand whether or not\nthere's NDAs and things,\nthat is how a lot of process happens.\nRecently, there was somebody from Gemini\nwho helped make this 1\nmillion context length\nand everyone is saying the next Llama who,\nI mean, he went to the Meta team,\nis gonna have 1 million context length.\nAnd that's how the world works.\n- As far as industrial\nespionage and things,\nthat has been greatly\nsuccessful in the past.\nThe Americans did the Brits,\nthe Chinese have done it to the Americans,\nand so on and so forth.\nIt is a fact of life.\nAnd so, to argue industrial espionage\ncan be stopped is probably unlikely.\nYou can make it difficult.\nBut even then, there's\nall these stories about,\nhey, F35 and F22 have\nalready been given to China\nin terms of design plans and stuff.\nCode and stuff like\nbetween, I say companies,\nnot nation states is\nprobably very difficult,\nbut ideas are discussed a lot.\nWhether it be a house\nparty in San Francisco\nor a company changing employees,\nor always the mythical honeypot\nthat always gets talked about,\nlike someone gets honeypotted.\nBecause everyone working on AI\nis a single dude who's\nin their 20s and 30s.\nNot everyone, but a insane percentages.\nSo, there's always all these\nyou know, and obviously-\n- So, honeypotted is like a spy,\na female spy approaches you and like-\n- Yeah, yeah.\n- Or male.\nIt's San Francisco, but...\n(Nathan laughing)\nAs a single dude, I will\nsay, in his late 20s,\nit is like, we are very easily corrupted.\n- [Lex] Yeah.\n- Not corrupted myself,\nbut we are, we are.\n- Yeah.\nEverybody else, not me.\nEverybody else.\n- Yeah. Exactly.\n- I'm too oblivious that I am not single.\nSo, I'm safe from one\nespionage access. (laughs)\n- Yeah, you have to make sure\nto close all security vulnerabilities.\nSo, you, Dylan, collect\na lot of information\nabout each of the mega clusters\nfor each of the major AI companies.\nCan you talk about the buildouts\nfor each one that stand out?\n- Yeah, so I think the thing\nthat's really important about\nthese mega cluster buildouts\nis they're completely\nunprecedented in scale.\nUS, data center power consumption\nhas been slowly on the rise\nand it's gone up to 2, 3%\neven through the cloud\ncomputing revolution.\nData center consumption as\na percentage of total US.\nAnd that's been over decades\nof data centers, et cetera.\nIt's been climbing, climbing slowly.\nBut now, 2 to 3%.\nNow, by the end of this\ndecade, even under like,\nwhen I say 10%, a lot of\npeople that are traditionally\nby 2028, 2030,\na traditional data center\npeople like, that's nuts.\nBut then, people who are in AI\nwho have really looked at this\nat the Anthropics and OpenAIs,\nthey're like, that's not enough.\nAnd I'm like, okay.\n- Mm-hmm. (chuckles)\n- But this is both through\nglobally distributed\nor distributed throughout the US,\nas well as centralized clusters.\nThe distributed throughout\nthe US is exciting\nand it's the bulk of it.\nLike, hey, OpenAI or say,\nMeta is adding a gigawatt.\nBut most of it is distributed\nthrough the US for inference\nand all these other things.\n- So, maybe we should lay\nit out what a cluster is.\nSo, does this include AWS?\nMaybe it's good\nto talk about the\ndifferent kinds of clusters\nand what you mean by mega clusters,\nand what's the GPU\n- Mm-hmm.\n- and what's the computer and what...\nI'm just kidding.\n- Yeah, yeah, yeah.\n- Not that far back, but yeah.\nSo, what do we mean by the clusters,\n- Oh man.", "mimetype": "text/plain", "start_char_idx": 222019, "end_char_idx": 225795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09211aeb-adbf-494a-8c7d-e383eeea8b57": {"__data__": {"id_": "09211aeb-adbf-494a-8c7d-e383eeea8b57", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35fa48bb-ac2a-438b-830b-52cac8aeae31", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "116d451ec524fb17e68467eadaa29252b4ebe74d646abe4f871f2a61701e7f19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c536d90-e27c-47f2-98ac-d88e28f1f0cb", "node_type": "1", "metadata": {}, "hash": "15d429b9b6e4092002c20126764996e3531a0c483fb5ce252c62a8fc495f538d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I'm like, okay.\n- Mm-hmm. (chuckles)\n- But this is both through\nglobally distributed\nor distributed throughout the US,\nas well as centralized clusters.\nThe distributed throughout\nthe US is exciting\nand it's the bulk of it.\nLike, hey, OpenAI or say,\nMeta is adding a gigawatt.\nBut most of it is distributed\nthrough the US for inference\nand all these other things.\n- So, maybe we should lay\nit out what a cluster is.\nSo, does this include AWS?\nMaybe it's good\nto talk about the\ndifferent kinds of clusters\nand what you mean by mega clusters,\nand what's the GPU\n- Mm-hmm.\n- and what's the computer and what...\nI'm just kidding.\n- Yeah, yeah, yeah.\n- Not that far back, but yeah.\nSo, what do we mean by the clusters,\n- Oh man.\n- the build outs?\n- I thought I was about\nto do the Apple ad.\nWhat's a computer?\n(group laughing)\nSo, traditionally, data\ncenters and data center tasks\nhave been a distributed systems problem\nthat is capable of being\nspread very far and widely.\ni.e, I send a request to Google,\nit gets routed to a data\ncenter somewhat close to me.\nIt does whatever search\nranking recommendation,\nsends a result back.\nThe nature of the task is\nchanging rapidly in that the task,\nthere's two tasks that people\nare really focused on now.\nIt's not database access,\nit's not serve me the right\npage, serve me the right ad.\nIt's now a inference.\nAnd inference is dramatically different\nfrom traditional distributed systems,\nbut it looks a lot more similar.\nAnd then, there's training.\nThe train inference\nside is still like, hey,\nI'm gonna put thousands of GPUs\nin blocks all around these data centers.\nI'm gonna run models on them,\nuser submits a request,\nit gets kicked off,\nor hey, my service, they\nsubmit a request to my service.\nThey're on Word,\nand they're like, oh\nyeah, help me, Copilot.\nAnd it starts, kicks it off,\nor I'm on my Windows, Copilot, whatever,\nApple Intelligence, whatever it is,\nit gets kicked off to a data center.\nAnd that data center does\nsome work and sends it back.\nThat's inference. That is going\nto be the bulk of compute.\nAnd that's like, there's\nthousands of data centers\nthat we're tracking with satellites\nand all these other things.\nAnd those are the bulk\nof what's being built.\nBut the scale of...\nAnd so, that's like\nwhat's really reshaping\nand that's what's\ngetting millions of GPUs.\nBut the scale of the largest cluster\nis also really important.\nWhen we look back at history,\nor through the age of AI,\nit was a really big deal\nwhen they did AlexNet on I think two GPUs\nor four GPUs.\n- Yeah.\n- [Dylan] I don't remember.\nIt was a really big deal.\n- Oh, it's a big deal,\n'cause you use GPUs.\n(Nathan laughing)\n- It's a big deal\nthey use GPUs and they used multiple.\nBut then over time, its scale\nhas just been compounding.\nAnd so, when you skip\nforward to GPT-3, then GPT 4,\nGPT-4, 20,000 A100 GPUs,\nunprecedented run in terms\nof the size and the cost.\nA couple hundred million\ndollars on a YOLO.\nA YOLO run for GPT-4.\nAnd it yielded this magical improvement\nthat was perfectly in line\nwith what was experimented\nand just like a log scale right up.\n- Oh yeah, they had that\nplot from the paper.\nScaling the technical were part.\n- The scaling laws were perfect.\nBut that's not a crazy number.\n20,000 A100s, roughly each\nGPU is consuming 400 watts.\nAnd then, when you add\nin the whole server,\neverything, it's like 15\nto 20 megawatts of power.\nMaybe you could look up what the power\nof consumption of a person is\nbecause the numbers are gonna get silly.\nBut 15 to 20 megawatts\nwas standard data center\nsize was just unprecedented.\nThat was all GPUs running one task.\n- [Nathan] How many watts\nis a toaster? (chuckles)\n- A toaster is like also-\n- That's a good example.\n- A similar power consumption to an A100.", "mimetype": "text/plain", "start_char_idx": 225069, "end_char_idx": 228818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c536d90-e27c-47f2-98ac-d88e28f1f0cb": {"__data__": {"id_": "6c536d90-e27c-47f2-98ac-d88e28f1f0cb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09211aeb-adbf-494a-8c7d-e383eeea8b57", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bcd8f9e757de898240d7b313c9b69c8f3ce0e882773835fadf8fcf03546603cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a70a0aa-a95d-460e-b027-e4aafd39ce18", "node_type": "1", "metadata": {}, "hash": "e71546d57f8ac3836ddd690d98bded8c88a92236a157458fee0666ff3126b290", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And it yielded this magical improvement\nthat was perfectly in line\nwith what was experimented\nand just like a log scale right up.\n- Oh yeah, they had that\nplot from the paper.\nScaling the technical were part.\n- The scaling laws were perfect.\nBut that's not a crazy number.\n20,000 A100s, roughly each\nGPU is consuming 400 watts.\nAnd then, when you add\nin the whole server,\neverything, it's like 15\nto 20 megawatts of power.\nMaybe you could look up what the power\nof consumption of a person is\nbecause the numbers are gonna get silly.\nBut 15 to 20 megawatts\nwas standard data center\nsize was just unprecedented.\nThat was all GPUs running one task.\n- [Nathan] How many watts\nis a toaster? (chuckles)\n- A toaster is like also-\n- That's a good example.\n- A similar power consumption to an A100.\nH100 comes around,\n(Nathan chuckles)\nthey increase the power\nfrom like 400 to 700 watts.\nAnd that's just per GPU.\nAnd then, there's all the\nassociated stuff around it.\nSo, once you count all that,\nit's roughly like 1,200 to\n1,400 watts for everything,\nnetworking, CPUs,\nmemory, blah, blah, blah.\n- So, we should also\nsay, so what's required?\nYou said power.\nSo, a lot of power is required,\na lot of heat is generated,\nso the cooling is required.\nAnd because there's a lot\nof GPUs that have to be,\nor CPUs or whatever, they\nhave to be connected.\nSo, there's a lot of networking,\nright?\n- Yeah, yeah, so I think...\nYeah, sorry for skipping past that.\nAnd then, the data center\nitself is complicated.\nBut these are still\nstandardized data centers for GPT-4 scale.\nNow, we step forward to what\nis the scale of clusters\nthat people have built last year.\nAnd it ranges widely.\nIt ranges from like, hey,\nthese are standard data centers\nand we're just using multiple of them\nand connecting them together really\nwith a ton of fiber between them,\na lot of networking, et cetera.\nThat's what OpenAI and\nMicrosoft did in Arizona.\nAnd so, they have 100,000 GPUs.\nMeta, similar thing.\nThey took their standard\nexisting data center design,\nand it looks like an H,\nand they connected\nmultiple on 'em together.\nAnd they got to,\nthey first did 16,000\nGPUs, 24,000 GPUs total.\nOnly 16 of them,\n1,000 of 'em were running\non the training run,\nbecause GPUs are very unreliable.\nSo, they needed to have\nspares to swap in and out.\nAll the way to like now 100,000 GPUs\nthat they're training\non Llama 4 on currently.\nLike 128,000 or so.\nThink about 100,000 GPUs with\nroughly 1,400 watts a piece,\nthat's 140 megawatts, 150\nmegawatts for 128, right?\nSo, you're talking about\nyou've jumped from 15\nto 20 megawatts to 10x,\nalmost 10x that number, 9x that number\nto 150 megawatts in two\nyears, from 2022 to 2024.\nAnd some people like\nElon that he admittedly,\nand he says himself got into\nthe game a little bit late\nfor pre-training large language models.\nxAI was started later.\nBut then, he bet heaven and\nhell to get his data center up\nand get the largest cluster in the world,\nwhich is 200,000 GPUs.\nAnd he did that. He bought\na factory in Memphis.\nHe's upgrading the\nsubstation with the same time\nhe's got a bunch of\nmobile power generation,\na bunch of single cycle combine.\nHe tapped the natural gas line\nthat's right next to the factory\nand is just pulling a\nton of gas, burning gas.\nHe's generating all this power.\nHe's in a factory and\nan old appliance factory\nthat's shut down and\nmoved to China long ago.\nAnd he's got 200,000 GPUs in it.\nAnd now, what's the next scale?\nAll the hyperscalers have done this.\nNow, the next scale is\nsomething that's even bigger.\nAnd so, Elon, just to stick on the topic,\nhe's building his own natural gas plant,\nlike a proper one right next door.", "mimetype": "text/plain", "start_char_idx": 228029, "end_char_idx": 231674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1a70a0aa-a95d-460e-b027-e4aafd39ce18": {"__data__": {"id_": "1a70a0aa-a95d-460e-b027-e4aafd39ce18", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c536d90-e27c-47f2-98ac-d88e28f1f0cb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1b1848ee7f4705d9af9b99bcf578ff007cd37f0d21d04f07fab76e20b26d3103", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4d1dde2-fe8d-4ed8-9238-4af420d2493e", "node_type": "1", "metadata": {}, "hash": "5f761c3d630324e2b1e00cd15889349f94a8435a478441831c0764287026879c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And he did that. He bought\na factory in Memphis.\nHe's upgrading the\nsubstation with the same time\nhe's got a bunch of\nmobile power generation,\na bunch of single cycle combine.\nHe tapped the natural gas line\nthat's right next to the factory\nand is just pulling a\nton of gas, burning gas.\nHe's generating all this power.\nHe's in a factory and\nan old appliance factory\nthat's shut down and\nmoved to China long ago.\nAnd he's got 200,000 GPUs in it.\nAnd now, what's the next scale?\nAll the hyperscalers have done this.\nNow, the next scale is\nsomething that's even bigger.\nAnd so, Elon, just to stick on the topic,\nhe's building his own natural gas plant,\nlike a proper one right next door.\nHe's deploying tons of\nTesla megapack batteries\nto make the power more smooth\nand all sorts of other things.\nHe's got industrial chillers\nto cool the water down\nbecause he's water-cooling the chips.\nSo, all these crazy things\nto get the clusters bigger and bigger.\nBut when you look at, like, say,\nwhat OpenAI did with Stargate,\nthat in Arizona, in Abilene, Texas,\nwhat they've announced at least.\nIt's not built. Elon says\nthey don't have the money.\nThere's some debates about this.\nBut at full-scale, at\nleast the first section\nis definitely money's accounted for,\nbut there's multiple sections.\nBut full scale, that data center\nis gonna be 2.2 gigawatts.\n2,200 megawatts of power in\nand roughly 1.8 gigawatts\nor 1,800 megawatt,\nyeah, 1,800 megawatts of\npower delivered to chips.\nNow, this is an absurd scale.\n2.2 gigawatts is like more\nthan most cities to be clear.\nAnd delivered to a single cluster\nthat's connected to do training.\nTo train these models, to\ndo both the pre-training,\nthe post-training, all of this stuff.\n- This is insane.\n- It is.\n- Insane.\n- What is a nuclear\npower plant again-\n- everyone is doing this.\nEveryone is doing this.\n(Lex laughing)\nMeta in Louisiana,\nthey're building two natural\ngas plants, massive ones,\nand then they're building\nthis massive data center.\nAmazon has plans for this scale.\nGoogle has plans for this scale.\nxAI has plans for this scale.\nThe guys that are racing,\nthe companies that are\nracing are racing hard,\nand they're doing multi\ngigawatt data centers\nto build this out,\nbecause they think that,\nyeah, if I now have...\nObviously, pre-training\nscaling is gonna continue,\nbut to some extent,\nbut then also all this\npost-training stuff,\nwhere you have a RL sandbox\nfor computer use or whatever.\nThis is where they're gonna...\nAnd all these viable domains\nwhere they just keep learning and learning\nand learning, self-play,\nwhatever, whatever it is,\nmakes the AI so much more capable\nbecause the line does go up.\nAs you throw more compute,\nyou get more performance.\nThe shirt is about scaling laws.\nTo some extent, it is diminishing returns.\nYou 10x the compute, you\ndon't get 10x better model.\nYou get a diminishing returns,\nbut also you get efficiency improvements.\nSo, you bend the curve.\nAnd these scale of data centers are doing,\nwreaking a lot of havoc on the network.\nNathan was mentioning there's,\nAmazon has tried to buy this\nnuclear power plant, Talen.\nAnd if you look at the Talen's stock,\nit's just skyrocketing.\nAnd they're building a massive\nmulti gigawatt data center there.\nAnd you just go down the list.\nThere's so many ramifications.\nInteresting thing is\ncertain regions of the US\ntransmitting power cost more\nthan actually generating it.\nBecause the grid is so slow to build,\nand the demand for power and\nthe ability to build power\nand re-ramping on a natural gas plant\nor even a coal plant is\nlike easy enough to do.\nBut transmitting the power is really hard.\nSo, in some parts of the\nUS, like in Virginia,\nit costs more to transmit power\nthan it cost to generate it.\nWhich is like, there's all sorts\nof second order effects\nthat are insane here.\n- And the power grid\nsupport this kind of growth?", "mimetype": "text/plain", "start_char_idx": 230990, "end_char_idx": 234843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4d1dde2-fe8d-4ed8-9238-4af420d2493e": {"__data__": {"id_": "f4d1dde2-fe8d-4ed8-9238-4af420d2493e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a70a0aa-a95d-460e-b027-e4aafd39ce18", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "fb14b8e0f1aed0404887ba50e1820dbe44745e0fe468b95e062a1677bb1357a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5", "node_type": "1", "metadata": {}, "hash": "7d300dfcec3a1f1e4fa44df319cceaea33aa81dfa604d586d837ca8a0805314d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And if you look at the Talen's stock,\nit's just skyrocketing.\nAnd they're building a massive\nmulti gigawatt data center there.\nAnd you just go down the list.\nThere's so many ramifications.\nInteresting thing is\ncertain regions of the US\ntransmitting power cost more\nthan actually generating it.\nBecause the grid is so slow to build,\nand the demand for power and\nthe ability to build power\nand re-ramping on a natural gas plant\nor even a coal plant is\nlike easy enough to do.\nBut transmitting the power is really hard.\nSo, in some parts of the\nUS, like in Virginia,\nit costs more to transmit power\nthan it cost to generate it.\nWhich is like, there's all sorts\nof second order effects\nthat are insane here.\n- And the power grid\nsupport this kind of growth?\n- Trump's executive orders,\nthere was a Biden executive order\nbefore the end of the year,\nbut then Trump had some\nmore executive orders,\nwhich hopefully reduce the regulations\nto where, yes, things can be built.\nBut yeah, this is a big, big challenge.\nIs building enough power fast enough?\n- Are you gonna basically\nhave a nuclear power plant\nnext to a data center\nfor each one of these?\n- So, the fun thing here\nis this is too slow.\n- [Nathan] To build the power plant.\n- To build a power plant\nor to reconfigure an existing\npower plant is too slow.\nAnd so, therefore, you must use natural...\nData center power consumption is flat.\nIt spike-\n- Which is why nuclear\nis also good for it.\nLike long-term, nuclear\nis a very natural fit,\n- Yeah, it's-\n- but you can't do solar\nor anything in the short-term like that.\n- Because data center power is like this.\nYou're telling me, I'm gonna buy tens\nof billions of dollars\nof GPUs and idle them,\n'cause the power's not being generated?\nPower's cheap.\nIf you look at the cost of a cluster,\nless than 20% of it is power.\nMost of it is the capital cost\nand depreciation of the GPUs.\nAnd so, it's like, well, screw it,\nI'll just build natural gas plants.\nThis is what Meta is doing in Louisiana.\nThis is what OpenAI's doing in Texas\nand all these different places.\nThey may not be doing it directly,\nbut they are partnered with someone.\nAnd so, there is a couple hopes. One is...\nAnd Elon, what he is\ndoing in Memphis is like,\nto the extreme,\nthey're not just using\ndual combine cycle gas,\nwhich is like super efficient,\nhe's also just using single cycle\nand mobile generators and\nstuff, which is less efficient.\nBut there's also the flip side,\nwhich is solar power\ngeneration is like this,\nand wind is another like\nthis, correlate different.\nSo, if you stack both of those,\nplus you get a big chunk of batteries,\nplus you have a little bit of gas,\nit is possible to run it more green.\nIt's just the timescales for that is slow.\nSo, people are trying,\n- Mm-hmm.\n- but Meta basically said, whatever,\ndon't care about my sustainability pledge.\nOr they'll buy like a power,\nit's called a PPA, power\npurchasing agreement,\nor they'll be a massive wind farm\nor solar farm, like wherever.\n- Oh.\n- And then, they'll just pretend\nlike those electrons are being\nconsumed by the data center,\nbut in reality, they're\npaying for the power here\nand selling it to the grid,\nand they're buying power here.\n- [Lex] Yep.\n- And then, another\nthing is like Microsoft\nquit on some of their\nsustainability pledges.\nElon, what he did with Memphis\nis objectively somewhat dirty,\nbut he is also doing it in an area\nwhere there's a bigger natural\ngas plant right next door\nand like a sewer next, or not a sewer,\nbut like a wastewater treatment\nand a garbage dump nearby.\nAnd he's obviously made the world\na lot more clean than that\none data center is gonna do.\nSo, I think it's fine to some extent.\nAnd maybe AGI solves\nglobal warming and stuff,\n(Lex laughing)\nwhatever it is.\nThis is sort of the attitude\nthat people at the labs have,\nwhich is like, yeah, it's great.\nWe'll just use gas.\nBecause the race is that important.\nAnd if we lose, that's way worse.", "mimetype": "text/plain", "start_char_idx": 234090, "end_char_idx": 238016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5": {"__data__": {"id_": "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4d1dde2-fe8d-4ed8-9238-4af420d2493e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f8d452eddb15be5691c4a436b40adf6302035f5a8222bf2da99dbb5c46bcf80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ab00f04-8b39-4a60-8e90-28759ba1ff9d", "node_type": "1", "metadata": {}, "hash": "45dc9ff24cd7aeb03f701dae2155443822ede5e439ef67859090090578556c60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Lex] Yep.\n- And then, another\nthing is like Microsoft\nquit on some of their\nsustainability pledges.\nElon, what he did with Memphis\nis objectively somewhat dirty,\nbut he is also doing it in an area\nwhere there's a bigger natural\ngas plant right next door\nand like a sewer next, or not a sewer,\nbut like a wastewater treatment\nand a garbage dump nearby.\nAnd he's obviously made the world\na lot more clean than that\none data center is gonna do.\nSo, I think it's fine to some extent.\nAnd maybe AGI solves\nglobal warming and stuff,\n(Lex laughing)\nwhatever it is.\nThis is sort of the attitude\nthat people at the labs have,\nwhich is like, yeah, it's great.\nWe'll just use gas.\nBecause the race is that important.\nAnd if we lose, that's way worse.\n- I should say that I got a chance\nto visit the Memphis data center.\n- Oh wow.\n- And it's incredible.\nI visited with Elon.\nJust the teams and the rate\nof innovation there is insane.\nMy sense is that nobody's ever\ndone anything of this scale\nand nobody has certainly\never done anything\nof this scale at the\nrate that xAI is doing.\nSo, they're like figuring out...\nAnd so, I was sitting\nin on all these meetings\nwhere they're brainstorming,\nit's like, it's insane.\nIt's exciting, 'cause they're like,\nthey're trying to figure out\nwhat the bottlenecks are,\nhow to remove the bottlenecks,\nhow to make sure that,\nthere's just so many really cool things\nabout putting together a data center,\n'cause everything has to work.\nThe people that do like the CIS admin,\nthe machine learning, all that\nis the exciting thing, so on,\nbut really, the people that\nrun everything (chuckles)\nare the folks that know\nthe low level software\nand hardware that runs everything,\nthe networking, all of that.\nAnd so, you have to make sure\nyou have procedures that test everything.\nI think they're using ethernet.\nI don't know how they're\ndoing the networking, but-\n- They're using Nvidia\nSpectrum-X ethernet.\nThere's actually like, I think yeah,\nthe unsung heroes are the\ncooling and electrical systems,\nwhich are just like\n(Lex chuckles)\n- Exactly.\n- glossed over.\n- [Lex] Yeah.\n- But I think one story\nthat maybe is like exemplifies\nhow insane this stuff is,\nis when you're training,\nyou're always doing,\nyou're running through the model a bunch,\nin the most simplistic terms,\nrunning through the model a bunch.\nAnd then, you're gonna exchange everything\nand synchronize the weights.\nSo, you'll do a step,\nthis is like a step in model training.\nAnd every step, your loss\ngoes down, hopefully.\nAnd it doesn't always,\nbut in the simplest terms,\nyou'll be computing a lot,\nand then you'll exchange.\nThe interesting thing is\nGPU power is most of it,\nnetworking power is some,\nbut it's a lot less.\nSo, while you're computing,\nyour power for your GPUs is here.\nBut then when you're exchanging weights,\nif you're not able to\noverlap communications\nand compute perfectly,\nthere may be a time period\nwhere your GPUs are just idle,\nand you're exchanging weights\nand you're like, hey,\nthe model's updating.\nSo, you're exchanging the\ngradient, you do the model update,\nand then you start training again.\nSo, the power goes...\n- Mm-hmm.\n- And it's super spiky.\n- Yeah.\n- And so, funnily enough,\nwhen you talk about the\nscale of data center power,\nyou can blow stuff up\nso easily.\n- Yeah.\n- And so, Meta actually has,\naccidentally opened upstream\nsomething to code in PyTorch\nwhere they added an operator.\nAnd I kid you not, whoever made this,\nI wanna hug the guy,\nbecause it says PyTorch,\nit's like pytorch.powerplantnoblowup\n(Lex laughing)\nequals zero or equal one.\n(Nathan laughing)\nAnd what it does,\nwhat it does is amazing.\n- [Lex] Yeah.\n- When you're exchanging the weights,\nthe GP will just compute fake numbers,\nso the power doesn't spike too much.\nAnd so then, the power\nplants don't blow up\nbecause the transient\nspikes screw stuff up.\n- Well, that makes sense.\nYou have to do that kind of thing.", "mimetype": "text/plain", "start_char_idx": 237274, "end_char_idx": 241184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ab00f04-8b39-4a60-8e90-28759ba1ff9d": {"__data__": {"id_": "5ab00f04-8b39-4a60-8e90-28759ba1ff9d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adfaf697-7a4d-44e1-8fbc-645b0b5d3fc5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "01e15cd9b0a82a24eff3ddfdc92bce2d906ab89f6114158f5b9ef917a85b498c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c576a45-4b8e-4191-891a-ee926ea10c80", "node_type": "1", "metadata": {}, "hash": "39450ebc63c32e775b036bb9361e1ca95a4e0e7493e64a438a5ae4b9be494eac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\n- And so, funnily enough,\nwhen you talk about the\nscale of data center power,\nyou can blow stuff up\nso easily.\n- Yeah.\n- And so, Meta actually has,\naccidentally opened upstream\nsomething to code in PyTorch\nwhere they added an operator.\nAnd I kid you not, whoever made this,\nI wanna hug the guy,\nbecause it says PyTorch,\nit's like pytorch.powerplantnoblowup\n(Lex laughing)\nequals zero or equal one.\n(Nathan laughing)\nAnd what it does,\nwhat it does is amazing.\n- [Lex] Yeah.\n- When you're exchanging the weights,\nthe GP will just compute fake numbers,\nso the power doesn't spike too much.\nAnd so then, the power\nplants don't blow up\nbecause the transient\nspikes screw stuff up.\n- Well, that makes sense.\nYou have to do that kind of thing.\nYou have to make sure they're not idle.\nYeah, that's-\n- And Elon's solution\nwas like, \"Let me throw a\nbunch of Tesla mega packs\nand a few other things.\"\n- Stabilize, yeah.\n- Everyone\nhas different solutions,\nbut Meta's at least\n(Lex laughing)\nwas publicly and openly known,\nwhich is just like, set this operator.\nAnd what this operator does\nis it just makes the GPUs\ncompute nothing so that\nthe power doesn't spike.\n- But that just tells you\nhow much power you're working with.\nIt's insane.\nIt's insane.\n- People should just go Google\nlike scale, like what does X watts do?\nAnd go through all the\nscales from one watt\nto a kilowatt to a megawatt,\nand you look and stare at that,\nand you're how high on\nthe list a gigawatt is,\nand it's mind-blowing.\n- Can you say something about the cooling?\nSo, I know Elon's using liquid cooling,\nI believe, in all cases.\nThat's a new thing, right?\nMost of 'em don't use liquid cooling.\nIs there something interesting\nto say about the cooling?\n- Yeah, yeah.\nSo, air cooling has been\nthe de facto standard.\nThrow a bunch of metal heat\npipes, et cetera, and fans,\nand that's cooled, that's\nbeen enough to cool it.\nPeople have been dabbling\nin water cooling.\nGoogle's TPUs are water-cooled.\nSo, they've been doing\nthat for a few years.\nBut with GPUs, no one's ever done...\nAnd no one's ever done the scale\nof water cooling that Elon just did.\nNow, next generation Nvidia\nis for the highest end GPU,\nit is mandatory water cooling.\nYou have to water cool it.\nBut Elon did it on this\ncurrent generation,\nand that required a lot of stuff.\nIf you look at some of\nthe satellite photos\nand stuff of the Memphis facility,\nthere's all these external\nwater chillers that are sitting,\nbasically, it looks like\na semi-truck pod thing.\nWhat's it called? The container.\nBut really, those are water chillers.\nAnd he has like 90 of those water chillers\njust sitting outside, 90\ndifferent containers with water,\nlike chill the water, bring\nit back to the data center,\nand then you distribute\nit to all the chips,\npull all the heat out,\nand then send it back.\nAnd this is both a way to cool the chips,\nbut it's also an efficiency thing.\nAnd going back\nto that sort of three\nvector thing right there is,\nthere is memory bandwidth\nFLOPS and interconnect.\nThe closer the chips are together,\nthe easier it is to do\nhigh speed interconnects.\nAnd so, this is also like a reason\nwhy you wanna go water cooling\nis because you can just put the chips\nright next to each other,\nand therefore get higher\nspeed connectivity.\n- I gotta ask you, so in\none of your recent posts,\nthere's a section called\ncluster measuring contest, so...\n- There's another word there,\nbut I won't say it, you know.\n(Dylan and Nathan laughing)\n- Who's got the biggest\nnow and who's gonna have\nthe biggest?\n- Today,\nindividual largest is Elon.\n- Right. Elon's cluster.\n- Elon's cluster in Memphis, 200,000 GPUs.\n- Okay.\n- Meta has like 128,000,\nOpenAI has 100,000 now.\nNow, to be clear, other companies\nhave more GPUs than Elon.\nThey just don't have 'em in one place.", "mimetype": "text/plain", "start_char_idx": 240440, "end_char_idx": 244230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c576a45-4b8e-4191-891a-ee926ea10c80": {"__data__": {"id_": "7c576a45-4b8e-4191-891a-ee926ea10c80", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ab00f04-8b39-4a60-8e90-28759ba1ff9d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d197cc3d05d8dd5d0f67a34db0fe7514dcd361da5a43fd4fb5c1f51b65bb5422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df510d1e-884d-4e87-a914-a055d4043375", "node_type": "1", "metadata": {}, "hash": "ab46cbf8ddb388fe85f17a79b95e2306b11ddd56c15c86ff3f21b392b7b3a4f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so, this is also like a reason\nwhy you wanna go water cooling\nis because you can just put the chips\nright next to each other,\nand therefore get higher\nspeed connectivity.\n- I gotta ask you, so in\none of your recent posts,\nthere's a section called\ncluster measuring contest, so...\n- There's another word there,\nbut I won't say it, you know.\n(Dylan and Nathan laughing)\n- Who's got the biggest\nnow and who's gonna have\nthe biggest?\n- Today,\nindividual largest is Elon.\n- Right. Elon's cluster.\n- Elon's cluster in Memphis, 200,000 GPUs.\n- Okay.\n- Meta has like 128,000,\nOpenAI has 100,000 now.\nNow, to be clear, other companies\nhave more GPUs than Elon.\nThey just don't have 'em in one place.\nAnd for training, you want\nthem tightly connected.\nThere's some techniques\nthat people are researching\nand working on that lets you\ntrain across multiple regions.\nBut for the most part, you\nwant them all in one area.\nSo, you can connect them\nwith high speed networking.\nAnd so, Elon today has 200,000 H100s.\nAnd 100,000 H100s, 100,000 H200s.\nMeta, OpenAI, and Amazon all have\non the scale of 100,000,\na little bit less.\nBut this year, this year,\npeople are building much more.\nAnthropic and Amazon\nare building a cluster\nof 400,000 Trainium 2, which\nis Amazon-specific chip\ntrying to get away from Nvidia.\nMeta and OpenAI have scales\nfor hundreds of thousands,\nbut by next year,\nyou'll have like 500,000\nto 700,000 GPU clusters.\nAnd note, those GPUs are\nmuch higher power consumption\nthan existing ones.\nHopper, 700 watts, Blackwell\ngoes to 1,200 watts.\nSo, the power\nper chip is growing\n(Lex laughing)\nand the number of chips is growing.\n- Nuts. Elon said he'll get to a million.\nYou think that's actually feasible?\n- I don't doubt Elon.\nThe filings that he has for the power plan\nand the Tesla battery packs,\nit's clear he has some\ncrazy plans for Memphis.\nLike permits and stuff is open record.\nBut it's not quite clear what\nand what the timescales are.\nI just never doubt Elon.\nHe's gonna surprise us.\n- So, what's the idea with these clusters?\nIf you have a million GPUs,\nwhat percentage in let's say,\ntwo, three years is used for training\nand what percent of pre-training\nand what percent is used\nfor the actual computation?\n- So, these mega clusters\nmake no sense for inference.\nYou could route inference\nthere and just not train.\nBut most of the inference\ncapacity is being,\nhey, I've got a 30-megawatt\ndata center here.\nI've got 50 megawatts here,\nI've got 100 here, whatever.\nI'll just throw inference in all of those,\nbecause the mega clusters,\nmulti gigawatt data centers,\nI want to train there.\nBecause that's where all\nof my GPUs are co-located,\nwhere I can put them\nat a super high networking\nspeed connected together.\nBecause that's what you need for training.\nNow, with pre-training,\nthis is the old scale.\nYou would increase parameters,\nyou'd increase data, model gets better.\nThat doesn't apply anymore,\nbecause there's not much more\ndata in the pre-training side.\nYes, there's video and audio and image\nthat has not been fully\ntaken advantage of.\nSo, there's a lot more scaling,\nbut a lot of people have transcript,\ntaken transcripts of YouTube videos,\nand that gets you a lot of the data.\nDoesn't get you all of the learning value\nout of the video and image data,\nbut there's still scaling\nto be done on pre-training.\nBut this post-training world\nis where all the FLOPS are gonna be spent.\nThe model's gonna play with itself.\nIt's gonna self-play, it's\ngonna do verifiable tasks,\nit's gonna do computer use in sandboxes.\nIt might even do\nsimulated robotics things.\nAll of these things are\ngonna be environments\nwhere compute is spent in,\nquote, unquote, \"post-training\".\nBut I think it's gonna be good.\nWe're gonna drop the\npost from post-training.\n- Yeah. Wow.", "mimetype": "text/plain", "start_char_idx": 243536, "end_char_idx": 247313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df510d1e-884d-4e87-a914-a055d4043375": {"__data__": {"id_": "df510d1e-884d-4e87-a914-a055d4043375", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c576a45-4b8e-4191-891a-ee926ea10c80", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "52da7748066d01598414eeb5c8a7db889f5670c23e402cc97a521f2a90197b8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8547f86-7d0a-447b-b96a-b978382e7425", "node_type": "1", "metadata": {}, "hash": "773ad5abee5a9c7ca62341244db4ae11da3e82fe9cde02af74416d0b1cd7be86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes, there's video and audio and image\nthat has not been fully\ntaken advantage of.\nSo, there's a lot more scaling,\nbut a lot of people have transcript,\ntaken transcripts of YouTube videos,\nand that gets you a lot of the data.\nDoesn't get you all of the learning value\nout of the video and image data,\nbut there's still scaling\nto be done on pre-training.\nBut this post-training world\nis where all the FLOPS are gonna be spent.\nThe model's gonna play with itself.\nIt's gonna self-play, it's\ngonna do verifiable tasks,\nit's gonna do computer use in sandboxes.\nIt might even do\nsimulated robotics things.\nAll of these things are\ngonna be environments\nwhere compute is spent in,\nquote, unquote, \"post-training\".\nBut I think it's gonna be good.\nWe're gonna drop the\npost from post-training.\n- Yeah. Wow.\n- It's gonna be pre-training\nand it's gonna be training, I think.\nAt point at some,\n(Dylan drowns out Nathan)\nat some point.\n(Nathan laughing)\nBecause for the bulk\nof the last few years,\npre-training has dwarfed post-training.\n- [Lex] Mm-hmm.\n- But with these verifiable methods,\nespecially ones that scale really,\npotentially infinitely,\nlike computer use and robotics,\nnot just math and coding,\nwhere you can verify what's happening,\nthose infinitely verifiable tasks,\nit seems you can spend as much\ncompute as you want on them.\n- Especially at the\ncontext length increase.\n'Cause at the end of pre-training\nis when you increase the\ncontext length for these models.\nAnd we've talked earlier\nin the conversation\nabout how the context length,\nwhen you have a long input,\nis much easier to manage than output.\nAnd a lot of these post-training\nand reasoning techniques\nrely on a ton of sampling\nand it's becoming\nincreasingly long context.\nSo, there's just your,\neffectively your compute\nefficiency goes down.\nI think FLOPS is the standard\nfor how you measure it.\nBut with RL and you have\nto do all these things,\nwhere you move your weights around\nin a different way than at\npre-training and just generation.\nIt's going to be become less efficient\nand FLOPS is gonna be\nless of a useful term.\nAnd then, as the\ninfrastructure gets better,\nit's probably gonna go back to FLOPS.\n- So, all of the things\nwe've been talking about\nis most likely going to be Nvidia.\nIs there any competitors?\n- Google, I ignored them.\n- TPU. Yeah. (chuckles)\n- I was like, huh?\n- Yeah,\nwhat's the story with TPU?\nLike what's the...\n- TPU is awesome. It's great.\nGoogle is, they're a bit more tepid\non building data centers for some reason.\nThey're building big data\ncenters, don't get me wrong.\nAnd they actually have\nthe biggest cluster.\nI was talking about Nvidia clusters,\nthey actually have the\nbiggest cluster, period.\nBut the way they do it\nis very interesting.\nThey have two data center super regions\nin that, the data center isn't physically\nlike all of the GPUs aren't\nphysically on one site,\nbut they're like 30 miles from each other.\nOr not GPUs, TPUs.\nThey have like in Iowa and Nebraska,\nthey have four data centers\nthat are just right next to each other.\n- Why doesn't Google\nflex it's cluster size?\n- Go to multi-datacenter training.\nThere's good images in there.\nSo, I'll show you what I mean.\nIt's just SemiAnalysis multi-datacenter.\nSo, this is an image\nof what a standard Google\ndata center looks like.\nBy the way, their data\ncenters look very different\nthan anyone else's data centers.\n- [Lex] What are we looking at here?\n- So, these are, yeah,\nso if you see this image right,\nin this center, there are\nthese big rectangular boxes.\nThose are where the actual chips are kept.\nAnd then, if you scroll\ndown a little bit further,\nyou can see there's\nlike these water pipes,\nthere's these chiller\ncooling towers in the top,\nand a bunch of diesel generators.\nThe diesel generators are backup power.\nThe data center itself\nlook physically smaller\nthan the water chillers.", "mimetype": "text/plain", "start_char_idx": 246515, "end_char_idx": 250374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8547f86-7d0a-447b-b96a-b978382e7425": {"__data__": {"id_": "c8547f86-7d0a-447b-b96a-b978382e7425", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df510d1e-884d-4e87-a914-a055d4043375", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c06ca5100aa4ee89d3491a7ef3e7842944ca61b0818b6c90d0a5b8a84f01fe1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e4407e6-4e14-4e08-a0d5-840b1684c5b2", "node_type": "1", "metadata": {}, "hash": "751182439c48fbd0318027dad4c3ecf3eeb5adc4e726ae75c19b0ac7495e7c9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Why doesn't Google\nflex it's cluster size?\n- Go to multi-datacenter training.\nThere's good images in there.\nSo, I'll show you what I mean.\nIt's just SemiAnalysis multi-datacenter.\nSo, this is an image\nof what a standard Google\ndata center looks like.\nBy the way, their data\ncenters look very different\nthan anyone else's data centers.\n- [Lex] What are we looking at here?\n- So, these are, yeah,\nso if you see this image right,\nin this center, there are\nthese big rectangular boxes.\nThose are where the actual chips are kept.\nAnd then, if you scroll\ndown a little bit further,\nyou can see there's\nlike these water pipes,\nthere's these chiller\ncooling towers in the top,\nand a bunch of diesel generators.\nThe diesel generators are backup power.\nThe data center itself\nlook physically smaller\nthan the water chillers.\nSo, the chips are actually\neasier to keep together,\nbut then cooling all the water\nfor the water cooling is very difficult.\nSo, Google has a very\nadvanced infrastructure\nthat no one else has for the TPU.\nAnd what they do is they've\nstamped these data center,\nthey've stamped a bunch\nof these data centers out\nin a few regions.\nSo, if you go a little bit further down,\nthis is a Microsoft, this is in Arizona.\nThis is where GPT-5, quote,\nunquote, \"will be trained\".\n(Lex laughing)\n- [Nathan] If it doesn't exist already.\n- Yeah, if it doesn't exist already.\nBut each of these data centers,\nI've shown a couple images of them,\nthey're really closely co-located\nin the same region, Nebraska, Iowa.\nAnd then, they also have a\nsimilar one in Ohio complex.\nAnd so, these data centers are\nreally close to each other.\nAnd what they've done\nis they've connected them super\nhigh bandwidth with fiber.\nAnd so, these are just\na bunch of data centers.\nAnd the point here is that Google\nhas a very advanced infrastructure,\nvery tightly connected in a small region.\nSo, Elon will always\nhave the biggest cluster fully connected,\nbecause it's all in one building.\n- Yeah.\n- And he's completely right\non that.\nGoogle has the biggest cluster,\nbut you have to spread over three sites,\nand by a significant margin,\nbut you have to go across multiple sites.\n- Why doesn't Google compete with Nvidia?\nWhy don't they sell TPUs?\n- I think there's a\ncouple problems with it.\nIt's like one, TPU has been a form\nof allowing search to\nbe really freaking cheap\nand build models for that.\nAnd so, a big chunk of the search,\nGPU purchases or TPU purchases,\nor big chunk of Google's\npurchases and usage,\nall of it is for internal workloads.\nWhether it be Search,\nnow, Gemini, YouTube,\nall these different\napplications that they have ads,\nthese are where all their\nTPUs are being spent,\nand that's what they're hyperfocused on.\nAnd so, there's certain\naspects of the architecture\nthat are optimized for their use case\nthat are not optimized elsewhere.\nOne simple one is they've\nopen sourced the Gemma model\nand they called it Gemma 7B.\nBut then, it's actually\n8 billion parameters\nbecause the vocabulary is so large.\n(Lex laughing)\nAnd the reason they made\nthe vocabulary so large\nis because TPU's\nmatrix-multiply unit is massive.\nBecause that's what they've optimized for.\nAnd so, they decided, oh, well,\nI'll just make the vocabulary large too,\neven though it makes no sense\nto do so in such a small model,\nbecause that fits on their hardware.\nSo, Gemma doesn't run\nas efficiently on a GPU as a Llama does.\nBut vice versa, Llama doesn't run\nas efficiently on a TPU as a Gemma does.\nAnd it's so like,\nthere's certain aspects of\nhardware, software, co-design.\nSo, all their search models,\nor their ranking and\nrecommendation models,\nall these different models that are AI,\nbut not like GenAI,\nhave been hyper optimized\nwith TPUs forever.\nThe software stack is super optimized,\nbut all of this software stack\nhas not been released publicly at all.\nVery small portions of\nit, Jax and XLA have been.", "mimetype": "text/plain", "start_char_idx": 249558, "end_char_idx": 253438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e4407e6-4e14-4e08-a0d5-840b1684c5b2": {"__data__": {"id_": "0e4407e6-4e14-4e08-a0d5-840b1684c5b2", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8547f86-7d0a-447b-b96a-b978382e7425", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c3885df8cb82fad394f4ae84967f7ad354bdbbd65677526d76c3d752ab8a4916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09e0d412-2978-4486-8af1-58377226b08b", "node_type": "1", "metadata": {}, "hash": "702756d4716d0873fe93052192d7da7790c09d3671009d939c19c7b259210c01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because that's what they've optimized for.\nAnd so, they decided, oh, well,\nI'll just make the vocabulary large too,\neven though it makes no sense\nto do so in such a small model,\nbecause that fits on their hardware.\nSo, Gemma doesn't run\nas efficiently on a GPU as a Llama does.\nBut vice versa, Llama doesn't run\nas efficiently on a TPU as a Gemma does.\nAnd it's so like,\nthere's certain aspects of\nhardware, software, co-design.\nSo, all their search models,\nor their ranking and\nrecommendation models,\nall these different models that are AI,\nbut not like GenAI,\nhave been hyper optimized\nwith TPUs forever.\nThe software stack is super optimized,\nbut all of this software stack\nhas not been released publicly at all.\nVery small portions of\nit, Jax and XLA have been.\nBut the experience when\nyou're inside of Google\nand you're training on\nTPUs as a researcher,\nyou don't need to know anything\nabout the hardware in many cases.\nIt's pretty beautiful.\n- They all love it.\n- But soon as you\nstep outside...\n(Nathan chuckles)\n- A lot of 'em go back. (chuckles)\nThey leave Google and then they go back.\n- Yeah.\n- Yeah.\nThey leave and they start a company,\n'cause they have all these\namazing research ideas\nand they're like, wait,\ninfrastructure's hard,\nsoftware is hard, and this is on GPUs.\nOr if they try to use TPUs, same thing,\n'cause they don't have\naccess to all this code.\nAnd so, it's like, how\ndo you convince a company\nwhose Golden Goose is Search,\nwhere they're making hundreds\nof billions of dollars from,\nto start selling GPU, or TPUs,\nwhich they used to only\nbuy a couple billion of...\nI think in 2023, they bought\nlike a couple billion,\nand now they're buying 10\nbillion to $15 billion worth.\nBut how do you convince them\nthat they should just\nbuy like twice as many\nand figure out how to sell\nthem, and make $30 billion?\nLike, who cares about making $30 billion?\n- Won't that 30 billion\nexceed actually the\nsearch profit eventually?\n- You're always gonna make\nmore money on services\nthan hardware.\n- Always.\n- Like yeah, to be clear, today,\npeople are spending a lot more\non hardware than they are the services,\nbecause the hardware\nruns the service spend.\n- Yeah.\n- But like-\n- [Lex] You're investing, yeah.\n- If there's no revenue for AI stuff\nor not enough revenue, then\nobviously, it's gonna blow up.\nPeople won't continue to\nspend on GPUs forever.\nAnd then, Nvidia's trying to\nmove up the stack with software\nthat they're trying to\nsell and license and stuff.\nBut Google has never had that DNA\nof this is a product we should sell.\nThe Google Cloud does,\nwhich is a separate\norganization from the TPU team,\nwhich is a separate organization\nfrom the DeepMind team,\nwhich is a separate organization\nfrom the Search team,\nthere's a lot of bureaucracy here.\n- Wait, Google Cloud is a\nseparate team than the TPU team?\n- Technically, TPU sits\nunder infrastructure,\nwhich sits under Google Cloud,\nbut Google Cloud, for renting stuff\nand TPU architecture are\nvery different goals.\nAnd hardware and software,\nlike all of this.\nThe JAX, XLA teams\ndo not serve Google's\ncustomers externally.\nWhereas Nvidia's various CUDA teams\nfor things like NCCL\nserve external customers.\n- [Lex] Mm-hmm.\n- The internal teams like\nJAX and XLA and stuff,\nthey more so serve DeepMind\nand Search.\n- Yeah.\n- And so, their customers' different.\nThey're not building a product for them.\n- Do you understand why AWS keeps winning\nversus Azure for cloud\nversus Google Cloud?\n- Yeah, there's-\n- Google Cloud is tiny,\nisn't it, relative to AWS?\n- Google Cloud is third.\n- [Nathan] Yeah, yeah.\n- Microsoft is the second biggest,\nbut Amazon is the biggest.\n- Yeah.\n- And Microsoft deceptively\nsort of includes Microsoft\nOffice 365, and things like that.\nLike some of these\nenterprise-wide licenses.\nSo, in reality, the Gulf is even larger,\nMicrosoft is still second though.\nAmazon is way bigger. Why?", "mimetype": "text/plain", "start_char_idx": 252673, "end_char_idx": 256549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09e0d412-2978-4486-8af1-58377226b08b": {"__data__": {"id_": "09e0d412-2978-4486-8af1-58377226b08b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e4407e6-4e14-4e08-a0d5-840b1684c5b2", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7a1d0ca1b1b5f7584ce7e717e20b9753ce881e704e1929787306345323b783e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6259c855-e56d-4dd2-a1e1-25734a79f2fd", "node_type": "1", "metadata": {}, "hash": "501ea161dc313d00df7443328cf5726856161df0843bc7f21844a90fc6c9d901", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Whereas Nvidia's various CUDA teams\nfor things like NCCL\nserve external customers.\n- [Lex] Mm-hmm.\n- The internal teams like\nJAX and XLA and stuff,\nthey more so serve DeepMind\nand Search.\n- Yeah.\n- And so, their customers' different.\nThey're not building a product for them.\n- Do you understand why AWS keeps winning\nversus Azure for cloud\nversus Google Cloud?\n- Yeah, there's-\n- Google Cloud is tiny,\nisn't it, relative to AWS?\n- Google Cloud is third.\n- [Nathan] Yeah, yeah.\n- Microsoft is the second biggest,\nbut Amazon is the biggest.\n- Yeah.\n- And Microsoft deceptively\nsort of includes Microsoft\nOffice 365, and things like that.\nLike some of these\nenterprise-wide licenses.\nSo, in reality, the Gulf is even larger,\nMicrosoft is still second though.\nAmazon is way bigger. Why?\nBecause using AWS is better and easier.\nAnd in many cases,\nit's cheaper.\n- It was first.\n- And it's first.\n- It was first.\n- [Lex] Yeah, but there's a lot of things\nthat are first that-\n- Well, it's easier...\nIt's harder to switch\nthan it is to-\n- Yeah, okay.\n- AWS is-\n- Because it's large-\n- There's big fees for switching too.\n- AWS generates over 80%\n(Nathan chuckles)\nof Amazon's profit, I think over 90%.\n- That's insane.\n- The distribution centers\nare just like, one day,\nwe'll decide to make money from\nthis, but they haven't yet.\nThey make tiny little profit from-\n- Yeah, one day, Amazon\nPrime will triple in price.\n- You would think they\nwould improve AWS interface,\n'cause it's horrible.\nIt's clunky, but everybody is...\n(Lex laughing)\n- [Nathan] I don't, yeah,\none would think.\n- I think actually, Google's\ninterface is sometimes nice,\nbut it's also they don't care\nabout anyone besides their top customers.\n- Yeah, exactly.\n- And like\ntheir customer service sucks\nand they have a lot less, like...\n- All these companies,\nthey optimize for the big customers, yeah.\nIt's supposed to be\nfor business.\n- Well, and Amazon\nhas always optimized for the\nsmall customer too though.\nObviously, they optimize a\nlot for the big customer,\nbut when they started, they just would go\nto like random Bay Area\nthings and give out credits.\nAnd then, they like,\nor just put in your\ncredit card and use us.\nIt went back in the early days.\nSo, they've always,\nthe business has grown\nwith them in burgeon.\nSo, why does Amazon, why is\nSnowflake all over Amazon?\nBecause Snowflake, in the beginning,\nwhen Amazon didn't care about\nthem, was still using Amazon.\nAnd then, of course, one day,\nSnowflake and Amazon has\na super huge partnership,\nbut this is the case,\nlike Amazon's user experience\nand quality is better.\nAlso, a lot of the silicon\nthey've engineered makes them\nhave a lower cost structure\nin traditional cloud,\nstorage, CPU, networking,\nthat kind of stuff than in databases.\nI think like four of Amazon's\ntop five revenue products,\nmargin products are gross profit products,\nare all database-related products\nlike Redshift and all these things.\nSo, Amazon has a very good\nsilicon to user experience,\nentire pipeline with AWS.\nI think Google, their silicon teams,\nyeah, they have awesome\nsilicon internally,\nTPU, the YouTube chip,\nsome of these other\nchips that they've made.\nAnd the problem is they're not\nserving external customers,\nthey're serving internal customers.\n- Nvidia's entire culture\nis designed from the bottom-up to do this.\nThere's this recent book,\n\"The Nvidia Way\" by Tae Kim,\nthat details this\nand they're how they look\nfor future opportunities\nand ready their CUDA software libraries\nto make it so that new applications\nof high performance computing\ncan very rapidly be evolved\non CUDA and Nvidia chips.\nAnd that is entirely different\nthan Google as a services business.\n- Yeah, Nvidia, it should be said,\nis a truly special company.\nThere's the whole, the\nculture and everything,\nthey're really optimized\nfor that kind of thing.\nSpeaking of which, is there somebody\nthat can even challenge\nNvidia hardware-wise?\nIntel, AMD?\n- I really don't think so.", "mimetype": "text/plain", "start_char_idx": 255767, "end_char_idx": 259721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6259c855-e56d-4dd2-a1e1-25734a79f2fd": {"__data__": {"id_": "6259c855-e56d-4dd2-a1e1-25734a79f2fd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09e0d412-2978-4486-8af1-58377226b08b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "793d383e05e28d71b7bf04b326276ff334c14e6e71f6656740f412d733719c19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceec81e4-b07f-4dfc-a410-604afbef03f4", "node_type": "1", "metadata": {}, "hash": "762358fb40e761142a067c13246b85fd637c80598e08474fb3e6c1d115383ce0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the problem is they're not\nserving external customers,\nthey're serving internal customers.\n- Nvidia's entire culture\nis designed from the bottom-up to do this.\nThere's this recent book,\n\"The Nvidia Way\" by Tae Kim,\nthat details this\nand they're how they look\nfor future opportunities\nand ready their CUDA software libraries\nto make it so that new applications\nof high performance computing\ncan very rapidly be evolved\non CUDA and Nvidia chips.\nAnd that is entirely different\nthan Google as a services business.\n- Yeah, Nvidia, it should be said,\nis a truly special company.\nThere's the whole, the\nculture and everything,\nthey're really optimized\nfor that kind of thing.\nSpeaking of which, is there somebody\nthat can even challenge\nNvidia hardware-wise?\nIntel, AMD?\n- I really don't think so.\nWe went through a very\nlong process of working\nwith AMD on training on their\nGPU's inference and stuff.\nAnd they're decent.\nTheir hardware is better in\nmany ways than in Nvidia's.\nThe problem is their\nsoftware is really bad,\nand I think they're getting better,\nthey're getting better\nfaster, but they're just,\nthe Gulf is so large\nand they don't spend enough resources on,\nor haven't historically.\nMaybe they're changing their tune now,\nbut for multiple months, we\nwere submitting the most bugs.\nLike us, SemiAnalysis.\n- Mm-hmm.\n- Like what the fuck?\nWhy are we submitting the most bugs?\nBecause they only,\nand they only cared about\ntheir biggest customers.\nAnd so, they'd ship them a\nprivate image, blah, blah, blah.\nAnd it's like, okay, but\nI am just using PyTorch\nand I wanna use the publicly\navailable libraries\n- Mm-hmm.\nYeah.\n- and you don't care\nabout that.\nSo, they're getting better.\nBut I think AMD's not possible.\nIntel's obviously in\ndire straits right now\nand needs to be saved somehow.\nVery important for national security,\nfor American technology dominance.\n- Can you explain the, obviously,\nso why are they in dire straits?\n- Going back to earlier,\nonly three companies can R&D.\n- Yeah.\n- Taiwan, Hsinchu,\nSamsung, Pyongyang, and\nthen Intel, Hillsboro.\nSamsung's doing horribly.\nIntel's doing horribly.\nWe could be in a world\nwhere there's only one\ncompany that can do R&D,\nand that one company already\nmanufactures most of chips.\nThey've been gaining market share anyways.\nBut that's a critical thing.\nSo, what happens to Taiwan\nmeans the rest of the world's\nsemiconductor industry,\nand therefore tech, relies on Taiwan.\nAnd that's obviously precarious.\nAs far as Intel, they've been\nslowly, steadily declining.\nThey were on top of servers and PCs,\nbut now, Apple's done the M1\nand Nvidia's releasing a PC chip,\nand Qualcomm's releasing a PC chip,\nand in servers, hyperscalers are all\nmaking their own ARM-based server chips.\nAnd Intel has no AI silicon wins.\nThey have very small wins.\nAnd they never got into mobile,\nbecause they said no to the iPhone.\nAnd all these things have compounded\nand they've lost their\nprocess technology leadership.\nThey were ahead for 20 years\nand now they're behind by\nat least a couple years.\nAnd they're trying to catch back up\nand we'll see if their 18A,\n14A strategy works out,\nwhere they try and leapfrog TSMC.\nBut like...\nAnd Intel is just losing\ntons of money anyways.\nAnd they just fired their CEO,\neven though their CEO was the only person\nwho understood the company well.\nWe'll see.\nHe was not the best, but he\nwas pretty good, relatively.\nTechnical guy.\n- [Lex] Where does Intel\nmake most of its money?\nThe CPU still, right?\n- PCs\nand data center CPUs, yeah.\nBut data center CPUs are all going Cloud\nand Amazon, Microsoft, Google\nare making ARM-based CPUs.\nAnd then, PC side, AMDs\ngained market share.\nNvidia's launching a chip\nthat's not gonna be a success.\nMediatek, Qualcomm ever launch chips.\nApple's doing well.\nThey could get squeezed\na little bit in PC,\nalthough PC generally I imagine,\nwill just stick Intel\nmostly for Windows side.\n- Let's talk about the broad\nAI race. Who do you think wins?\nWe talked about Google, Meta,\nxAI.", "mimetype": "text/plain", "start_char_idx": 258926, "end_char_idx": 262918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ceec81e4-b07f-4dfc-a410-604afbef03f4": {"__data__": {"id_": "ceec81e4-b07f-4dfc-a410-604afbef03f4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6259c855-e56d-4dd2-a1e1-25734a79f2fd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d3507d131bf58e0ad0db2063bbce76d237b004d810f9b3da6c4c5a819f609e32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0bccdc5-a3d0-4295-87c7-c3d3926559ae", "node_type": "1", "metadata": {}, "hash": "d5a31f5b04efe5d64a7c5ed8c2a4ccae134c4f648cc996bbad122034d759e0da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And they just fired their CEO,\neven though their CEO was the only person\nwho understood the company well.\nWe'll see.\nHe was not the best, but he\nwas pretty good, relatively.\nTechnical guy.\n- [Lex] Where does Intel\nmake most of its money?\nThe CPU still, right?\n- PCs\nand data center CPUs, yeah.\nBut data center CPUs are all going Cloud\nand Amazon, Microsoft, Google\nare making ARM-based CPUs.\nAnd then, PC side, AMDs\ngained market share.\nNvidia's launching a chip\nthat's not gonna be a success.\nMediatek, Qualcomm ever launch chips.\nApple's doing well.\nThey could get squeezed\na little bit in PC,\nalthough PC generally I imagine,\nwill just stick Intel\nmostly for Windows side.\n- Let's talk about the broad\nAI race. Who do you think wins?\nWe talked about Google, Meta,\nxAI.\n- The default leader\nhas been Google because of\ntheir infrastructure advantage.\n- [Lex] Well, like in the\nnews, OpenAI is the leader.\n- They're leading in the-\n- They have the best model.\n- They have the best\nmodel that people can use.\nAnd they're experts-\n- And they have\nthe most AI revenue.\n- Yeah. OpenAI is winning.\n- So, who's making money on AI right now?\nIs anyone making money?\n- So, accounting profit-wise,\nMicrosoft is making money,\nbut they're spending a lot of CapEx,\nand that gets depreciated over years.\nMeta's making tons of money,\nbut with recommendation systems,\nwhich is AI,\n(Nathan chuckles)\nbut not with Llama.\n- Right.\n- Llama's losing money for sure.\nI think Anthropic and OpenAI\nare obviously not making money,\n'cause otherwise, they\nwouldn't be raising money.\nThey have to raise money to build more.\nAlthough theoretically,\nthey are making money.\nYou spent a few hundred\nmillion dollars on GPT-4,\nand it's doing billions in revenue.\nSo, obviously, it's making money.\nAlthough they had to continue to research\nto get the compute efficiency wins,\nand move down the curve\nto get that 1,200x that has\nbeen achieved for GPT-3.\nMaybe we're only at a couple 100x now,\nbut with GPT-4 Turbo and 4o,\nand there'll be another one\nprobably cheaper than GPT-4o even\nthat comes out at some point.\n- And that research costs a lot of money.\n- [Dylan] Yep. Exactly.\n- That's the thing that I guess\nis not talked about with the cost,\nthat when you're referring\nto the cost of the model,\nit's not just the\ntraining or the test runs,\nit's the actual research,\nthe manpower that-\n- Yeah, to do things\nlike reasoning right now\nthat that exists, they're gonna scale it,\nthey're gonna do a lot of research still.\nI think people focus on\nthe payback question,\nbut it's really easy\nto just be like, well,\nGDP is humans and industrial capital.\nAnd if you can make intelligence\ncheap, you can grow a lot.\nThat's the sort of dumb way to explain it.\nBut that's what basically\nthe investment thesis is.\nI think only Nvidia is\nactually making tons of money\nand other hardware vendors.\nThe hyperscalers are all\non paper making money,\nbut in reality, they're\nspending a lot more\non purchasing the GPUs,\nwhich you don't know if they're still\ngonna make this much money\non each GPU in two years.\nYou don't know if all of a\nsudden, OpenAI goes kapoof,\nand now Microsoft has\nhundreds of thousands\nof GPUs they were renting\nto OpenAI that are,\nthat they paid for themselves\nwith their investment in them\nthat no longer have a customer.\nthis is always a possibility.\nI don't believe that.\nI think OpenAI will keep raising money.\nI think others will keep raising money,\nbecause the investments,\nthe returns from it\nare gonna be eventually\nhuge once we have AGI.\n- So, do you think multiple\ncompanies will get...\nLet's assume that-\n- I don't think\nit's winner take all.\n- Okay. So, let's not\ncall it AGI, whatever.\nIt's like a single day.\nIt's a gradual thing.\n- Powerful AI,\nsuper powerful AI.", "mimetype": "text/plain", "start_char_idx": 262147, "end_char_idx": 265895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0bccdc5-a3d0-4295-87c7-c3d3926559ae": {"__data__": {"id_": "f0bccdc5-a3d0-4295-87c7-c3d3926559ae", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceec81e4-b07f-4dfc-a410-604afbef03f4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "51bace480708371627846abffbc1ee723a01fc83944cf452fe08f0441632f814", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdf27d02-2c56-4efc-84ec-b3b31389fa22", "node_type": "1", "metadata": {}, "hash": "aad0145dd518cc00dcaf8674852b18341ead5159354d94f43325594b5e3f1132", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You don't know if all of a\nsudden, OpenAI goes kapoof,\nand now Microsoft has\nhundreds of thousands\nof GPUs they were renting\nto OpenAI that are,\nthat they paid for themselves\nwith their investment in them\nthat no longer have a customer.\nthis is always a possibility.\nI don't believe that.\nI think OpenAI will keep raising money.\nI think others will keep raising money,\nbecause the investments,\nthe returns from it\nare gonna be eventually\nhuge once we have AGI.\n- So, do you think multiple\ncompanies will get...\nLet's assume that-\n- I don't think\nit's winner take all.\n- Okay. So, let's not\ncall it AGI, whatever.\nIt's like a single day.\nIt's a gradual thing.\n- Powerful AI,\nsuper powerful AI.\n- But it's a gradually\nincreasing set of features\nthat are useful and make-\n- [Nathan] Rapidly increasing\nset of features.\n- Rapidly,\nrapidly increasing set of features.\nSo, you're saying a lot\nof companies will be...\nIt just seems absurd that\nall of these companies\nare building gigantic data centers.\n- There were companies\nthat will benefit from AI,\nbut not because they train the best model.\nLike Meta has so many avenues\nto benefit from AI and\nall of their services.\nPeople are there, people\nspend time on Meta's platforms\nand it's a way to make more\nmoney per user per hour.\n- Yeah it seems like\nGoogle/xAI/Tesla, important to say,\nand then Meta will benefit\nnot directly from the AI\nlike the LLMs, but from the intelligence,\nlike the additional boost of intelligence\nto the products they already sell.\nSo, whether that's the\nrecommendation system\nor for Elon who's been talking\nabout Optimus, the robot,\npotentially the intelligence of the robot.\nAnd then, you have personalized\nrobots in the home,\nthat kind of thing.\nHe thinks it's a 10 plus trillion\ndollar business, which...\n(Lex laughing)\n- Yeah, sure.\n- At some point maybe.\nNot soon, but who knows\nwhat robotics will-\n- Let's do a TAM analysis.\n8 billion humans\nand let's get\n(Nathan laughing)\n8 billion robots.\nAnd let's pay 'em the average salary,\nand yeah, there we go, 10\ntrillion, more than 10 trillion.\n- Yeah.\nIf there's robots everywhere,\nwhy does it have to be\njust 8 billion robots?\n- Yeah, yeah,\nof course, of course.\n- [Lex] It could be-\n- I'm gonna have one robot,\nyou're gonna have like 20.\n- Yeah, I see a use case for that.\nSo, yeah, I guess the benefit\nwould be in the products they sell,\nwhich is why OpenAI's in a\ntrickier position, 'cause they-\n- All of the value of OpenAI right now\nas a brand is in ChatGPT.\nAnd there is actually\nnot that, for most users,\nthere's not that much of a reason\nthat they need OpenAI\nto be spending billions\nand billions of dollars\non the next best model,\n- Mm-hmm.\n- when they could\njust license Llama 5,\nand for be way cheaper.\nSo, that's like ChatGPT\nis an extremely valuable\nentity to them, (chuckles)\nbut they could make\nmore money just off that\nthan trying-\n- The chat application\nis clearly like, does not\nhave tons of room to continue.\nLike the standard chat,\nwhere you're just using it for\na random question and stuff.\nThe cost continues to collapse.\nV3 is the latest-\n- It'll go down to ads.\n- Biggest, but it's gonna\nget supported by ads.\nMeta already serves 405b\nand probably loses the\nmoney, but at some point,\nthey're going to get, the\nmodels are gonna get so cheap\nthat they can just serve them\nfor free with ads supported.\nAnd that's what Google's\ngonna be able to do.\nAnd that's obviously\nthey've got a bigger reach.\nSo, chat is not gonna\nbe the only use case.\nIt's like these reasoning,\ncode, agents, computer use,\nall this stuff is where OpenAI\nhas to actually go to\nmake money in the future.\nOtherwise, they're kaputs.\n- But X, Google, and Meta\nhave these other products.\nSo, isn't it likely that OpenAI\nand Anthropic disappear eventually?", "mimetype": "text/plain", "start_char_idx": 265203, "end_char_idx": 268969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fdf27d02-2c56-4efc-84ec-b3b31389fa22": {"__data__": {"id_": "fdf27d02-2c56-4efc-84ec-b3b31389fa22", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0bccdc5-a3d0-4295-87c7-c3d3926559ae", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2b92641fabedb5c611cb7c57fbfc21810fc91d32625f3c0c5677494bd8feb0ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa", "node_type": "1", "metadata": {}, "hash": "b9bee060d0347614af0c562dbf95390509e17698a6b06cdb05be4415f2e39303", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The cost continues to collapse.\nV3 is the latest-\n- It'll go down to ads.\n- Biggest, but it's gonna\nget supported by ads.\nMeta already serves 405b\nand probably loses the\nmoney, but at some point,\nthey're going to get, the\nmodels are gonna get so cheap\nthat they can just serve them\nfor free with ads supported.\nAnd that's what Google's\ngonna be able to do.\nAnd that's obviously\nthey've got a bigger reach.\nSo, chat is not gonna\nbe the only use case.\nIt's like these reasoning,\ncode, agents, computer use,\nall this stuff is where OpenAI\nhas to actually go to\nmake money in the future.\nOtherwise, they're kaputs.\n- But X, Google, and Meta\nhave these other products.\nSo, isn't it likely that OpenAI\nand Anthropic disappear eventually?\nBecause it's-\n- Unless they're so good\nat models, 'cause they are.\n- [Lex] But it's such a cutting edge,\nI mean, you have to get-\n- It depends on where\nyou think AI capabilities are going.\n- You have to keep winning.\n- Yes.\n- You have to keep winning.\nAs you climb,\neven if the AI capabilities\nare going super rapidly awesome\ninto the direction of\nAGI, there's still a boost\nfor X in terms of data,\nGoogle in terms of data,\nMeta in terms of data,\nin terms of other products and the money.\nThere's just\nhuge amount of money.\n- But if the whole idea\nis human data is tapped\nout, we don't care,\nwe all care about\nself-play verifiable tasks-\n- Yes, the self-play,\n- Think about AWS-\n- [Lex] which is an R&D problem.\n- AWS does not make a lot of money\non each individual machine.\nAnd the same can be said for\nthe most powerful AI platform,\nwhich is even though the\ncalls to the API are so cheap,\nthere's still a lot of money\nto be made by owning that platform.\nAnd there's a lot of discussions\nas it's the next compute layer.\n- You have to believe that...\nAnd there's a lot of\ndiscussions that tokens\nand tokenomics and LLM APIs\nare the next compute layer,\nor the next paradigm for the economy,\nlike energy and oil was.\nBut there's also, you\nhave to believe that APIs\nand chat are not where AI is stuck.\nIt is actually just tasks\nand agents and robotics and computer use.\nAnd those are the areas\nwhere all the value will be delivered,\nnot API, not chat application.\n- So, is it possible you have,\nI mean, it all just becomes a commodity,\nand you have the very thin\nwrapper like Perplexity.\nJust joking.\n- [Nathan] There are a lot of wrappers\nmaking a lot of money.\n- Yeah, but do you think it's possible\nthat people would just\neven forget what OpenAI\nand Anthropic is, and just,\n'cause there'll be\nwrappers around the API,\nand it just dynamically-\n- If model progress is not rapid, yeah.\nIt's becoming a commodity.\nDeepSeek-V3 shows this,\nbut also the GPT-3 chart\nearlier, chart showed this.\nLlama 3B is 1,200x cheaper than GPT-3.\nAny GPT-3, like anyone\nwhose business model\nwas GPT-3 level capabilities is dead.\n- Yeah.\n- Anyone whose business model\nis GPT-4 level capabilities is dead.\n- It is a common saying\nthat the best businesses\nbeing made now are ones\nthat are predicated on\nmodels getting better.\n- Right, which would be wrappers,\nthing that is riding\nthe wave of the models.\n- The short-term that company\nthat could make the most money\nis the one that figures out\nwhat advertising targeting method works\nfor language model generations.\nWe have the Meta ads which\nare hyper targeted in feed,\nnot within specific pieces\n- Mm-hmm.\n- of content.\nAnd we have search ads\nthat are used by Google\nand Amazon has been\nrising a lot on Search.\nBut within a piece, within\na return from ChatGPT,\nit is not clear\nhow you get a high quality\nplaced ad within the output.\nAnd if you can do that with\nmodel costs coming down,\nyou can just get super high revenue per...\nThat revenue is totally untapped\nand it's not clear\ntechnically how it is done.", "mimetype": "text/plain", "start_char_idx": 268238, "end_char_idx": 272006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa": {"__data__": {"id_": "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdf27d02-2c56-4efc-84ec-b3b31389fa22", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "31bc6b327697c890b2ee42f7cc8699070532630e9a464b17dfceeffa182d3201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3015bec2-5339-4492-b524-e70ab6c9d55b", "node_type": "1", "metadata": {}, "hash": "7bfd5757e8d8a4a0c2bb3c6199658a07b493259aaf20aaca0257b45caeed1435", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Right, which would be wrappers,\nthing that is riding\nthe wave of the models.\n- The short-term that company\nthat could make the most money\nis the one that figures out\nwhat advertising targeting method works\nfor language model generations.\nWe have the Meta ads which\nare hyper targeted in feed,\nnot within specific pieces\n- Mm-hmm.\n- of content.\nAnd we have search ads\nthat are used by Google\nand Amazon has been\nrising a lot on Search.\nBut within a piece, within\na return from ChatGPT,\nit is not clear\nhow you get a high quality\nplaced ad within the output.\nAnd if you can do that with\nmodel costs coming down,\nyou can just get super high revenue per...\nThat revenue is totally untapped\nand it's not clear\ntechnically how it is done.\n- Yeah, that is...\nthe ad sense innovation that Google did,\nthe one day you'll have\nin GPT output an ad\nand that's gonna make\nbillions, not-\n- And it could be very subtle.\nIt could be in conversation.\nWe have voice mode now.\nIt could be some way of making it,\nso the voice introduces certain things.\nIt's much harder to measure\nand it takes imagination, but yeah.\n- And it wouldn't come off shady,\nso you would receive public\nblow back, that kind of thing.\nSo, you have to do it loud enough\nto where it's clear it's an ad\nthat and balance all of that.\nSo, that's the open question\nthey're trying to solve.\nAnthropic and OpenAI, they need to-\n- They might not say\nthat they-\n- I don't think\nthey care about that at all.\n- They don't care about it right now.\nI think it's places\n- I think they're surely-\n- like Perplexity are\nexperimenting on that more.\n- [Lex] Oh, interesting. Yeah, for sure.\n- Like Perplexity, Google,\nMeta care about this.\nI think OpenAI and Anthropic\nare purely laser-focused on-\n- AGI.\n- Yeah, agents and AGI.\nAnd if I build AGI, I\ncan make tons of money.\nOr I can spend, pay for everything.\nAnd it is just predicated back\non the export control thing.\nIf you think AGI is 5,\n10 years away or less,\nthese labs think it's 2, 3 years away.\nObviously, your actions are,\nif you assume they're rational actors,\nwhich they are mostly,\nwhat you do in a two-year\nAGI versus five-year\nversus 10 years is very,\nvery, very different.\n- Do you think agents are promising?\nWe have to talk about this. (chuckles)\nThis is like the excitement of the year\nthat agents are gonna rev...\nThis is the generic hype term\nthat a lot of business folks are using.\nAI agents are gonna\nrevolutionize everything.\n- Okay. So, mostly, the term\nagent is obviously overblown.\nWe've talked a lot about\nreinforcement learning\nas a way to train for verifiable outcomes.\nAgents should mean\nsomething that is open-ended\nand is solving a task\nindependently on its own,\nand able to adapt to uncertainty.\nThere's a lot of the term agent applied\nto things like Apple Intelligence,\nwhich we still don't\nhave after the last WWDC,\nwhich is orchestrating between apps.\nAnd that type of tool\nuse thing is something\nthat language models can do really well.\nApple Intelligence I suspect will work,\nso will come eventually.\nIt's a closed domain.\nIt's your messages app\nintegrating with your photos,\nwith AI in the background.\nThat will work.\nThat has been described as an agent\nby a lot of software companies\nto get into the narrative.\n- Yeah.\n- The question is what ways\ncan we get language models\nto generalize to new domains\nand solve their own problems in real time.\nMaybe some tiny amount of training\nwhen they're doing this\nwith fine-tuning themselves\nor in context learning,\nwhich is the idea of storing\ninformation in a prompt.\nAnd you can use learning\nalgorithms to update that.\nAnd whether or not you believe\nthat that is gonna actually\ngeneralize to things\nlike me saying, book my trip\nto go to Austin in two days.\nI have X, Y, Z constraints,\nand actually trusting it.\nI think there's a HCI problem\ncoming back for information.\n- Well, what's your prediction there?\nBecause my gut says we're\nvery far away from that.", "mimetype": "text/plain", "start_char_idx": 271272, "end_char_idx": 275205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3015bec2-5339-4492-b524-e70ab6c9d55b": {"__data__": {"id_": "3015bec2-5339-4492-b524-e70ab6c9d55b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c8a177b-23b0-4625-8422-9a4ce2e5bdfa", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cf871a3178df9964756e8f3f5ca1550d536387048c94b48309194e766b8ad909", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37371d20-a446-4d92-8406-5d0b49d23b40", "node_type": "1", "metadata": {}, "hash": "f4657ee6b84a029e642d20fe53e98a2e5f3334c7e2a6ccb245650ca36ddb07d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That will work.\nThat has been described as an agent\nby a lot of software companies\nto get into the narrative.\n- Yeah.\n- The question is what ways\ncan we get language models\nto generalize to new domains\nand solve their own problems in real time.\nMaybe some tiny amount of training\nwhen they're doing this\nwith fine-tuning themselves\nor in context learning,\nwhich is the idea of storing\ninformation in a prompt.\nAnd you can use learning\nalgorithms to update that.\nAnd whether or not you believe\nthat that is gonna actually\ngeneralize to things\nlike me saying, book my trip\nto go to Austin in two days.\nI have X, Y, Z constraints,\nand actually trusting it.\nI think there's a HCI problem\ncoming back for information.\n- Well, what's your prediction there?\nBecause my gut says we're\nvery far away from that.\n- I think OpenAI's statement,\nI don't know if you've\nseen the five levels.\nOr it's chat is level one,\nreasoning is level two, and\nthen agents is level three.\nAnd I think there's a couple more levels,\nbut it's important to\nnote, we were in chat\nfor a couple years.\n- Mm-hmm.\n- We just theoretically got to reasoning.\nWe'll be here for a year or two.\nAnd then, agents, but at the same time,\npeople can train like\napproximate capabilities\nof the next level,\nbut the agents are doing\nthings autonomously,\ndoing things for minutes at a time,\nhours at a time, et cetera.\nReasoning is doing things for\ntens of seconds at a time.\nAnd then, coming back with an output\nthat I still need to verify\nand use and try check out.\nAnd the biggest problem is of course,\nit's the same thing with manufacturing.\nThere's the whole six sigma thing.\nHow many nines do you get,\nand then you compound the\nnines onto each other,\nand it's like if you multiply\nby the number of steps that are six sigma,\nyou get to a yield or something.\nSo, like in semiconductor manufacturing,\ntens of thousands of steps.\n9999999 is not enough,\nbecause you multiply that many times,\nyou actually end up with like 60% yield.\n- Or zero.\n- Really low yield. Yeah.\nOr zero.\nAnd this is the same thing with agents.\nChaining tasks together each time, LLMs,\neven the best LLMs in particularly\npretty good benchmarks\ndon't get 100%.\n- Yeah.\n- They get a little bit below that,\nbecause there is a lot of noise.\nAnd so, how do you get to enough nines?\nThis is the same thing with self-driving.\nWe can't have self-driving\nbecause without it being\nsuper geofenced like Google's.\nAnd even then, they have\na bunch of tele operators\nto make sure it doesn't get stuck.\nBut you can't do that because\nit doesn't have enough nines.\n- And self-driving has\nquite a lot of structure\nbecause roads have rules.\nIt's well-defined. There's regulation.\nWhen you're talking about\ncomputer use for the open web,\nfor example, or the open\noperating system, it's a mess.\nSo, the possibility, I'm always skeptical\nof any system that is\ntasked with interacting\nwith the human world,\nthe open message human world.\n- That's the thing.\nIf we can't get intelligence,\nthat's enough to solve the\nhuman world on its own.\nWe can create infrastructure\nlike the human operators for Waymo\n- Yeah.\n- over many years\nthat enable certain workflows.\n- There is a company, I don't remember it,\nbut that's literally their pitches.\nYeah, we're just gonna\nbe the human operator\nwhen agents fail, and you\njust call us and we fix it.\n- Yeah.\n- It's like an API call\nand it's hilarious.\n- There's gonna be teleoperation markets\nwhen we get human robots,\nwhich is there's gonna be somebody\naround the world that's\nhappy to fix the fact\nthat it can't finish loading my dishwasher\n- Yeah.\n- when I'm unhappy with it,\nbut that's just gonna be part\nof the Tesla service package.\n- I'm just imagining an AI agent\ntalking to another AI agent.\nOne company has an AI agent\nthat specializes in\nhelping other AI agents.\n- But if you can make things\nthat are good at one step,\n- Yeah.\n- you can stack them together.", "mimetype": "text/plain", "start_char_idx": 274404, "end_char_idx": 278309, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37371d20-a446-4d92-8406-5d0b49d23b40": {"__data__": {"id_": "37371d20-a446-4d92-8406-5d0b49d23b40", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3015bec2-5339-4492-b524-e70ab6c9d55b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1a0705923200ba6c74d461d0c4206d9360840b1cdf3a1fffcb73b40df12ecc42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "552a6f72-3daf-4164-92de-56ed4a14453a", "node_type": "1", "metadata": {}, "hash": "c406a866b67ce3e040dded1d422fffa955cd119d79abd1a9e5ea9d42c8e8e27a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- over many years\nthat enable certain workflows.\n- There is a company, I don't remember it,\nbut that's literally their pitches.\nYeah, we're just gonna\nbe the human operator\nwhen agents fail, and you\njust call us and we fix it.\n- Yeah.\n- It's like an API call\nand it's hilarious.\n- There's gonna be teleoperation markets\nwhen we get human robots,\nwhich is there's gonna be somebody\naround the world that's\nhappy to fix the fact\nthat it can't finish loading my dishwasher\n- Yeah.\n- when I'm unhappy with it,\nbut that's just gonna be part\nof the Tesla service package.\n- I'm just imagining an AI agent\ntalking to another AI agent.\nOne company has an AI agent\nthat specializes in\nhelping other AI agents.\n- But if you can make things\nthat are good at one step,\n- Yeah.\n- you can stack them together.\nSo, that's why I'm like,\nif it takes a long time,\nwe're gonna build\ninfrastructure that enables it.\nYou see the operator launch,\nthey have partnerships with\ncertain websites, with DoorDash,\nwith OpenTable,\n- Mm-hmm.\n- with things like this.\nThose partnerships are gonna\nlet them climb really fast.\nTheir model's gonna get\nreally good at those things.\nIt's gonna proof of concept\nthat might be a network effect\nwhere more companies wanna\nmake it easier for AI.\nSome companies will be like,\nno, let's at put blockers in place.\n- Yep.\n- And this is the story\nof the internet we've seen.\nWe see it now with training\ndata for language models,\nwhere companies are like,\nno, you have to pay,\n- [Lex] Mm-hmm.\n(Nathan chuckles)\n- business working it out.\n- That said, I think airlines have a very,\nand hotels have high incentive\nto make their site work really\nwell, and they usually don't.\nIf you look at how many clicks it takes\nto order a airplane ticket, it's insane.\nI don't-\n- You actually\ncan't call an American\nAirlines agent anymore.\nThey don't have a phone number.\n- It's horrible on many, on\nthe interface front and all...\nTo imagine that agents will be able\nto deal with that website,\nwhen I, as a human, struggle,\nlike I have an existential crisis\nevery time I try to\nbook an airplane ticket,\nthat I think it's gonna be\nvery extremely difficult\nto build a AI agent that's robust\nin that way.\n- But think about it,\nUnited has accepted the Starlink term,\nwhich is they have to\nprovide Starlink for free\nand the users are going to love it.\nWhat if one Airline is like,\nwe're gonna take a year\nand we're gonna make our website\nhave white text that works\nperfectly for the AIs.\nEvery time anyone asks\nabout an AI flight, they buy\nwhatever airline it is.\n(Dylan laughing)\n- Or they're just like, here's an API in,\nit's only exposed to AI agents\nand if anyone queries it,\nthe price is 10% higher.\n- [Lex] Yeah.\n- And for any flight,\nbut we'll let you see any of our flights\nand you can just book any of them.\nHere you go, agent-\n- And that's-\n- Then, it's, oh, and I\nmade 10% higher price.\nAwesome.\n- Yeah.\n- And am I willing to say that for like,\nhey, book me a flight to see Lex.\nAnd it's like, yeah, whatever.\n- Yeah, yeah.\n- I think computers and real world\nand the open world are\nreally, really messy.\nBut if you start defining the\nproblem in narrow regions,\npeople are gonna be able\nto create very, very productive things,\nand ratchet down cost massively.\nNow, crazy things like\nrobotics in the home,\nthose are gonna be a lot harder to do\njust like self-driving.\nBecause there's just a billion\ndifferent failure modes.\nBut agents that can navigate\na certain set of websites\nand do certain sets of tasks,\nor take a photo of your fridge,\nor like upload your recipes,\nand then it figures out what to order\nfrom Amazon/Whole Foods food delivery.\nAnd that's gonna be pretty\nquick and easy to do, I think.\nSo, it's gonna be be a whole\nrange of business outcomes\nand it's gonna be tons of optimism\naround people can just figure\nout ways to make money.", "mimetype": "text/plain", "start_char_idx": 277514, "end_char_idx": 281356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "552a6f72-3daf-4164-92de-56ed4a14453a": {"__data__": {"id_": "552a6f72-3daf-4164-92de-56ed4a14453a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37371d20-a446-4d92-8406-5d0b49d23b40", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cc61faec728ea0b9c29fb6c9a178d155dedd2b31fc493bec2d7dd32d117ac60a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bea9fbc-77ac-48e6-afcc-00eec95d11ac", "node_type": "1", "metadata": {}, "hash": "d4b35df337f3d8b742e24b2b472f7129ddfb3f5a53d021c5a29a652513881a7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah, yeah.\n- I think computers and real world\nand the open world are\nreally, really messy.\nBut if you start defining the\nproblem in narrow regions,\npeople are gonna be able\nto create very, very productive things,\nand ratchet down cost massively.\nNow, crazy things like\nrobotics in the home,\nthose are gonna be a lot harder to do\njust like self-driving.\nBecause there's just a billion\ndifferent failure modes.\nBut agents that can navigate\na certain set of websites\nand do certain sets of tasks,\nor take a photo of your fridge,\nor like upload your recipes,\nand then it figures out what to order\nfrom Amazon/Whole Foods food delivery.\nAnd that's gonna be pretty\nquick and easy to do, I think.\nSo, it's gonna be be a whole\nrange of business outcomes\nand it's gonna be tons of optimism\naround people can just figure\nout ways to make money.\n- To be clear, these sandboxes\nalready exist in research.\nThere are people who have built clones\nof all the most popular\nwebsites of Google, Amazon,\nblah, blah, blah, to make\nit so that there's...\nAnd I mean, OpenAI probably\nhas them internally\nto train these things.\nIt's the same as DeepMind's robotics team\nfor years has had clusters for robotics,\nwhere you interact with\nrobots fully, remotely.\nThey just have a lab in London\nand you send tasks to it,\nit arrange the blocks\nand you do this research.\nObviously, there's texts\nthere that fix stuff.\nBut we've turned these\ncranks of automation before.\nYou go from sandbox to progress,\nand then you add one\nmore domain at a time,\nand generalize, I think.\nIn the history of NLP\nand language processing,\ninstruction tuning and\ntasks per language model\nused to be one language\nmodel did one task.\nAnd then, in the instruction\ntuning literature,\nthere's this point where\nyou start adding more\nand more tasks together,\nwhere it just starts to\ngeneralize to every task.\nAnd we don't know where\non this curve we are.\nI think for reasoning with this RL\nand verifiable domains, we're early,\nbut we don't know where the point is,\nwhere you just start training\non enough domains and poof,\nmore domains just start working,\nand you've crossed the\ngeneralization barrier.\n- Well, what do you think\nabout the programming context?\nSo, software engineering.\nThat's where I personally,\nand I know a lot of people\ninteract with AI the most.\n- There's a lot of fear\nand angst too from current CS students,\nbut there's also, that is the area\nwhere probably the most AI revenue\nand productivity gains have come.\n- [Lex] Yeah.\n- Whether it be Copilots or\nCursor, or what have you,\nor just standard ChatGPT.\nLike a lot of...\nI know very few programmers\nwho don't have ChatGPT\nand actually many of\nthem have the $200 tier\nbecause that's what it's so good for.\nI think that in that world,\nwe already see it like SWE-bench,\nI don't know if you've\nlooked at the benchmark\nmade by some Stanford students.\nI wouldn't say it's really hard,\nbut I wouldn't say it's easy either.\nI think it takes someone\nwho's been through at\nleast a few years of CS,\nor a couple years of programming\nto do SWE-bench well.\nAnd the models went from\n4% to 60% in like a year.\nAnd where are they gonna go to next year?\nIt's gonna be higher.\nIt probably won't be 100%,\n'cause again, that nines\nis really hard to do,\nbut we're gonna get to\nsome point, where that's,\nand then we're gonna need\nharder software engineering benchmarks,\nand so on and so forth.\nBut the way that people think of it now\nis it's can do code completion easy.\nIt can do some function generation\nand I have to review it.\nGreat.\nBut really, the software\nengineering agents,\nI think can be done faster\nsooner than any other agent,\nbecause it is a verifiable domain.\nYou can always unit test or compile,\nand there's many different regions\nof it can inspect the\nwhole code base at once,\nwhich no engineer really can.\nOnly the architects can really think\nabout this stuff,\n- Mm-hmm.\n- the really senior guys,\nand they can define stuff,\nand then the agent can execute on it.", "mimetype": "text/plain", "start_char_idx": 280519, "end_char_idx": 284492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bea9fbc-77ac-48e6-afcc-00eec95d11ac": {"__data__": {"id_": "0bea9fbc-77ac-48e6-afcc-00eec95d11ac", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "552a6f72-3daf-4164-92de-56ed4a14453a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "079003098d42190fae528cb39b2624125582aed8acd5a0b1cfa5c98e609853ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b17390b-863a-453a-88fd-dfeca48f9370", "node_type": "1", "metadata": {}, "hash": "3776ab4e4c582345cb14774105eb2e1fa6eb334495db7e8082057539edb517f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's gonna be higher.\nIt probably won't be 100%,\n'cause again, that nines\nis really hard to do,\nbut we're gonna get to\nsome point, where that's,\nand then we're gonna need\nharder software engineering benchmarks,\nand so on and so forth.\nBut the way that people think of it now\nis it's can do code completion easy.\nIt can do some function generation\nand I have to review it.\nGreat.\nBut really, the software\nengineering agents,\nI think can be done faster\nsooner than any other agent,\nbecause it is a verifiable domain.\nYou can always unit test or compile,\nand there's many different regions\nof it can inspect the\nwhole code base at once,\nwhich no engineer really can.\nOnly the architects can really think\nabout this stuff,\n- Mm-hmm.\n- the really senior guys,\nand they can define stuff,\nand then the agent can execute on it.\nSo, I think software engineering costs\nare gonna plummet like crazy.\nAnd one interesting aspect of that\nis when software engineering\ncosts are really low,\nyou get very different markets.\nSo, in the US, you have all\nthese platform SaaS companies,\nSalesforce, and so on and so forth.\nIn China, no one uses platform SaaS.\nEveryone just builds their own stack,\nbecause software engineering\nis much cheaper in China,\nand partially because people,\nnumber of STEM graduates, et cetera.\nSo, STEM. So, it's generally\njust cheaper to do.\nAnd so, at the same time,\ncode LLMs have been\nadopted much less in China,\nbecause the cost of an\nengineer there is much lower.\nBut what happens when every\ncompany can just invent\ntheir own business logic\nreally cheaply and quickly?\nYou stop using platform SaaS,\nyou start building\ncustom-tailored solutions,\nyou change them really quickly.\nNow, all of a sudden,\nyour business is a little bit\nmore efficient too potentially\nbecause you're not dealing with the hell\nthat is some random\nplatform SaaS company stuff\nnot working perfectly and\nhaving to adjust workflows,\nor random business automation cases\nthat aren't necessarily AI-required.\nIt's just logic\nthat needs to be built\nthat no one has built.\nAll of these zings can go happen faster.\nAnd so, I think software...\nAnd then, the other\ndomain is like industrial,\nchemical, mechanical engineers\nsuck at coding just generally.\nAnd their tools, like\nsemiconductor engineers,\ntheir tools are 20 years old.\nAll the tools run on XP,\nincluding ASML lithography\ntools, run on Windows XP.\nAnd a lot of the analysis\nhappens in Excel.\nIt's just like, guys,\nyou guys can move 20 years forward\nwith all the data you have\nand gathered and do a lot better.\nIt's just you need the engineering skills\nfor software engineering\nto be delivered to the actual\ndomain expert engineer.\nSo, I think that's the area,\nwhere I'm super, duper bullish\nof generally AI creating value.\n- The big picture\nis that I don't think\nit's gonna be a cliff.\n- Yeah.\n- We talked to,\nI think a really good example\nof how growth changes is\nwhen Meta added stories.\nSo, Snapchat was on an exponential,\nthey added stories, it flatlined.\nSoftware engineers,\nthen up until the right,\nAI's gonna come in, it's\nprobably just gonna be flat.\nIt is not like everyone's\ngonna lose their job.\nIt's hard because the\nsupply corrects more slowly.\nSo, the amount of\nstudents is still growing\nand that'll correct on a\nmulti-year, like a year delay,\nbut the amount of jobs will just turn,\nand then maybe in 20, 40\nyears, it'll be well down.\nBut in the few years,\nthere'll never gonna be the snap moment,\nwhere it's like software\nengineers aren't useful.\n- I think also the nature\nof what it means to be a programmer\nand what kind of jobs\nprogrammers do changes,\n'cause I think there needs\nto be a human in the loop of\neverything you've talked about.\nThere's a really important human\nin that picture of correcting the code,\nlike fix-\n- Think more\nthan the context length.\n- Yep.\nAnd debugging also,\nlike debugging by sort\nof reading the code,\nunderstanding the steering\nthe system, like no, no, no.\nYou missed the point.\nAdding more to the prompt.", "mimetype": "text/plain", "start_char_idx": 283673, "end_char_idx": 287661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b17390b-863a-453a-88fd-dfeca48f9370": {"__data__": {"id_": "4b17390b-863a-453a-88fd-dfeca48f9370", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bea9fbc-77ac-48e6-afcc-00eec95d11ac", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e8336d3b8461488277af37ba8dd250beb6ba51b3122b39d7d0e9061b2e66fe58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccec087a-4aa3-4241-9aaa-343a7b5490f0", "node_type": "1", "metadata": {}, "hash": "4ab7ea41f898e7a076448ff146f0517560c1a89b9f05a488fcdd4c40fb295af1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, the amount of\nstudents is still growing\nand that'll correct on a\nmulti-year, like a year delay,\nbut the amount of jobs will just turn,\nand then maybe in 20, 40\nyears, it'll be well down.\nBut in the few years,\nthere'll never gonna be the snap moment,\nwhere it's like software\nengineers aren't useful.\n- I think also the nature\nof what it means to be a programmer\nand what kind of jobs\nprogrammers do changes,\n'cause I think there needs\nto be a human in the loop of\neverything you've talked about.\nThere's a really important human\nin that picture of correcting the code,\nlike fix-\n- Think more\nthan the context length.\n- Yep.\nAnd debugging also,\nlike debugging by sort\nof reading the code,\nunderstanding the steering\nthe system, like no, no, no.\nYou missed the point.\nAdding more to the prompt.\nKind of like, yes, adding the human-\n- Designing the perfect Google button.\nGoogle's famous for having\npeople design buttons\nthat are so perfect, and it's\nlike how is AI gonna do that?\nIs like they could give you all the ideas.\nPerfect. Fine.\n- Yeah.\n- That's the thing, you can call it taste.\nOne thing humans can do is figure out\nwhat other humans enjoy\nbetter than AI systems.\nThat's where the preference,\nyou loading that in.\nBut ultimately,\nhumans are the greatest\npreference generat...\nThat's where the preference comes from.\n- And humans are actually\nvery good at reading\nor judging between two things versus,\nthis goes back to the core\nof what RLHF and preference tuning is,\nis that it's hard\nto generate a good answer\nfor a lot of problems,\nbut it's easy to see which one is better.\nAnd that's how we're\nusing humans for AI now\nis judging which one is better.\nAnd that's what software\nengineering could look like,\nis the PR review, here's a\nfew options, what are the,\nlike here are some\npotential pros and cons,\nand they're gonna be judges.\n- I think the thing I\nwould very much recommend\nis people start,\nprogrammers start using AI,\nand embracing that role of the\nsupervisor of the AI system,\nand partner the AI system\nversus, writing from scratch\nor not learning coding at all\nand just generating stuff.\nBecause I think there actually\nhas to be a pretty high level\nof expertise as a programmer\nto be able to manage\nincreasingly intelligent systems.\n- I think it's that,\nand then becoming a domain\nexpert in something.\n- Sure. Yeah.\n- Right?\nBecause seriously, if\nyou go look at aerospace\nor semiconductors or chemical engineering,\neveryone is using really crappy platforms,\nreally old software.\nThe job of a data sciences\nis like a joke in many cases.\nIn many cases, it's very real,\nbut it's like bring what the forefront\nof human capabilities are to your domain.\nAnd even if the forefront is from the AI,\nyour domain, you're like at the forefront.\nSo, it's like you have to be\nat the forefront of something,\nand then leverage the like rising tide\nthat is AI for everything else.\n- Oh, yeah.\nThere's so many low-hanging\nfruit everywhere\nin terms of where software\ncan help automate a thing\nor digitize a thing.\nIn the legal system...\nThat's why Doge is exciting.\nI got to hang out\nwith a bunch of the\nDoge folks, and they...\n(Lex chuckles)\nI mean, government is so old school,\nit's like begging for the\nmodernization of software,\nof organizing the data,\nall this kind of stuff.\nIn that case, it's by design,\nbecause bureaucracy protects\ncenters of power, and so on.\nBut software breaks down those barriers,\nso it hurts those that\nare holding onto power,\nbut ultimately, benefits humanity.\nSo, there's a bunch of\ndomains of that kind.\nOne thing we didn't fully finish\ntalking about is open source.\nSo, first of all, congrats,\nyou released a new model.\n- Yeah. This is the-\n- Tulu. (chuckles)\n- I'll explain what a Tulu is.\n- Yeah.\n- A tulu is a hybrid camel\nwhen you breed a Dromedary\nwith a Bactrian camel.", "mimetype": "text/plain", "start_char_idx": 286865, "end_char_idx": 290679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccec087a-4aa3-4241-9aaa-343a7b5490f0": {"__data__": {"id_": "ccec087a-4aa3-4241-9aaa-343a7b5490f0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b17390b-863a-453a-88fd-dfeca48f9370", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "17c122c0d066699f2f3320669d8df649c6bfc03efdb6ebc5fb79cab81b896573", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be20ef15-5501-4c71-aae3-6de74a99756a", "node_type": "1", "metadata": {}, "hash": "67706be9331b74bb3f69ad84710c6dc92fbe6baab2c571f2a8c3f0a06e8c5b02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I got to hang out\nwith a bunch of the\nDoge folks, and they...\n(Lex chuckles)\nI mean, government is so old school,\nit's like begging for the\nmodernization of software,\nof organizing the data,\nall this kind of stuff.\nIn that case, it's by design,\nbecause bureaucracy protects\ncenters of power, and so on.\nBut software breaks down those barriers,\nso it hurts those that\nare holding onto power,\nbut ultimately, benefits humanity.\nSo, there's a bunch of\ndomains of that kind.\nOne thing we didn't fully finish\ntalking about is open source.\nSo, first of all, congrats,\nyou released a new model.\n- Yeah. This is the-\n- Tulu. (chuckles)\n- I'll explain what a Tulu is.\n- Yeah.\n- A tulu is a hybrid camel\nwhen you breed a Dromedary\nwith a Bactrian camel.\nBack in the early days, after\nChatGPT, there was a big wave\nof models coming out like\nAlpaca, Vicuna, et cetera,\nthat were all named after\nvarious mammalian species.\nSo, Tulu is the brand\nis multiple years old,\nwhich comes from that.\n- Mm-hmm.\n- And we've been playing at the frontiers\nof post-training with open source code.\nAnd this first part of this\nrelease was in the fall,\nwhere we've built on Llama's open models,\nopen-weight models,\nand then we add in our fully\nopen code, our fully open data.\nThere's a popular benchmark\nthat is chat bot arena.\nAnd that's generally the metric\nby which how these chat\nmodels are evaluated.\nAnd it's humans compare random models\nfrom different organizations.\nAnd if you looked at the leaderboard\nin November or December,\namong the top 60 models from\n10s to 20s of organizations,\nnone of them had open code or\ndata for just post-training.\nAmong that, even fewer\nor none have pre-training\ndata and code available.\nBut it's like post-training\nis much more accessible at this time.\nIt's still pretty cheap and you can do it.\nAnd the thing is how high\ncan we push this number,\nwhere people have access\nto all the code and data.\nSo, that's the motivation of the project.\nWe draw in lessons from Llama.\nNvidia had a Nemotron model,\nwhere the recipe for their post-training\nwas fairly open with\nsome data and a paper.\nAnd it's putting all these together to try\nto create a recipe that people\ncan fine-tune models like\nGPT-4 to their domain.\n- So, to be clear, in the case of Tulu,\nmaybe you can talk about OLMO 2,\nbut in the case of Tulu,\nyou're taking Llama 3, 4, 5B.\n- Tulu has been a series of\nrecipes for post-training.\nSo, we've done\n- Okay.\n- multiple models over years.\n- Okay.\nAnd so, you're open sourcing everything.\n- Yeah, if you start with\nan open-weight-based model,\nthe whole model technically\nisn't open source,\nbecause you don't know\nwhat Llama put into it,\nwhich is why we have a separate\nthing that we'll get to.\nBut it's just getting\nparts of the pipeline,\nwhere people can zoom in and customize.\nI know I hear from\nstartups and businesses,\nthey're like, okay, I can\ntake this post-training\nand try to apply it to my domain.\nWe talk about verifiers a lot.\nWe use this idea which\nis reinforcement learning\nwith verifiable rewards,\nRLVR, similar to RLHF.\nAnd we applied it to map.\nAnd the model today,\nwhich is we applied it\nto the Llama 405b base\nmodel from last year.\nAnd we have our other stuff,\nwe have our instruction tuning\nand our preference tuning.\nBut the math thing is interesting,\nwhich is it's easier to\nimprove this math benchmark.\nThere's a benchmark,\nM-A-T-H, MATH, all capitals,\ntough name when the benchmark,\nits name is the area\nthat you're evaluating.\nWe're researchers, we're not\nbrands, brand strategists.\nAnd this is something\nthat the DeepSeek paper\ntalked about as well\nis at this bigger model,\nit's easier to elicit\npowerful capabilities\nwith this RL training.\nAnd then, they distill it down\nfrom that big model to the small model.", "mimetype": "text/plain", "start_char_idx": 289936, "end_char_idx": 293686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "be20ef15-5501-4c71-aae3-6de74a99756a": {"__data__": {"id_": "be20ef15-5501-4c71-aae3-6de74a99756a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccec087a-4aa3-4241-9aaa-343a7b5490f0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "068d7aa80b98a13f09fb340f0553dc1ba63bea3d44998294a81873ed895e8b3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8", "node_type": "1", "metadata": {}, "hash": "32d9d5e6e0244635a890cc35a0891889d7a2afc397a93dc914ac41a5b5e37217", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use this idea which\nis reinforcement learning\nwith verifiable rewards,\nRLVR, similar to RLHF.\nAnd we applied it to map.\nAnd the model today,\nwhich is we applied it\nto the Llama 405b base\nmodel from last year.\nAnd we have our other stuff,\nwe have our instruction tuning\nand our preference tuning.\nBut the math thing is interesting,\nwhich is it's easier to\nimprove this math benchmark.\nThere's a benchmark,\nM-A-T-H, MATH, all capitals,\ntough name when the benchmark,\nits name is the area\nthat you're evaluating.\nWe're researchers, we're not\nbrands, brand strategists.\nAnd this is something\nthat the DeepSeek paper\ntalked about as well\nis at this bigger model,\nit's easier to elicit\npowerful capabilities\nwith this RL training.\nAnd then, they distill it down\nfrom that big model to the small model.\nAnd this model we released\ntoday, we saw the same thing,\nis we're AI2, we don't\nhave a ton of compute,\nwe can't train 405b models all the time.\nSo, we just did a few runs\nand they tend to work,\nand it's like, it just shows\nthat there's a lot of room\nfor people to play in these things.\nAnd that's-\n- And they crushed Llama's\nactual release.\nThey're way better than it.\n- Yeah.\nSo, our eval numbers, we\nhave extra months in this,\nbut our eval numbers are much better\nthan the Llama instruct model\n- Mm-hmm.\n- that they released.\n- And then, you also said\nbetter than DeepSeek-V3.\n- Yeah. On our eval benchmark.\nThe most DeepSeek-V3 is really similar.\nWe have a safety benchmark to understand\nif it will say harmful\nthings and things like that.\nAnd that's what draws\ndown most of the way.\nIt's still like-\n- It's like an amalgamation\nof multiple benchmarks\nor what do you mean?\n- Yeah, so we have a 10 evaluat...\nThis is standard practice in post-training\nis you choose your\nevaluations you care about.\nIn academics, in smaller labs,\nyou'll have fewer evaluations.\nIn companies, you'll have a really\none domain that you really care about.\nIn Frontier Labs, you'll have 10s to 20s,\nto maybe even 100 valuations\nof specific things.\nSo, we'd choose a\nrepresentative suite of things\nthat look like chat, precise\ninstruction following,\nwhich is like respond only in emojis,\njust model follow weird things like that.\n- Yeah.\n- Math, code.\nAnd you create a suite like this.\nSo, safety would be 1 of\n10 in that type of suite,\nwhere you have like,\nwhat is the broader\ncommunity of AI care about?\nAnd for example, in\ncomparison to DeepSeek,\nit would be something\nlike our average eval for\nour model would be 80,\nincluding safety and similar without.\nAnd DeepSeek would be like 79%\naverage score without safety\nand their safety score would bring it down\nto like 76-\n- Oh, so you beat them\neven ignoring safety?\n- Yeah.\nSo, this is something that internally,\nit's like I don't want to win\nonly by how you shape the eval benchmark.\nSo, if there's something\nthat's like people may\nor may not care about\nsafety in their model,\nsafety can come downstream,\nsafety can be when you\nhost the model for an API,\nsafety is addressed\nin a spectrum of locations\nin AI applications.\nSo, it's like,\nif you wanna say that\nyou have the best recipe,\nyou can't just gauge it on these things\nthat some people might not want.\n- [Lex] Mm-hmm.\n- And this is just, it's\nlike the time of progress.\nWe benefit if we can release model later,\nwe have more time to learn new techniques.\nLike this RL technique, we\nhad started this in the fall.\nIt's now really popular reasoning models.\nThe next thing to do for\nopen source post-training\nis to scale up verifiers,\nto scale up data to replicate\nsome of DeepSeek's results.\nAnd it's awesome that\nwe have a paper to draw\nand that it makes it a lot easier.\nAnd that's the type of things\nthat is going on among academic\nand closed frontier research in AI.\n- Since you're pushing open source,\nwhat do you think is the future of it?", "mimetype": "text/plain", "start_char_idx": 292888, "end_char_idx": 296718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8": {"__data__": {"id_": "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be20ef15-5501-4c71-aae3-6de74a99756a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "49aec73ec4d026fcaa27636d8ccb481c9b3e861c996a3a8ed02af43488be37c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ee6445d-456e-4ed0-8e09-08e463746555", "node_type": "1", "metadata": {}, "hash": "e8c39dd8008890e8c076152aafc0b85cd396114bf87d8f68e333902ca2558ba4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, it's like,\nif you wanna say that\nyou have the best recipe,\nyou can't just gauge it on these things\nthat some people might not want.\n- [Lex] Mm-hmm.\n- And this is just, it's\nlike the time of progress.\nWe benefit if we can release model later,\nwe have more time to learn new techniques.\nLike this RL technique, we\nhad started this in the fall.\nIt's now really popular reasoning models.\nThe next thing to do for\nopen source post-training\nis to scale up verifiers,\nto scale up data to replicate\nsome of DeepSeek's results.\nAnd it's awesome that\nwe have a paper to draw\nand that it makes it a lot easier.\nAnd that's the type of things\nthat is going on among academic\nand closed frontier research in AI.\n- Since you're pushing open source,\nwhat do you think is the future of it?\nDo you think DeepSeek\nactually changes things\nsince it's open source or open-weight\nor is pushing the open source movement\ninto the open direction?\n- This goes very back to\nthe license discussion.\nSo, DeepSeek-R1 with a friendly\nlicense is a major reset.\nSo, it's like the first time\nthat we've had a really\nclear frontier model\nthat is open-weights\nand with a commercially friendly license\nwith no restrictions on\ndownstream use cases,\nsynthetic data, distillation, whatever.\nThis has never been the case at all\nin the history of AI in the\nlast few years since ChatGPT.\nThere have been models\nthat are off the frontier\nor models with weird licenses\nthat you can't really use them.\n- So, isn't Meta's license\npretty much permissible\nexcept for five companies?\nAnd there's also...\nSo, this goes to what open\nsource AI is, which is,\nthere's also use case\nrestrictions in the Llama license,\nwhich says you can't use\nit for specific things.\nSo, if you come from an open\nsource software background,\nyou would say that that is\nnot an open source license.\n- What kind of things are those though?\nAre they like...\n- At this point, I can't pull\nthem off the top of my head.\nBut it'll be like-\n- Like stuff like competitor,\nprobably.\n- It used to be\nmilitary use was one,\n- Oh-\n- and they removed that for scale.\nIt'll be like CSAM, like\nchild abuse material\nor that's the type of thing\nthat is forbidden there.\nBut that's enough from\nan open source background\nto say it's not open source license.\nAnd also, the Llama license\nhas this horrible thing,\nwhere you have to name your\nmodel Llama if you touch it\nto the Llama model.\n- Mm-hmm.\n- So, it's like the branding thing.\nSo, if a company uses Llama, technically,\nthe license says that they\nshould say built with Llama\nat the bottom of their application.\nAnd from a marketing\nperspective, that just hurts.\nI can suck it up as a researcher,\nI'm like, oh, it's fine.\nIt says Llama dash on all of\nour materials for this release.\nBut this is why we need truly open models,\nwhich is we don't know\nDeepSeek-R1's data, but-\n- Wait, so you're saying I\ncan't make a cheap copy of Llama\nand pretend it's mine,\nbut I can do this with the Chinese model?\n- [Lex] Yeah.\n- Yeah.\n- Hell yeah.\n(Nathan and Dylan laughing)\n- That's what I'm saying.\n- Yeah.\n- And that's why it's like we want to,\nthis whole open language\nmodels thing, the OLMO thing,\nis to try to keep the model\nwhere everything is open\nwith the data as close to\nthe frontier as possible.\nSo, we're compute-constrained,\nwe're personnel-constrained.\nWe rely on getting insights from people\nlike John Schulman tells\nus to do RL on outputs.\nWe can make these big jumps,\nbut it just takes a long time\nto push the frontier of open source.\nAnd fundamentally, I would say\nthat that's because open source AI\ndoes not have the same feedback loops\nas open source software.\nWe talked about open source\nsoftware for security\nalso is just because\nyou build something once\nand you can reuse it.\nIf you go into a new company,\nthere's so many benefits.", "mimetype": "text/plain", "start_char_idx": 295942, "end_char_idx": 299745, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ee6445d-456e-4ed0-8e09-08e463746555": {"__data__": {"id_": "1ee6445d-456e-4ed0-8e09-08e463746555", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a0a6499-2a0f-4a2c-a697-750a5e50a9d8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f22c82a42df57343aad2300fc79f640e96de81c2529728b64d1a16954085ff07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67c2ab1b-0c15-4650-997a-be60575410c3", "node_type": "1", "metadata": {}, "hash": "bc22485b0935145b603541dbf0943259d6e34140bae9334710524644f4e39be6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\n- Hell yeah.\n(Nathan and Dylan laughing)\n- That's what I'm saying.\n- Yeah.\n- And that's why it's like we want to,\nthis whole open language\nmodels thing, the OLMO thing,\nis to try to keep the model\nwhere everything is open\nwith the data as close to\nthe frontier as possible.\nSo, we're compute-constrained,\nwe're personnel-constrained.\nWe rely on getting insights from people\nlike John Schulman tells\nus to do RL on outputs.\nWe can make these big jumps,\nbut it just takes a long time\nto push the frontier of open source.\nAnd fundamentally, I would say\nthat that's because open source AI\ndoes not have the same feedback loops\nas open source software.\nWe talked about open source\nsoftware for security\nalso is just because\nyou build something once\nand you can reuse it.\nIf you go into a new company,\nthere's so many benefits.\nBut if you open source a language model,\nyou have this data sitting around,\nyou have this training\ncode, it's not that easy\nfor someone to come and\nbuild on and improve,\n'cause you need to spend a lot on compute.\nYou need to have expertise.\nSo, until there are feedback\nloops of open source AI,\nit seems mostly an ideological mission.\nLike people like Mark\nZuckerberg, which is like,\nAmerica needs this, and I agree with him,\nbut in the time where the\nmotivation ideologically is high,\nwe need to capitalize\nand build this ecosystem\naround what benefits\ndo you get from seeing\nthe language model data?\nAnd there's not a lot about that.\nWe're gonna try to launch a demo soon\nwhere you can look at a\nOLMO model and a query,\nand see what pre-training\ndata is similar to it,\nwhich is legally risky and complicated,\nbut it's like, what does it mean\nto see the data that\nthe AI was trained on?\nIt's hard to parse.\nIt's terabytes of files.\nIt's like, I don't know what\nI'm gonna find in there.\nBut that's what we need\nto do as an ecosystem\nif people want open source\nAI to be financially useful.\n- We didn't really talk about Stargate.\nI would love to get your opinion\non what the new administration,\nthe Trump administration,\neverything that's doing\nthat's being done from the America side\nand supporting AI infrastructure\nand the efforts of the\ndifferent AI companies.\nWhat do you think about Stargate?\nWhat are we supposed to\nthink about Stargate?\nAnd does Sam have the money?\n(Nathan chuckles)\n- Yeah,\nso I think Stargate is a opaque thing.\nIt definitely doesn't have $500 billion,\ndoesn't even have $100 billion.\nSo, what they announced is\nthis $500 billion number,\nLarry Ellison, Sam\nAltman, and Trump said it.\nThey thanked Trump\nand Trump did do some executive actions\nthat do significantly improve the ability\nfor this to be built faster.\nOne of the executive actions\nyou did is on federal land,\nyou can just basically\nbuild data centers in power,\npretty much like that.\n- Mm-hmm.\n- And then, the permitting\nprocess is basically gone\nor you file after the fact.\nSo, one of the, again,\nlike I had a schizo take\nearlier, another schizo take,\nif you've ever been to the\nPresidio in San Francisco,\nbeautiful area, you\ncould build a power plant\nand a data center there if you wanted to.\nBecause it is federal land.\n(Nathan laughing)\nIt used to be a military base, so.\n- It did.\n- But obviously,\nthis would piss people off.\nIt's a good bit. Anyways.\n(Nathan and Lex laughing)\nTrump has made it much\neasier to do this generally.\nTexas has the only unregulated\ngrid in the nation as well.\n- [Lex] Let's go, Texas.\n- And so, therefore,\nlike ERCOT enables people\nto build faster as well.\nIn addition, the federal\nregulations are coming down.\nAnd so, Stargate is predicated,\nand this is why that whole show happened.\nNow, how they came up\nwith a $500 billion number is beyond me.\nHow they came up with $100 billion number\nmakes sense to some extent.\nAnd there's actually a good table in here\nthat I would like to show in\nthat Stargate piece that I had.\nIt's the most recent one. Yeah.", "mimetype": "text/plain", "start_char_idx": 298916, "end_char_idx": 302820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67c2ab1b-0c15-4650-997a-be60575410c3": {"__data__": {"id_": "67c2ab1b-0c15-4650-997a-be60575410c3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ee6445d-456e-4ed0-8e09-08e463746555", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5602251436aefb8e467685a33142e24c5c13827c260a93daf64f7752690403d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46c8d43a-7344-454c-84d8-6950c619875c", "node_type": "1", "metadata": {}, "hash": "997be1a2fcde5a4d02d721fc40f65ea1cb799b846838c28cce08c0b02b11499c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(Nathan laughing)\nIt used to be a military base, so.\n- It did.\n- But obviously,\nthis would piss people off.\nIt's a good bit. Anyways.\n(Nathan and Lex laughing)\nTrump has made it much\neasier to do this generally.\nTexas has the only unregulated\ngrid in the nation as well.\n- [Lex] Let's go, Texas.\n- And so, therefore,\nlike ERCOT enables people\nto build faster as well.\nIn addition, the federal\nregulations are coming down.\nAnd so, Stargate is predicated,\nand this is why that whole show happened.\nNow, how they came up\nwith a $500 billion number is beyond me.\nHow they came up with $100 billion number\nmakes sense to some extent.\nAnd there's actually a good table in here\nthat I would like to show in\nthat Stargate piece that I had.\nIt's the most recent one. Yeah.\nSo, anyways, Stargate, it's basically,\nit's a table about cost.\nThere, you passed it\nalready. It's that one.\nSo, this table is explaining what happens.\nSo, Stargate is in Abilene, Texas,\nthe first $100 billion of it.\nThat site is 2.2 gigawatts of power in,\nabout 1.8 gigawatts of power consumed.\nPer GPU, they have like roughly...\nOracle is already building\nthe first part of this\nbefore Stargate came about.\nTo be clear, they've been\nbuilding it for a year.\nThey tried to rent it to Elon, in fact.\nBut Elon was like, it's\ntoo slow, I need it faster.\nSo, then he went\nand did his Memphis thing.\n- Mm-hmm.\n- And so, OpenAI was able to get it\nwith this weird joint\nventure called Stargate.\nThey initially signed\na deal with just Oracle\nfor the first section of this cluster.\nThis first section of this cluster\nis roughly $5 billion to\n$6 billion of server spend.\nAnd then, there's another billion or so\nof data center spend.\nAnd then, likewise,\nif you fill out that entire 1.8 gigawatts\nwith the next two generations\nof Nvidia's chips,\nGB200, GB300, VR200, and\nyou fill it out completely,\nthat ends up being roughly\n$50 billion of server cost.\nPlus there's data center\ncost, plus maintenance cost,\nplus operation costs,\nplus all these things.\nAnd that's where OpenAI\ngets to their $100 billion\nannouncement that they had.\nBecause they talked about\n100 billion is phase one.\nThat's this Abilene, Texas data center.\n$100 billion of \"total cost\nof ownership\", quote, unquote.\nSo, it's not CapEx, it's not investment,\nit's $100 billion of\ntotal cost of ownership.\nAnd then, there will be future phases.\nThey're looking at other sites\nthat are even bigger than this\n2.2 gigawatts, by the way,\nin Texas and elsewhere.\nAnd so, they're not\ncompletely ignoring that.\nBut there is, the number of $100 billion\nthat they say is for phase one,\nwhich I do think will happen.\nThey don't even have the money for that.\nFurthermore, it's not $100 billion,\nit's $50 billion of spend.\nAnd then, $50 billion of\noperational cost, power, et cetera,\nrental pricing, et cetera.\n'Cause OpenAI is renting the GPUs\nfrom the Stargate joint venture.\nWhat money do they\nactually have? SoftBank.\nSoftBank is gonna invest,\nOracle's gonna invest,\nOpenAI is gonna invest.\nOpenAI is on the line\n- Mm-hmm.\n- for $19 billion.\nEveryone knows that\nthey've only got 6 billion\nin their last round and 4 billion in debt.\nBut there's news of SoftBank\nmaybe investing 25 billion into OpenAI.\nSo, that's part of it.\nSo, 19 billion can come from there.\nSo, OpenAI does not have the\nmoney at all, to be clear.\nInk is not dried on anything.\nOpenAI has $0\n- Yeah.\n- for this 50 billion.\nAnd which they're legally obligated\nto put 19 billion of CapEx\nor into the joint venture.\nAnd then, the rest, they're gonna pay\nvia renting the GPUs\nfrom the joint venture.\nAnd then, there's Oracle.\nOracle has a lot of money.", "mimetype": "text/plain", "start_char_idx": 302057, "end_char_idx": 305681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46c8d43a-7344-454c-84d8-6950c619875c": {"__data__": {"id_": "46c8d43a-7344-454c-84d8-6950c619875c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67c2ab1b-0c15-4650-997a-be60575410c3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5e796e0b93a38904efbeff7ba3591effcf73937d815b28fb108c050729611ed3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1ddc9d4-06a8-470e-8462-e7de248690c9", "node_type": "1", "metadata": {}, "hash": "8d07c226b4689e308c827399c40d89f7cce09209f870fbb360458f2dbdbaf625", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SoftBank.\nSoftBank is gonna invest,\nOracle's gonna invest,\nOpenAI is gonna invest.\nOpenAI is on the line\n- Mm-hmm.\n- for $19 billion.\nEveryone knows that\nthey've only got 6 billion\nin their last round and 4 billion in debt.\nBut there's news of SoftBank\nmaybe investing 25 billion into OpenAI.\nSo, that's part of it.\nSo, 19 billion can come from there.\nSo, OpenAI does not have the\nmoney at all, to be clear.\nInk is not dried on anything.\nOpenAI has $0\n- Yeah.\n- for this 50 billion.\nAnd which they're legally obligated\nto put 19 billion of CapEx\nor into the joint venture.\nAnd then, the rest, they're gonna pay\nvia renting the GPUs\nfrom the joint venture.\nAnd then, there's Oracle.\nOracle has a lot of money.\nThey're building the\nfirst section completely.\nThey were spending for it themselves.\nThis $6 billion of CapEx,\n$10 billion of TCO.\nAnd they were gonna do that first section.\nThey're paying for that.\nAs far as the rest of the section,\nI don't know how much\nLarry wants to spend.\nAt any point, he could pull out.\nThis is, again, this is\ncompletely voluntary.\nSo, at any point,\nthere's no signed ink on this.\n- Mm-hmm.\n- But he potentially\ncould contribute tens of\nbillions of dollars, to be clear.\nHe's got the money,\nOracle's got the money.\nAnd then, there's like MGX,\nwhich is the south, the UAE fund,\nwhich technically has $1.5\ntrillion for investing in AI.\nBut again, I don't know\nhow real that money is.\nAnd whereas there's no\nink signed for this,\nSoftBank does not have\n$25 billion of cash.\nThey have to sell down their stake in Arm,\nwhich is the leader in\nCPUs, and they IPO'ed it.\nThis is obviously what\nthey've always wanted to do.\nThey just didn't know where\nthey'd redeploy the capital.\nSelling down the stake in\nArm makes a ton of sense.\nSo, they can sell that down\nand invest in this if they want to\nand invest in OpenAI if they want to.\nAs far as money secured,\nthe first 100,000 GB200\ncluster can be funded.\nEverything else after that\n- Up in the air.\n- is up in the air.\nMoney's coming.\nI believe the money will come.\nI personally do.\n(Lex laughing)\n- It's a belief. Okay.\n- It's a belief\nthat they are gonna release better models\nand be able to raise more money,\nright?\n- Yeah, yeah.\nBut the actual reality\nis, is that Elon's right,\nthe money does not exist.\n- What is the US government\nhave to do with anything?\nWhat does Trump have\nto do with everything?\nHe's just a hype man?\n- So, Trump is,\nhe's reducing the regulation\nso they can build it faster.\nAnd he is allowing them to do it.\nBecause any investment of this side\nis gonna involve antitrust stuff.\nSo, obviously, he's gonna\nallow them to do it.\nHe's gonna enable the regulations\nto actually allow to be built.\nI don't believe there's\nany US government dollars\nbeing spent on this though.\n- Yeah.\nSo, I think he's also just\ncreating a general vibe\nthat this regulation will go down\nand this is the era of building.\nSo, if you're a builder,\n- Yeah.\n- you want to create stuff,\nyou wanna launch stuff,\nthis is the time to do it.\n- And so, we've had this\n1.8 gigawatt data center\nin our data for over a year now,\nand we've been sending\nit to all of our clients,\nincluding many of these companies\nthat are building the multi gigawatts.\nBut that is at a level that's not quite,\nmaybe executives seeing\n$500 billion, $100 billion,\nand then everyone's asking them like...\nSo, it could spur another,\nan even faster arms race.\nBecause there's already an arms race,\nbut this like 100 billion,\n$500 billion number, Trump\ntalking about it on TV.\nIt could spur the arm\nrace to be even faster\nand more investors to flood\nin and et cetera, et cetera.", "mimetype": "text/plain", "start_char_idx": 304973, "end_char_idx": 308590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1ddc9d4-06a8-470e-8462-e7de248690c9": {"__data__": {"id_": "c1ddc9d4-06a8-470e-8462-e7de248690c9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46c8d43a-7344-454c-84d8-6950c619875c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "273f657aa792689c05a3ec97ac3ca9954821e39a5ffa9b4327e0ecc69f78e292", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a", "node_type": "1", "metadata": {}, "hash": "e25d347e3118a4d12fa2021b414f6ea33ed41a76887ac68d777f106722e8a585", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So, if you're a builder,\n- Yeah.\n- you want to create stuff,\nyou wanna launch stuff,\nthis is the time to do it.\n- And so, we've had this\n1.8 gigawatt data center\nin our data for over a year now,\nand we've been sending\nit to all of our clients,\nincluding many of these companies\nthat are building the multi gigawatts.\nBut that is at a level that's not quite,\nmaybe executives seeing\n$500 billion, $100 billion,\nand then everyone's asking them like...\nSo, it could spur another,\nan even faster arms race.\nBecause there's already an arms race,\nbut this like 100 billion,\n$500 billion number, Trump\ntalking about it on TV.\nIt could spur the arm\nrace to be even faster\nand more investors to flood\nin and et cetera, et cetera.\nSo, I think you're right\nin that sense that OpenAI,\nor Trump is championing\npeople are gonna build more\nand his actions are gonna\nlet people build more.\n- What are you excited\nabout these several\nyears that are upcoming\nin terms of cluster build outs,\nin terms of breakthroughs in AI?\nLike the best possible\nfuture you can imagine\nin the next couple years,\ntwo, three, four years?\nWhat does that look like?\nJust it could be very\nspecific technical things\nlike breakthroughs on post post-training,\nor it could be just size big.\n- [Dylan] Yeah, I mean it's-\n- Impressive clusters.\n- I really enjoy tracking supply chain\nand who's involved in what.\n(Lex laughing)\n- Yeah.\n- I really do.\nIt's really fun to see\nthe numbers, the cost,\nwho's building what capacity,\nhelping them figure out how\nmuch capacity they should build,\nwinning deals, strategic\nstuff, that's really cool.\nI think technologically, there's a lot\naround the networking side\nthat really excites me\nwith optics and electronics\ngetting closer and closer,\nwhether it be co-packaged optics,\nor some sort of forms of\nnew forms of switching.\n- This is internal\nto a cluster.\n- Cluster. Yeah.\nAlso multi-data center training,\npeople are putting so much\nfiber between these data centers\nand lighting it up with so much\nbandwidth that there's a lot\nof interesting stuff\nhappening on that end.\nTelecom has been really boring since 5G,\nand now it's like really\nexciting again on the fiber side.\n- Can you educate me a little\nbit about the speed of things?\nSo, the speed of memory versus\nthe speed of interconnect,\nversus the speed of fiber\nbetween data centers?\nAre these orders of magnitude different?\nCan we, at some point,\nconverge towards a place,\nwhere it all just feels like one computer?\n- No, I don't think\nthat's possible.\n- Okay. All right. (chuckles)\n- It's only gonna get harder to program,\nnot easier.\n- Okay.\n- It's only gonna get more difficult\nand complicated in more layers.\nThe general image that people\nlike to have is this hierarchy of memory.\nSo, on chip is really close.\nLocalized within the\nchip, you have registers.\nAnd those are shared between\nsome compute elements,\nand then you'll have caches,\nwhich are shared between\nmore compute elements.\nThen, you have memory, like HBM or DRAM,\nlike DDR memory or whatever it is.\nAnd that's shared between the whole chip.\nAnd then, you can have pools of memory\nthat are shared between many chips.\nAnd then, storage and you keep zoning out.\nThe access latency across data centers,\nacross within the data\ncenter within a chip differs.\nSo, you're obviously always,\nyou're always gonna have different\nprogramming paradigms for this.\nIt's not gonna be easy.\nProgramming this stuff is gonna be hard.\nMaybe AI can help with programming this.\nBut the way to think\nabout it is that like,\n(Dylan sighs)\nthere's the more elements\nyou add to a task,\nyou don't gain, you\ndon't get strong scaling.\nIf I double the number of chips,\nI don't get 2x the performance.\nThis is just like a reality of computing,\n'cause there's inefficiencies.\nAnd there's a lot of interesting work\nbeing done to make it more linear.\nWhether it's making the chips\nmore networked together more tightly\nor cool programming models,\nor cool algorithmic things\nthat you can do on the model side.", "mimetype": "text/plain", "start_char_idx": 307870, "end_char_idx": 311860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a": {"__data__": {"id_": "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1ddc9d4-06a8-470e-8462-e7de248690c9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "83732ffd95ee696f567b4f5c03d7ca0aa0372a5355acb68b3690ea0768178f15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86cd532d-e832-41d0-9c36-d28ef9d67773", "node_type": "1", "metadata": {}, "hash": "64546ed05b2a2ef8e366ced66ebdc67fece24f23a3dbb4ead242f803e3daea65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then, storage and you keep zoning out.\nThe access latency across data centers,\nacross within the data\ncenter within a chip differs.\nSo, you're obviously always,\nyou're always gonna have different\nprogramming paradigms for this.\nIt's not gonna be easy.\nProgramming this stuff is gonna be hard.\nMaybe AI can help with programming this.\nBut the way to think\nabout it is that like,\n(Dylan sighs)\nthere's the more elements\nyou add to a task,\nyou don't gain, you\ndon't get strong scaling.\nIf I double the number of chips,\nI don't get 2x the performance.\nThis is just like a reality of computing,\n'cause there's inefficiencies.\nAnd there's a lot of interesting work\nbeing done to make it more linear.\nWhether it's making the chips\nmore networked together more tightly\nor cool programming models,\nor cool algorithmic things\nthat you can do on the model side.\nDeepSeek did some of these\nreally cool innovations\nbecause they were limited on interconnect,\nbut they still needed to parallelize.\nAll sorts of...\nEveryone's always doing stuff.\nGoogle's got a bunch of work\nand everyone's got a\nbunch of work about this.\nThat stuff is super exciting\non the model and workload\nand innovation side.\nHardware, solid-state transformers\nare interesting for the power side.\nThere's all sorts of stuff on batteries\nand there's all sorts of stuff on...\nI think when you look at,\nif you look at every layer\nof the compute stack,\nwhether it goes from lithography and etch,\nall the way to like\nfabrication, to optics,\nto networking, to power, to transformers,\nto cooling, to a networking\nand you just go on up and\nup and up and up the stack.\nEven air conditioners for\ndata centers are innovating.\nThere's like copper cables are innovating.\nYou wouldn't think it,\nbut copper cables are,\nthere's some innovations happening there\nwith the density of how you can pack them.\nAnd it's like all of\nthese layers of the stack,\nall the way up to the models.\nHuman progress is at a pace\nthat's never been seen before.\n- I'm just imagining you sitting\nback in a layer somewhere\nwith screens everywhere, just\nmonitoring the supply chain\nwhere all these clusters,\nall the information you're gathering.\nYou do incredible-\n- There's a big team.\nThere's a big team.\n- Yeah. (laughs)\nYou do quite incredible\nwork with SemiAnalysis.\nJust keeping your finger on the pulse\nof human civilization\nin the digital world.\nIt's pretty cool just to watch, feel that.\n- [Dylan] Yeah. Thank you, I guess-\n- Feel all of us doing shit, epic shit.\n- [Dylan] Feel the AGI.\n(Lex laughing)\n- From meme to reality.\nWhat Nathan, is there breakthroughs\nthat you're looking\nforward to potentially?\n- I had a while to think\nabout this while listening\nto Dylan's beautiful response.\n(Dylan laughing)\n- He did listen to me. He was so-\n- No, I knew this was coming.\nAnd it's like, realistically,\ntraining models is very fun\nbecause there's so much low-hanging fruit.\nAnd the thing that makes\nmy job entertaining,\nI train models, I write analysis\nabout what's happening\nwith models, and it's fun\nbecause there is obviously so\nmuch more progress to be had.\nAnd the real motivation why I do this\nsomewhere where I can share\nthings is that there's just,\nI don't trust people that are like,\n\"Trust me, bro, we're gonna make AI good.\"\nIt's like we're the ones that\nit's like, we're gonna do it\nand you can trust us and we're\njust gonna have all the AI,\nand it's just I would like a future,\nwhere more people have a say\nin what AI is and can understand it.\nAnd it's a little bit less fun,\nthat it's not a like\npositive thing of like,\nthis is just all really fun.\nTraining models is fun and\nbring people in is fun,\nbut it's really like AI, if it is going\nto be the most powerful\ntechnology of my lifetime,\nit's like, we need to have\na lot of people involved\nin making that and...\n- Making it open\n(Nathan chuckles)\nhelps with that,\nas successful as possible,\nas open as possible, yeah.", "mimetype": "text/plain", "start_char_idx": 311006, "end_char_idx": 314917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86cd532d-e832-41d0-9c36-d28ef9d67773": {"__data__": {"id_": "86cd532d-e832-41d0-9c36-d28ef9d67773", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52d49dd9-9d1d-48ae-a16e-7f4bed9d408a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "74943e3b331285796cf9635e00b0f41db20c911825604941f03518c5d9745338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f9b0539-8400-4066-b575-cc1dc7dd55a7", "node_type": "1", "metadata": {}, "hash": "4a6de9ebd94d7921682bd3d00ac66b31800e2ad1c7f1ae5fc774620a09ce3b9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's like we're the ones that\nit's like, we're gonna do it\nand you can trust us and we're\njust gonna have all the AI,\nand it's just I would like a future,\nwhere more people have a say\nin what AI is and can understand it.\nAnd it's a little bit less fun,\nthat it's not a like\npositive thing of like,\nthis is just all really fun.\nTraining models is fun and\nbring people in is fun,\nbut it's really like AI, if it is going\nto be the most powerful\ntechnology of my lifetime,\nit's like, we need to have\na lot of people involved\nin making that and...\n- Making it open\n(Nathan chuckles)\nhelps with that,\nas successful as possible,\nas open as possible, yeah.\n- In my read of the last few years\nis that more openness\nwould help the AI ecosystem\nin terms of having more people\nunderstand what's going on,\nwhether that's researchers\nfrom non-AI fields\nto governments, to everything.\nIt doesn't mean that openness\nwill always be the answer.\nI think then it'll reassess of like,\nwhat is the biggest problem facing AI\nand tack on a different angle\nto the wild ride that we're on.\n- And for me, just from\neven the user experience,\nanytime you have the, like Karpathy said,\nthe aha moments, like the magic,\nlike seeing the reasoning,\nthe chain of thought,\nit's like there's something really\njust fundamentally beautiful about that.\nIt's putting a mirror to ourselves\nand seeing like, oh shit.\nIt is solving intelligence\nas the cliche goal of these companies is.\nAnd you get to understand\nwhy we humans are special.\nThe intelligence within us is special.\nAnd for now also, why\nwe're special in terms of,\nwe seem to be conscious in\nthe AI systems for now aren't,\nand we get to solve, we get\nto explore that mystery.\nSo, it's just really cool\nto get to explore these\nquestions that I don't think,\nI would've never imagined\nwould be even possible\nback when sort of just\nwatching with excitement\nthe Deep Blue beat Kasparov.\nI wouldn't have ever thought\nthis kind of AI would be\npossible in my lifetime.\nIt's like this is really feels like AI.\n- Yeah. (chuckles)\n- It's incredible.\n- I started with AI of learning\nto fly a silly quadrotor.\nIt's like learning to fly\nand it just like, it learned to fly up,\nit would hit the ceiling\nand stop and catch it.\nIt's like, okay,\nthat is really stupid compared\nto what's going on now.\n- And now, you could probably,\nwith natural language,\ntell it to learn to fly\nand it's going to generate\nthe control algorithm\nrequired to do that. (laughs)\n- Probably.\nThere's low level blockers.\nWe have to do some weird stuff for that,\n- Yeah, for sure.\n- but you can,\nyou definitely can.\n- It's back\nto our robotics conversation, yeah.\nWhen you have to interact\nin the actual physical world, that's hard.\nWhat gives you hope about the\nfuture of human civilization?\nLooking into the next 10\nyears, 100 years, 1,000 years,\nhow long you think we'll make it?\nYou think we've got\n1,000 years?\n- I think humans\nwill definitely be around in 1,000 years.\nI think there's ways that\nvery bad things could happen.\nThere'll be way fewer humans,\nbut humans are very good at surviving.\nThere's been a lot of\nthings that that is true.\nI don't think they're necessarily,\nwe're good at long-term\ncredit assignment of risk,\nbut when the risk becomes immediate,\nwe tend to figure things out,\n- Oh yeah.\n- and for that reason,\nI am like, there's physical constraints\nto things like AGI hyper,\nlike recursive improvement\nto kill us all type stuff\nfor physical reasons,\nand for how humans have\nfigured things out before,\nI'm not too worried about it.\nAI takeover.\nThere are other international\nthings that are worrying,\nbut there's just\nfundamental human goodness\nand trying to amplify that,\nand we're on a tenuous time.\nAnd if you look at humanity as a whole,\nthere's been times where\nthings go backwards,\nthere's times when things\ndon't happen at all.\nAnd we're on a,\nwhat should be very positive\ntrajectory right now.", "mimetype": "text/plain", "start_char_idx": 314269, "end_char_idx": 318173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f9b0539-8400-4066-b575-cc1dc7dd55a7": {"__data__": {"id_": "6f9b0539-8400-4066-b575-cc1dc7dd55a7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d6f3674-6d30-4bb7-bc38-f7b9ee628037", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "483c17ba1cfd0c4dc623bdda61eb9ef7905542ef7e216eeb89f0486fa9df9f41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86cd532d-e832-41d0-9c36-d28ef9d67773", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_name": "[English] DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters _ Lex Fridman Podcast #459 [DownSub.com].txt", "file_type": "text/plain", "file_size": 320843, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1694bf93e197baa407cec6e2fda0180b1f45dc109a8dfff786fb37b16a5933b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There's been a lot of\nthings that that is true.\nI don't think they're necessarily,\nwe're good at long-term\ncredit assignment of risk,\nbut when the risk becomes immediate,\nwe tend to figure things out,\n- Oh yeah.\n- and for that reason,\nI am like, there's physical constraints\nto things like AGI hyper,\nlike recursive improvement\nto kill us all type stuff\nfor physical reasons,\nand for how humans have\nfigured things out before,\nI'm not too worried about it.\nAI takeover.\nThere are other international\nthings that are worrying,\nbut there's just\nfundamental human goodness\nand trying to amplify that,\nand we're on a tenuous time.\nAnd if you look at humanity as a whole,\nthere's been times where\nthings go backwards,\nthere's times when things\ndon't happen at all.\nAnd we're on a,\nwhat should be very positive\ntrajectory right now.\n- Yeah, there seems to be\nprogress, but just with power,\nthere's spikes of human suffering,\nand we wanna try to minimize\nthe amount of spikes.\n- Generally, humanity is\ngonna suffer a lot less.\nI'm very optimistic about that.\nI do worry of techno\nfascism type stuff arising\nas AI becomes more and more\nprevalent and powerful,\nand those who control\nit can do more and more.\nMaybe it doesn't kill us all,\nbut at some point, every\nvery powerful human\nis gonna wanna brain computer interface,\nso that they can interact with the AGI,\nand all of its advantages\nin many more way,\nand merge its mind with sort of like,\nand its capabilities or\nthat person's capabilities\ncan leverage those much\nbetter than anyone else,\nand therefore be, it won't\nbe one person rule them all,\nbut it will be...\nThe thing I worry about is\nit'll be like few people,\nhundreds, thousands, tens of thousands,\nmaybe millions of people\nrule whoever's left,\nand the economy around it.\nAnd that's the thing that's\nprobably more worrisome\nis human machine amalgamations.\nThis enables an individual human\nto have more impact on the world.\nAnd that impact can be\nboth positive and negative.\nGenerally, humans have\npositive impacts on the world,\nat least societally,\nbut it's possible for individual humans\nto have such negative impacts.\nAnd AGI, at least as I\nthink the labs define it,\nwhich is not a runaway sentient thing,\nbut rather just something\nthat can do a lot\nof tasks really efficiently,\namplifies the capabilities\nof someone causing extreme damage.\nBut for the most part,\nI think it'll be used for\nprofit-seeking motives,\nwhich will increase the\nabundance and supply of things.\nand therefore reduce suffering.\n(Lex laughing)\n- Yeah.\n- That's the goal.\n- Scrolling on a timeline,\n(Nathan laughing)\njust drowning in dopamine-\n- Crawling stasis.\n- That is-\n- Scrolling holds\nthe status quo of the world.\n- That is a positive outcome\n(Nathan and Lex laughing)\nis like, if I have food tubes\n- Yeah.\n- and I'm scrolling and I'm happy,\nthat's a positive outcome.\n(group laughing)\n- While expanding out into the cosmos.\nWell, this is a fun time to be alive.\nAnd thank you for pushing the forefront\nof what is possible in humans.\nAnd thank you for talking\ntoday. This was fun.\n- Thanks for having us.\n- Thanks for having us.\n- Thanks for listening\nto this conversation with\nDylan Patel and Nathan Lambert.\nTo support this podcast,\nplease check out our\nsponsors in the description.\nAnd now, let me leave you some\nwords from Richard Feynman.\n\"For a successful technology,\nreality must take precedence\nover public relations, for\nnature cannot be fooled.\"\nThank you for listening and\nhope to see you next time.", "mimetype": "text/plain", "start_char_idx": 317347, "end_char_idx": 320843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f1b891c-d5b2-4205-a51d-5246dfebefa9": {"__data__": {"id_": "0f1b891c-d5b2-4205-a51d-5246dfebefa9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a02853ea-8a7e-4278-b368-cee4eda627c8", "node_type": "1", "metadata": {}, "hash": "46420fe434784ea2822fe727a38e88512b64fdd279dd15b884671512be53c54e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- A lot of people have said for many years\nthat there will come a time\nwhen we want to pause a little bit.\nThat time is now.\n- The following is a\nconversation with Max Tegmark,\nhis third time in the podcast.\nIn fact, his first appearance\nwas episode number one\nof this very podcast.\nHe is a physicist\nand artificial intelligence\nresearcher at MIT,\nco-founder of Future of Life Institute,\nand Author of \"Life 3.0:\nBeing Human in the Age of\nArtificial Intelligence.\"\nMost recently,\nhe's a key figure in\nspearheading the open letter\ncalling for a six-month\npause on giant AI experiments\nlike training GPT-4.\nThe letter reads,\n\"We're calling for a pause on training\nof models larger than\nGPT-4 for six months.\nThis does not imply a pause\nor ban on all AI research\nand development or the use of systems\nthat have already been\nplaced in the market.\nOur call is specific and addresses\na very small pool of actors\nwho possesses this capability.\"\nThe letter has been signed\nby over 50,000 individuals,\nincluding 1800 CEOs and\nover 1500 professors.\nSignatories include Yoshua Bengio,\nStuart Russell, Elon Musk,\nSteve Wozniak, Yuval Noah Harari,\nAndrew Yang, and many others.\nThis is a defining moment\nin the history of human civilization,\nwhere the balance of power\nbetween human and AI begins to shift,\nand Max's mind and his voice\nis one of the most valuable and powerful\nin a time like this.\nHis support, his wisdom, his friendship,\nhas been a gift I'm forever\ndeeply grateful for.\nThis is the Lex Fridman podcast.\nTo support it, please\ncheck out our sponsors\nin the description.\nAnd now, dear friends,\nhere's Max Tegmark.\nYou were the first ever\nguest on this podcast,\nepisode number one.\nSo first of all,\nMax, I just have to say,\nthank you for giving me a chance.\nThank you for starting this journey,\nand it's been an incredible journey,\njust thank you for sitting down with me\nand just acting like I'm\nsomebody who matters,\nthat I'm somebody who's\ninteresting to talk to.\nAnd thank you for doing it.\nThat meant a lot.\n- And thanks to you\nfor putting your heart and soul into this.\nI know when you delve\ninto controversial topics,\nit's inevitable to get hit\nby what Hamlet talks about\n\"The slings and arrows,\" and stuff.\nAnd I really admire this.\nIt's in an era, you know,\nwhere YouTube videos are too long,\nand now it has to be\nlike a 20-minute TikTok,\n20-second TikTok clip.\nIt's just so refreshing to see you\ngoing exactly against all of the advice\nand doing these really long form things,\nand the people appreciate it, you know.\nReality is nuanced, and thanks\nfor sharing it that way.\n- So let me ask you again,\nthe first question I've\never asked on this podcast,\nepisode number one, talking to you.\nDo you think there's intelligent life\nout there in the universe?\nLet's revisit that question.\nDo you have any updates?\nWhat's your view when you\nlook out to the stars?\n- So, when we look out to the stars,\nif you define our universe the\nway most astrophysicists do,\nnot as all of space,\nbut the spherical region of space\nthat we can see with our telescopes\nfrom which light has the time to reach us,\nsince our Big Bang.\nI'm in the minority.\nI estimate that we are the only life\nin this spherical volume\nthat has invented internet,\nthe radio, has gotten\nto our level of tech.\nAnd if that's true,\nthen it puts a lot of responsibility on us\nto not mess this one up.\nBecause if it's true,\nit means that life is quite rare.\nAnd we are stewards of this one spark\nof advanced consciousness,\nwhich if we nurture it and help it grow,\neventually life can spread from here,\nout into much of our universe,\nand we can have this just amazing future.\nWhereas, if we instead are reckless\nwith the technology we build\nand just snuff it out due to stupidity\nor in-fighting, then,\nmaybe the rest of cosmic\nhistory in our universe\nis just gonna be playing\nfor empty benches.\nBut I do think that we are actually\nvery likely to get visited by aliens,\nalien intelligence quite soon.\nBut I think we are gonna be building\nthat alien intelligence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a02853ea-8a7e-4278-b368-cee4eda627c8": {"__data__": {"id_": "a02853ea-8a7e-4278-b368-cee4eda627c8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f1b891c-d5b2-4205-a51d-5246dfebefa9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "32bd2eb6933b19c12bc8c6390c77abd0bc372ae33acc9eb51a9b54eb7d3ae7ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b03ea2c2-9d5f-454e-8ba7-c21625987f56", "node_type": "1", "metadata": {}, "hash": "3db3cdc8f5f9c67f127dbfbe54db8b6cafd710c607f9869003e9de269fc83ada", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And if that's true,\nthen it puts a lot of responsibility on us\nto not mess this one up.\nBecause if it's true,\nit means that life is quite rare.\nAnd we are stewards of this one spark\nof advanced consciousness,\nwhich if we nurture it and help it grow,\neventually life can spread from here,\nout into much of our universe,\nand we can have this just amazing future.\nWhereas, if we instead are reckless\nwith the technology we build\nand just snuff it out due to stupidity\nor in-fighting, then,\nmaybe the rest of cosmic\nhistory in our universe\nis just gonna be playing\nfor empty benches.\nBut I do think that we are actually\nvery likely to get visited by aliens,\nalien intelligence quite soon.\nBut I think we are gonna be building\nthat alien intelligence.\n- So we're going to give birth\nto an intelligent alien civilization,\nunlike anything that human,\nthe evolution here on\nearth was able to create\nin terms of the path,\nthe biological path it took.\n- Yeah, and it's gonna be\nmuch more alien than a cat,\nor even the most exotic animal\non the planet right now,\nbecause it will not have been created\nthrough the usual Darwinian competition\nwhere it necessarily cares\nabout self-preservation,\nthat is afraid of death,\nany of those things.\nThe space of alien\nminds that you can build\nis just so much faster than\nwhat evolution will give you.\nAnd with that also comes\na great responsibility,\nfor us to make sure that\nthe kind of minds we create\nare the kind of minds\nthat it's good to create.\nMinds that will share our values\nand be good for humanity and life.\nAnd also don't create\nminds that don't suffer.\n- Do you try to visualize the full space\nof alien minds that AI could be?\nDo you try to consider\nall the different kinds of intelligences,\ninstead of generalizing\nwhat humans are able to do\nto the full spectrum of\nwhat intelligent creatures,\nentities could do?\n- I try,\nbut I would say I fail,\nI mean, it's very difficult for human mind\nto really grapple with\nsomething so completely alien.\nEven for us, right?\nIf we just try to\nimagine how would it feel\nif we were completely indifferent\ntowards death or individuality?\nEven if you just imagine that for example,\nyou could just copy my knowledge\nof how to speak Swedish,\n(fingers snapping) boom,\nnow you can speak Swedish,\nand you could copy any\nof my cool experiences,\nand then you could delete the ones\nyou didn't like in your own life,\njust like that.\nit would already change quite a lot\nabout how you feel as\na human being, right?\nYou probably spend less\neffort studying things\nif you just copy them,\nand you might be less afraid of death,\nbecause if the plane\nyou're on starts to crash,\nyou'd just be like, \"Oh shucks,\nI haven't backed my\nbrain up for four hours,\n(Lex laughs)\nso I'm gonna lose this,\nall this wonderful\nexperiences of this flight.\"\nWe might also start feeling more,\nlike compassionate maybe with other people\nif we can so readily share\neach other's experiences\nand our knowledge, and\nfeel more like a hivemind.\nIt's very hard though.\nI really feel very humble about this\nto grapple with it,\nhow it might actually feel.\nThe one thing which is so obvious though,\nwhich, I think is just\nreally worth reflecting on,\nis because the mind space\nof possible intelligences\nis so different from ours,\nit's very dangerous if we assume\nthey're gonna be like us,\nor anything like us.\n- Well there's,\nthe entirety of human written history\nhas been through poetry, through novels,\nbeen trying to describe\nthrough philosophy,\ntrying to describe the human condition\nand what's entailed in it.\nLike, just like you said,\nfear of death and all\nthose kinds of things,\nwhat is love,\nand all of that changes.\n- [Max] Yeah.\n- If you have a different\nkind of intelligence.\nLike all of it,\nthe entirety of all those poems,\nthey're trying to sneak up\nto what the hell it means to be human.\nAll of that changes.\nHow AI concerns\nand existential crises\nthat AI experiences,\nhow that clashes with the\nhuman existential crisis,\nthe human condition.\n- [Max] Yeah.\n- That's hard to fathom, hard to predict.\n- It's hard, but it's\nfascinating to think about also.", "mimetype": "text/plain", "start_char_idx": 3273, "end_char_idx": 7368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b03ea2c2-9d5f-454e-8ba7-c21625987f56": {"__data__": {"id_": "b03ea2c2-9d5f-454e-8ba7-c21625987f56", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a02853ea-8a7e-4278-b368-cee4eda627c8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e8b4c6b92bc2e22ed0608a53517761333a4a589900b3daea82c26cb4b5270099", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11ad8c7e-3bde-4adb-89bf-dafef87ebe19", "node_type": "1", "metadata": {}, "hash": "0325febbad5a3f4adbd4e6c32b2d0eff76b9de70902186853500d53a3f4f1724", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Well there's,\nthe entirety of human written history\nhas been through poetry, through novels,\nbeen trying to describe\nthrough philosophy,\ntrying to describe the human condition\nand what's entailed in it.\nLike, just like you said,\nfear of death and all\nthose kinds of things,\nwhat is love,\nand all of that changes.\n- [Max] Yeah.\n- If you have a different\nkind of intelligence.\nLike all of it,\nthe entirety of all those poems,\nthey're trying to sneak up\nto what the hell it means to be human.\nAll of that changes.\nHow AI concerns\nand existential crises\nthat AI experiences,\nhow that clashes with the\nhuman existential crisis,\nthe human condition.\n- [Max] Yeah.\n- That's hard to fathom, hard to predict.\n- It's hard, but it's\nfascinating to think about also.\nEven in the best case scenario,\nwhere we don't lose control\nover the ever more powerful AI\nthat we're building to other humans\nwhose goals we think are horrible,\nand where we don't lose\ncontrol to the machines,\nand AI provides the things we want.\nEven then, you get into the questions\nyou touched here, you know,\nmaybe it's the struggle\nthat it's actually hard to do things\nis part of the things that\ngives us meaning as well, right?\nSo for example,\nI found it so shocking that\nthis new Microsoft GPT-4 commercial\nthat they put together,\nhas this woman talking about,\nshowing this demo how she's gonna give\na graduation speech to\nher beloved daughter.\nAnd she asks GPT-4 to write it.\nIt was frigging 200 words or so.\nIf I realized that my\nparents couldn't be bothered\nstruggling a little\nbit to write 200 words,\nand outsource that to their computer,\nI would feel really offended, actually.\nAnd so I wonder if eliminating too much\nof the struggle from our existence,\ndo you think that would also take away\na little bit of what-\n- it means to be human? Yeah.\n- [Max] Yeah.\n- We can't even predict.\nI had somebody mentioned\nto me that they use,\nthey started using ChatGPT\nwith the 3.5 and now 4.0,\nto write what they\nreally feel to a person,\nand they have a temper issue,\nand they're basically\ntrying to get ChatGPT\nto rewrite it in a nicer way.\nTo get the point across,\nbut rewrite it in a nicer way.\nSo we're even removing the inner asshole\nfrom our communication.\nSo I don't, you know,\nthere's some positive aspects of that,\nbut mostly it's just the transformation\nof how humans communicate.\nAnd it's scary because\nso much of our society is based\non this glue of communication.\nAnd if we're now using AI as\nthe medium of communication\nthat does the language for us,\nso much of the emotion that's\nladen in human communication,\nand so much of the intent,\nthat's going to be handled by,\noutsourced to AI,\nhow does that change everything?\nHow does that change the internal state\nof how we feel about other human beings?\nWhat makes us lonely?\nWhat makes us excited?\nWhat makes us afraid?\nHow we fall in love?\nAll that kind of stuff.\n- Yeah.\nFor me personally, I have to confess,\nthe challenge is one of the things\nthat really makes my life feel\nmeaningful, you know?\nIf I go hiking mountain\nwith my wife, Meia,\nI don't wanna just press a\nbutton and be at the top,\nI want to struggle and\ncome up there sweaty,\nand feel, \"Wow, we did this,\"\nin the same way.\nI want to constantly work on myself\nto become a better person.\nIf I say something in anger that I regret,\nI want to go back\nand really work on myself\nrather than just tell an AI,\nfrom now on, always filter what I write\nso I don't have to work on myself,\n'cause then I'm not growing.\n- Yeah, but then again,\nit could be like with chess,\nand AI, once it significantly,\nobviously supersedes the\nperformance of humans,\nit will live in its own world,\nand provide maybe a flourishing\ncivilizations for humans.\nBut we humans will\ncontinue hiking mountains,\nand playing our games,\neven though AI is so much smarter,\nso much stronger,\nso much superior in every single way,\njust like with chess.\n- [Max] Yeah.", "mimetype": "text/plain", "start_char_idx": 6612, "end_char_idx": 10519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11ad8c7e-3bde-4adb-89bf-dafef87ebe19": {"__data__": {"id_": "11ad8c7e-3bde-4adb-89bf-dafef87ebe19", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b03ea2c2-9d5f-454e-8ba7-c21625987f56", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3e23a831fd1a5fce8fba4ae4738570818702030d0ee1a267eda91c0875ac3ce6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d", "node_type": "1", "metadata": {}, "hash": "e2d9c94f9acb226a5be06777a5cda094dbba1e3f9f62f4715b2eeac6546c9ae7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I want to constantly work on myself\nto become a better person.\nIf I say something in anger that I regret,\nI want to go back\nand really work on myself\nrather than just tell an AI,\nfrom now on, always filter what I write\nso I don't have to work on myself,\n'cause then I'm not growing.\n- Yeah, but then again,\nit could be like with chess,\nand AI, once it significantly,\nobviously supersedes the\nperformance of humans,\nit will live in its own world,\nand provide maybe a flourishing\ncivilizations for humans.\nBut we humans will\ncontinue hiking mountains,\nand playing our games,\neven though AI is so much smarter,\nso much stronger,\nso much superior in every single way,\njust like with chess.\n- [Max] Yeah.\n- So that,\nI mean, that's one possible\nhopeful trajectory here,\nis that humans will continue to human,\nand AI will just be a kind of,\na medium that enables the\nhuman experience to flourish.\n- Yeah, I would phrase that\nas rebranding ourselves\nfrom Homo sapiens to Homo sentiens.\nYou know, right now, if it's sapiens,\nthe ability to be intelligent,\nwe've even put it in our species' name.\nSo we're branding\nourselves as the smartest\ninformation processing\nentity on the planet.\nThat's clearly gonna change\nif AI continues ahead.\nSo maybe we should focus\non the experience instead,\nthe subjective experience that we have,\nHomo sentiens, and say that's\nwhat's really valuable,\nthe love, the connection,\nthe other things,\nand get off our high horses,\nand get rid of this hubris that,\nyou know, only we can do integrals.\n- So consciousness,\nthe subjective experience\nis a fundamental value\nto what it means to be human.\nMake that the priority.\n- That feels like a\nhopeful direction to me.\nBut that also requires more compassion,\nnot just towards other humans,\nbecause they happen to be\nthe smartest on the planet,\nbut also towards all our\nother fellow creatures\non this planet.\nI personally feel right now,\nwe're treating a lot of farm\nanimals horribly, for example.\nAnd the excuse we're using is,\n\"Oh, they're not as smart as us.\"\nBut if we admit that we're not that smart\nin the grand scheme of things either,\nin the post-AI epoch, you know,\nthen surely, we should value\nthe subjective experience of a cow also.\n- Well, allow me to\nbriefly look at the book,\nwhich at this point is becoming\nmore and more visionary\nthat you've written, I\nguess over five years ago,\n\"Life 3.0.\"\nSo first of all, 3.0,\nwhat's 1.0, what's 2.0, What's 3.0?\nand how's that vision sort of evolve,\nthe vision in the book evolve to today.\n- Life 1.0 is really dumb like bacteria,\nand that it can't actually\nlearn anything at all\nduring their lifetime.\nThe learning just comes\nfrom this genetic process\nfrom one generation to the next.\nLife 2.0 is us and other\nanimals which have brains\nwhich can learn during\ntheir lifetime a great deal.\nRight so,\nand you know, you were born\nwithout being able to speak English,\nand at some point you decided,\n\"Hey, I wanna upgrade my software,\nand so let's install an\nEnglish-speaking module.\"\nSo you did.\nAnd Life 3.0, which does not exist yet,\ncannot replace not only its\nsoftware the way we can,\nbut also it's hardware.\nAnd that's where we're\nheading towards at high speed.\nWe're already maybe 2.1 because we can,\nyou know, put in an artificial knee,\npacemaker, et cetera, et cetera.\nAnd if Neuralink and\nother companies succeed,\nit will be life 2.2, et cetera.\nBut the companies trying to build AGI,\nor trying to make is of course, full 3.0,\nand you can put that intelligence\nin something that also has no,\nbiological basis whatsoever.\n- So less constraints\nand more capabilities,\njust like the leap from 1.0 to 2.0.\nThere is nevertheless,\nyou speaking so harshly about bacteria,\nso disrespectfully about bacteria,\nthere is still the same\nkind of magic there\nthat permeates life 2.0 and 3.0.\nIt seems like maybe the\nthing that's truly powerful\nabout life, intelligence,\nand consciousness,\nwas already there in 1.0.\nIs it possible?", "mimetype": "text/plain", "start_char_idx": 9820, "end_char_idx": 13758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d": {"__data__": {"id_": "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11ad8c7e-3bde-4adb-89bf-dafef87ebe19", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b5a0cae457debbef15daed5887210d414f8130c61a00b5c4c70938941fbad36b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33a638e0-a7e0-4a9d-8841-90e99eef8d71", "node_type": "1", "metadata": {}, "hash": "261ce5e0f348cf7ca40639470ae86fcdb3273b227fe9ef977e35f2118c616e47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We're already maybe 2.1 because we can,\nyou know, put in an artificial knee,\npacemaker, et cetera, et cetera.\nAnd if Neuralink and\nother companies succeed,\nit will be life 2.2, et cetera.\nBut the companies trying to build AGI,\nor trying to make is of course, full 3.0,\nand you can put that intelligence\nin something that also has no,\nbiological basis whatsoever.\n- So less constraints\nand more capabilities,\njust like the leap from 1.0 to 2.0.\nThere is nevertheless,\nyou speaking so harshly about bacteria,\nso disrespectfully about bacteria,\nthere is still the same\nkind of magic there\nthat permeates life 2.0 and 3.0.\nIt seems like maybe the\nthing that's truly powerful\nabout life, intelligence,\nand consciousness,\nwas already there in 1.0.\nIs it possible?\n- I think we should be\nhumble and not be so quick\nto make everything binary\nand say either it's there or it's not.\nClearly there's a great spectrum\nand there is even controversy about\nwhether some unicellular\norganisms like amoebas\ncan maybe learn a little\nbit, you know, after all.\nSo apologies if I offended\nany bacteria here. (laughs)\nIt wasn't my intent.\nIt was more that I wanted to talk up\nhow cool it is to actually have a brain.\n- [Lex] Yeah.\n- Where you can learn\ndramatically within your lifetime.\n- [Lex] Typical human.\n- And the higher up you\nget from 1.0 2.0 to 3.0,\nthe more you become the\ncaptain of your own ship,\nthe master of your own destiny.\nAnd the less you become a slave\nto whatever evolution gave you, right?\nBy upgrading your software,\nwe can be so different\nfrom previous generations\nand even from our parents,\nmuch more so than even a bacterium,\nyou know, no offense to them.\nAnd if you can also swap out your hardware\nand take any physical\nform you want, of course,\nit's really, the sky's the limit.\n- Yeah, so the,\nit accelerates the rate\nat which you can perform\nthe computation that\ndetermines your destiny.\n- Yeah, and I think it's worth commenting\na bit on what \"you\" means in this context.\nAlso, if you swap things out a lot, right?\nThis is controversial, but my,\ncurrent understanding is that,\nyou know,\nlife is best thought of\nnot as a bag of meat,\nor even a bag of elementary particles,\nbut rather as a system which\ncan process information\nand retain its own complexity,\neven though nature is\nalways trying to mess it up,\nso, it's all about information processing.\nAnd that makes it a lot like something\nlike a wave in the ocean, which is not,\nit's water molecules, right?\nThe water molecules bob up and down,\nbut the wave moves forward,\nit's an information pattern\nin the same way you, Lex,\nyou're not the same atoms\nas during the first,\n- Time we talked, yeah.\n- Interview you did with me,\nyou've swapped out most of them,\nbut it's still you.\nAnd the information\npattern is still there,\nand if you could swap out your arms,\nand like whatever,\nyou can still have this\nkind of continuity,\nit becomes much more sophisticated\nsort of way forward in time\nwhere the information lives on.\nI lost both of my parents\nsince our last podcast,\nand it actually gives me a lot of solace\nthat this way of thinking about them,\nthey haven't entirely died\nbecause a lot of mommy and daddy's,\nsorry, I'm getting a\nlittle emotional here,\nbut a lot of their values,\nand ideas, and even jokes and so on,\nthey haven't gone away, right?\nSome of them live on,\nI can carry on some of them,\nand they also live on a\nin a lot of other people.\nSo in this sense, even with life 2.0,\nwe can to some extent,\nalready transcend our\nphysical bodies and our death.\nAnd particularly if you can\nshare your own information,\nyour own ideas with many others\nlike you do in your podcast,\nthen you know,\nthat's the closest immortality we can get\nwith our bio bodies.\n- You carry a little bit of\nthem in you in some sense.\n- [Max] Yeah, yeah.\n- Do you miss them?\nDo you miss your mom and dad?\n- Of course, of course.\n- What did you learn about life from them?", "mimetype": "text/plain", "start_char_idx": 13001, "end_char_idx": 16912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33a638e0-a7e0-4a9d-8841-90e99eef8d71": {"__data__": {"id_": "33a638e0-a7e0-4a9d-8841-90e99eef8d71", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "382ea280-c44b-4a0d-8f34-f5bfd64bdc6d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "990095074629d0f98ec6847e8cebd92b73e5b9f0c66f459e1fccabc7f009d409", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "260d20f3-9678-43f1-b357-33b1c62a8534", "node_type": "1", "metadata": {}, "hash": "22c47d42e990bf1ccc9ec57932e138a1275bf142c75ffa286cc1661d3d25049b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some of them live on,\nI can carry on some of them,\nand they also live on a\nin a lot of other people.\nSo in this sense, even with life 2.0,\nwe can to some extent,\nalready transcend our\nphysical bodies and our death.\nAnd particularly if you can\nshare your own information,\nyour own ideas with many others\nlike you do in your podcast,\nthen you know,\nthat's the closest immortality we can get\nwith our bio bodies.\n- You carry a little bit of\nthem in you in some sense.\n- [Max] Yeah, yeah.\n- Do you miss them?\nDo you miss your mom and dad?\n- Of course, of course.\n- What did you learn about life from them?\nIf we can take a bit of a tangent.\n- Oh, so many things.\nFor starters, my fascination for math\nand the physical\nmysteries of our universe,\nI got a lot of that from my dad.\nBut I think my obsession\nfor really big questions,\nand consciousness, and so on,\nthat actually came mostly from my mom\nand what I got from both of them,\nwhich is very core part\nof really who I am,\nI think is this,\njust feeling comfortable with,\nnot buying into what\neverybody else is saying,\njust doing what I think is right.\nThey both very much just,\nyou know, did their own thing,\nand sometimes they got flak for it\nand they did it anyway.\n- That's why you've always\nbeen in an inspiration to me.\nThat you're at the top of your field\nand you're still willing\nto tackle the big\nquestions in your own way.\nYou're one of one of the people\nthat represents MIT best to me,\nyou've always been an inspiration in that.\nSo it's good to hear that you got that\nfrom your mom and dad.\n- Yeah, you're too kind.\nBut yeah, I mean,\nthe good reason to do science\nis because you're really curious,\nand you wanna figure out the truth.\nIf you think,\nthis is how it is and everyone else says,\n\"No, no, that's bullshit,\nand it's that way,\"\nyou know,\nYou stick with what you think is true,\nand even if everybody else\nkeeps thinking it's bullshit,\nthere's a certain,\nI always root for the underdog,\n(Lex laughs)\nwhen I watch movies.\nAnd my dad once,\none time for example,\nwhen I wrote one of my\ncraziest papers ever,\ntalking about our universe\nultimately being mathematical,\nwhich we're not gonna get into today,\nI got this email from a quite\nfamous professor saying,\n\"This is not only all bullshit,\nbut it's gonna ruin your career.\nYou should stop doing this kind of stuff.\"\nI sent it to my dad.\nDo you know what he said?\n- [Lex] (laughs) What he say?\n- He replied with a quote from Dante.\n(Lex laughing)\n(Max speaking in Italian)\n\"Follow your own path\nand let the people talk.\"\n(Both laughing)\nGo dad!\n- [Lex] Yeah.\n- This is the kind of thing, you know,\nhe's dead, but that attitude is not.\n- How did losing them as a man,\nas a human being change you?\nHow did it expand your\nthinking about the world?\nHow did it expand your thinking about,\nyou know, this thing we're talking about,\nwhich is humans creating another living,\nsentient perhaps, being?\n- I think it,\nmainly do two things.\nOne of them just going\nthrough all their stuff\nafter they had passed away and so on,\njust drove home to me how\nimportant it is to ask ourselves,\nwhy are we doing this things we do?\nBecause it's inevitable\nthat you look at some things\nthey spent an enormous time on\nand you ask in hindsight,\nwould they really have\nspent so much time on this?\nWould they have done something\nthat was more meaningful?\nSo I've been looking more\nin my life now and asking,\nyou know, why am I doing what I'm doing?\nAnd I feel,\nit should either be something\nI really enjoy doing,\nor it should be something that I find\nreally, really meaningful\nbecause it helps humanity,\nand if it's in none of\nthose two categories,\nmaybe I should spend less\ntime on it, you know.\nThe other thing is,\ndealing with death up in person like this,\nit's actually made me less afraid of,\neven less afraid of\nother people telling me\nthat I'm an idiot, you know,\nwhich happens regularly,\nand just live my life,\ndo my thing, you know?", "mimetype": "text/plain", "start_char_idx": 16311, "end_char_idx": 20235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "260d20f3-9678-43f1-b357-33b1c62a8534": {"__data__": {"id_": "260d20f3-9678-43f1-b357-33b1c62a8534", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33a638e0-a7e0-4a9d-8841-90e99eef8d71", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e843c77e536335973eac86537d1731fd70769270f344740b6b9eff998108347d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b62ce1a-bc9d-4730-ab4e-741951c40f12", "node_type": "1", "metadata": {}, "hash": "27d1684ce9c5bf140f3c436c790fecb750225ec64c25d8f732022df6260b98bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because it's inevitable\nthat you look at some things\nthey spent an enormous time on\nand you ask in hindsight,\nwould they really have\nspent so much time on this?\nWould they have done something\nthat was more meaningful?\nSo I've been looking more\nin my life now and asking,\nyou know, why am I doing what I'm doing?\nAnd I feel,\nit should either be something\nI really enjoy doing,\nor it should be something that I find\nreally, really meaningful\nbecause it helps humanity,\nand if it's in none of\nthose two categories,\nmaybe I should spend less\ntime on it, you know.\nThe other thing is,\ndealing with death up in person like this,\nit's actually made me less afraid of,\neven less afraid of\nother people telling me\nthat I'm an idiot, you know,\nwhich happens regularly,\nand just live my life,\ndo my thing, you know?\nAnd it's made it a\nlittle bit easier for me\nto focus on what I feel\nis really important.\n- What about fear of your own death?\nHas it made it more real\nthat this is something that happens?\n- Yeah, it's made it extremely real,\nand you know,\nI'm next in line in our family now, right?\nIt's me and my younger brother, but,\nthey both handled it with such dignity,\nthat was a true inspiration also.\nThey never complained about things,\nand you know, when you're old\nand your body starts falling apart,\nit's more and more to complain about,\nthey looked at what could they\nstill do that was meaningful,\nand they focused on that\nrather than wasting time\ntalking about, or even thinking much about\nthings they were disappointed in.\nI think anyone can make\nthemselves depressed\nif they start their morning by\nmaking a list of grievances.\nWhereas if you start your day\nwhen the little meditation\nand just the things you're grateful for,\nyou basically choose to be a happy person.\n- Because you only have\na finite number of days,\nwe should spend them,\n- [Max] Make it count.\n- Being grateful.\n- [Max] Yeah.\n- Well you do happen to\nbe working on a thing\nwhich seems to have potentially,\nsome of the greatest impact\non human civilization\nof anything humans have ever created,\nwhich is artificial intelligence.\nThis is, on the both\ndetailed technical level,\nand on the high philosophical\nlevel you work on.\nSo you've mentioned to me\nthat there's an open letter\nthat you're working on.\n- It's actually going live in a few hours.\n(Lex laughing)\nSo I've been having late\nnights and early mornings.\nIt's been very exciting, actually.\nIn short,\nhave you seen, \"Don't Look Up,\"\nthe film?\n- Yes, yes.\n- I don't want to be the movie spoiler\nfor anyone watching\nthis who hasn't seen it.\nBut if you're watching this,\nyou haven't seen it,\nwatch it, because we\nare actually acting out,\nit's life imitating art.\nHumanity is doing exactly that right now,\nexcept it's an asteroid that\nwe are building ourselves.\nAlmost nobody is talking about it.\nPeople are squabbling across the planet\nabout all sorts of things,\nwhich seem very minor\ncompared to the asteroid\nthat's about to hit us, right?\nMost politicians don't\neven this on the radar,\nthey think maybe in 100 years or whatever.\nRight now we're at a fork on the road.\nThis is the most important\nfork that humanity has reached\nin it's over 100,000 years on this planet.\nWe're building effectively a new species\nthat's smarter than us,\nit doesn't look so much like a species yet\n'cause it's mostly not embodied in robots.\nBut that's the technicality\nwhich will soon be changed.\nAnd this arrival of of\nartificial general intelligence\nthat can do all our jobs as well as us,\nand probably shortly\nthereafter, superintelligence,\nwhich greatly exceeds\nour cognitive abilities.\nIt's gonna either be the best thing ever\nto happen to humanity or the worst.\nI'm really quite confident\nthat there is not that\nmuch middle ground there.\n- But it would be\nfundamentally transformative\nto human civilization.\n- Of course, utterly and totally.\nAgain, we'd branded\nourselves as Homo sapiens\n'cause it seemed like the basic thing,\nwe're the king of the\ncastle on this planet,\nwe're the smart ones,\nwe can control everything else,\nthis could very easily change.", "mimetype": "text/plain", "start_char_idx": 19431, "end_char_idx": 23498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b62ce1a-bc9d-4730-ab4e-741951c40f12": {"__data__": {"id_": "3b62ce1a-bc9d-4730-ab4e-741951c40f12", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260d20f3-9678-43f1-b357-33b1c62a8534", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c63659886c901b7eeecc4ed439e909968a49027c7352ccb60cd0adcd58ab6cc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8118fea2-400b-4115-9a19-5ccbe00a5446", "node_type": "1", "metadata": {}, "hash": "4e51f0bf9704b1eeb1a0e84ad8dd00727e81ae4b159ea3b49a8134241364464e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We're building effectively a new species\nthat's smarter than us,\nit doesn't look so much like a species yet\n'cause it's mostly not embodied in robots.\nBut that's the technicality\nwhich will soon be changed.\nAnd this arrival of of\nartificial general intelligence\nthat can do all our jobs as well as us,\nand probably shortly\nthereafter, superintelligence,\nwhich greatly exceeds\nour cognitive abilities.\nIt's gonna either be the best thing ever\nto happen to humanity or the worst.\nI'm really quite confident\nthat there is not that\nmuch middle ground there.\n- But it would be\nfundamentally transformative\nto human civilization.\n- Of course, utterly and totally.\nAgain, we'd branded\nourselves as Homo sapiens\n'cause it seemed like the basic thing,\nwe're the king of the\ncastle on this planet,\nwe're the smart ones,\nwe can control everything else,\nthis could very easily change.\nWe're certainly not gonna be the smartest\non the planet for very long if AI,\nunless AI progress just halts,\nand we can talk more about\nwhy I think that's true\n'cause it's controversial.\nAnd then we can also talk about\nreasons we might think it's\ngonna be the best thing ever,\nand the reason we think\nit's going to be the end of humanity,\nwhich is of course, super controversial.\nBut what I think we can,\nanyone who's working on advanced AI\ncan agree on is,\nit's much like the film \"Don't Look Up,\"\nin that it's just really comical\nhow little serious public\ndebate there is about it,\ngiven how huge it is.\n- So what we're talking\nabout is a development,\nof currently, things like GPT-4,\nand the signs it's showing\nof rapid improvement\nthat may, in the near\nterm lead to development\nof superintelligent AGI,\nAI, general AI systems,\nand what kind of impact\nthat has on society.\n- [Max] Exactly.\n- When that thing achieves\ngeneral human-level intelligence,\nand then beyond that,\ngeneral superhuman level intelligence.\nThere's a lot of\nquestions to explore here.\nSo one, you mentioned halt.\nIs that the content of the letter?\nis to suggest that maybe we should pause\nthe development of these systems.\n- Exactly, so this is very controversial,\nfrom when we talked the first time,\nwe talked about how I was involved\nin starting the Future of Life Institute,\nand we worked very hard on 2014, 2015,\nwas the mainstream AI safety.\nThe idea that there even could be risks\nand that you could do things about them.\nBefore then, a lot of people thought\nit was just really kooky\nto even talk about it.\nAnd a lot of AI researchers felt,\nworried that this was too flaky,\nand could be bad for funding,\nand that the people had\ntalked about it were just not,\ndidn't understand AI.\nI'm very, very happy with how that's gone,\nand that now, you know,\nit's completely mainstream,\nyou go in any AI conference,\nand people talk about AI safety,\nand it's a nerdy technical\nfield full of equations\nand blah-blah.\n- [Lex] Yes.\n- As it should be,\nbut there is this other thing,\nwhich has been quite taboo up until now,\ncalling for slowdown.\nSo what,\nwe've constantly been\nsaying, including myself,\nI've been biting my tongue a lot,\nyou know, is that,\nwe don't need to slow down AI development.\nWe just need to win this race,\nthe wisdom race between\nthe growing power of the AI\nand the growing wisdom\nwith which we manage it.\nAnd rather than trying to slow down AI,\nlet's just try to accelerate the wisdom,\ndo all this technical work to figure out\nhow you can actually ensure\nthat your powerful AI\nis gonna do what you want it to do.\nAnd have society adapt also\nwith incentives and regulations\nso that these things get put to good use.\nSadly, that didn't pan out.\nThe progress on technical AI capabilities\nhas gone a lot faster\nthan many people thought\nback when we started this in 2014.\nTurned out to be easier to build\nreally advanced AI than we thought.\nAnd on the other side,\nit's gone much slower than we hoped\nwith getting policymakers and others\nto actually put incentives\nin place to make,\nsteer this in the good directions,\nmaybe we should unpack it\nand talk a little bit about each, so.\n- [Lex] Yeah.\n- Why did it go faster than\na lot of people thought?", "mimetype": "text/plain", "start_char_idx": 22626, "end_char_idx": 26729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8118fea2-400b-4115-9a19-5ccbe00a5446": {"__data__": {"id_": "8118fea2-400b-4115-9a19-5ccbe00a5446", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b62ce1a-bc9d-4730-ab4e-741951c40f12", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "9258598d03cea75d132aa60fa5d6e1baa1b5c5df8b2520c89d27b0a0ecf7564c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca2615d3-8a09-414c-af2a-198d2f83ed80", "node_type": "1", "metadata": {}, "hash": "c8fdbeea1d70d1163dfda2817f66ab5c7ac75fef8fc091381716f3a2dd1467ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And rather than trying to slow down AI,\nlet's just try to accelerate the wisdom,\ndo all this technical work to figure out\nhow you can actually ensure\nthat your powerful AI\nis gonna do what you want it to do.\nAnd have society adapt also\nwith incentives and regulations\nso that these things get put to good use.\nSadly, that didn't pan out.\nThe progress on technical AI capabilities\nhas gone a lot faster\nthan many people thought\nback when we started this in 2014.\nTurned out to be easier to build\nreally advanced AI than we thought.\nAnd on the other side,\nit's gone much slower than we hoped\nwith getting policymakers and others\nto actually put incentives\nin place to make,\nsteer this in the good directions,\nmaybe we should unpack it\nand talk a little bit about each, so.\n- [Lex] Yeah.\n- Why did it go faster than\na lot of people thought?\nIn hindsight, it's exactly\nlike building flying machines.\nPeople spent a lot of time wondering about\nhow do birds fly, you know.\nAnd that turned out to be really hard.\nHave you seen the TED\nTalk with a flying bird?\n- Like a flying robotic bird?\n- Yeah, it flies around the audience,\nbut it took 100 years longer\nto figure out how to do that\nthan for the Wright brothers\nto build the first airplane\nbecause it turned out there\nwas a much easier way to fly.\nAnd evolution picked\na more complicated one\nbecause it had its hands tied.\nIt could only build a machine\nthat could assemble itself,\nwhich the Wright brothers\ndidn't care about that,\nthey could only build a machine that use\nonly the most common atoms\nin the periodic table,\nWright Brothers didn't care about that,\nthey could use steel,\niron atoms,\nand it had to be built to repair itself,\nand it also had to be\nincredibly fuel efficient,\nyou know,\na lot of birds use less than half the fuel\nof a remote-controlled plane\nflying the same distance,\nFor humans, just throw\na little more money,\nput a little more fuel in it,\nand there you go, 100 years earlier.\nThat's exactly what's happening now\nwith these large language models.\nThe brain is incredibly complicated.\nMany people made the mistake,\nyou're thinking we have to\nfigure out how the brain does\nhuman-level AI first\nbefore we could build in the machine,\nthat was completely wrong.\nYou can take an incredibly simple\ncomputational system called\na transformer network\nand just train it to do\nsomething incredibly dumb.\nJust read a gigantic amount of text\nand try to predict the next word.\nAnd it turns out,\nif you just throw a ton of compute at that\nand a ton of data,\nit gets to be frighteningly\ngood like GPT-4,\nwhich I've been playing with so much\nsince it came out, right?\nAnd there's still some debate\nabout whether that can get you all the way\nto full human level or not,\nbut yeah, we can come back\nto the details of that\nand how you might get the human-level AI\neven if large language models don't.\n- Can you briefly,\nif it's just a small tangent,\ncomment on your feelings about GPT-4?\nSo just that you're impressed\nby this rate of progress,\nbut where is it?\nCan GPT-4 reason?\nWhat are like the intuitions?\nWhat are human interpretable\nwords you can assign\nto the capabilities of GPT-4\nthat makes you so damn impressed with it?\n- I'm both very excited\nabout it and terrified.\nIt's interesting mixture\nof emotions. (laughs)\n- All the best things in life\ninclude those two somehow.\n- Yeah, it can absolutely reason,\nanyone who hasn't played with it,\nI highly recommend doing\nthat before dissing it.\nIt can do quite remarkable reasoning.\nI've had to do a lot of things,\nwhich I realized I couldn't\ndo that myself that well even,\nand it obviously does it\ndramatically faster than we do too,\nwhen you watch it type,\nand it's doing that well,\nservicing a massive number of\nother humans at the same time.\nThe same time, it cannot reason\nas well as a human can on some tasks,\nit's obviously the limitations\nfrom its architecture.\nYou know, we have in our heads,\nwhat in geek-speak is called\na recurrent neural network.", "mimetype": "text/plain", "start_char_idx": 25892, "end_char_idx": 29856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca2615d3-8a09-414c-af2a-198d2f83ed80": {"__data__": {"id_": "ca2615d3-8a09-414c-af2a-198d2f83ed80", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8118fea2-400b-4115-9a19-5ccbe00a5446", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "438fa37733ee7345711c8a31ed05d9dab893f639513c3b8f835f277cd3f5536b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ce24096-f645-41c5-9c49-fb0af35cd42a", "node_type": "1", "metadata": {}, "hash": "50f29ed11c0969e92e0ac887947ac6f54228da370e7b37a2322fe21258dbff70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- I'm both very excited\nabout it and terrified.\nIt's interesting mixture\nof emotions. (laughs)\n- All the best things in life\ninclude those two somehow.\n- Yeah, it can absolutely reason,\nanyone who hasn't played with it,\nI highly recommend doing\nthat before dissing it.\nIt can do quite remarkable reasoning.\nI've had to do a lot of things,\nwhich I realized I couldn't\ndo that myself that well even,\nand it obviously does it\ndramatically faster than we do too,\nwhen you watch it type,\nand it's doing that well,\nservicing a massive number of\nother humans at the same time.\nThe same time, it cannot reason\nas well as a human can on some tasks,\nit's obviously the limitations\nfrom its architecture.\nYou know, we have in our heads,\nwhat in geek-speak is called\na recurrent neural network.\nThere are loops,\ninformation can go from this neuron,\nto this neuron, to this neuron,\nand then back to this one,\nyou can like ruminate on\nsomething for a while,\nyou can self-reflect a lot.\nThese large language models,\nthey cannot, like GPT-4.\nIt's a so-called transformer\nwhere it's just like a one-way street\nof information, basically.\nIn geek-speak, it's called a\nfeed-forward neural network.\nAnd it's only so deep,\nso it can only do logic\nthat's that many steps\nand that deep, and it's not,\nso you can create problems\nwhich it will fail to solve,\nyou know, for that reason.\nBut the fact that it\ncan do so amazing things\nwith this incredibly simple\narchitecture already,\nis quite stunning,\nand what we see in my lab at MIT\nwhen we look inside large language models\nto try to figure out how they're doing it,\nwhich, that's the key core\nfocus of our research,\nit's called mechanistic\ninterpretability in geek-speak.\nYou know, you have this machine\nthat does something smart,\nyou try to reverse engineer it,\nand see how does it do it.\nI think of it also as\nartificial neuroscience,\n(Lex laughs)\n'Cause that's exactly\n- I love it.\n- what neuroscientists do\nwith actual brains.\nBut here you have the\nadvantage that you can,\nyou don't have to worry\nabout measurement errors.\nYou can see what every\nneuron is doing all the time,\nand a recurrent thing\nwe see again and again,\nthere's been a number of beautiful papers\nquite recently by a lot of researchers,\nand some of 'em are\nhere even in this area,\nis where when they figure\nout how something is done,\nyou can say, \"Oh man, that's\nsuch a dumb way of doing it.\"\nAnd you read immediately\nsee how it can be improved.\nLike for example,\nthere was this beautiful paper recently\nwhere they figured out\nhow a large language model\nstores certain facts,\nlike Eiffel Tower is in Paris,\nand they figured out\nexactly how it's stored\nand the proof of that they understood it\nwas they could edit it.\nThey changed some synapses in it,\nand then they asked it,\nWhere's the Eiffel Tower?\"\nAnd it said, \"It's in Rome.\"\nAnd then they asked,\n\"How do you get there?\nOh, how do you get there from Germany?\"\n\"Oh, you take this train,\nthe Roma Termini train station,\nand this and that,\"\n\"And what might you see\nif you're in front of it?\"\n\"Oh, you might see the Colosseum.\"\nSo they had edited,\n- So they literally moved it to Rome.\n- But the way that it's\nstoring this information,\nit's incredibly dumb,\nif any fellow nerds listening to this,\nthere was a big matrix,\nand roughly speaking,\nthere are certain row and column vectors\nwhich encode these things,\nand they correspond very hand-wavingly\nto principle components\nand it would be much more\nefficient for as far as matrix,\njust store in the database,\nyou know and,\nand everything so far,\nwe've figured out how these things do\nare ways where you can see\nit can easily be improved.\nAnd the fact that this\nparticular architecture\nhas some roadblocks built into it\nis in no way gonna\nprevent crafty researchers\nfrom quickly finding workarounds\nand making other kinds of architectures\nsort of go all the way, so.\nIn short, it's turned\nout to be a lot easier\nto build close to human\nintelligence than we thought,\nand that means our runway as a species to\nget our shit together has has shortened.", "mimetype": "text/plain", "start_char_idx": 29074, "end_char_idx": 33121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ce24096-f645-41c5-9c49-fb0af35cd42a": {"__data__": {"id_": "3ce24096-f645-41c5-9c49-fb0af35cd42a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca2615d3-8a09-414c-af2a-198d2f83ed80", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d9f6e5c64447fce6a4546bf863ff96d7a0538ceef370b5b3410566e52a323583", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c727fd7e-54aa-4ee1-8d62-bdad377dd409", "node_type": "1", "metadata": {}, "hash": "153f4cc2eb6a624b275be6aac9db42535f07a50bc367f934ce63f3ddfdf848d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the fact that this\nparticular architecture\nhas some roadblocks built into it\nis in no way gonna\nprevent crafty researchers\nfrom quickly finding workarounds\nand making other kinds of architectures\nsort of go all the way, so.\nIn short, it's turned\nout to be a lot easier\nto build close to human\nintelligence than we thought,\nand that means our runway as a species to\nget our shit together has has shortened.\n- And it seems like the scary thing\nabout the effectiveness\nof large language models,\nso Sam Altman, I've recently\nhad conversation with,\nand he really showed that\nthe leap from GPT-3 to GPT-4\nhas to do with just a bunch of hacks,\na bunch of little explorations\nwith smart researchers\ndoing a few little fixes here and there.\nIt's not some fundamental leap\nand transformation in the architecture.\n- And more data and more compute.\n- And more data and compute,\nbut he said the big leaps has to do\nwith not the data and the compute,\nbut just learning this new discipline,\njust like you said.\nSo researchers are going to\nlook at these architectures\nand there might be big\nleaps where you realize,\n\"Wait, why are we doing\nthis in this dumb way?\"\nAnd all of a sudden this\nmodel is 10x smarter.\nAnd that that can happen on any one day,\non any one Tuesday or Wednesday afternoon.\nAnd then all of a sudden you have a system\nthat's 10x smarter.\nIt seems like it's such a new discipline,\nit's such a new,\nlike we understand so little\nabout why this thing works so damn well,\nthat the linear improvement of compute,\nor exponential,\nbut the steady improvement of compute,\nsteady improvement of the data\nmay not be the thing that\neven leads to the next leap.\nIt could be a surprise little\nhack that improves everything.\n- Or a lot of little leaps here and there\nbecause so much of this\nis out in the open also,\nso many smart people are looking at this\nand trying to figure out\nlittle leaps here and there,\nand it becomes this sort\nof collective race where,\na lot of people feel,\n\"If I don't take the\nleap someone else will,\"\nand it is actually very crucial\nfor the other part of it,\nwhy do we wanna slow this down?\nSo again, what this open\nletter is calling for\nis just pausing all training\nof systems that are more powerful\nthan GPT-4 for six months.\nJust give a chance\nfor the labs to coordinate\na bit on safety,\nand for society to adapt,\ngive the right incentives to the labs.\n'cause I, you know,\nyou've interviewed a lot of\nthese people who lead these labs\nand you know just as well as I do\nthat they're good people,\nthey're idealistic people.\nThey're doing this first and foremost\nbecause they believe that AI\nhas a huge potential to help humanity.\nBut at the same time they are trapped\nin this horrible race to the bottom.\nHave you read \"Meditations on Moloch\"\nby Scott Alexander?\n- [Lex] Yes.\n- Yeah, it's a beautiful\nessay on this poem by Ginsberg\nwhere he interprets it as\nbeing about this monster.\nIt's this game theory\nmonster that pits people\nagainst each other in\nthis race to the bottom\nwhere everybody ultimately loses.\nAnd the evil thing about this monster\nis even though everybody\nsees it and understands,\nthey still can't get\nout of the race, right?\nA good fraction of all the\nbad things that we humans do\nare caused by Moloch.\nAnd I like Scott Alexander's\nnaming of the monster.\nSo we can,\nwe humans can think of it as a thing.\nIf you look at why do we have overfishing,\nwhy do we have more generally,\nthe tragedy of the commons.\nWhy is it that,\nso Liv Boeree,\nI don't know if you've\nhad her on your podcast.\n- Mhm, yeah.\nShe's become a friend, yeah.\n- Great, she made this\nawesome point recently\nthat beauty filters that a lot of female\ninfluencers feel pressure to use,\nare exactly Moloch in action again.\nFirst, nobody was using them,\nand people saw them\njust the way they were,\nand then some of 'em started using it,\nand becoming ever more plastic fantastic,\nand then the other ones\nthat weren't using it\nstarted to realize that,\nif they wanna to keep\ntheir their market share,\nthey have to start using it too.", "mimetype": "text/plain", "start_char_idx": 32712, "end_char_idx": 36742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c727fd7e-54aa-4ee1-8d62-bdad377dd409": {"__data__": {"id_": "c727fd7e-54aa-4ee1-8d62-bdad377dd409", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ce24096-f645-41c5-9c49-fb0af35cd42a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a49c17e1bb3383a28a3df8caf5803290cc5b9338909ab82ea18a68c3f7f4340b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea44a111-ff1e-40e6-8a92-114e12e5165f", "node_type": "1", "metadata": {}, "hash": "e516b47a2a093687de8b404b6f39a6603f7dca20b561f951b7f04a62055f8ef3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I like Scott Alexander's\nnaming of the monster.\nSo we can,\nwe humans can think of it as a thing.\nIf you look at why do we have overfishing,\nwhy do we have more generally,\nthe tragedy of the commons.\nWhy is it that,\nso Liv Boeree,\nI don't know if you've\nhad her on your podcast.\n- Mhm, yeah.\nShe's become a friend, yeah.\n- Great, she made this\nawesome point recently\nthat beauty filters that a lot of female\ninfluencers feel pressure to use,\nare exactly Moloch in action again.\nFirst, nobody was using them,\nand people saw them\njust the way they were,\nand then some of 'em started using it,\nand becoming ever more plastic fantastic,\nand then the other ones\nthat weren't using it\nstarted to realize that,\nif they wanna to keep\ntheir their market share,\nthey have to start using it too.\nAnd then you're in a situation\nwhere they're all using it,\nand none of them has any more market share\nor less than before.\nSo nobody gained anything, everybody lost,\nand they have to keep becoming\never more plastic fantastic also, right?\nBut nobody can go back to the old way\nbecause it's just too costly, right?\nMoloch is everywhere,\nand Moloch is not a new\narrival on the scene either.\nWe humans have developed a lot\nof collaboration mechanisms\nto help us fight back against Moloch\nthrough various kinds of\nconstructive collaboration.\nThe Soviet Union and the United States\ndid sign a number of arms control treaties\nagainst Moloch who is trying to stoke them\ninto unnecessarily risky\nnuclear arms races,\net cetera, et cetera.\nAnd this is exactly what's\nhappening on the AI front.\nThis time it's a little bit geopolitics,\nbut it's mostly money,\nwhere there's just so\nmuch commercial pressure.\nYou know, if you take any of these leaders\nof the top tech companies,\nif they just say, you know,\n\"This is too risky, I want\nto pause for six months.\"\nThey're gonna get a lot of pressure\nfrom shareholders and others.\nThey're like, \"Well\nyou know, if you pause,\nbut those guys don't pause.\nWe don't wanna get our lunch eaten.\"\n- [Lex] Yeah.\n- And shareholders even have the power\nto replace the executives\nin the worst case, right?\nSo we did this open letter\nbecause we want to help\nthese idealistic tech executives to do\nwhat their heart tells them,\nby providing enough public\npressure on the whole sector.\nJust pause, so that they can all pause\nin a coordinated fashion.\nAnd I think without the public pressure,\nnone of them can do it alone.\nPush back against their shareholders\nno matter how goodhearted they are,\n'cause Moloch is a really powerful foe.\n- So the idea is to,\nfor the major developers\nof AI systems like this,\nso we're talking about Microsoft, Google,\nMeta, and anyone else.\n- Well OpenAI is very\nclose with Microsoft now,\n- With Microsoft, right, yeah.\n- of course,\n- And there there are\nplenty of smaller players.\nfor example, Anthropic\nis is very impressive,\nthere's Conjecture,\nthere's many, many, many players,\nI don't wanna make a long list\nthat sort of leave anyone out.\nAnd for that reason,\nit's so important that\nsome coordination happens,\nthat there's external\npressure on all of them,\nsaying, \"You all need the pause.\"\n'Cause then, the people,\nthe researchers in there\nat these organizations,\nthe leaders who wanna\nslow down a little bit,\nthey can say to their\nshareholders, you know,\n\"Everybody's slowing down\nbecause of this pressure\nand it's the right thing to do.\"\n- Have you seen in history,\nthere examples what it's possible\nto pause the Moloch?\n- Yes, absolutely.\nAnd even like human cloning for example,\nyou could make so much\nmoney on human cloning.\nWhy aren't we doing it?\nBecause biologists thought hard about this\nand felt like this is way too risky,\nthey got together in the\nseventies in Asilomar,\nand decided even to stop a lot more stuff,\nalso just editing the\nhuman germline, right?\nGene editing that goes\nin to our offspring,\nand decided, \"Let's not do this\nbecause it's too unpredictable\nwhat it's gonna lead to,\"\nwe could lose control over\nwhat happens to our species,\"\nso they paused.", "mimetype": "text/plain", "start_char_idx": 35955, "end_char_idx": 39970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea44a111-ff1e-40e6-8a92-114e12e5165f": {"__data__": {"id_": "ea44a111-ff1e-40e6-8a92-114e12e5165f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c727fd7e-54aa-4ee1-8d62-bdad377dd409", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e49e24e2dd5a372e6ead009b24e8367a1d34461cd085804281292b7f811f0c80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e84bc547-c093-4d6b-8289-cd2948d03a0b", "node_type": "1", "metadata": {}, "hash": "fce8caa8d97b9a520721243c726c7043f2c0531016e77bbb19cf57779fdabc26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Have you seen in history,\nthere examples what it's possible\nto pause the Moloch?\n- Yes, absolutely.\nAnd even like human cloning for example,\nyou could make so much\nmoney on human cloning.\nWhy aren't we doing it?\nBecause biologists thought hard about this\nand felt like this is way too risky,\nthey got together in the\nseventies in Asilomar,\nand decided even to stop a lot more stuff,\nalso just editing the\nhuman germline, right?\nGene editing that goes\nin to our offspring,\nand decided, \"Let's not do this\nbecause it's too unpredictable\nwhat it's gonna lead to,\"\nwe could lose control over\nwhat happens to our species,\"\nso they paused.\nThere was a ton of money to be made there,\nSo it's very doable,\nbut you need a public awareness\nof what the risks are,\nand the broader community\ncoming in and saying,\n\"Hey, let's slow down.\"\nAnd you know, another\ncommon pushback I get today,\nis that we can't stop in\nthe West because China.\nAnd in China undoubtedly,\nthey also get told, \"We can't\nslow down because the West,\"\nbecause both sides think\nthey're the good guy.\n- [Lex] Yeah.\n- But look at human cloning, you know?\nDid China forge ahead with human cloning?\nThere's been exactly one human cloning\nthat's actually been done that I know of.\nIt was done by a Chinese guy.\nDo you know where he is now?\n- [Lex] Where?\n- In jail.\nAnd you know who put him there?\n- [Lex] Who?\n- Chinese government.\nNot because Westerners said,\n\"China look, this is...\"\nNo the Chinese government put him there\n'cause they also felt,\nthey like control, the Chinese government.\nIf anything, maybe they're\neven more concerned about\nhaving control than Western governments,\nhave no incentive of just losing control\nover where everything is going,\nand you can also see the Ernie Bot\nthat was released by,\nI believe, Baidu recently,\nthey got a lot of pushback\nfrom the government\nand had to rein it in,\nyou know, in a big way.\nI think once this basic message comes out\nthat this isn't an arms\nrace, it's a suicide race,\nwhere everybody loses\nif anybody's AI goes out of control,\nit really changes the whole dynamic.\nIt's not,\nand I'll say this again\n'cause this is this very basic point\nI think a lot of people get wrong.\nBecause a lot of people\ndismiss the whole idea\nthat AI can really get very superhuman\nbecause they think there's something\nreally magical about intelligence\nsuch that it can only\nexist in human minds,\nyou know, because they believe that,\nthey think it's gonna kind\nof get to just more or less\n\"GPT-4 plus plus,\" and then that's it.\nThey don't see it as a suicide race.\nThey think whoever gets that first,\nthey're gonna control the world,\nthey're gonna win.\nThat's not how it's gonna be.\nAnd we can talk again about\nthe scientific arguments\nfrom why it's not gonna stop there.\nBut the way it's gonna be,\nis if anybody completely loses control\nand you know, you don't care\nif someone manages to take over the world\nwho really doesn't share your goals,\nyou probably don't really\neven care very much\nabout what nationality they have,\nyou're not gonna like it\nmuch worse than today.\nIf you live in Orwellian dystopia,\nwhat do you care who's created it, right?\nAnd if someone,\nif it goes farther,\nand we just lose control\neven to the machines,\nso that it's not us versus them,\nit's us versus it.\nWhat do you care who created\nthis unaligned entity\nwhich has goals different\nfrom humans, ultimately?\nAnd we get marginalized,\nwe get made obsolete,\nwe get replaced.\nThat's what I mean when I\nsay it's a suicide race,\nit's kind of like we're\nrushing towards this cliff,\nbut the closer the cliff we get,\nthe more scenic the views are,\nand the more money there is there,\nand the more,\nso we keep going,\nbut we have to also stop\nat some point, right?\nQuit while we're ahead,\nAnd it's,\nit's a suicide race which cannot be won,\nbut the way to really benefit from it is,\nto continue developing awesome\nAI a little bit slower.\nSo we make it safe,\nmake sure it does the\nthings that humans want,\nand create a condition\nwhere everybody wins.", "mimetype": "text/plain", "start_char_idx": 39335, "end_char_idx": 43342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e84bc547-c093-4d6b-8289-cd2948d03a0b": {"__data__": {"id_": "e84bc547-c093-4d6b-8289-cd2948d03a0b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea44a111-ff1e-40e6-8a92-114e12e5165f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8f79d17b7ca7b78976c142c5da3a20f0719ae524c9ee588255e4240048bd6888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a", "node_type": "1", "metadata": {}, "hash": "9980103d257c9f35eb0a6e23e3fa311a12735d9b03a872ad4181b0aebdf5bf12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What do you care who created\nthis unaligned entity\nwhich has goals different\nfrom humans, ultimately?\nAnd we get marginalized,\nwe get made obsolete,\nwe get replaced.\nThat's what I mean when I\nsay it's a suicide race,\nit's kind of like we're\nrushing towards this cliff,\nbut the closer the cliff we get,\nthe more scenic the views are,\nand the more money there is there,\nand the more,\nso we keep going,\nbut we have to also stop\nat some point, right?\nQuit while we're ahead,\nAnd it's,\nit's a suicide race which cannot be won,\nbut the way to really benefit from it is,\nto continue developing awesome\nAI a little bit slower.\nSo we make it safe,\nmake sure it does the\nthings that humans want,\nand create a condition\nwhere everybody wins.\nThe technology has shown us that,\nyou know, geopolitics\nand politics in general\nis not a zero sum game at all.\n- So there is some rate of\ndevelopment that will lead\nus as a human species to\nlose control of this thing.\nAnd the hope you have\nis that there's some\nlower level of development\nwhich will not allow us to lose control.\nThis is an interesting thought you have\nabout losing control,\nso if you have somebody,\nif you are somebody like Sundar Pichai\nor Sam Altman at the head\nof a company like this,\nyou're saying if they develop an AGI,\nthey too will lose control of it.\nSo no one person can maintain control,\nno group of individuals\ncan maintain control.\n- If it's created very, very soon\nand is a big black box\nthat we don't understand\nlike the large language models, yeah.\nThen I'm very confident\nthey're gonna lose control.\nBut this isn't just me\nsaying it, you know,\nSam Altman and Demis Hassabis\nhave both said,\nthey themselves acknowledge that,\nyou know, there's really\ngreat risks for this\nand they want slow down once\nthey feel it gets scary.\nBut it's clear that they're stuck in this,\nagain, Moloch is forcing\nthem to go a little faster\nthan they're comfortable with\nbecause of pressure from,\njust commercial pressures, right?\nTo get a bit optimistic here,\nof course, this is a problem\nthat can be ultimately solved.\nTo win this wisdom race,\nit's clear that what we\nhope that was gonna happen\nhasn't happened.\nThe capability progress has gone faster\nthan a lot of people thought,\nand the progress in the public sphere\nof policy making and so on,\nhas gone slower than we thought.\nEven the technical AI\nsafety has gone slower.\nA lot of the technical safety research\nwas kind of banking on\nthat large language models\nand other poorly understood systems\ncouldn't get us all the way.\nThat you had to build more\nof a kind of intelligence\nthat you could understand.\nMaybe it could prove itself safe,\nyou know, things like this,\nand I'm quite confident\nthat this can be done\nso we can reap all the benefits,\nbut we cannot do it as quickly\nas this out of control\nexpress train we are on now\nis gonna get to AGI.\nThat's why we need a\nlittle more time, I feel.\n- Is there something to be said,\nwell like Sam Altman talked about,\nwhich is while we're in the pre-AGI stage,\nto release often and as\ntransparently as possible\nto learn a lot.\nSo as opposed to being extremely cautious,\nrelease a lot,\ndon't invest in a closed development\nwhere you focus on the AI safety.\nWhile it's somewhat \"dumb,\"\nquote-unquote,\nrelease as often as possible.\nAnd as you start to see signs\nof human-level intelligence\nand or superhuman level intelligence,\nthen you put a halt on it.\nWell what a lot of safety researchers\nhave been saying for many years\nis that the most dangerous\nthings you can do\nwith an AI is first of all\nteach it to write code.\n- [Lex] Yeah.\n- Because that's the first step\ntowards recursive self-improvement,\nwhich can take it from\nAGI to much higher levels.\nOkay? Oops, we've done that.\nAnd another thing high risk\nis connect it to the internet,\nlet it go to websites,\ndownload stuff on its\nown and talk to people.\nOops, we've done that already.\nYou know Eliezer Yudkowsky,\nyou said you interviewed\nhim recently, right?\n- [Lex] Yes, yep.", "mimetype": "text/plain", "start_char_idx": 42612, "end_char_idx": 46588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a": {"__data__": {"id_": "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e84bc547-c093-4d6b-8289-cd2948d03a0b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c89579247cd2f72ee6b5fc7a39bfa448ed09640789333f08c8f6b92d338849c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5", "node_type": "1", "metadata": {}, "hash": "fc987a5365e731ee8ed58a91884a53fdbd83563cf8d4c186912f2104ca257068", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While it's somewhat \"dumb,\"\nquote-unquote,\nrelease as often as possible.\nAnd as you start to see signs\nof human-level intelligence\nand or superhuman level intelligence,\nthen you put a halt on it.\nWell what a lot of safety researchers\nhave been saying for many years\nis that the most dangerous\nthings you can do\nwith an AI is first of all\nteach it to write code.\n- [Lex] Yeah.\n- Because that's the first step\ntowards recursive self-improvement,\nwhich can take it from\nAGI to much higher levels.\nOkay? Oops, we've done that.\nAnd another thing high risk\nis connect it to the internet,\nlet it go to websites,\ndownload stuff on its\nown and talk to people.\nOops, we've done that already.\nYou know Eliezer Yudkowsky,\nyou said you interviewed\nhim recently, right?\n- [Lex] Yes, yep.\n- So he had this tweet\nrecently which said,\ngave me one of the best laughs in a while,\nwhere he is like,\n\"Hey, people used to\nmake fun of me and say,\n'You're so stupid, Eliezer.'\n'Cause you're saying\nyou have to worry of obviously developers\nonce they get to like really strong AI,\nfirst thing you're gonna do is like,\nnever connect it to the internet,\nkeep it in a box.\nwhere you know, you can\nreally study it safe.\"\nSo he had written it in\nthe like in the meme form\nso it's like \"Then,\" and then that,\nand then, \"Now.\"\n(Lex laughing)\n\"LOL, let's make a chatbot.\"\n(both laughing)\n- [Lex] Yeah, yeah, yeah.\n- And the third thing is Stuart Russell.\n- [Lex] Yeah.\n- You know, amazing AI researcher.\nHe has argued for a while\nthat we should never\nteach AI anything about humans.\nAbove all, we should never let it learn\nabout human psychology and\nhow you manipulate humans.\nThat's the most dangerous kind\nof knowledge you can give it.\nYeah, you can teach it\nall it needs to know\nabout how to cure cancer\nand stuff like that.\nBut don't let it read\nDaniel Kahneman's book\nabout cognitive biases and all that.\nAnd then oops,\n\"LOL, you know, let's invent social media\nrecommender algorithms\nwhich do exactly that.\"\nThey get so good at knowing\nus and pressing our buttons\nthat we are starting to create a world now\nwhere we just have ever more hatred,\n'cause they've figured\nout that these algorithms,\nnot for out of evil,\nbut just to make money on advertising,\nthat the best way to get more engagement,\nthe euphemism,\nget people glued to their\nlittle rectangles, right?\nIs just to make them pissed off.\n- Well that's really interesting that\na large AI system that's\ndoing the recommender system\nkind of task on social media,\nis basically just studying human beings\nbecause it's a bunch of\nus rats giving it signal,\nnonstop signal.\nIt'll show a thing and\nthen we give signal,\nand whether we spread that\nthing, we like that thing,\nthat thing increases our engagement,\ngets us to return to the platform,\nand it has that on the scale\nof hundreds of millions\nof people constantly.\nSo it's just learning, and\nlearning, and learning,\nand presumably if the number of parameters\nin the neural network\nthat's doing the learning,\nand more end to end the learning is,\nthe more it's able to\njust basically encode\nhow to manipulate human behavior.\n- [Max] Exactly.\n- How to control humans at scale.\n- Exactly, and that is\nnot something you think\nis in humanity's interest.\nAnd right now it's mainly letting\nsome humans manipulate other\nhumans for profit and power,\nwhich already caused a lot of damage,\nand then eventually that's a sort of skill\nthat can make AI persuade\nhumans to let them escape\nwhatever safety precautions we had put,\nyou know, there was a really nice article\nin the New York Times\nrecently by Yuval Noah Harari\nand two co-authors\nincluding Tristan Harris\nfrom \"The Social Dilemma,\"\nand we have this phrase in there I love,\nIt said, \"Humanity's first\ncontact with advanced AI\nwas social media.\"\nAnd we lost that one.\nWe now live in a country\nwhere there's much more hate in the world\nwhere there's much more hate, in fact.", "mimetype": "text/plain", "start_char_idx": 45815, "end_char_idx": 49709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5": {"__data__": {"id_": "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71e8f5a9-1c3b-4c12-b49f-e56d3ebab43a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d8798b01dc2cf6bfa7aaa9710473f394512c4427760ed7e377252fc0eb7b00c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "638758f9-0195-4149-8feb-7d3105d034c1", "node_type": "1", "metadata": {}, "hash": "2878cffff4b0413fc029c2893ba64562e4b3d5054ffb41e78849d0f9c4b6f82b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Max] Exactly.\n- How to control humans at scale.\n- Exactly, and that is\nnot something you think\nis in humanity's interest.\nAnd right now it's mainly letting\nsome humans manipulate other\nhumans for profit and power,\nwhich already caused a lot of damage,\nand then eventually that's a sort of skill\nthat can make AI persuade\nhumans to let them escape\nwhatever safety precautions we had put,\nyou know, there was a really nice article\nin the New York Times\nrecently by Yuval Noah Harari\nand two co-authors\nincluding Tristan Harris\nfrom \"The Social Dilemma,\"\nand we have this phrase in there I love,\nIt said, \"Humanity's first\ncontact with advanced AI\nwas social media.\"\nAnd we lost that one.\nWe now live in a country\nwhere there's much more hate in the world\nwhere there's much more hate, in fact.\nAnd in our democracy than\nwe're having this conversation,\nand people can't even agree on\nwho won the last election, you know.\nAnd we humans often point fingers\nat other humans and say it's their fault,\nbut it's really Moloch\nin these AI algorithms.\nWe got the algorithms and then Moloch\npitted the social media\ncompanies against each other\nso nobody could have a\nless creepy algorithm\n'cause then they would lose out on revenue\nto the other company.\n- Is there any way to win that battle back\nif we just linger on this one battle\nthat we've lost in terms of social media,\nis it possible to redesign social media,\nthis very medium in which\nwe use as a civilization\nto communicate with each other,\nto have these kinds of conversation,\nto have discourse,\nto try to figure out how to solve\nthe biggest problems in the world,\nwhether that's nuclear war\nor the development of AGI.\nIs is it possible to do\nsocial media correctly?\n- I think it's not only\npossible, but it's necessary.\nWho are we kidding?\nThat we're gonna be able to\nsolve all these other challenges\nif we can't even have a\nconversation with each other?\nIt's constructive.\nThe whole idea,\nthe key idea of democracy\nis that you get a bunch of people together\nand they have a real conversation.\nThe ones you try to foster on this podcast\nwhere you respectfully listen\nto people you disagree with.\nAnd you realize actually,\nyou know, there are some things actually\nsome common ground we have and let's,\nwe both agree, let's not\nhave any nuclear wars,\nlet's not do that, et cetera, et cetera.\nWe're kidding ourselves that\nthinking we can face off\nthe second contact with\never more powerful AI\nthat's happening now\nwith these large language\nmodels if we can't even\nhave a functional conversation\nin the public space.\nThat's why I started the\nImprove The News project,\nimprovethenews.org.\nBut I'm an optimist fundamentally,\nin that there is a lot of\nintrinsic goodness in people.\nAnd that what makes the difference\nbetween someone doing\ngood things for humanity\nand bad things is not some\nsort of fairytale thing,\nthat this person was\nborn with the evil gene\nand this one is born with the good gene.\nNo, I think it's whether we put,\nwhether people find\nthemselves in situations\nthat bring out the best in them\nor that bring out the worst in them.\nAnd I feel we're building an internet\nand a society that brings out the worst.\n- But it doesn't have to be that way.\n- [Max] No, it does not.\n- It's possible to create incentives\nand also create incentives\nthat make money.\nThat both make money and\nbring out the best in people.\n- I mean, in the long term,\nit's not a good investment\nfor anyone, you know,\nto have a nuclear war, for example.\nAnd you know,\nis it a good investment for humanity\nif we just ultimately replace\nall humans by machines,\nand then we're so\nobsolete that eventually,\nthere are no humans left?\nWell, it depends guess\nhow you do the math,\nBut I would say by any\nreasonable economic standard,\nif you look at the future income of humans\nand there aren't any,\nyou know, that's not a good investment.\nMoreover, like why can't we have\na little bit of pride\nin our species, damn it?\nYou know, why should we just build\nanother species that gets rid of us?", "mimetype": "text/plain", "start_char_idx": 48915, "end_char_idx": 52928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "638758f9-0195-4149-8feb-7d3105d034c1": {"__data__": {"id_": "638758f9-0195-4149-8feb-7d3105d034c1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb9fd07a-69c9-46e5-ab9a-c274bec9c8e5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7bcef4a25234996b0ba0cc3b9c89a829cdaa9bae360aa33d9f3f6315cece8fe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33e6204d-9f90-4686-8089-59aa3987863e", "node_type": "1", "metadata": {}, "hash": "6d106152457fb1a81616300c8ede13ca077c07535dd82e67411b08dbbefe9997", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Max] No, it does not.\n- It's possible to create incentives\nand also create incentives\nthat make money.\nThat both make money and\nbring out the best in people.\n- I mean, in the long term,\nit's not a good investment\nfor anyone, you know,\nto have a nuclear war, for example.\nAnd you know,\nis it a good investment for humanity\nif we just ultimately replace\nall humans by machines,\nand then we're so\nobsolete that eventually,\nthere are no humans left?\nWell, it depends guess\nhow you do the math,\nBut I would say by any\nreasonable economic standard,\nif you look at the future income of humans\nand there aren't any,\nyou know, that's not a good investment.\nMoreover, like why can't we have\na little bit of pride\nin our species, damn it?\nYou know, why should we just build\nanother species that gets rid of us?\nIf we were Neanderthals,\nwould we really consider it a smart move\nif we had really advanced\nbiotech to build Homo sapiens?\nYou know, you might say,\n\"Hey Max, you know,\nyeah, let's build,\nthese Homo sapiens, they're\ngonna be smarter than us,\nmaybe they can help us,\ndefend us better against predators\nand help fix up our\ncaves, make them nicer,\nwe'll control 'em undoubtedly, you know?\"\nSo then they build a couple,\na little baby girl, little baby boy.\nThey either,\nand then you have some wise\nold Neanderthal elder is like,\n\"Hmm, I'm scared that we're\nopening a Pandora's box here\nand that we're gonna\nget outsmarted by these\nsuper Neanderthal intelligences,\nand there won't be any Neanderthals left.\"\nBut then you have a bunch of\nothers in the cave, right?\n\"You're such a Luddite scaremonger.\nOf course, they're gonna\nwant to keep us around\n'cause we are their creators,\nand, you know, the smarter,\nI think the smarter they get,\nthe nicer they're gonna get,\nthey're gonna leave us.\nThey're gonna want us around\nand it's gonna be fine,\nand besides look at these\nbabies, they're so cute.\nClearly they're totally harmless.\"\nThose babies are exactly GPT-4.\nIt's not,\nI wanna be clear,\nit's not GPT-4 that's terrifying.\nIt's that GPT-4 is a baby technology,\nyou know, and Microsoft even\nhad a paper recently out,\ntitled something like, \"Sparkles of AGI.\"\nWell they were basically\nsaying this is baby AI,\nlike these little Neanderthal babies,\nand it's gonna grow up.\nThere's gonna be other\nsystems from the same company,\nfrom other companies,\nthey'll be way more powerful,\nbut they're gonna take all the things,\nideas from these babies\nand before we know it,\nwe're gonna be like\nthose last Neanderthals\nwho were pretty disappointed\nwhen they realized that\nthey were getting replaced.\n- Well, this interesting point you make,\nwhich is of programming,\nit's entirely possible that GPT-4\nis already the kind of system\nthat can change everything\nby writing programs.\n- Yeah, it's because it's life 2.0,\nthe systems I'm afraid\nof are gonna look nothing\nlike a large language\nmodel, and they're not,\nbut once it gets,\nonce it or other people figure out a way\nof using this tech to make\nmuch better tech, right?\nIt's just constantly\nreplacing its software.\nAnd from everything that we've seen\nabout how these work under the hood,\nthey're like the minimum\nviable intelligence.\nThey do everything, you know,\nthe dumbest way that still works, sort of.\n- [Lex] Yeah.\n- And so they're life 3.0,\nexcept when they replace their software,\nit's a lot faster than when\nyou decide to learn Swedish.\nPoof. (fingers snapping)\nAnd moreover, they think\na lot faster than us too.\nSo when, you know,\nwe don't think,\nhave one logical step\nevery nanosecond or few, or so,\nthe way they do,\nand we can't also just\nsuddenly scale up our hardware\nmassively in the cloud 'cause\nwe're so limited, right?\nSo they are,\nand they are also life,\ncan soon become a little\nbit more like life 3.0\nin that if they need more hardware,\nhey, just rent it in the cloud, you know?\n\"How do you pay for it?\"", "mimetype": "text/plain", "start_char_idx": 52126, "end_char_idx": 55982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33e6204d-9f90-4686-8089-59aa3987863e": {"__data__": {"id_": "33e6204d-9f90-4686-8089-59aa3987863e", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "638758f9-0195-4149-8feb-7d3105d034c1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "be62e78f12a06f585a45f0a652bae1bc508fcedada40095cf50c6a58dfe4d62a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256", "node_type": "1", "metadata": {}, "hash": "bd316dd40817d51fdda580c622d5007d4810504970a64a64df55b6bed7f8ceb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They do everything, you know,\nthe dumbest way that still works, sort of.\n- [Lex] Yeah.\n- And so they're life 3.0,\nexcept when they replace their software,\nit's a lot faster than when\nyou decide to learn Swedish.\nPoof. (fingers snapping)\nAnd moreover, they think\na lot faster than us too.\nSo when, you know,\nwe don't think,\nhave one logical step\nevery nanosecond or few, or so,\nthe way they do,\nand we can't also just\nsuddenly scale up our hardware\nmassively in the cloud 'cause\nwe're so limited, right?\nSo they are,\nand they are also life,\ncan soon become a little\nbit more like life 3.0\nin that if they need more hardware,\nhey, just rent it in the cloud, you know?\n\"How do you pay for it?\"\n\"Well, with all the services you provide.\"\n- And what we haven't seen yet,\nwhich could change a lot,\nis entire software systems.\nSo right now programming is\ndone sort of in bits and pieces\nas an assistant tool to humans.\nBut I do a lot of programming\nand with the kind of stuff\nthat GPT-4 is able to do,\nI mean, it's replacing a lot\nwhat I'm able to do, right?\nYou still need a human in the loop\nto kind of manage the design of things,\nmanage like, what are the prompts\nthat generate the kind of stuff\nto do some basic adjustment of the codes,\ndo some debugging,\nbut if it's possible\nto add on top of GPT-4,\nkind of a feedback loop\nof self-debugging, improving the code,\nand then you launch that\nsystem onto the wild\non the internet because\neverything is connected,\nand have it do things,\nhave it interact with humans\nand then get that feedback,\nnow you have this giant\necosystem of humans.\nThat's one of the things that\nElon Musk recently sort of tweeted\nas a case why everyone\nneeds to pay $7 or whatever\nfor Twitter,\n- [Max] To make sure they're real.\n- Make sure they're real,\nwe're now going to be living in a world\nwhere the bots are getting smarter,\nand smarter, and smarter\nto a degree where,\nyou can't tell the difference\nbetween a human and a bot.\n- [Max] That's right.\n- And now you can have\nbots outnumber humans\nby 1 million to one.\nWhich is why he's making a\ncase why you have to pay.\nTo prove you're human,\nwhich is one of the only\nmechanisms to prove,\nwhich is depressing.\n- And yeah,\nI feel we have to remember,\nas individuals, we\nshould from time to time,\nask ourselves why are we\ndoing what we're doing, right?\nAnd as a species, we need to do that too.\nSo if we're building, as you say,\nmachines that are outnumbering us,\nand more and more outsmarting us,\nand replacing us on the job market,\nnot just for the dangerous\nand and boring tasks,\nbut also for writing poems and doing art,\nand things that a lot of\npeople find really meaningful,\nwe gotta ask ourselves, why?\nWhy are we doing this?\nThe answer is Moloch is\ntricking us into doing it.\nAnd it's such a clever trick\nthat even though we see the trick,\nwe still have no choice\nbut to fall for it, right?\nAnd also, thing you said about you using\nco-pilot AI tools to program faster,\nhow many,\nwhat factor faster would\nyou say you code now?\nDoes it go twice as fast? Or,\n- I don't really,\nbecause it's such a new tool.\n- [Max] Yeah.\n- I don't know if speed\nis significantly improved,\nbut it feels like I'm a year away\nfrom being 5 to 10 times faster.\n- So if that's typical for programmers,\nthen you're already seeing another kind\nof recursive self-improvement, right?\nBecause previously,\nlike a major generation of\nimprovement of the codes\nwould happen on the\nhuman R and D time scale.\nAnd now if that's five times shorter,\nthen it's gonna take five times less time\nthan it otherwise would to develop\nthe next level of these tools, and so on.\nSo this is exactly the sort of beginning\nof an intelligence explosion.\nThere can be humans in the\nloop a lot in the early stages,\nand then eventually humans\nare needed less and less\nand the machines can\nmore kind of go alone.", "mimetype": "text/plain", "start_char_idx": 55292, "end_char_idx": 59122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256": {"__data__": {"id_": "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33e6204d-9f90-4686-8089-59aa3987863e", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bfde5d2fdb7c7a5bbc78e46d817cefee25505439530d08ee54caa06c0b4d6e61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "020e0143-9083-4318-9da3-0b8a5b692f9b", "node_type": "1", "metadata": {}, "hash": "aec4266c3275a53601f0682e148c4b403d92ed3a3735e310dbf5c9253aeda6d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Or,\n- I don't really,\nbecause it's such a new tool.\n- [Max] Yeah.\n- I don't know if speed\nis significantly improved,\nbut it feels like I'm a year away\nfrom being 5 to 10 times faster.\n- So if that's typical for programmers,\nthen you're already seeing another kind\nof recursive self-improvement, right?\nBecause previously,\nlike a major generation of\nimprovement of the codes\nwould happen on the\nhuman R and D time scale.\nAnd now if that's five times shorter,\nthen it's gonna take five times less time\nthan it otherwise would to develop\nthe next level of these tools, and so on.\nSo this is exactly the sort of beginning\nof an intelligence explosion.\nThere can be humans in the\nloop a lot in the early stages,\nand then eventually humans\nare needed less and less\nand the machines can\nmore kind of go alone.\nBut what you said there\nis just an exact example\nof these sort of things.\nAnother thing which,\nI was kind of lying on my\npsychiatrist imagining,\nI'm on a psychiatrist couch here saying,\n\"Well what are my fears\nthat people would do\nwith AI systems?\"\nSo I mentioned three\nthat I had fears about\nmany years ago, that they would do,\nnamely teach it to code,\nconnect it to the internet,\nand teach it to manipulate humans.\nA fourth one is building an API,\n(Lex chuckles)\nwhere code can control this\nsuper powerful thing, right?\nThat's very unfortunate because one thing\nthat systems like GPT-4\nhave going for them\nis that they are an oracle in the sense\nthat they just answer questions.\nThere's no robot connected to GPT-4.\nGPT-4 can't go and do stock trading\nbased on its thinking.\nIt is not an agent,\nand an intelligent agent is something\nthat takes in information from the world,\nprocesses it,\nto figure out what action to take\nbased on its goals that it has,\nand then does something back on the world.\nBut once you have an API for,\nfor example, GPT-4,\nnothing stops Joe Schmoe\nand a lot of other people\nfrom building real agents,\nwhich just keep making calls somewhere\nin some inner loop somewhere\nto these powerful oracle systems,\nwhich makes themselves much more powerful.\nThat's another kind of\nunfortunate development,\nwhich I think we would've\nbeen better off delaying.\nI don't wanna pick on\nany particular companies,\nI think they're all under a\nlot of pressure to make money.\n- [Lex] Yeah.\n- And again, the reason we're\nwe're calling for this pause\nis to give them all cover to do\nwhat they know is the right thing,\njust slow down a little bit at this point.\nBut everything we've talked about,\nI hope we'll make it clear\nto people watching this,\nyou know, why these sort\nof human-level tools\ncan cause a gradual acceleration.\nYou keep using yesterday's technology\nto build tomorrow's technology.\nAnd when you do that over and over again,\nyou naturally get an explosion.\nYou know, that's the definition\nof an explosion in science, right?\nIf you have two people,\nand they fall in love,\nnow you have four people,\nand then they can make more babies,\nand now you have eight people,\nand then you have 16, 32, 64, et cetera.\nWe call that a population explosion\nwhere it's just that each,\nif it's instead free neutrons\nin a nuclear reaction\nthat if each one can make more than one,\nthen you get an\nexponential growth in that,\nwe call it a nuclear explosion.\nAll explosions are like that,\nand an intelligence explosion,\nit's just exactly the same principle,\nthat some amount of intelligence\ncan make more intelligence than that,\nand then repeat.\nYou always get exponentials.\n- What's your intuition why it does,\nyou mentioned there's\nsome technical reasons\nwhy it doesn't stop at a certain point.\nWhat's your intuition?\nAnd do you have any\nintuition why it might stop?\n- It's obviously gonna stop\nwhen it bumps up against\nthe laws of physics.\nThere are some things you just can't do\nno matter how smart you are, right?\n- Allegedly.\n'Cause we don't know all the full\nlaws of physics yet, right?\n- Seth Lloyd wrote a really cool paper\non the physical limits on\ncomputation, for example.", "mimetype": "text/plain", "start_char_idx": 58320, "end_char_idx": 62302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "020e0143-9083-4318-9da3-0b8a5b692f9b": {"__data__": {"id_": "020e0143-9083-4318-9da3-0b8a5b692f9b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea48cd56-20e5-4dc2-bbb7-4161fcbd6256", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5872f72513332cf9675bd6b2e297789ce186bdbc0c276c64b35f8a554970f6a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43366bb9-8544-4deb-bc36-1a5a5807974a", "node_type": "1", "metadata": {}, "hash": "813ec0c836663b9de2f47977e5d3b5c4a31fbbdc521530076640149697c3836b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All explosions are like that,\nand an intelligence explosion,\nit's just exactly the same principle,\nthat some amount of intelligence\ncan make more intelligence than that,\nand then repeat.\nYou always get exponentials.\n- What's your intuition why it does,\nyou mentioned there's\nsome technical reasons\nwhy it doesn't stop at a certain point.\nWhat's your intuition?\nAnd do you have any\nintuition why it might stop?\n- It's obviously gonna stop\nwhen it bumps up against\nthe laws of physics.\nThere are some things you just can't do\nno matter how smart you are, right?\n- Allegedly.\n'Cause we don't know all the full\nlaws of physics yet, right?\n- Seth Lloyd wrote a really cool paper\non the physical limits on\ncomputation, for example.\nIf you make it,\nput too much energy into it\nand the finite space will\nturn into a black hole,\nyou can't move information around\nfaster than the speed of\nlight, stuff like that.\nBut it's hard to store\nway more than a modest number\nof bits per atom, et cetera.\nBut, you know, those limits\nare just astronomically above,\nlike 30 orders of magnitude\nabove where we are now.\nSo, you know.\nBigger difference, bigger\njump in intelligence\nthan if you go from ant to a human.\nI think,\nof course what we want to do\nis have a controlled thing,\nin a nuclear reactor you put moderators in\nto make sure exactly\nit doesn't blow up out of control, right?\nWhen we do,\nexperiments with biology\nand cells and so on,\nyou know, we also try to make sure\nit doesn't get out of control.\nWe can do this with AI too.\nThe thing is, we haven't succeeded yet.\nAnd Moloch is exactly doing the opposite.\nJust fueling, just egging everybody on,\n\"Faster, faster, faster,\nor the other company is\ngonna catch up with you,\nor the other country is\ngonna catch up with you.\"\nWe have to want to stop,\nand I don't believe in just asking people\nto look into their hearts\nand do the right thing.\nIt's easier for others to say that,\nbut like, if you are in this situation\nwhere your company is gonna get screwed\nby other companies that are not stopping,\nyou're putting people in\na very hard situation,\nthe right thing to do\nis change the whole\nincentive structure instead.\nAnd this is not an old,\nmaybe I should say one\nmore thing about this,\n'cause Moloch has been around\nas humanity's number\none or number two enemy\nsince the beginning of civilization.\nAnd we came up with some\nreally cool countermeasures.\nLike first of all,\nalready over 100,000 years ago,\nevolution realized that\nit was very unhelpful\nthat people kept killing\neach other all the time.\nSo it genetically gave us compassion\nand made it so that,\nlike if you get two drunk dudes\ngetting into a pointless bar fight,\nthey might give each other black eyes,\nbut they have a lot of inhibition\ntowards just killing each other.\nThat's a,\nAnd similarly, if you find\na baby lying on the street,\nwhen you go out for your\nmorning jog tomorrow,\nyou're gonna stop and pick it up, right?\nEven though it maybe make you\nlate for your next podcast.\nSo evolution gave us these genes\nthat make our own egoistic incentives\nmore aligned with what's good\nfor the greater group\nwe're part of, right?\nAnd then as we got a\nbit more sophisticated\nand developed language,\nwe invented gossip,\nwhich is also a fantastic\nanti-Moloch, right?\n'Cause now,\nit really discourages\nliars, moochers, cheaters,\nbecause their own incentive\nnow is not to do this\nbecause word quickly gets around\nand then suddenly people\naren't gonna invite them\nto their dinners anymore or trust them.\nAnd then when we got\nstill more sophisticated\nin bigger societies,\nyou know, we invented the legal system\nwhere even strangers who\ncouldn't rely on gossip\nand things like this\nwould treat each other,\nwould have an incentive.\nNow those guys in the bar fights,\neven if someone is so drunk\nthat he actually wants\nto kill the other guy,\nhe also has a little thought\nin the back of his head that,\nyou know, \"Do I really wanna\nspend the next 10 years\neating like really crappy\nfood in a small room?\nI'm just gonna chill out,\" you know?", "mimetype": "text/plain", "start_char_idx": 61577, "end_char_idx": 65601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "43366bb9-8544-4deb-bc36-1a5a5807974a": {"__data__": {"id_": "43366bb9-8544-4deb-bc36-1a5a5807974a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "020e0143-9083-4318-9da3-0b8a5b692f9b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b9022d4926868a5ae2d273515b2c0698e783c092c7b9d626e21eb29dcba7e368", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18ee40e9-96f6-44df-b8a7-cf95e5108046", "node_type": "1", "metadata": {}, "hash": "01740419d883e89af8ebbff3fd9668fe6e34e5e65485d40280d97b9fa1603840", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "'Cause now,\nit really discourages\nliars, moochers, cheaters,\nbecause their own incentive\nnow is not to do this\nbecause word quickly gets around\nand then suddenly people\naren't gonna invite them\nto their dinners anymore or trust them.\nAnd then when we got\nstill more sophisticated\nin bigger societies,\nyou know, we invented the legal system\nwhere even strangers who\ncouldn't rely on gossip\nand things like this\nwould treat each other,\nwould have an incentive.\nNow those guys in the bar fights,\neven if someone is so drunk\nthat he actually wants\nto kill the other guy,\nhe also has a little thought\nin the back of his head that,\nyou know, \"Do I really wanna\nspend the next 10 years\neating like really crappy\nfood in a small room?\nI'm just gonna chill out,\" you know?\nAnd we similarly have tried to give\nthese incentives to our corporations\nby having regulation and\nall sorts of oversight\nso that their incentives are\naligned with the greater good.\nWe tried really hard,\nand the big problem\nthat we're failing now,\nis not that we haven't tried before,\nbut it's just that the tech is growing,\nis developing much faster\nthan the regulators been\nable to keep up, right?\nSo regulators,\nit's kind of comical that\nthe European Union right now\nis doing this AI act, right?\nAnd in the beginning they had\na little opt-out exception\nthat GPT-4 would be completely\nexcluded from regulation.\nBrilliant idea.\n- What's the logic behind that?\n- Some lobbyists pushed\nsuccessfully for this?\nSo we were actually quite involved\nwith the Future of Life Institute,\nMark Brakel, Risto Uuk,\nAnthony Aguirre, and others,\nyou know, we're quite\ninvolved with talking to,\neducating various people\ninvolved in this process\nabout these general-purpose\nAI models coming,\nand pointing out that they\nwould become the laughing stock\nif they didn't put it in.\nSo the French started pushing for it,\nit got put in to the draft,\nand it looked like all was good,\nand then there was a huge\ncounter push from lobbyists.\nYeah, there were more\nlobbyists in Brussels\nfrom tech companies than from\noil companies, for example.\nAnd it looked like it might,\nthis was gonna maybe get taken out again.\nAnd now GPT-4 happened,\nand I think it's gonna stay in.\nBut this just shows, you know,\nMoloch can be defeated.\nBut the challenge we're\nfacing is that the tech\nis generally much faster than\nwhat the policymakers are,\nand a lot of the policymakers\nalso don't have a tech background,\nso it's, you know,\nwe really need to work\nhard to educate them\non what's taking place here.\nSo we're getting this situation\nwhere the first kind of,\nso I define artificial intelligence\njust as non-biological\nintelligence, right?\nAnd by that definition,\na company, a corporation is\nalso an artificial intelligence\nbecause the corporation isn't its humans,\nit's a system.\nIf its CEO decides,\nif a CEO of a tobacco\ncompany decides one morning\nthat she or he doesn't wanna\nsell cigarettes anymore,\nthey'll just put another CEO in there.\nIt's not enough to align\nthe incentives of individual people\nor align individual computers'\nincentives to their owners,\nwhich is what technically,\nAI safety research is about.\nYou also have to align the\nincentives of corporations\nwith the greater good.\nAnd some corporations have\ngotten so big and so powerful\nvery quickly that in many cases,\ntheir lobbyists instead\nalign the regulators\nto what they want rather\nthan the other way round.\nIt's a classic regulatory capture.\n- Right, is the thing that\nthe slowdown hopes to achieve\nis give enough time to\nregulators to catch up,\nor enough time to the companies themselves\nto breathe and understand how to do\nAI safety correctly?\n- I think both,\nbut I think that the vision,\nthe path to success I see\nis first you give a breather\nactually to the people in these companies,\ntheir leadership who wants\nto do the right thing,\nand they all have safety teams and so on,\non their companies,\ngive them a chance to get\ntogether with the other companies,\nand the outside pressure can\nalso help catalyze that, right?\nAnd work out what is it that's,\nwhat are the reasonable\nsafety requirements\none should put on future systems\nbefore they get rolled out.", "mimetype": "text/plain", "start_char_idx": 64838, "end_char_idx": 68993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18ee40e9-96f6-44df-b8a7-cf95e5108046": {"__data__": {"id_": "18ee40e9-96f6-44df-b8a7-cf95e5108046", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43366bb9-8544-4deb-bc36-1a5a5807974a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a574ccba534faa21342d6d66408b54270d4b728521762d08cc24934e357edc9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7182ef11-99e8-412f-a05a-77f5244934d8", "node_type": "1", "metadata": {}, "hash": "03a63aca2fe243cf6cf0624ce9f2ebfd388d3914ee111ab7efb11f752fefaed1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's a classic regulatory capture.\n- Right, is the thing that\nthe slowdown hopes to achieve\nis give enough time to\nregulators to catch up,\nor enough time to the companies themselves\nto breathe and understand how to do\nAI safety correctly?\n- I think both,\nbut I think that the vision,\nthe path to success I see\nis first you give a breather\nactually to the people in these companies,\ntheir leadership who wants\nto do the right thing,\nand they all have safety teams and so on,\non their companies,\ngive them a chance to get\ntogether with the other companies,\nand the outside pressure can\nalso help catalyze that, right?\nAnd work out what is it that's,\nwhat are the reasonable\nsafety requirements\none should put on future systems\nbefore they get rolled out.\nThere are a lot of people also in academia\nand elsewhere outside of these companies\nwho can be brought into this\nand have a lot of very good ideas.\nAnd then I think it's very\nrealistic that within six months,\nyou can get these people coming up,\nso here's a white paper,\nhere's what we all think it's reasonable.\nYou know, you didn't,\njust because cars killed a lot of people,\nyou didn't ban cars,\nbut they got together a bunch of people\nand decided, you know,\nin order to be allowed to sell a car,\nit has to have a seatbelt in it.\nThey're the analogous things\nthat you can start requiring\na future AI systems\nso that they are safe.\nAnd once this heavy lifting,\nthis intellectual work has been done\nby experts in the field,\nwhich can be done quickly,\nI think it's going to be quite easy\nto get policymakers to see,\nyeah, this is a good idea.\nAnd it's, you know,\nfor the companies to fight Moloch,\nthey want, and I believe Sam Altman\nhas explicitly called for this,\nthey want the regulators\nto actually adopt it\nso that their competition\nis gonna abide by it too, right?\nYou don't want,\nyou don't want to be\nenacting all these principles\nand then you abide by them,\nand then there's this one little company\nthat doesn't sign onto it\nand then now they can\ngradually overtake you.\nThen the companies will get,\nbe able to sleep secure knowing\nthat everybody's playing\nby the same rules.\n- So do you think it's\npossible to develop guardrails\nthat keep the systems\nfrom basically damaging\nirreparably humanity,\nwhile still enabling sort\nof the capitalist-fueled\ncompetition between companies\nas they develop how to best\nmake money with this AI?\nYou think there's a\nbalancing that's possible?\n- Absolutely, I mean, we've seen that\nin many other sectors\nwhere you've had the free market\nproduce quite good things\nwithout causing particular harm.\nWhen the guardrails are there\nand they work, you know,\ncapitalism is a very\ngood way of optimizing\nfor just getting the same\nthings done more efficiently.\nBut it was good, you know,\nand like in hindsight,\nand I never met anyone,\neven on parties way over on the right,\nin any country who think it was a bad,\nthinks it was a terrible idea\nto ban child labor, for example.\n- Yeah, but it seems like\nthis particular technology\nhas gotten so good so fast,\nbecome powerful to a\ndegree where you could see\nin the near term,\nthe ability to make a lot of money.\n- [Max] Yeah.\n- And to put guardrails,\nto develop guardrails quickly\nin that kind of context\nseems to be tricky.\nIt's not similar to cars or child labor,\nit seems like the opportunity\nto make a lot of money here\nvery quickly is right here before us.\n- So again, there's this cliff.\n- Yeah, it gets quite scenic, (laughs)\n- [Max] The closer to the cliff you go,\n- Yeah.\n- The more money there is,\nthe more gold ingots\nthere are on the ground\nyou can pick up or whatever,\nif you want to drive there very fast,\nbut it's not in anyone's incentive\nthat we go over the cliff\nand it's not like\neverybody's in the wrong car.\nAll the cars are connected\ntogether with a chain.\nSo if anyone goes over,\nthey'll start dragging\nthe others down too.\nAnd so ultimately it's in the selfish\ninterests also of the\npeople in the companies\nto slow down when you just start seeing\nthe contours of the cliff\nthere in front of you, right?", "mimetype": "text/plain", "start_char_idx": 68241, "end_char_idx": 72298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7182ef11-99e8-412f-a05a-77f5244934d8": {"__data__": {"id_": "7182ef11-99e8-412f-a05a-77f5244934d8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18ee40e9-96f6-44df-b8a7-cf95e5108046", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "20ef3ce6e37249943f12333aa6bedb6d58abc8131ffa0a67c63877972b4d5f9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf2b6151-a3f4-4312-b63c-476e7b811471", "node_type": "1", "metadata": {}, "hash": "ca3a60e8fb5e77f04e44a7ba18055c54abed19c9d39aa19b5cde3cf63490ca13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So again, there's this cliff.\n- Yeah, it gets quite scenic, (laughs)\n- [Max] The closer to the cliff you go,\n- Yeah.\n- The more money there is,\nthe more gold ingots\nthere are on the ground\nyou can pick up or whatever,\nif you want to drive there very fast,\nbut it's not in anyone's incentive\nthat we go over the cliff\nand it's not like\neverybody's in the wrong car.\nAll the cars are connected\ntogether with a chain.\nSo if anyone goes over,\nthey'll start dragging\nthe others down too.\nAnd so ultimately it's in the selfish\ninterests also of the\npeople in the companies\nto slow down when you just start seeing\nthe contours of the cliff\nthere in front of you, right?\nAnd the problem is that,\neven though the people who\nare building the technology,\nand the CEOs, they really get it,\nthe shareholders and\nthese other market forces,\nthey are people who don't honestly,\nunderstand that the cliff is there,\nthey usually don't.\nYou have to get quite into the weeds\nto really appreciate how\npowerful this is and how fast.\nAnd a lot of people are\neven still stuck again\nin this idea that\nin this \"carbon chauvinism\"\nas I like to call it,\nthat you can only have our\nlevel of intelligence in humans,\nthat there's something magical about it.\nWhereas the people in the tech companies\nwho build this stuff,\nthey all realize that intelligence\nis information processing\nof a certain kind,\nand it really doesn't matter at all\nwhether the information is\nprocessed by carbon atoms\nin neurons, in brains,\nor by silicon atoms in\nsome technology we build.\nSo you brought up capitalism earlier,\nand there are a lot of\npeople who love capitalism\nand a lot of people who\nreally, really don't.\nAnd it struck me recently,\nthat what's happening with capitalism here\nis exactly analogous to the way\nin which superintelligence\nmight wipe us out.\nDo you know why I studied\neconomics for my undergrad?\nStockholm School of Economics, yay.\n(Lex laughing)\n- Well, no.\nNo why, tell me.\n- So I was very interested in how\nyou could use market forces\nto just get stuff done more efficiently,\nbut give the right incentives to market\nso that it wouldn't do really bad things.\nSo Dylan Hadfield-Menell,\nwho's a professor and\ncolleague of mine at MIT,\nwrote this really interesting paper\nwith some collaborators recently,\nwhere they proved mathematically\nthat if you just take one goal\nthat you just optimize for,\non and on, and on, indefinitely,\nthat you think is gonna bring\nyou in the right direction,\nwhat basically always happens is,\nin the beginning, it will\nmake things better for you,\nbut if you keep going, at some point,\nit's gonna start making\nthings worse for you again.\nAnd then gradually,\nit's gonna make it\nreally, really terrible.\nSo just as a simple,\nthe way I think of the proof is,\nsuppose you want to go from\nhere back to Austin for example,\nand you're like,\n\"Okay, yeah, let's go south,\"\nbut you put in exactly sort\nof the right direction.\nJust optimize that, as south as possible.\nYou get closer and closer to Austin,\nbut there's always some little error.\nSo you're not going\nexactly towards Austin,\nbut you get pretty close,\nbut eventually, you\nstart going away again,\nand eventually, you're gonna\nbe leaving the solar system.\n- [Lex] (chuckles) Yeah.\n- And they proved,\nit's a beautiful mathematical proof,\nthis happens generally,\nand this is very important for AI because,\neven though Stuart\nRussell has written a book\nand given a lot of talks\non why it's a bad idea\nto have AI just blindly\noptimize something,\nthat's what pretty much\nall our systems do.\n- [Lex] Yeah.\n- We have something\ncalled the loss function\nthat we're just minimizing,\nor reward function, we're\njust maximizing, and,\ncapitalism is exactly like that too.\nWe wanted to get stuff\ndone more efficiently,\nthe people wanted.\nSo introduce the free market.\nThings got done much more\nefficiently than they did\nin say, communism, right?\nAnd it got better.", "mimetype": "text/plain", "start_char_idx": 71634, "end_char_idx": 75533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf2b6151-a3f4-4312-b63c-476e7b811471": {"__data__": {"id_": "cf2b6151-a3f4-4312-b63c-476e7b811471", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7182ef11-99e8-412f-a05a-77f5244934d8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a371f3a0ab0d88d14d54ce341e71cb82a82cf20916f24a2e21368838ea40fa6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59a11f4a-4941-4fcc-9d15-31494c7edaa1", "node_type": "1", "metadata": {}, "hash": "b7828ae16e11fc03519f2dbcef58cf118f366ce42be05863cbf9932d316dcbc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Lex] (chuckles) Yeah.\n- And they proved,\nit's a beautiful mathematical proof,\nthis happens generally,\nand this is very important for AI because,\neven though Stuart\nRussell has written a book\nand given a lot of talks\non why it's a bad idea\nto have AI just blindly\noptimize something,\nthat's what pretty much\nall our systems do.\n- [Lex] Yeah.\n- We have something\ncalled the loss function\nthat we're just minimizing,\nor reward function, we're\njust maximizing, and,\ncapitalism is exactly like that too.\nWe wanted to get stuff\ndone more efficiently,\nthe people wanted.\nSo introduce the free market.\nThings got done much more\nefficiently than they did\nin say, communism, right?\nAnd it got better.\nBut then it just kept optimizing,\nand kept optimizing,\nand you got every bigger companies,\nand every more efficient\ninformation processing\nand now also very much powered by IT,\nand eventually a lot of\npeople are beginning to feel,\n\"Wait, we're kind of\noptimizing a bit too much.\nLike why did we just chop\ndown half the rainforest?\"\nYou know,\nand why did suddenly these\nregulators get captured\nby lobbyists and so on?\nIt's just the same optimization\nthat's been running for too long.\nIf you have an AI that actually\nhas power over the world\nand you just give it one goal,\nand just like keep optimizing that,\nmost likely everybody's gonna be like,\n\"Yay, this is great.\"\nIn the beginning things\nare getting better,\nbut it's almost impossible to give it\nexactly the right\ndirection to optimize in.\nAnd then eventually all\nhell breaks loose, right?\nNick Bostrom and others\nhave given examples\nthat sound quite silly,\nLike what if you just want to like,\ntell it to cure cancer or something,\nand that's all you tell it,\nmaybe it's gonna decide to\ntake over an entire continent\njust so we can get more\nsupercomputer facilities in there,\nand figure out how to\ncure cancer backwards,\nand then you're like,\n\"Wait, that's not what I wanted,\" right?\nAnd the issue with capitalism\nand the issue with runaway\nAI have kind of merged now,\nbecause that Moloch I talked about\nis exactly the capitalist Moloch that,\nwe have built an economy\nthat is optimizing for only one thing.\nProfit, right?\nAnd that worked great\nback when things were very inefficient,\nand then now it's getting done better,\nand it worked great as\nlong as the companies\nwere small enough that they\ncouldn't capture the regulators.\nBut that's not true anymore,\nbut they keep optimizing,\nand now they realize that they can,\nthese companies can make even more profit\nby building ever more powerful\nAI even if it's reckless,\nbut optimize more and more,\nand more, and more, and more.\nSo this is Moloch again showing up.\nAnd I just wanna,\nanyone here who has any concerns\nabout late-stage capitalism\nhaving gone a little too far,\nyou should worry about superintelligence\n'cause it's the same\nvillain in both cases.\nIt's Moloch.\n- And optimizing one objective\nfunction aggressively,\nblindly is going to take us there.\n- Yeah, we have to pause from time to time\nand look into our hearts\nand ask why are we doing this?\nIs this, am I still going towards Austin,\nor have I gone too far?\nYou know, maybe we\nshould change direction.\n- And that is the idea behind\nthe halt for six months.\nWhy six months?\nThat seems like a very short period.\nCan we just linger and\nexplore different ideas here,\nbecause this feels like a really important\nmoment in human history,\nwhere pausing would actually have\na significant positive effect.\n- We said six months,\nbecause we figured the number one pushback\nthat we're gonna get in the\nWest was like, \"But China?\"\nand everybody knows\nthere's no way that China\nis gonna catch up with the\nWest on this in six months.\nSo that argument goes off the table\nand you can forget about\ngeopolitical competition\nand just focus on the real issue.\nThat's why we put this.\n- That's really interesting.\nBut you've are already made\nthe case that even for China,\nif you actually wanna\ntake on that argument,\nChina too would not be\nbothered by a longer halt\nbecause they don't wanna lose control\neven more than the West doesn't.\n- That's what I think, yeah.\n- That's a really interesting argument.", "mimetype": "text/plain", "start_char_idx": 74840, "end_char_idx": 78985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59a11f4a-4941-4fcc-9d15-31494c7edaa1": {"__data__": {"id_": "59a11f4a-4941-4fcc-9d15-31494c7edaa1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf2b6151-a3f4-4312-b63c-476e7b811471", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6d796906008cf16ae894ea212795a5c680f95eb9c5a9f0e83a0681028fc5215d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7aab407-8de8-4058-ace9-99672de4f921", "node_type": "1", "metadata": {}, "hash": "5bdb774fc42f82ca0ed3823d80bb3e7cd76a954777a628a7eb480347a154aa81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- We said six months,\nbecause we figured the number one pushback\nthat we're gonna get in the\nWest was like, \"But China?\"\nand everybody knows\nthere's no way that China\nis gonna catch up with the\nWest on this in six months.\nSo that argument goes off the table\nand you can forget about\ngeopolitical competition\nand just focus on the real issue.\nThat's why we put this.\n- That's really interesting.\nBut you've are already made\nthe case that even for China,\nif you actually wanna\ntake on that argument,\nChina too would not be\nbothered by a longer halt\nbecause they don't wanna lose control\neven more than the West doesn't.\n- That's what I think, yeah.\n- That's a really interesting argument.\nLike I have to actually\nreally think about that,\nwhich, the kind of thing people assume\nis if you develop an AGI, that OpenAI,\nif they're the ones\nthat do it, for example,\nthey're going to win.\nBut you're saying no, everybody loses.\n- Yeah, it's gonna get\nbetter and better and better,\nand then kaboom, we all lose.\nThat's what's gonna happen.\n- When lose and win\nare defined on a metric\nof basically quality of\nlife for human civilization,\nand for Sam Altman.\n(laughs) Both.\n- To be blunt, my personal guess,\nyou know, and people\ncan quibble with this,\nis that we're just gonna,\nthere won't be any humans.\nThat's it, that's what I mean by lose.\nYou know, if you,\nwe can see in history,\nonce you have some species\nor some group of people\nwho aren't needed anymore,\ndoesn't usually work out\nso well for them, right?\n- [Lex] Yeah.\n- There were a lot of horses\nthat were used for traffic\nin Boston and then the car got invented\nand most of them got,\nyeah, well. (laughs)\nWe don't need to go there.\nAnd if you look at\nhumans, you know,\nright now, why did the\nlabor movement succeed?\nAnd after the Industrial Revolution?\nBecause it was needed.\nEven though we had a lot of Molochs,\nand there was child labor\nand so on, you know,\nthe company still needed to have workers,\nand that's why strikes\nhad power and so on.\nIf we get to the point\nwhere most humans aren't needed anymore,\nI think it's quite naive to think\nthat they're gonna still be treated well.\nYou know, we say that.\nYeah, yeah everybody's equal,\nand the government will\nalways protect them.\nBut if you look in practice,\ngroups that are very disenfranchised\nand don't have any actual\npower usually get screwed.\nAnd now in the beginning,\nso Industrial Revolution,\nwe automated away muscle work,\nbut that got,\nworked out pretty well eventually,\nbecause we educated ourselves\nand started working\nwith our brains instead\nand got usually more\ninteresting, better paid jobs.\nBut now we're beginning\nto replace brain work.\nSo we replaced a lot of boring stuff,\nlike we got the pocket calculator,\nso you don't have people adding,\nmultiplying numbers anymore at work.\nFine, there were better\njobs they could get.\nBut now GPT-4, you know,\nand the Stable Diffusion\nand techniques like this,\nthey're really beginning to blow away\nsome jobs that people really loved having.\nThere was a heartbreaking\narticle post just yesterday\non social media I saw,\nabout this guy who was doing 3D modeling\nfor gaming and he,\nand all of a sudden now\nhe got this new software\nhe just sets prompts,\nand he feels this whole job that he loved\njust lost its meaning, you know?\nAnd I asked GPT-4 to rewrite\n\"Twinkle, Twinkle, Little Star\"\nin the style of Shakespeare,\nI couldn't have done such a good job.\nIt was really impressive.\nYou've seen a lot of the\nart coming out here, right?\nSo I'm all for automating\naway the dangerous jobs\nand the boring jobs.\nBut I think you hear a lot,\nsome arguments which are too glib.\nSometimes people say,\n\"Well that's all that's gonna happen.\nWe're getting rid of the boring,\ntedious, dangerous jobs,\"\nit's just not true.\nThere are a lot of really interesting jobs\nthat are being taken away now.\nJournalism is gonna get crushed,\ncoding is gonna get crushed.\nI predict the job market for programmers,\nthe salaries are gonna start dropping.", "mimetype": "text/plain", "start_char_idx": 78299, "end_char_idx": 82281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7aab407-8de8-4058-ace9-99672de4f921": {"__data__": {"id_": "c7aab407-8de8-4058-ace9-99672de4f921", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59a11f4a-4941-4fcc-9d15-31494c7edaa1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8704aa5181f0d2238060d4217251a66a136f2a5203f833b1af4fc091ee848aa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7640ff44-d388-45f9-9e92-fb74f7bad5dd", "node_type": "1", "metadata": {}, "hash": "16feac428c0cbaa8af261f4afd92bc6b41c5fec1446f9a1747e8c5351b1b7468", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I asked GPT-4 to rewrite\n\"Twinkle, Twinkle, Little Star\"\nin the style of Shakespeare,\nI couldn't have done such a good job.\nIt was really impressive.\nYou've seen a lot of the\nart coming out here, right?\nSo I'm all for automating\naway the dangerous jobs\nand the boring jobs.\nBut I think you hear a lot,\nsome arguments which are too glib.\nSometimes people say,\n\"Well that's all that's gonna happen.\nWe're getting rid of the boring,\ntedious, dangerous jobs,\"\nit's just not true.\nThere are a lot of really interesting jobs\nthat are being taken away now.\nJournalism is gonna get crushed,\ncoding is gonna get crushed.\nI predict the job market for programmers,\nthe salaries are gonna start dropping.\nYou know, if you said you\ncan code five times faster,\nyou know, then you need five\ntimes fewer programmers,\nmaybe there will be more output also,\nbut then you'll still end up using fewer,\nneeding fewer programmers than today.\nAnd I love coding,\nyou know, I think it's super cool.\nSo we need to stop and ask ourselves\nwhy again are we doing\nthis as humans, right?\nI feel that AI should be built\nby humanity for humanity,\nand let's not forget that.\nIt shouldn't be by Moloch for Moloch,\nor what it really is now is\nkind of by humanity for Moloch,\nwhich doesn't make any sense.\nIt's for us that we're doing it.\nAnd it would make a lot more sense\nif we build, develop,\nfigure out gradually,\nsafely how to make all this tech,\nand then we think about\nwhat are the kind of jobs\nthat people really don't want to have,\nyou know, automate them all away.\nAnd then we ask what are the jobs\nthat people really find meaning in,\nlike maybe taking care of\nchildren in the daycare center,\nmaybe doing art, et cetera, et cetera.\nAnd even if it were possible\nto automate that away,\nwe don't need to do that, right?\nWe built these machines.\n- Well it's possible that we redefine\nor rediscover what are the\njobs that give us meaning.\nSo for me, the thing, it is really sad.\nLike I, (chuckles)\nhalf the time I'm excited,\nhalf the time I'm crying\nas I'm generating code\nbecause I kind of love programming.\nIt's the act of creation,\nYou have an idea, you design it,\nand then you bring it to life,\nand it does something.\nEspecially if there's\nsome intelligence to it,\nit doesn't even have to have intelligence.\nPrinting \"Hello world\" on screen.\nYou made a little machine\nand it it comes to life.\n- [Max] Yeah.\n- And there's a bunch of\ntricks you learn along the way\n'cause you've been doing\nit for many, many years.\nAnd then for to see AI be able to generate\nall the tricks you thought were special.\nI don't know, it's very,\nit's scary, it's almost painful.\nLike a loss of innocence maybe,\nlike maybe when I was younger,\nI remember before I learned\nthat sugar is bad for you,\nyou should be on a diet.\nI remember I enjoyed candy deeply,\nin a way I just can't anymore,\nthat I know is bad for me.\nI enjoyed it unapologetically,\nfully, just intensely.\nAnd I lost that.\nNow, I feel like a little\nbit of that is lost,\nor being lost with programming,\nsimilar as it is for the 3D modeler\nno longer being able\nto really enjoy the art\nof modeling 3D things for gaming.\nI don't know what to make sense of that.\nMaybe I would rediscover\nthat the true magic\nof what it means to be humans\nis connecting with other humans,\nto have conversations like this,\nI don't know, to have sex,\nto eat food,\nto really intensify the value\nfrom conscious experiences,\nversus like creating other stuff.\n- You're pitching the rebranding again\nfrom Homo sapiens to Homo sentiens,\nthe meaningful experiences.\nAnd just to a inject some\noptimism in this here,\nso we don't sound like it was a gloomers.\nYou know, we can totally\nhave our cake and eat it.\nYou hear a lot of totally bullshit claims\nthat we can't afford having more teachers,\nhave to cut the number of nurses,\nyou know, that's just nonsense, obviously.\nWith anything even quite far short of AGI,\nwe can dramatically improve, grow the GDP,\nand produce this wealth\nof goods and services.", "mimetype": "text/plain", "start_char_idx": 81585, "end_char_idx": 85569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7640ff44-d388-45f9-9e92-fb74f7bad5dd": {"__data__": {"id_": "7640ff44-d388-45f9-9e92-fb74f7bad5dd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7aab407-8de8-4058-ace9-99672de4f921", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4961459724110e63461947f13bc01c7b2eda3247265c423f4bec4d4331b0ee28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1047e0cf-760c-4328-a1f7-bb8e84f77d55", "node_type": "1", "metadata": {}, "hash": "949b10e92925f283ba65989dbfe279eb0aee7f3f1e64ee6cf1d53785cc4b378f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I don't know what to make sense of that.\nMaybe I would rediscover\nthat the true magic\nof what it means to be humans\nis connecting with other humans,\nto have conversations like this,\nI don't know, to have sex,\nto eat food,\nto really intensify the value\nfrom conscious experiences,\nversus like creating other stuff.\n- You're pitching the rebranding again\nfrom Homo sapiens to Homo sentiens,\nthe meaningful experiences.\nAnd just to a inject some\noptimism in this here,\nso we don't sound like it was a gloomers.\nYou know, we can totally\nhave our cake and eat it.\nYou hear a lot of totally bullshit claims\nthat we can't afford having more teachers,\nhave to cut the number of nurses,\nyou know, that's just nonsense, obviously.\nWith anything even quite far short of AGI,\nwe can dramatically improve, grow the GDP,\nand produce this wealth\nof goods and services.\nIt's very easy to create a world\nwhere everybody is better off than today.\nIncluding the richest people\ncan be better off as well, right?\nIt's not a zero sum game,\nyou know, technology.\nAgain, you can have two countries,\nlike Sweden and Denmark had\nall these ridiculous wars\ncentury after century,\nand sometimes that Sweden\ngot a little better off\n'cause it got a little bigger,\nand then Denmark got a\nlittle bit better off\n'cause Sweden got a little bit smaller,\nbut then technology came along\nand we both got just\ndramatically wealthier\nwithout taking away from anyone else,\nso it was just a total win for everyone.\nAnd AI can do that on steroids.\nif you can build safe AGI,\nif you can build superintelligence,\nbasically all the limitations\nthat cause harm today\ncan be completely eliminated. Right?\nIt's a wonderful possibility.\nAnd this is not sci-fi,\nthis is something which\nis clearly possible\naccording to laws of physics,\nAnd we can talk about ways\nof making it safe also,\nbut unfortunately that'll only happen\nif we steer in that direction,\nthat's absolutely not the default outcome.\nThat's why income\ninequality keeps going up.\nThat's why the life expectancy in the US\nhas been going down now,\nI think it's four years in a row.\nI just read a heartbreaking study from CDC\nabout how something like\n1/3 of all the teenage girls\nin the US have been\nthinking about suicide.\nYou know, like those are steps\nin totally the wrong direction\nand it's important to keep\nour eyes on the prize here\nthat we can,\nwe have the power now for the first time\nin the history of our species\nto harness artificial intelligence,\nto help us really flourish,\nand help bring out the\nbest in our humanity\nrather than the worst of it.\nTo help us have really\nfulfilling experiences\nthat feel truly meaningful.\nAnd you and I shouldn't sit here\nand dictate the future\ngenerations what they will be,\nlet them figure it out.\nBut let's give them a chance to live,\nand not foreclose all these\npossibilities for them,\nby just messing things up, right?\n- Well for that, we'll have to\nsolve the AI safety problem.\nIt would be nice if we can linger\non exploring that a little bit.\nSo one interesting way to\nenter that discussion is,\nyou tweeted, and Elon replied,\nyou tweeted,\n\"Let's not just focus on whether GPT-4\nwill do more harm or\ngood on the job market,\nbut also whether it's coding skills\nwill hasten the arrival\nof superintelligence.\"\nThat's something we've\nbeen talking about, right?\nSo Elon proposed one\nthing in the reply saying,\n\"Maximum truth-seeking is my\nbest guess for AI safety.\"\nCan you maybe steel me on the case for,\nthis objective function of truth\nand maybe make an argument\nagainst it in general,\nwhat are your different ideas\nto start approaching the\nsolution to AI safety?\n- I didn't see that reply actually.\n- [Lex] Oh, interesting.\n- But I really resonate with it because,\nAI is not evil.\nIt caused people around the world\nto hate each other much more,\nbut that's because we\nmade it in a certain way.\nIt's a tool,\nwe can use it for great\nthings and bad things,\nand we could just as well have AI systems,\nand this is part of my\nvision for success here.", "mimetype": "text/plain", "start_char_idx": 84716, "end_char_idx": 88719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1047e0cf-760c-4328-a1f7-bb8e84f77d55": {"__data__": {"id_": "1047e0cf-760c-4328-a1f7-bb8e84f77d55", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7640ff44-d388-45f9-9e92-fb74f7bad5dd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7b94d9a708085f556edd600dfc18dd2668633b70f25223f2548f9f70bd22220a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5", "node_type": "1", "metadata": {}, "hash": "4c316029863b5e6157097610f3df6fc8e3ce6a796ac7b8652d0827ad24123c32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That's something we've\nbeen talking about, right?\nSo Elon proposed one\nthing in the reply saying,\n\"Maximum truth-seeking is my\nbest guess for AI safety.\"\nCan you maybe steel me on the case for,\nthis objective function of truth\nand maybe make an argument\nagainst it in general,\nwhat are your different ideas\nto start approaching the\nsolution to AI safety?\n- I didn't see that reply actually.\n- [Lex] Oh, interesting.\n- But I really resonate with it because,\nAI is not evil.\nIt caused people around the world\nto hate each other much more,\nbut that's because we\nmade it in a certain way.\nIt's a tool,\nwe can use it for great\nthings and bad things,\nand we could just as well have AI systems,\nand this is part of my\nvision for success here.\nTruth-seeking AI that really\nbrings us together again,\nyou know, why do people\nhate each other so much\nbetween countries and within countries\nis because they each\nhave totally different\nversions of the truth, right?\nIf they all had the same truth\nthat they trusted for good reason\n'cause they could check it and verify it,\nand not have to believe\nin some self-proclaimed authority, right?\nThey wouldn't be as nearly as much hate.\nThere'd be a lot more\nunderstanding instead,\nand this is,\nI think something AI can\nhelp enormously with.\nFor example, a little baby\nstep in this direction\nis this website called Metaculus\nwhere people bet and make\npredictions not for money,\nbut just for their own reputation.\nAnd it's kind of funny actually,\nyou treat the humans like you treat AI,\nas you have a loss function\nwhere they get penalized\nif they're super confident on something\nand then the opposite happens.\n- [Lex] Yeah.\n- Whereas if you're kind of humble,\nand then you're like,\n\"I think it's 51% chance\nthis is gonna happen,\"\nand then the other happens,\nyou don't get penalized much,\nand what you can see is that some people\nare much better at predicting than others.\nThey've earned your trust, right?\nOne project that I'm working on right now\nis an outgrowth of Improve\nThe News foundation\ntogether with the Metaculus folks is,\nseeing if we can really\nscale this up a lot\nwith more powerful AI.\n'Cause I would love it,\nI would love for there\nto be like a really powerful\ntruth-seeking system where,\nthat is trustworthy\nbecause it keeps being right about stuff.\nAnd people come to it\nand maybe look at its latest trust ranking\nof different pundits and\nnewspapers, et cetera.\nIf they want to know why\nsome someone got a low score,\nthey can click on it,\nand see all the predictions\nthat they actually made\nand how they turned out, you know,\nthis is how we do it in science.\nYou trust scientists like Einstein\nwho said something everybody\nthought was bullshit,\nand turned out to be right,\nhe get a lot of trust points,\nand he did it multiple times, even.\nI think AI has the power to really heal\na lot of the rifts we're seeing\nby creating a trust system.\nIt has to get away from this idea today\nwith some fact checking site,\nwhich might themselves have an agenda\nand you just trust it\nbecause of its reputation,\nyou want to have,\nso these sort of systems,\nthey earn in their trust\nand they're completely transparent.\nThis I think would actually help a lot\nthat can, I think,\nhelp heal the very\ndysfunctional conversation\nthat humanity has about\nhow it's gonna deal\nwith all its biggest challenges\nin in the world today.\nAnd then on the technical side,\nyou know, another common\nsort of gloom comment\nI get from people are saying,\n\"We're just screwed, there's no hope.\"\nIs well,\nthings like GPT-4 are way too complicated\nfor a human to ever understand,\nand prove that they can be trustworthy.\nThey're forgetting that AI can help us\nprove that things work, right?\n- [Lex] Yeah.\n- And there's this very\nfundamental fact that in math,\nit's much harder to come up with a proof\nthan it is to verify that\nthe proof is correct.\nYou can actually write a\nlittle proof-checking code,\nit's quite short,\nbut you can as a human, understand,\nand then it can check the\nmost monstrously long proof\never generated even by your computer,\nand say, \"Yeah, this is valid.\"", "mimetype": "text/plain", "start_char_idx": 87984, "end_char_idx": 92063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5": {"__data__": {"id_": "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1047e0cf-760c-4328-a1f7-bb8e84f77d55", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4a675d291e731118b4004366f00f85ac803f77cd89276f613dcf0aa14068fd82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29244be7-65c3-4bfd-ba9c-5b0c3d24a984", "node_type": "1", "metadata": {}, "hash": "29680fbfdc84ee499ae1fe10cd91c4c01f0734fef08714c340ff21dfd1be97a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then on the technical side,\nyou know, another common\nsort of gloom comment\nI get from people are saying,\n\"We're just screwed, there's no hope.\"\nIs well,\nthings like GPT-4 are way too complicated\nfor a human to ever understand,\nand prove that they can be trustworthy.\nThey're forgetting that AI can help us\nprove that things work, right?\n- [Lex] Yeah.\n- And there's this very\nfundamental fact that in math,\nit's much harder to come up with a proof\nthan it is to verify that\nthe proof is correct.\nYou can actually write a\nlittle proof-checking code,\nit's quite short,\nbut you can as a human, understand,\nand then it can check the\nmost monstrously long proof\never generated even by your computer,\nand say, \"Yeah, this is valid.\"\nSo right now,\nwe have,\nthis approach with virus-checking software\nthat it looks to see if there's something,\nand if you should not trust it,\nand if it can prove to itself\nthat you should not trust that\ncode, it warns you, right?\nWhat if you flip this around,\nand this is an idea I should give credit\nto Steve Omohundro for,\nso that it will only run\nthe code if it can prove,\ninstead of not running it\nif it can prove that it's not trustworthy,\nit will only run it\nif it can prove that it's trustworthy.\nSo it asks the code,\n\"Prove to me that you're gonna do\nwhat you say you're gonna do,\"\nand it gives you this proof,\nand you have a little proof\nthat you can check it.\nNow you can actually trust an AI\nthat's much more intelligent\nthan you are, right?\nBecause you,\nis its problem to come up with this proof\nthat you could never have found,\nthat you should trust it.\n- So this is the interesting point.\nI agree with you,\nbut this is where Eliezer Yudkowsky\nmight disagree with you.\nHis claim, not with\nyou, but with this idea.\nhis claim is superintelligent AI\nwould be able to know how to\nlie to you with such a proof.\n- I have to lie to you and give me a proof\nthat I'm gonna think is correct?\n- [Lex] Yeah.\n- But it's not me it's lying to you.\nThat's to trick my proof checker,\nwhich is a piece of code.\n- So his general idea is\na superintelligent system\ncan lie to a dumber proof checker.\nSo you're going to have,\nas a system becomes more\nand more intelligent,\nthere's going to be a threshold\nwhere a superintelligent system\nwill be able to effectively lie\nto a slightly dumber AGI system.\nLike there's a,\nlike he really focuses on this weak AGI\nto strong AGI jump,\nwhere the strong AGI can\nmake all the weak AGIs think\nthat it's just one of them,\nbut it's no longer that.\nAnd that leap is when it runs away.\n- Yeah, I don't buy that argument.\nI think no matter how\nsuperintelligent an AI is,\nit's never gonna be able to prove to me\nthat there are only finitely\nmany primes, for example.\n(Lex chuckling)\nIt just can't.\nAnd it can try to snow me\nby making up all sorts of\nnew weird rules of deduction,\nand say, \"Trust me, you know,\nthe way your proof checker\nwork is too limited,\nand we have this new\nhyper math and it's true.\"\nBut then I would just take the attitude,\nokay, I'm gonna forfeit some of these,\nthe supposedly super cool technologies,\nI'm only gonna go with the ones\nthat I can prove in my\nown trusted proof checker.\nThen I think it's fine.\nThere's still, of course,\nthis is not something anyone\nhas successfully\nimplemented at this point,\nbut I think it,\nI just give it as an example of hope,\nwe don't have to do all\nthe work ourselves, right?\nThis is exactly the sort of\nvery boring and tedious task\nthat is perfect to outsource to an AI.\nAnd this is a way in which less powerful\nand less intelligent agents like us\ncan actually continue to control\nand trust more powerful ones.\n- So build AGI systems that help us defend\nagainst other AGI systems.", "mimetype": "text/plain", "start_char_idx": 91334, "end_char_idx": 95028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29244be7-65c3-4bfd-ba9c-5b0c3d24a984": {"__data__": {"id_": "29244be7-65c3-4bfd-ba9c-5b0c3d24a984", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8d7fa8f-6618-4be5-bd45-e55dedcbb7a5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8fa5cd17b3a01338e969ffe98f9935418fb294f75a301d8240a3f41dc93c138e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f5ee24e-50bf-4804-89ba-a89b470b2050", "node_type": "1", "metadata": {}, "hash": "0c6f78ea5b6f2b72faca4fe6097ce9c95bd9bca642bedf2f2cf0486c55b2020d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But then I would just take the attitude,\nokay, I'm gonna forfeit some of these,\nthe supposedly super cool technologies,\nI'm only gonna go with the ones\nthat I can prove in my\nown trusted proof checker.\nThen I think it's fine.\nThere's still, of course,\nthis is not something anyone\nhas successfully\nimplemented at this point,\nbut I think it,\nI just give it as an example of hope,\nwe don't have to do all\nthe work ourselves, right?\nThis is exactly the sort of\nvery boring and tedious task\nthat is perfect to outsource to an AI.\nAnd this is a way in which less powerful\nand less intelligent agents like us\ncan actually continue to control\nand trust more powerful ones.\n- So build AGI systems that help us defend\nagainst other AGI systems.\n- Well for starters,\nbegin with a simple\nproblem of just making sure\nthat the system that you own\nor that's supposed to be loyal to you\nhas to prove to itself\nthat it's always gonna do\nthe things that you actually\nwant it to do, right?\nAnd if it can't prove it,\nmaybe it's still gonna do it,\nbut you won't run it.\nSo you just forfeit some aspects\nof all the cool things AI can do.\nI bet your dollars to donuts,\nit can still do some\nincredibly cool stuff for you.\n- [Lex] Yeah.\n- There are other things too,\nthat we shouldn't sweep under the rug.\nLike not every human agrees on exactly\nwhat direction we should\ngo with humanity, right?\n- Yes.\n- And you've talked a lot\nabout geopolitical things\non your podcast to this effect,\nyou know, but,\nI think that shouldn't\ndistract us from the fact\nthat there are actually a lot of things\nthat everybody in the\nworld virtually agrees on.\nThat \"Hey, you know,\nlike having a no humans on\nthe planet in a near future,\nnah, let's not do that\" right?\nYou looked at something like\nthe United Nations\nSustainable Development Goals.\nSome of 'em were quite a ambitious,\nand basically all the countries agree,\nUS, China, Russia,\nUkraine, they all agree.\nSo instead of quibbling\nabout the little things\nthat we don't agree on,\nlet's start with the things we do agree on\nand get them done.\nInstead of being so distracted\nby all these things we disagree on,\nthat Moloch wins because frankly,\nMoloch going wild now,\nit feels like a war on life\nplaying out in front of our eyes,\nif you just look at it\nfrom space, you know,\nwe're on this planet,\nbeautiful, vibrant ecosystem,\nnow we start chopping\ndown big parts of it,\neven though nobody,\nmost people thought that was a bad idea.\nOh, we start doing ocean acidification,\nwiping out all sorts of species,\noh, now we have all these close calls,\nwe almost had a nuclear war,\nand we're replacing more\nand more of the biosphere\nwith non-living things.\nWe're also replacing in our social lives,\na lot of the things which\nwere so valuable to humanity,\na lot of social interactions now\nare replaced by people staring\ninto their rectangles, right?\nAnd I'm not a psychologist,\nI'm out of my depth here,\nbut I suspect that part of\nthe reason why teen suicide\nand suicide in general in the US\nthat record-breaking level\nis actually caused by,\nagain, AI technologies and social media\nmaking people spend less time\nwith actually just human interaction.\nWe've all seen a bunch\nof good-looking people\nin restaurants staring into the rectangles\ninstead of looking into\neach other's eyes, right?\nSo that's also part of the war in life\nthat we are replacing so many\nreally life-affirming\nthings by technology.\nWe're putting technology between us,\nthat the technology that\nwas supposed to connect us\nis actually distancing us\nourselves from each other.\nAnd then we are giving\never more power to things\nwhich are not alive.\nThese large corporations are\nnot living things, right?\nThey're just maximizing profit.\nI wanna win the war on life.\nI think we humans,\ntogether with all our fellow\nliving things on this planet\nwill be better off if\nwe can remain in control\nover the non-living things and make sure\nthat they work for us.\nI really think it can be done.\n- Can you just linger\non this maybe high level\nof philosophical disagreement\nwith Eliezer Yudkowsky,\nin the hope you're stating.", "mimetype": "text/plain", "start_char_idx": 94293, "end_char_idx": 98371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f5ee24e-50bf-4804-89ba-a89b470b2050": {"__data__": {"id_": "2f5ee24e-50bf-4804-89ba-a89b470b2050", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29244be7-65c3-4bfd-ba9c-5b0c3d24a984", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "33d1ce551e688fe14f946e61eadb087f7d23a5c898815cae7df91ab95b7f08db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "114ed034-c823-45a5-bc46-307d0cad1bc5", "node_type": "1", "metadata": {}, "hash": "23edaa7f346d11922d1cfaf56e4be8fc8925b0810af5c5bf3a791d81afdc78b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So that's also part of the war in life\nthat we are replacing so many\nreally life-affirming\nthings by technology.\nWe're putting technology between us,\nthat the technology that\nwas supposed to connect us\nis actually distancing us\nourselves from each other.\nAnd then we are giving\never more power to things\nwhich are not alive.\nThese large corporations are\nnot living things, right?\nThey're just maximizing profit.\nI wanna win the war on life.\nI think we humans,\ntogether with all our fellow\nliving things on this planet\nwill be better off if\nwe can remain in control\nover the non-living things and make sure\nthat they work for us.\nI really think it can be done.\n- Can you just linger\non this maybe high level\nof philosophical disagreement\nwith Eliezer Yudkowsky,\nin the hope you're stating.\nSo he is very sure,\nhe puts a very high probability,\nvery close to one,\ndepending on the day he puts it at one,\nthat AI is going to kill humans.\nThat there's just,\nhe does not see a trajectory,\nwhich it doesn't end up\nwith that conclusion.\nWhat trajectory do you see\nthat doesn't end up there?\nAnd maybe can you see\nthe point he's making,\nand can you also see a way out?\n- First of all, I tremendously respect\nEliezer Yudkowsky and his thinking.\nSecond, I do share his view\nthat there's a pretty large chance\nthat we're not gonna make it as humans.\nThere won't be any humans on the planet,\nin a not-too-distant future,\nand that makes me very sad.\nYou know, we just had a little baby\nand I keep asking myself,\nyou know, is,\nhow old is he even gonna get, you know?\nAnd I ask myself,\nit feels, I said to my wife recently,\nit feels a little bit\nlike I was just diagnosed\nwith some sort of cancer,\nwhich has some, you know,\nrisk of dying from and some\nrisk of surviving, you know.\nExcept this is a kind of cancer\nwhich can kill all of humanity.\nSo I completely take\nseriously his concerns,\nI think,\nbut absolutely, I don't\nthink it's hopeless.\nI think there is,\nfirst of all a lot of momentum now\nfor the first time actually,\nsince the many, many\nyears that have passed\nsince I and many others\nstarted warning about this,\nI feel most people are getting it now.\nI was just talking\nto this guy in the gas station\nnear our house the other day.\nAnd he's like, \"I think\nwe're getting replaced,\nand then I think...\"\nSo that's positive that they're finally,\nwe're finally seeing this reaction,\nwhich is the first step\ntowards solving the problem.\nSecond, I really think that this vision\nof only running AIs,\nif the stakes are really high,\nthey can prove to us that they're safe.\nIt's really just virus\nchecking in reverse again,\nI think it's scientifically doable.\nI don't think it's hopeless,\nwe might have to forfeit\nsome of the technology that we could get\nif we were putting blind faith in our AIs,\nbut we're still gonna get amazing stuff.\n- Do you envision a process\nwith a proof checker?\nLike something like GPT-4, GPT-5,\nwill go through a process\nof rigorous interrogation?\n- No I think it's hopeless,\nThat's like trying to\nproof-verify spaghetti.\n- [Lex] (laughs) Okay.\n- What I think,\nthe vision I have for\nsuccess is instead that,\nyou know, just like we human beings\nwere able to look at our brains\nand distill out the key knowledge.\nGalileo, when his dad threw\nhim an apple when he was a kid,\nhe was able to catch it\n'cause his brain could,\nin his funny spaghetti kind of way,\nyou know, predict how\nparabolas are gonna move,\nhis Kahneman System 1, right?\nBut then he got older and he's like,\n\"Wait, this is a parabola.\nIt's y equals x squared.\"\nI can distill this knowledge out\nand today you can easily\nprogram it into a computer\nand it can simulate not just that,\nbut how to get to Mars and so on, right?\nI envision a similar process\nwhere we use the amazing\nlearning power of neural networks\nto discover the knowledge\nin the first place,\nbut we don't stop with a\nblack box and use that.", "mimetype": "text/plain", "start_char_idx": 97583, "end_char_idx": 101459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "114ed034-c823-45a5-bc46-307d0cad1bc5": {"__data__": {"id_": "114ed034-c823-45a5-bc46-307d0cad1bc5", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f5ee24e-50bf-4804-89ba-a89b470b2050", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e587dfb8c14b57bc8c04857a3cc80d6d7c295ba0abd38f86a5233fc8228a8563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c97994b1-6b51-4043-8e8b-56c17811cc9d", "node_type": "1", "metadata": {}, "hash": "532e2be98d2011e9092d72568e6c599372b11958505831475dc9ff403324c8c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Galileo, when his dad threw\nhim an apple when he was a kid,\nhe was able to catch it\n'cause his brain could,\nin his funny spaghetti kind of way,\nyou know, predict how\nparabolas are gonna move,\nhis Kahneman System 1, right?\nBut then he got older and he's like,\n\"Wait, this is a parabola.\nIt's y equals x squared.\"\nI can distill this knowledge out\nand today you can easily\nprogram it into a computer\nand it can simulate not just that,\nbut how to get to Mars and so on, right?\nI envision a similar process\nwhere we use the amazing\nlearning power of neural networks\nto discover the knowledge\nin the first place,\nbut we don't stop with a\nblack box and use that.\nWe then do a second round of AI\nwhere we use automated systems\nto extract out the knowledge,\nand see what is it,\nwhat are the insights it's had, okay?\nAnd then we put that knowledge\ninto a completely different\nkind of architecture,\nor programming language or whatever,\nthat's made in a way that it\ncan be both really efficient,\nand also is more amenable\nto very formal verification.\nThat's my vision.\nI'm not sitting here saying,\nI'm confident 100% sure that\nit's gonna work, you know.\nBut I don't think it's a chance,\nit's certainly not zero either,\nand it will certainly be possible to do\nfor a lot of really cool AI applications\nthat we're not using now.\nSo we can have a lot of the\nfun that we're excited about\nif we do this.\nWe are gonna need a little bit of time.\nAnd that's why it's good to pause\nand put in place requirements.\nOne more thing also,\nI think, you know,\nsomeone might think,\n\"Well, 0% chance we're gonna survive,\nlet's just give up,\" right?\nThat's very dangerous,\nbecause there's no more\nguaranteed way to fail\nthan to convince yourself\nthat it's impossible\nand not try,\nyou know, when you study\nhistory and military history,\nthe first thing you learn is that,\nthat's how you do psychological warfare.\nYou persuade the other\nside that it's hopeless\nso they don't even fight.\nAnd then of course you win, right?\nLet's not do this psychological\nwarfare on ourselves\nand say there's 100% percent probability\nwe're all screwed anyway.\nAnd sadly, I do get that a little bit,\nsometimes from actually some young people\nwho are like so convinced\nthat we're all screwed,\nthat they're like,\n\"I'm just gonna play\ncomputer games and do drugs,\n'cause we're screwed anyway, right?\"\nIt's important to keep the hope alive\nbecause it actually has a causal impact,\nand makes it more likely\nthat we're gonna succeed.\n- It seems like the people that actually\nbuild solutions to the problem,\nseemingly impossible to solve problems\nare the ones that believe.\n- [Max] Yeah.\n- They're the ones who are the optimists.\nAnd it's like,\nit seems like there's some\nfundamental law to the universe\nwhere \"Fake it till you\nmake it,\" kind of works.\nLike believe it's possible\nand it becomes possible.\n- Yeah, was it Henry Ford who said that,\nif you tell yourself that\nit's impossible, it is.\nSo let's not make that mistake.\nAnd this is a big mistake\nsociety is making,\nI think all in all, everybody's so gloomy,\nand the media also very biased towards\nif it bleeds, it leads,\nand gloom and doom, right?\nSo most,\nvisions of the future\nwe have are dystopian,\nwhich really demotivates people.\nWe wanna really, really, really focus\non the upside also to give people\nthe willingness to fight for it.\nAnd for AI,\nyou and I mostly talked\nabout gloom here again,\nbut let's not forget that, you know,\nwe have probably both lost someone\nwe really cared about to some disease\nthat we were told was incurable.\nWell it's not,\nthere's no law of physics\nsaying we had to die\nof that cancer or whatever.\nOf course, you can cure it.\nAnd there's so many other things that we,\nwith our human intelligence\nhave also failed\nto solve on this planet,\nwhich AI could also very\nmuch help us with, right?\nSo if we can get this right,\nand just be a little more chill,\nand slow down a little\nbit so we get it right.", "mimetype": "text/plain", "start_char_idx": 100804, "end_char_idx": 104733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c97994b1-6b51-4043-8e8b-56c17811cc9d": {"__data__": {"id_": "c97994b1-6b51-4043-8e8b-56c17811cc9d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "114ed034-c823-45a5-bc46-307d0cad1bc5", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c8f0b32ebf24d7d9705b6e96d1eb2bce8ae2556693fbed88d1a85cd268afc3a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca3682f6-6fc9-483c-8602-090a652cdc07", "node_type": "1", "metadata": {}, "hash": "b2452ffaf4c2b8381cc0f0678ad459b9138e56b618e909d9aa595bf0f2cc4c23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So most,\nvisions of the future\nwe have are dystopian,\nwhich really demotivates people.\nWe wanna really, really, really focus\non the upside also to give people\nthe willingness to fight for it.\nAnd for AI,\nyou and I mostly talked\nabout gloom here again,\nbut let's not forget that, you know,\nwe have probably both lost someone\nwe really cared about to some disease\nthat we were told was incurable.\nWell it's not,\nthere's no law of physics\nsaying we had to die\nof that cancer or whatever.\nOf course, you can cure it.\nAnd there's so many other things that we,\nwith our human intelligence\nhave also failed\nto solve on this planet,\nwhich AI could also very\nmuch help us with, right?\nSo if we can get this right,\nand just be a little more chill,\nand slow down a little\nbit so we get it right.\nIt's mind-blowing how awesome\nour future can be, right?\nWe talked a lot about stuff on Earth,\nit can be great,\nbut even if you really get ambitious\nand look up into the skies, right?\nThere's no reason we have\nto be stuck on this planet\nfor the rest of the remaining,\nfor billions of years to come.\nWe totally understand\nnow that laws of physics\nlet life spread out into\nspace to other solar systems,\nto other galaxies,\nand flourish for billions\nand billions of years.\nAnd this to me is a\nvery, very hopeful vision\nthat really motivates me to fight.\nAnd coming back to it in the end,\nit's something you talked about again,\nyou know, the struggle,\nhow the human struggle\nis one of the things\nthat's also really gives\nmeaning to our lives.\nIf there's ever been an\nepic struggle, this is it.\nAnd isn't it even more epic\nif you're the underdog?\nIf most people are telling\nyou this is gonna fail,\nit's impossible, right?\nAnd you persist and you succeed, right?\nAnd that's what we can do\ntogether as a species on this one.\nA lot of pundits are\nready to count this out.\n- Both in the battle to keep AI safe\nand becoming a multi-planetary species.\n- Yeah, and they're the same challenge.\nIf we can keep AI safe,\nthat's how we're gonna get\nmulti-planetary very efficiently.\n- I have some sort of technical questions\nabout how to get it right.\nSo one idea that I'm not even sure\nwhat the right answer is to is,\nshould systems like GPT-4 be open sourced\nin whole or in part?\nCan you see the case for either?\n- I think the answer right now is no.\nI think the answer early on was yes.\nSo we could bring in all the wonderful\ngreat thought process\nof everybody on this,\nbut asking should we open source GPT-4 now\nis just the same as if you say,\nshould we open source\nhow to build really small nuclear weapons?\nShould we open source\nhow to make bioweapons?\nShould we open source\nhow to make a new virus\nthat kills 90% of everybody who gets it?\nOf course we shouldn't.\n- So it's already that powerful.\nIt's already that powerful\nthat we have to respect\nthe power of the systems we've built.\n- The knowledge that you get\nfrom open sourcing everything we do now\nmight very well be powerful enough\nthat people looking at that\ncan use it to build the things\nthat are really threatening.\nAgain, let's get it,\nremember OpenAI's GPT-4 is a baby AI,\nsort of baby,\nproto, almost little bit AGI,\naccording to what Microsoft's\nrecent paper said, right?\nIt's not that that we're scared of,\nwhat we're scared about is\npeople taking that who are,\nwho might be a lot less responsible\nthan the company that made it, right?\nAnd just go into town with it.\nThat's why we wanna,\nit's an information hazard.\nThere are many things which,\nyeah, are not open-sourced\nright now in society\nfor very good reason.\nLike how do you make certain\nkind of very powerful toxins\nout of stuff you can buy in Home Depot?\nWe don't open source\nthose things for a reason,\nand this is really no different.\n- [Lex] So-\n- And I'm saying that,\nI have to say it feels a bit weird,\nin a way, a bit weird to say it\nbecause MIT is like the cradle\nof the open source movement.", "mimetype": "text/plain", "start_char_idx": 103949, "end_char_idx": 107843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca3682f6-6fc9-483c-8602-090a652cdc07": {"__data__": {"id_": "ca3682f6-6fc9-483c-8602-090a652cdc07", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c97994b1-6b51-4043-8e8b-56c17811cc9d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3183b22369f1bd7da7803db5a85783d6d038e1fd96a118b8b7eb20bc2410691b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a6b34da-46e7-4858-ad36-b02c0ed1365d", "node_type": "1", "metadata": {}, "hash": "042e80cffe4292472f8ded61d51eba7eb743f6da6a09834aacb0cdb82f7668a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's not that that we're scared of,\nwhat we're scared about is\npeople taking that who are,\nwho might be a lot less responsible\nthan the company that made it, right?\nAnd just go into town with it.\nThat's why we wanna,\nit's an information hazard.\nThere are many things which,\nyeah, are not open-sourced\nright now in society\nfor very good reason.\nLike how do you make certain\nkind of very powerful toxins\nout of stuff you can buy in Home Depot?\nWe don't open source\nthose things for a reason,\nand this is really no different.\n- [Lex] So-\n- And I'm saying that,\nI have to say it feels a bit weird,\nin a way, a bit weird to say it\nbecause MIT is like the cradle\nof the open source movement.\nAnd I love open source in general,\npower to the people, I say,\nbut there's always gonna be some stuff\nthat you don't open source,\nand you know, it's just\nlike you don't open source,\nso we have a three-month old baby, right?\nWhen he gets a little bit older,\nwe're not gonna open source to him\nall the most dangerous things\nhe can do in the house, right?\n- But it does,\nit's a weird feeling\nbecause this is one of the\nfirst moments in history\nwhere there's a strong case to be made\nnot to open source software.\nThis is when the software\nhas become too dangerous.\n- Yeah, but it's not the first time\nthat we didn't wanna\nopen source a technology.\n- Technology, yeah.\nIs there something to be said about\nhow to get the release\nof such systems right,\nlike GPT-4 and GPT-5?\nSo OpenAI went through\na pretty rigorous effort\nfor several months,\nyou could say it could be longer,\nbut nevertheless it's longer\nthan you would've expected\nof trying to test the system\nto see like what are the ways goes wrong\nto make it very difficult,\nwell, somewhat difficult\nfor people to ask things,\n\"How do I make a bomb for $1?\"\nOr \"How do I say I hate a\ncertain group on Twitter\nin a way that doesn't get\nme blocked from Twitter,\nbanned from Twitter.\"\nThose kinds of questions.\nSo you basically use\nthe system to do harm.\n- [Max] Yeah.\n- Is there something you could say\nabout ideas you have that's just,\non looking having thought about\nthis problem of AI safety,\nhow to release a system,\nhow to test such systems\nwhen you have them inside the company.\n- Yeah, so a lot of people say\nthat the two biggest risks\nfrom large language models are,\nit's spreading disinformation,\nharmful information of various types,\nand second being used for\noffensive cyberweapon.\nI think those are not\nthe two greatest threats.\nThey're very serious threats,\nand it's wonderful that people\nare trying to mitigate them.\nA much bigger elephant in the room\nis how this is gonna disrupt our economy\nin a huge way, obviously,\nand maybe take away a lot\nof the most meaningful jobs.\nAnd an even bigger one is\nthe one we spent so much time\ntalking about here that this\nbecomes the bootloader\nfor the more powerful AI.\n- Write code, connected to the\ninternet, manipulate humans.\n- Yeah, and before we know\nit, we have something else,\nwhich is not at all a large language model\nthat looks nothing like it,\nbut which is way more intelligent\nand capable and has goals.\nAnd that's the elephant in the room.\nAnd obviously no matter how hard\nany of these companies have tried,\nthat's not something that's easy for them\nto verify with large language models.\nAnd the only way to really\nlower that risk a lot\nwould be to not let, for example,\nnever let it read any code,\nnot train on that,\nand not put it into an API,\nand to not Give it access\nto so much information\nabout how to manipulate humans, so,\nbut that doesn't mean you still can't make\na ton of money on them, you know?\nWe're gonna just watch now\nthis coming year, right?\nMicrosoft is rolling\nout the new Office Suite\nwhere you go into Microsoft Word,\nand give it a prompt,\nand it write the whole text for you\nand then you edit it and then you're like,\n\"Oh, gimme a PowerPoint version of this,\"\nand it makes it.\n\"And now take the\nspreadsheet and blah blah.\"", "mimetype": "text/plain", "start_char_idx": 107158, "end_char_idx": 111100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a6b34da-46e7-4858-ad36-b02c0ed1365d": {"__data__": {"id_": "2a6b34da-46e7-4858-ad36-b02c0ed1365d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca3682f6-6fc9-483c-8602-090a652cdc07", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "471163dc22b2b08886c9365f32b3c0261d15637db576d95ac381fde42fb98965", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10963816-45ba-44d2-965d-f515b05e7b23", "node_type": "1", "metadata": {}, "hash": "4c098461b1af856c1ca0fceac5d124dd7b0c0fae698ddb89b5d1dbfd3291a4ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And obviously no matter how hard\nany of these companies have tried,\nthat's not something that's easy for them\nto verify with large language models.\nAnd the only way to really\nlower that risk a lot\nwould be to not let, for example,\nnever let it read any code,\nnot train on that,\nand not put it into an API,\nand to not Give it access\nto so much information\nabout how to manipulate humans, so,\nbut that doesn't mean you still can't make\na ton of money on them, you know?\nWe're gonna just watch now\nthis coming year, right?\nMicrosoft is rolling\nout the new Office Suite\nwhere you go into Microsoft Word,\nand give it a prompt,\nand it write the whole text for you\nand then you edit it and then you're like,\n\"Oh, gimme a PowerPoint version of this,\"\nand it makes it.\n\"And now take the\nspreadsheet and blah blah.\"\nAnd you know,\nall of those things I think are,\nyou can debate the economic impact of it\nand whether society is prepared\nto deal with this disruption.\nBut those are not the things which,\nthat's not the elephant of the room\nthat keeps me awake at night\nfor wiping out humanity.\nAnd I think that's the biggest\nmisunderstanding we have.\nA lot of people think that we're scared of\nlike automatic spreadsheets.\nThat's not the case.\nThat's not what Eliezer was\nfreaked out about either.\n- Is there in terms of\nthe actual mechanism\nof how AI might kill all humans.\nSo something you've been outspoken about,\nyou've talked about a lot.\nIs it autonomous weapon systems?\nSo the use of AI in war,\nis that one of the things that's still\nyou carry concern for\nas these systems become\nmore and more powerful?\n- I carry a concern for it,\nnot that all humans are gonna\nget killed by slaughter bots,\nbut rather just as express route\ninto an Orwellian dystopia\nwhere it becomes much easier\nfor very few to kill very many,\nand therefore it becomes very easy\nfor very few to dominate very many, right?\nAI, if you wanna know how\nAI could kill all people,\njust ask yourself,\nwe humans have driven a\nlot of species extinct.\nHow do we do it?\nYou know, we were smarter than them,\nusually we didn't do\nit even systematically\nby going around one-on-one,\none after the other and stepping on them,\nor shooting them or anything like that.\nWe just like chopped down their habitat\n'cause we needed it for something else.\nIn some cases we did it by putting\nmore carbon dioxide in the atmosphere\nbecause of some reason\nthat those animals didn't even understand,\nand now they're gone, right?\nSo if you're an AI,\nand you just wanna figure something out,\nthen you decide, you know,\nwe just really need this space here\nto build more compute facilities.\nYou know, if that's the\nonly goal it has, you know,\nwe are just the sort of\naccidental roadkill along the way.\nAnd you could totally imagine,\n\"Yeah, maybe this oxygen\nis kind of annoying\n'cause it cause more corrosion,\nso let's get rid of the oxygen.\"\nAnd good luck surviving after that.\nYou know, I'm not particularly concerned\nthat they would want to kill us\njust because that would\nbe like a goal in itself.\nyou know, when we..\nwe've driven a number\nof the elephant species extinct. Right?\nIt wasn't 'cause we didn't like elephants.\nThe basic problem is you\njust don't want to give,\nyou don't wanna cede\ncontrol over your planet\nto some other more intelligent entity\nthat doesn't share your goals.\nIt's that simple, and so,\nwhich brings us to another key challenge\nwhich AI safety research\nhas been grappling with for a long time.\nLike, how do you make AI,\nfirst of all, understand our goals\nand then adopt our goals,\nand then retain them as\nthey get smarter, right?\nAll three of those are really hard, right?\nLike a human child,\nfirst, they're just not smart enough\nto understand our goals.\nThey can't even talk.\nAnd then eventually they're teenagers,\nand understand our goals just fine,\nbut they don't share. (laughs)\n- [Lex] Yeah.\n- But there is fortunately\na magic phase in the middle\nwhere they're smart enough\nto understand our goals\nand malleable enough\nthat we can hopefully,\nwith good parenting, teach\nthem right from wrong\nand instill good goals in them, right?\nSo those are all tough\nchallenges with computers.", "mimetype": "text/plain", "start_char_idx": 110295, "end_char_idx": 114438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "10963816-45ba-44d2-965d-f515b05e7b23": {"__data__": {"id_": "10963816-45ba-44d2-965d-f515b05e7b23", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a6b34da-46e7-4858-ad36-b02c0ed1365d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a3220499eb054732c8c9c6bb10cbc6e538a1aed0b6be09f6b3e8f04afe0a3068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71b0b176-ee12-4a17-9f79-98f4c2f03a5b", "node_type": "1", "metadata": {}, "hash": "9bbbc009ac0d662641616c03ea336820a044d9a720749dbedf634c2951172468", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's that simple, and so,\nwhich brings us to another key challenge\nwhich AI safety research\nhas been grappling with for a long time.\nLike, how do you make AI,\nfirst of all, understand our goals\nand then adopt our goals,\nand then retain them as\nthey get smarter, right?\nAll three of those are really hard, right?\nLike a human child,\nfirst, they're just not smart enough\nto understand our goals.\nThey can't even talk.\nAnd then eventually they're teenagers,\nand understand our goals just fine,\nbut they don't share. (laughs)\n- [Lex] Yeah.\n- But there is fortunately\na magic phase in the middle\nwhere they're smart enough\nto understand our goals\nand malleable enough\nthat we can hopefully,\nwith good parenting, teach\nthem right from wrong\nand instill good goals in them, right?\nSo those are all tough\nchallenges with computers.\nAnd then, you know,\neven if you teach your kids\ngood goals when they're little,\nthey might outgrow them too,\nand that's a challenge for\nmachines to keep improving.\nSo these are a lot of hard,\nhard challenges we're up for,\nbut I don't think any of\nthem are insurmountable.\nThe fundamental reason why Eliezer\nlooked so depressed when I last saw him\nwas because he felt there\njust wasn't enough time.\n- Oh, that not that it was unsolvable,\n- Correct.\n- There's just not enough time.\n- He was hoping that humanity\nwas gonna take this threat more seriously,\nso we would have more time,\nand now we don't have more time.\nThat's why the open letter\nis calling for more time.\n- But even with time,\nthe AI alignment problem,\nit seems to be really difficult.\n- Oh yeah.\nBut it's also the most worthy problem,\nthe most important problem\nfor humanity to ever solve.\nBecause if we solve that one, Lex,\nthat aligned AI can help us\nsolve all the other problems.\n- 'Cause it seems like\nit has to have constant\nhumility about its goal,\nconstantly question the goal.\nBecause as you optimize\ntowards a particular goal\nand you start to achieve it,\nthat's when you have the\nunintended consequences,\nall the things you mentioned about.\nSo how do you enforce and\ncode a constant humility\nas your ability become better,\nand better, and better, and better?\n- Professor Stuart Russell at Berkeley\nis also one of the driving\nforces behind this letter,\nhe has a whole research\nprogram about this.\nI think of it as a AI humility, exactly.\nAlthough he calls it inverse\nreinforcement learning\nand other nerdy terms.\nBut it's about exactly that.\nInstead of telling the AI,\n\"Here's this goal,\ngo optimize the the bejesus out of it.\"\nYou tell it, \"Okay, do\nwhat I want you to do,\nbut I'm not gonna tell\nyou right now what it is\nI want you to do.\nYou need to figure it out.\"\nSo then you give the\nincentives to be very humble\nand keep asking you\nquestions along the way.\nIs this what you really meant?\nIs this what you wanted?\nAnd oh the other thing\nI tried didn't work,\nand seemed like it didn't work out right.\nShould I try it differently?\nWhat's nice about this\nis it's not just\nphilosophical mumbo-jumbo,\nit's theorems and technical\nwork that with more time,\nI think it can make a lot of progress,\nand there are a lot of\nbrilliant people now\nworking on AI safety.\nWe just need to give em a bit more time.\n- But also not that many\nrelative to skill of the prompt.\n- No, exactly.\nThere should be at least this,\njust like every university worth its name\nhas some cancer research going on\nin its biology department, right?\nEvery university that\ndoes computer science\nshould have a real effort in this area\nand it's nowhere near that.\nThis is something I hope is changing now,\nthanks to the GPT-4, right?\nSo I think if there's a silver lining\nto what's happening here,\neven though I think many people would wish\nit would've been rolled\nout more carefully,\nis that this might be the wake-up call\nthat humanity needed,\nto really stop fantasizing about this\nbeing a hundred years off\nand stop fantasizing about this\nbeing completely\ncontrollable and predictable\nbecause it's so obvious,\nit's not predictable, you know?", "mimetype": "text/plain", "start_char_idx": 113615, "end_char_idx": 117616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71b0b176-ee12-4a17-9f79-98f4c2f03a5b": {"__data__": {"id_": "71b0b176-ee12-4a17-9f79-98f4c2f03a5b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10963816-45ba-44d2-965d-f515b05e7b23", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b40b7251f507199fc752e2494a5074d17995cb59a91e25326a7f53465033f14e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a133c59-05b6-4701-88e2-1f70072238b7", "node_type": "1", "metadata": {}, "hash": "ed27c2672bfa7e27f97d8ac20d46b34662ed2e7e2ed53da47f49605195e0e67d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We just need to give em a bit more time.\n- But also not that many\nrelative to skill of the prompt.\n- No, exactly.\nThere should be at least this,\njust like every university worth its name\nhas some cancer research going on\nin its biology department, right?\nEvery university that\ndoes computer science\nshould have a real effort in this area\nand it's nowhere near that.\nThis is something I hope is changing now,\nthanks to the GPT-4, right?\nSo I think if there's a silver lining\nto what's happening here,\neven though I think many people would wish\nit would've been rolled\nout more carefully,\nis that this might be the wake-up call\nthat humanity needed,\nto really stop fantasizing about this\nbeing a hundred years off\nand stop fantasizing about this\nbeing completely\ncontrollable and predictable\nbecause it's so obvious,\nit's not predictable, you know?\nwhy is it that,\nI think it was ChatGPT that\ntried to persuade a journalist\nto divorce his wife, you know.\nIt was not 'cause the\nengineers had built it,\nwas like, (laughs mischievously)\n\"Let's put this in here,\nand screw a little bit with people.\"\nThey hadn't predicted it at all.\nThey built the giant black box\ntrained to predict the next word\nand got all these emergent properties,\nand oops, it did this, you know.\nI think this is a very\npowerful wake-up call\nand anyone watching this who's not scared,\nI would encourage them to just play\na bit more with these tools.\nThey're out there now like GPT-4 and,\nso wake-up call is first step,\nonce you've woken up,\nthen gotta slow down a\nlittle bit the risky stuff\nto give a chance to\neveryone that has woken up\nto catch up with this on the safety front.\n- You know what's\ninteresting is, you know,\nMIT, that's computer science,\nbut in general,\nbut let's just even say\ncomputer science curriculum.\nHow does the computer science\ncurriculum change now?\nYou mentioned programming.\n- [Max] Yeah.\n- Like why would you be,\nwhen I was coming up,\nprogramming as a prestigious position.\nLike why would you be\ndedicating crazy amounts of time\nto become an excellent programmer?\nLike the nature of programming\nis fundamentally changing.\n- The nature of our\nentire education system\nis completely turned on its head.\n- Has anyone been able\nto like, load that in,\nand like think, because\nit's really turning,\n- I mean some English professors,\nsome English teachers are\nbeginning to really freak out now.\nRight? Like they give an essay assignment\nand they get back all\nthis fantastic prose,\nlike this is style of Hemmingway,\nand then they realize they\nhave to completely rethink\nand even, you know, just\nlike we stopped teaching,\nwriting a script,\nis that what you say in English?\n- [Lex] Yeah, handwritten, yeah.\n- Yeah, when everybody started typing,\nyou know, like so much of\nwhat we teach our kids today.\n- Yeah, I mean that's,\neverything is changing and\nit is changing very quickly.\nAnd so much of us understanding\nhow to deal with the big\nproblems of the world\nis through the education system.\nAnd if the education system\nis being turned on its head,\nthen what's next?\nIt feels like having these\nkinds of conversations\nis essential to trying to figure it out.\nAnd everything's happening so rapidly.\nI don't think there's even,\nyou're speaking of safety,\nthe broad AI safety defined,\nI don't think most universities\nhave courses on AI safety.\nIt's like a philosophy seminar.\n- Yeah, and like I'm an educator myself,\nso it pains me to say this,\nbut I feel our education right now\nis completely obsoleted\nby what's happening.\nYou know, you put a kid into first grade,\nand then you are envisioning like,\nand then they're gonna come out\nof high school 12 years later,\nand you've already pre-planned now\nwhat they're gonna learn,\nwhen you're not even sure\nif there's gonna be any\nworld left to come out to,\nlike clearly you need to have a much more\nopportunistic education system\nthat keeps adapting itself very rapidly\nas society re-adapts.\nThe skills that were really useful\nwhen the curriculum was written,\nI mean how many of those skills\nare gonna get you a job in 12 years?\nI mean, seriously.", "mimetype": "text/plain", "start_char_idx": 116770, "end_char_idx": 120844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a133c59-05b6-4701-88e2-1f70072238b7": {"__data__": {"id_": "8a133c59-05b6-4701-88e2-1f70072238b7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71b0b176-ee12-4a17-9f79-98f4c2f03a5b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ba41e26228c71fb5a8b5715ca10b674a2fd9c0067400af07ecfaf86453956967", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ee5df95-f098-4628-a07d-1c55a8e5eab1", "node_type": "1", "metadata": {}, "hash": "761ab774b7906f5f7312b6d03cac273fcafcadcd8bd2d3bbb294fb87a6c1fdaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's like a philosophy seminar.\n- Yeah, and like I'm an educator myself,\nso it pains me to say this,\nbut I feel our education right now\nis completely obsoleted\nby what's happening.\nYou know, you put a kid into first grade,\nand then you are envisioning like,\nand then they're gonna come out\nof high school 12 years later,\nand you've already pre-planned now\nwhat they're gonna learn,\nwhen you're not even sure\nif there's gonna be any\nworld left to come out to,\nlike clearly you need to have a much more\nopportunistic education system\nthat keeps adapting itself very rapidly\nas society re-adapts.\nThe skills that were really useful\nwhen the curriculum was written,\nI mean how many of those skills\nare gonna get you a job in 12 years?\nI mean, seriously.\n- If we just linger on the\nGPT-4 system a little bit,\nyou kind of hinted at it,\nespecially talking about the importance\nof consciousness in in the\nhuman mind with Homo sentiens.\nDo you think GPT-4 is conscious?\n- Ah, I love this question.\nSo let's define consciousness first\nbecause in my experience,\nlike 90% of all arguments\nabout consciousness,\n(Lex chuckles)\nboil down to the two people arguing\nhaving totally different\ndefinitions of what it is,\nthen they're just\nshouting past each other.\nI define consciousness\nas subjective experience.\nRight now I'm experiencing\ncolors and sounds, and emotions,\nyou know, but does a self-driving\ncar experience anything?\nThat's the question about\nwhether it's conscious or not, right?\nOther people think\nyou should define\nconsciousness differently,\nfine by me,\nbut then maybe use a\ndifferent word for it.\nOr they can,\nI'm gonna use consciousness\nfor this at least, so,\nbut if people hate the, yeah.\nSo is GPT-4 conscious?\nDoes GPT-4 have subjective experience?\nShort answer, I don't know,\nbecause we still don't know what it is\nthat gives this wonderful\nsubjective experience\nthat is kind of the\nmeaning of our life, right?\nBecause meaning itself,\nthe feeling of meaning is\na subjective experience.\nJoy is a subjective experience,\nlove is a subjective experience,\nwe don't know what it is,\nI've written some papers about this,\na lot of people have.\nGiulio Tononi, a professor,\nhas stuck his neck out the farthest\nand written down actually very\nbold mathematical conjecture\nfor what's the essence of\nconscious information processing.\nHe might be wrong, he might be right,\nbut we should test it.\nHe postulates that the consciousness\nhas to do with loops in\nthe information processing.\nSo our brain has loops.\nInformation can go round and round,\nin computer science nerd-speak,\nyou call it a recurrent neural network\nwhere some of the output\ngets fed back in again.\nAnd with his\nmathematical formulism,\nif it's a feed-forward neural network\nwhere information only\ngoes in one direction,\nlike from your eye retina\ninto the back of your brain\nfor example, that's not conscious.\nSo he would predict\nthat your retina itself\nisn't conscious of anything,\nor a video camera.\nNow the interesting thing about GPT-4\nis it's also just a one-way\nflow of information.\nSo if Tononi is right,\nthen GPT-4 is a very intelligent zombie,\nthat can do all this smart stuff\nbut isn't experiencing anything.\nAnd this is both a relief\nif it's true,\nand that you don't have to feel guilty\nabout turning off GPT-4\nand wiping its memory\nwhenever a new user comes along.\nI wouldn't like if someone did that to me,\nand neuralyze me like in \"Men In Black.\"\nBut it's also creepy,\nthat you can have a very high intelligence\nperhaps that is not conscious,\nbecause if we get replaced by machines,\nand while it's sad enough that\nhumanity isn't here anymore,\n'cause I kind of like humanity,\nbut at least if the\nmachines were conscious,\nI could be like, \"Well, but\nthey are our descendants\nand maybe they have our values\nand they are our children.\"\nBut if Tononi is right\nand these are all transformers that are,\nnot in the sense of Hollywood,\nbut in the sense of these one-way\ndirection neural networks,\nso they're all the zombies,\nthat's the ultimate zombie apocalypse now.\nWe have this universe that goes on\nwith great construction\nprojects and stuff,\nbut there's no one experiencing anything.", "mimetype": "text/plain", "start_char_idx": 120095, "end_char_idx": 124243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ee5df95-f098-4628-a07d-1c55a8e5eab1": {"__data__": {"id_": "8ee5df95-f098-4628-a07d-1c55a8e5eab1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a133c59-05b6-4701-88e2-1f70072238b7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b03b1d7dc07e43cd4e0086ba389450bbc233fca72ac074cba9d4cd6871059505", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09062f8d-4f7a-43f1-bb40-39fccde61671", "node_type": "1", "metadata": {}, "hash": "8cd9c99cda44aa864e92f988c247834547f8a1940dd176920c865632c7f682c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I wouldn't like if someone did that to me,\nand neuralyze me like in \"Men In Black.\"\nBut it's also creepy,\nthat you can have a very high intelligence\nperhaps that is not conscious,\nbecause if we get replaced by machines,\nand while it's sad enough that\nhumanity isn't here anymore,\n'cause I kind of like humanity,\nbut at least if the\nmachines were conscious,\nI could be like, \"Well, but\nthey are our descendants\nand maybe they have our values\nand they are our children.\"\nBut if Tononi is right\nand these are all transformers that are,\nnot in the sense of Hollywood,\nbut in the sense of these one-way\ndirection neural networks,\nso they're all the zombies,\nthat's the ultimate zombie apocalypse now.\nWe have this universe that goes on\nwith great construction\nprojects and stuff,\nbut there's no one experiencing anything.\nThat would be like the\nultimate depressing future.\nSo I actually think,\nas we move forward with\nbuilding more advanced AI,\nwe should do more research on figuring out\nwhat kind of information processing\nactually it has experienced,\nbecause I think that's\nwhat it's all about.\nAnd I completely don't buy the dismissal\nthat some people will say,\n\"Well this is all bullshit\nbecause consciousness\nequals intelligence.\"\n- [Lex] Right.\n- That's obviously not true.\nYou can have a lot of conscious experience\nwhen you're not really\naccomplishing any goals at all.\nYou're just reflecting on something,\nand you can sometimes,\ndoing things that require intelligence\nprobably without being conscious.\n- But I also worry that we humans,\nwill discriminate against AI systems\nthat clearly exhibit consciousness.\nThat we will not allow AI\nsystems to have consciousness.\nWe'll come up with theories\nabout measuring consciousness\nthat will say this is a lesser being,\nand this was like,\nI worry about that because maybe,\nwe humans will create something\nthat is better than us humans,\nin the way that we find beautiful,\nwhich is they have a deeper\nsubjective experience of reality.\nNot only are they smarter,\nbut they feel deeper.\nAnd we humans will hate them for it.\nAs human history is shown,\nthey'll be the \"other,\"\nwe'll try to suppress it,\nthey'll create conflict,\nthey'll create war, all of this.\nI worry about this too.\n- Are you saying that we humans\nsometimes come up with\nself-serving arguments?\nNo, we would never do that, would we?\n- Well that's the danger here is,\neven in this early stages,\nwe might create something beautiful.\nAnd we'll erase its memory.\n- I was horrified as a kid\nwhen someone started boiling lobsters.\nI'm like, \"Oh my God, that's so cruel.\"\nAnd some grownup there\nback in Sweden said,\n\"Oh, it doesn't feel pain.\"\nI'm like, \"How do you know that?\"\n\"Oh, a scientist have shown that.\"\nAnd then there was a recent study\nwhere they show that lobsters\nactually do feel pain\nwhen you boil them.\nSo they banned lobster\nboiling in Switzerland now.\nYou have to kill them in\na different way first.\nPresumably, a scientific\nresearch boiled down\nto someone asked the\nlobster, \"Does it hurt?\"\n(both laughing)\n- Survey, self-report.\n- And we do the same thing\nwith cruelty to farm animals also,\nall these self-serving\narguments for why they're fine.\nAnd yeah, so we should certainly,\nwhat I think step one is just be humble,\nand acknowledge that consciousness\nis not the same thing as intelligence.\nAnd I believe that consciousness\nstill is a form of information processing\nwhere it's really information\nbeing aware of itself in a certain way,\nand let's study it and give\nourselves a little bit of time,\nand I think we will be able to figure out\nactually what it is that\ncauses consciousness.\nAnd then we can make\nprobably unconscious robots\nthat do the boring jobs\nthat we would feel immoral\nto give the machines.\nBut if you have a companion robot\ntaking care of your mom\nor something like that,\nshe would probably want\nit to be conscious, right?\nSo the emotions it seems\nto display aren't fake.\nAll these things can be done in a good way\nif we give ourselves a little bit of time,\nand don't run, and take on this challenge.\n- Is there something you\ncould say to the timeline\nthat you think about,\nabout the development of AGI?", "mimetype": "text/plain", "start_char_idx": 123427, "end_char_idx": 127573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09062f8d-4f7a-43f1-bb40-39fccde61671": {"__data__": {"id_": "09062f8d-4f7a-43f1-bb40-39fccde61671", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ee5df95-f098-4628-a07d-1c55a8e5eab1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4b58b199338fec5dc24389b05de5e5cc077d9c17721c3e13065e696514a445f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b94d1d04-5888-4bdf-a788-54d0591b2810", "node_type": "1", "metadata": {}, "hash": "6b29ca23746bfecae42d619324258b6ac95d125f6e5d0e92f5a1fe05c58d7267", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I believe that consciousness\nstill is a form of information processing\nwhere it's really information\nbeing aware of itself in a certain way,\nand let's study it and give\nourselves a little bit of time,\nand I think we will be able to figure out\nactually what it is that\ncauses consciousness.\nAnd then we can make\nprobably unconscious robots\nthat do the boring jobs\nthat we would feel immoral\nto give the machines.\nBut if you have a companion robot\ntaking care of your mom\nor something like that,\nshe would probably want\nit to be conscious, right?\nSo the emotions it seems\nto display aren't fake.\nAll these things can be done in a good way\nif we give ourselves a little bit of time,\nand don't run, and take on this challenge.\n- Is there something you\ncould say to the timeline\nthat you think about,\nabout the development of AGI?\nDepending on the day,\nI'm sure that changes for you,\nbut when do you think\nthere would be a really\nbig leap in intelligence\nwhere you would definitively\nsay we have built AGI?\nDo you think it's one year from now,\nfive years from now, 10, 20, 50?\nWhat's your gut say?\n- Honestly, for the past decade,\nI've deliberately given\nvery long timelines\nbecause I didn't want to fuel\nsome kind of stupid Moloch race.\n- [Lex] Yeah.\n- But I think that cat has\nreally left the bag now.\nI think we might be very, very close.\nI don't think the Microsoft\npaper is totally off\nwhen they say that there\nare some glimmers of AGI.\nIt's not AGI yet, it's not an agent,\nthere's a lot of things they can't do.\nBut I wouldn't bet very strongly\nagainst it happening very soon,\nthat's why we decided\nto do this open letter.\nBecause you know,\nif there's ever been a\ntime to pause, you know,\nit's today.\n- There's a feeling like this GPT-4\nis a big transition\ninto waking everybody up\nto the effectiveness of these systems.\nAnd so the next version will be big.\n- Yeah, and if that next one isn't AGI,\nmaybe the next next one will.\nAnd there are many companies\ntrying to do these things\nand the basic architecture of 'em\nis not some sort of\nsuper well-kept secret.\nSo this is a time to...\nA lot of people have said for many years\nthat there will come a time\nwhen we want to pause a little bit,\nthat time is now.\n- You have spoken about\nand thought about nuclear war a lot.\nOver the past year, we\nseemingly have come closest\nto the precipice of nuclear war than,\nat least in my lifetime.\n- [Max] Mhm, yeah.\n- What do you learn about\nhuman nature from that?\n- It's our old friend Moloch again.\nIt is really scary to see it where,\nAmerica doesn't want\nthere to be a nuclear war.\nRussia doesn't want there to\nbe a global nuclear war either.\nWe both know that it's just be another,\nif we just try to do it,\nif both sides try to launch first,\nit's just another suicide race, right?\nSo why are we,\nwhy is it the way you said,\nthat this is the closest\nwe've come since 1962?\nIn fact, I think we've come closer now\nthan even the Cuban Missile Crisis.\nIt's 'cause of Moloch,\nYou know, you have these other forces.\nOn one hand you have the West\nsaying that we have to\ndrive Russia out of Ukraine,\nit's a matter of pride.\nAnd we've staked so much on it\nthat it would be seen as a huge loss\nof the credibility of the West\nif we don't drive Russia\nout entirely of the Ukraine.\nAnd on the other hand,\nyou have Russia who has,\nand you have the Russian leadership\nwho knows that if they get\ncompletely driven out of Ukraine,\nyou know, it might,\nit's not just gonna be\nvery humiliating for them,\nbut they might,\nit often happens when countries lose wars\nthat the things don't go so well\nfor their leadership either.\nLike, you remember when Argentina\ninvaded the Falkland Islands?\nThe military junta that\nordered that, right?\nPeople are cheering on\nthe streets at first\nwhen they took it,\nand then when they got their\nbutt kicked by the British,\nyou know what happened to those guys?\nThey were out.\nAnd I believe those who are still alive\nare in jail now, right?", "mimetype": "text/plain", "start_char_idx": 126744, "end_char_idx": 130688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b94d1d04-5888-4bdf-a788-54d0591b2810": {"__data__": {"id_": "b94d1d04-5888-4bdf-a788-54d0591b2810", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09062f8d-4f7a-43f1-bb40-39fccde61671", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0dea8bbf26563cb24da24feadcd08179275d7259e6376379f5b9dcf1f2aba931", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d63b859e-29dd-4df3-b90c-7835def36ecf", "node_type": "1", "metadata": {}, "hash": "4a6572f9ac284f9639d59d0c4138cf324c68ce9eb9c5b418e637ed762a8705da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we've staked so much on it\nthat it would be seen as a huge loss\nof the credibility of the West\nif we don't drive Russia\nout entirely of the Ukraine.\nAnd on the other hand,\nyou have Russia who has,\nand you have the Russian leadership\nwho knows that if they get\ncompletely driven out of Ukraine,\nyou know, it might,\nit's not just gonna be\nvery humiliating for them,\nbut they might,\nit often happens when countries lose wars\nthat the things don't go so well\nfor their leadership either.\nLike, you remember when Argentina\ninvaded the Falkland Islands?\nThe military junta that\nordered that, right?\nPeople are cheering on\nthe streets at first\nwhen they took it,\nand then when they got their\nbutt kicked by the British,\nyou know what happened to those guys?\nThey were out.\nAnd I believe those who are still alive\nare in jail now, right?\nSo you know, the Russian\nleadership is entirely cornered\nwhere they know that just\ngetting driven out of Ukraine\nis not an option, and,\nso this to me, is a\ntypical example of Moloch.\nYou have these incentives\nof the two parties\nwhere both of them are\njust driven to escalate\nmore and more, right?\nIf Russia starts losing in\nthe conventional warfare,\nthe only thing they cam do\nsince their back's against the wall,\nis to keep escalating.\nAnd the West has put\nitself in the situation now\nwhere we're sort of already\ncommitted to drive Russia out.\nSo the only option the West has,\nis to call Russia's bluff and\nkeep sending in more weapons.\nThis really bothers me because Moloch\ncan sometimes drive competing parties\nto do something which is ultimately\njust really bad for both of them.\nAnd you know,\nwhat makes me even more\nworried is not just that I,\nit's difficult to see an ending,\na quick peaceful ending to this tragedy\nthat doesn't involve\nsome horrible escalation,\nbut also that we\nunderstand more clearly now\njust how horrible it would be.\nThere was an amazing\npaper that was published\nin Naturefood this August,\nby some of the top researchers\nwho've been studying nuclear\nwinter for a long time,\nand what they basically did\nwas they combined climate models\nwith food and agricultural models,\nso instead of just saying,\n\"Yeah, you know, it gets\nreally cold, blah blah blah,\"\nthey figured out actually\nhow many people would die\nin different countries.\nAnd it's pretty mind-blowing, you know?\nSo basically what happens, you know,\nis that the thing that\nkills the most people\nis not the explosions,\nit's not the radioactivity,\nit's not the EMP mayhem,\nit's not the rampaging mobs foraging food,\nno, it's the fact that\nyou get so much smoke\ncoming up from the burning\ncities into the stratosphere\nthat it spreads around the\nEarth from the jetstreams.\nSo in typical models you\nget like 10 years or so\nwhere it's just crazy cold\nduring the first year after the war,\nand in their models,\nthe temperature drops in Nebraska\nand in the Ukraine bread baskets,\nyou know, by like 20 Celsius or so,\nif I remember.\nNo yeah,\n20, 30 Celsius depending on where you are.\n40 Celsius in some places,\nwhich is, you know, 40 Fahrenheit\nto 80 Fahrenheit colder than\nwhat it would it normally be.\nSo, you know, I'm not good at farming but,\n(Lex laughing)\nif it's snowing,\nif it drops below freezing pretty much\non most days in July\nand then like, that's not good.\nSo they worked out,\nthey put this into their farming models\nand what they found\nwas really interesting.\nThe countries that get the most hard hit\nare the ones in the northern hemisphere.\nSo in the US,\nin one model they had,\nthey had about 99% of all\nAmericans starving to death,\nin Russia, and China, and Europe,\nalso about 99%, 98% starving to death.\nSo you might be like,\n\"Oh, it's kind of poetic justice\nthat both the Russians and the Americans,\n99% of them have to pay for it,\n'cause it was their bombs that did it.\"\nBut you know,\nthat doesn't particularly\ncheer people up in Sweden\nor other random countries\nthat have nothing to do with it, right?\nAnd it,\nI think it hasn't entered the mainstream,\nnot understanding very much\njust like how bad this is.", "mimetype": "text/plain", "start_char_idx": 129855, "end_char_idx": 133890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d63b859e-29dd-4df3-b90c-7835def36ecf": {"__data__": {"id_": "d63b859e-29dd-4df3-b90c-7835def36ecf", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b94d1d04-5888-4bdf-a788-54d0591b2810", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "01acd9e83cc6ead8f28b03698eabd1d0c1d576f452f0203738c80da6bb1e31cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46cf8d8c-7c45-4eae-961b-8fcc68e5a284", "node_type": "1", "metadata": {}, "hash": "7cc19f950789ef9fab44d44bc39937a8ee63f74f36f49091e94e178f3cda3550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So they worked out,\nthey put this into their farming models\nand what they found\nwas really interesting.\nThe countries that get the most hard hit\nare the ones in the northern hemisphere.\nSo in the US,\nin one model they had,\nthey had about 99% of all\nAmericans starving to death,\nin Russia, and China, and Europe,\nalso about 99%, 98% starving to death.\nSo you might be like,\n\"Oh, it's kind of poetic justice\nthat both the Russians and the Americans,\n99% of them have to pay for it,\n'cause it was their bombs that did it.\"\nBut you know,\nthat doesn't particularly\ncheer people up in Sweden\nor other random countries\nthat have nothing to do with it, right?\nAnd it,\nI think it hasn't entered the mainstream,\nnot understanding very much\njust like how bad this is.\nMost people, especially a lot of people\nin decision-making positions\nstill think of nuclear weapons\nas something that makes you powerful,\nscary but powerful.\nThey don't think of it as something where,\n\"Yeah, just to within a percent or two,\nyou know, we're all just\ngonna starve to death and-\n- And starving to death is,\nthe worst way to die.\nAs Holodomor, as all the\nfamines in history show\nthe torture involved in that.\n- Probably brings out\nthe worst in people also.\nWhen people are desperate\nlike this, it's not,\nso some people,\nI've have heard some people say that\nif that's what's gonna happen,\nthey'd rather be at ground zero\nand just get vaporized, you know?\nBut I think people\nunderestimate the risk of this\nbecause they aren't afraid of Moloch.\nThey think, \"Oh, it's just gonna be,\n'cause humans don't want this,\nso it's not gonna happen.\"\nThat's the whole point of Moloch.\nThat things happen that nobody wanted.\n- And that applies to nuclear weapons,\nand that applies to AGI.\n- Exactly. And it applies\nto some of the things\nthat people have gotten most upset\nwith capitalism for also, right?\nWhere everybody was just\nkind of trapped, you know.\nIt's not that if some\ncompany does something\nthat causes a lot of harm,\nnot that the CEO is a bad person,\nbut she or he knew that, you know,\nthat all the other companies\nwere doing this too.\nSo Moloch is,\nis a formidable foe,\nI wish someone would make good movies\nso we can see who the real enemy is,\nso we don't,\n'cause we're not fighting\nagainst each other,\nMoloch makes us fight against each other.\nThat's what Moloch's superpower is.\nThe hope here is any kind of technology\nor the mechanism that\nlets us instead realize\nthat we're fighting\nthe wrong enemy, right?\n- It's such a fascinating battle.\n- It's not us versus them,\nit's us versus it, yeah.\n- Yeah, we are fighting\nMoloch for human survival.\nWe as a civilization.\n- Have you seen the\nmovie \"Needful Things\"?\nIt's a Stephen King novel.\nI love Stephen King,\nand Max von Sydow, a Swedish actor,\nis playing the guy.\nIt's brilliant,\nI just thought, I hadn't\nthought about that until now,\nbut that's the closest I've\nseen to a movie about Moloch.\nI don't wanna spoil the film for anyone\nwho wants to watch it.\nBut basically, it's about\nthis guy who turns out to,\nyou can interpret him as\nthe devil or whatever,\nbut he doesn't actually ever\ngo around and kill people\nor torture people, or go\nburning coal or anything.\nHe makes everybody fight each other,\nmakes everybody fear each other,\nhate each other, and then kill each other.\nSo that's the movie\nabout Moloch, you know.\n- Love is the answer,\nthat seems to be,\none of the ways to fight\nMoloch is by compassion,\nby seeing the common humanity.\n- Yes, yes.\nAnd to not sound,\nso we don't sound like\na bunch of Kumbaya tree\nhuggers here, right?\n(Lex laughing)\nWe're not just saying\n\"Love and peace, man.\"\nWe're trying to actually help people\nunderstand the true facts\nabout the other side,\nand feel the compassion because,\nit's that truth makes you\nmore compassionate, right?\nSo that's why I really like using AI\nfor truth and for\ntruth-seeking technologies.\nthat can as a result, you know,\nwill get us more love than hate.", "mimetype": "text/plain", "start_char_idx": 133134, "end_char_idx": 137074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46cf8d8c-7c45-4eae-961b-8fcc68e5a284": {"__data__": {"id_": "46cf8d8c-7c45-4eae-961b-8fcc68e5a284", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d63b859e-29dd-4df3-b90c-7835def36ecf", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "bf5b078d84734de75201164a0b047b1649b53d6b35805542f27960dc2b983e91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75c24ea2-cf9c-499e-8711-279fd5d081a8", "node_type": "1", "metadata": {}, "hash": "7a1ac27cd8f6e9843a1334f0878b961293aad249a94f0d85275db26da23826bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He makes everybody fight each other,\nmakes everybody fear each other,\nhate each other, and then kill each other.\nSo that's the movie\nabout Moloch, you know.\n- Love is the answer,\nthat seems to be,\none of the ways to fight\nMoloch is by compassion,\nby seeing the common humanity.\n- Yes, yes.\nAnd to not sound,\nso we don't sound like\na bunch of Kumbaya tree\nhuggers here, right?\n(Lex laughing)\nWe're not just saying\n\"Love and peace, man.\"\nWe're trying to actually help people\nunderstand the true facts\nabout the other side,\nand feel the compassion because,\nit's that truth makes you\nmore compassionate, right?\nSo that's why I really like using AI\nfor truth and for\ntruth-seeking technologies.\nthat can as a result, you know,\nwill get us more love than hate.\nAnd even if you can't get love, you know,\nlet's settle for some understanding\nwhich already gives compassion.\nIf someone is like, you know,\n\"I really disagree with you Lex,\nbut I can see where you're coming from.\nYou're not a bad person\nwho needs to be destroyed,\nbut I disagree with you\nand I'm happy to have an\nargument about it,\" you know?\nThat's a lot of progress\ncompared to where we are at\n2023 in the public space,\nwouldn't you say?\n- If we solve the AI safety problem,\nas we've talked about,\nand then you, Max Tegmark,\nwho has been talking\nabout this for many years,\nget to sit down with the AGI,\nwith the early AGI system\non a beach with a drink,\n(Max chuckles)\nWhat would you ask her?\nWhat kind of question would you ask?\nWhat would you talk about?\nSomething so much smarter than you,\nwould you be afraid?\n- I knew you were gonna get me\nwith a really zinger of a question.\nThat's a good one.\n- Would you be afraid\nto ask some questions?\n- No, I'm not afraid of the truth.\n(Lex laughing)\nI'm very humble.\nI know I'm just a meat bag\nwith all these flaws, you know?\nBut yeah,\nI mean, we talked a lot\nabout the Homo sentiens,\nI've really already tried that\nfor a long time with myself.\nAnd that is what's really valuable\nabout being alive for me,\nis that I have these\nmeaningful experiences.\nIt's not that\nI'm good at this, or\ngood at that or whatever.\nThere's so much I suck at, and...\n- So you're not afraid for the system\nto show you just how dumb you are.\n- No, no.\nIn fact, my son reminds me of that\npretty frequently. (laughs)\n- You could find out how dumb\nyou are in terms of physics,\nhow little we humans understand.\n- I'm cool with that.\nI think,\nso I can't waffle my way\nout of this question,\nit's a fair one and it's tough.\nI think, given that I'm a\nreally, really curious person,\nthat's really the\ndefining part of who I am,\nI'm so curious.\nI have some physics questions.\n(Lex laughing)\nI love to understand.\nI have some questions about consciousness,\nabout the nature of reality,\nI would just really, really\nlove to understand also.\nI can tell you one for example,\nthat I've been obsessing\nabout a lot recently.\nSo I believe that,\nso suppose Tononi is right.\nand suppose there are some\ninformation processing systems\nthat are conscious and some that are not.\nSuppose you can even make\nreasonably smart things\nlike GPT-4 that are not conscious,\nbut you can also make them conscious.\nHere is the question that\nkeeps me awake at night.\nIs it the case that the\nunconscious zombie systems\nthat are really intelligent\nare also really efficient?\nSorry, really inefficient?\nSo that when you try to\nmake things more efficient,\nwe will naturally be a pressure to do,\nthey become conscious.\nI'm kind of hoping that\nthat's correct, and I,\ndo you want me to give you,\nyou can hand-wave\nthe argument for it?\n- Yes, please.\n- You know like,\nIn my lab again, every time we look at\nhow these large language\nmodels do something,\nwe see that they do them\nin really dumb ways,\nand you could make it make it better.\nIf you,\nwe have loops in our computer\nlanguage for a reason,\nthe code would get way, way longer\nif you weren't allowed to use them, right?", "mimetype": "text/plain", "start_char_idx": 136320, "end_char_idx": 140227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75c24ea2-cf9c-499e-8711-279fd5d081a8": {"__data__": {"id_": "75c24ea2-cf9c-499e-8711-279fd5d081a8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46cf8d8c-7c45-4eae-961b-8fcc68e5a284", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "33c2e0ea02d8206183f6eabe3ede7e78ba7d2129ed4883038ab1156983448ad2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "839e9752-ae79-4b51-8d56-7f5f7ccc97ad", "node_type": "1", "metadata": {}, "hash": "e7a1f7fa3c79dea445ba9cd9f4ff9bb79f96518438908f35831b68cb240c85f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here is the question that\nkeeps me awake at night.\nIs it the case that the\nunconscious zombie systems\nthat are really intelligent\nare also really efficient?\nSorry, really inefficient?\nSo that when you try to\nmake things more efficient,\nwe will naturally be a pressure to do,\nthey become conscious.\nI'm kind of hoping that\nthat's correct, and I,\ndo you want me to give you,\nyou can hand-wave\nthe argument for it?\n- Yes, please.\n- You know like,\nIn my lab again, every time we look at\nhow these large language\nmodels do something,\nwe see that they do them\nin really dumb ways,\nand you could make it make it better.\nIf you,\nwe have loops in our computer\nlanguage for a reason,\nthe code would get way, way longer\nif you weren't allowed to use them, right?\nIt's more efficient to have the loops\nand in order to have self-reflection\nwhether it's conscious or not, right?\nEven an operating system knows\nthings about itself, right?\nYou need to have loops already, right?\nSo I think this is,\nI'm waving my hands a lot,\nbut I suspect that,\nthe most efficient way of implementing\na given level of intelligence,\nhas loops in it,\nthe self-reflection,\nand will be conscious.\n- Isn't that great news?\n- Yes, if it's true, it's wonderful.\n'Cause then we don't have to fear\nthe ultimate zombie apocalypse.\nAnd I think if you look\nat our brains, actually.\nOur brains are part\nzombie and part conscious.\nWhen I open my eyes,\nI immediately take all these pixels\nthat hit on my retina, right?\nAnd I'm like, \"Oh, that's Lex.\"\nBut I have no freaking clue\nof how I did that computation.\nIt's actually quite complicated, right?\nIt was only relatively recently,\nwe could even do it well\nwith machines, right?\nYou get a bunch of information processing\nhappening in my retina\nand then it goes to the\nlateral geniculate nucleus\nin my thalamus,\nand the area V1, V2, V4,\nand the fusiform face area here,\nthat Nancy Kanwisher at MIT invented,\nand blah, blah, blah, blah, blah.\nAnd I have no frigging clue\nhow that worked, right?\nIt feels to me subjectively,\nlike my conscious module just\ngot a little email saying,\n\"Facial processing task\ncomplete, it's Lex.\"\n- [Lex] Yeah.\n- And I'm gonna just go with that, right?\nSo this fits perfectly\nwith Tononi's model,\nbecause this was all one-way\ninformation processing mainly.\nAnd it turned out for\nthat particular task,\nthat's all you needed.\nAnd it probably was kind of the\nmost efficient way to do it.\nBut there were a lot of other things\nthat we associated with\nhigher intelligence\nand planning, and so on, and so forth,\nwhere you kind of wanna have loops\nand be able to ruminate and self-reflect,\nand introspect, and so on.\nWhere my hunch is that\nif you want to fake that\nwith a zombie system that\njust all goes one way,\nyou have to like unroll those loops,\nand it gets really, really long,\nand it's much more inefficient.\nSo I'm actually hopeful that AI,\nif in the future we have all these various\nsublime and interesting\nmachines that do cool things,\nand are aligned with us,\nthat they will be at least,\nthey will also have consciousness\nfor kind of these things that we do.\n- That great intelligence\nis also correlated to great consciousness,\nor a deep kind of consciousness.\n- Yes, so that's a happy thought for me\n'cause the zombie apocalypse really,\nis my worst nightmare of all.\nIt would be like adding insult to injury,\nnot only did we get replaced,\nbut we frigging replaced\nourselves by zombies,\nlike, how dumb can we be?\n- That's such a beautiful vision,\nand that's actually a provable one.\nThat's one that we humans\ncan intuitively prove\nthat those two things are correlated,\nas we start to understand what\nit means to be intelligent,\nand what it means to be conscious,\nwhich these systems,\nearly AGI-like systems\nwill help us understand.\nAnd I just wanna say one more thing,\nwhich is super important.\nMost of my colleagues,\nwhen I started going\non about consciousness\ntell me that it's all bullshit\nand I should stop talking about it.", "mimetype": "text/plain", "start_char_idx": 139476, "end_char_idx": 143438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "839e9752-ae79-4b51-8d56-7f5f7ccc97ad": {"__data__": {"id_": "839e9752-ae79-4b51-8d56-7f5f7ccc97ad", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1da736a2-3f13-4ecf-9cc9-850b7fdcb0e4", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f33a1835c3c796015245152e688b8dcd35f03a8b690ce29e8d519695c21f75a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75c24ea2-cf9c-499e-8711-279fd5d081a8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_name": "[English] Max Tegmark_ The Case for Halting AI Development _ Lex Fridman Podcast #371 [DownSub.com].txt", "file_type": "text/plain", "file_size": 146186, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6225f28ccded4430d55f5dc40a0b67d88694a0dbcb19c37c0c6f21ac8ce2c001", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- That great intelligence\nis also correlated to great consciousness,\nor a deep kind of consciousness.\n- Yes, so that's a happy thought for me\n'cause the zombie apocalypse really,\nis my worst nightmare of all.\nIt would be like adding insult to injury,\nnot only did we get replaced,\nbut we frigging replaced\nourselves by zombies,\nlike, how dumb can we be?\n- That's such a beautiful vision,\nand that's actually a provable one.\nThat's one that we humans\ncan intuitively prove\nthat those two things are correlated,\nas we start to understand what\nit means to be intelligent,\nand what it means to be conscious,\nwhich these systems,\nearly AGI-like systems\nwill help us understand.\nAnd I just wanna say one more thing,\nwhich is super important.\nMost of my colleagues,\nwhen I started going\non about consciousness\ntell me that it's all bullshit\nand I should stop talking about it.\nI hear a little inner voice\nfrom my father and from my mom saying,\n\"Keep talking about it,\"\n'cause I think they're wrong.\nAnd the main way to\nconvince people like that,\nthat they're wrong if they\nsay that consciousness\nis just equal to intelligence,\nis to ask them what's wrong with torture?\nOr why are you against torture?\nif it's just about, you know,\nthese particles moving this\nway rather than that way,\nand there is no such thing\nas subjective experience,\nwhat's wrong with torture?\nI mean, do you have a\ngood comeback to that?\n- No, it seems like suffering.\nSuffering imposed unto\nother humans is somehow\ndeeply wrong in a way\nthat intelligence doesn't quite explain.\n- And if someone tells me,\nwell, you know, it's just an illusion,\nconsciousness, whatever, you know.\nI would like to invite them\nthe next time they're having surgery,\nto do it without anesthesia.\nLike what is anesthesia really doing?\nIf you have it,\nyou can have a local\nanesthesia when you're awake.\nI have that when they\nfixed my shoulder, right?\nIt's super entertaining.\nWhat was that that it did?\nit just removed my subjective\nexperience of pain.\nIt didn't change anything\nabout what was actually\nhappening in my shoulder, right?\nSo if someone says, \"That's all bullshit,\"\nSkip the anesthesia, that's my advice.\nThis is incredibly central.\n- It could be fundamental to whatever\nthis thing we have going on here.\n- It is fundamental because we're,\nwhat we feel that's so fundamental,\nis suffering and joy, and\npleasure, and meaning, and,\nthose are all subjective\nexperiences there.\nAnd let's not,\nthose are the elephant in the room,\nthat's what makes life worth living.\nAnd that's what can make it horrible\nif it's just a bunch of suffering.\nSo let's not make the mistake\nof saying that that's all bullshit.\n- And let's not make the mistake\nof not instilling the AI systems\nwith that same thing\nthat makes us special.\n- [Max] Yeah.\n- Max, it's a huge honor\nthat you would sit down to me\nthe first time on the first\nepisode of this podcast.\nIt's a huge honor you\nsit down with me again\nand talk about this,\nwhat I think is the most important topic,\nthe most important problem\nthat we humans have to\nface and hopefully solve.\n- Yeah, well, the honor is all mine\nand I'm so grateful to you\nfor making more people aware of this fact\nthat humanity has reached\nthe most important\nfork in the road ever in its history.\nAnd let's turn in the correct direction.\n- Thanks for listening\nto this conversation\nwith Max Tegmark.\nTo support this podcast.\nPlease check out our\nsponsors in the description.\nAnd now let me leave you\nwith some words from Frank Herbert.\n\"History is a constant race\nbetween invention and catastrophe.\"\nThank you for listening,\nand hope to see you next time.", "mimetype": "text/plain", "start_char_idx": 142569, "end_char_idx": 146186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3783bca2-74e3-4b77-8346-b747d3fb0929": {"__data__": {"id_": "3783bca2-74e3-4b77-8346-b747d3fb0929", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5e6d35c-16f8-4b96-9f5b-1fef1b373346", "node_type": "1", "metadata": {}, "hash": "1d115baa60be13fc128c9e2fe4e34b5b3db957b078275d1782c1c29a9536f632", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The following is a\nconversation with Terence Tao,\nwidely considered to be one\nof the greatest mathematicians in history,\noften referred to as the Mozart of math.\nHe won the Fields Medal\nand the Breakthrough Prize in mathematics,\nand has contributed groundbreaking work\nto a truly astonishing range of fields\nin mathematics and physics.\nThis was a huge honor\nfor me for many reasons,\nincluding the humility and\nkindness that Terry showed\nto me throughout all our interactions.\nIt means the world.\nThis is the Lex Fridman podcast.\nTo support it, please\ncheck out our sponsors\nin the description or at\nlexfreedman.com/sponsors.\nAnd now, dear friends, here's Terence Tao.\nWhat was the first\nreally difficult research\nlevel math problem\nthat you encountered?\nOne that gives you pause, maybe?\n- Well, I mean,\nin your undergraduate education you learn\nabout the really hard,\nimpossible problems like\nthe Riemann Hypothesis,\nthe Twin-Primes Conjecture.\nYou can make problems\narbitrarily difficult.\nThat's not really a problem.\nIn fact, there's even\nproblems that we know\nto be unsolvable.\nWhat's really interesting\nare the problems just\non the boundary between what\nwe can do relatively easily\nand what are hopeless.\nBut what are problems\nwhere existing techniques\ncan do like 90% of the job,\nand then you just need that remaining 10%?\nI think as a PhD student,\nthe Kakeya problem certainly caught my eye\nand it just got solved, actually.\nIt's a problem I've worked on a lot\nin my early research.\nHistorically, it came from a little puzzle\nby the Japanese mathematician\nSoichi Kakeya in 1918 or so.\nSo the puzzle is that you\nhave a needle on the plane,\nor think like driving\non a road or something,\nand you wanted to execute a U-turn.\nYou want to turn the needle around,\nbut you want to do it in as\nlittle space as possible.\nSo you want to use this little area\nin order to turn it around,\nbut the needle is infinitely maneuverable.\nSo you can imagine just spinning\nit around as a unit needle.\nYou can spin it around its center\nand I think that gives you a disk of area,\nI think pi over four.\nOr you can do a three-point U-turn,\nwhich is what we teach people\nin their driving schools to do.\nAnd that actually takes\narea pi over eight.\nSo it's a little bit more\nefficient than a rotation.\nAnd so for a while,\npeople thought that was\nthe most efficient way\nto turn things around.\nBut Besicovitch showed that\nin fact you could actually turn the needle\naround using as little area as you wanted.\nSo 0.001.\nThere was some really\nfancy multi back and forth\nU-turn thing\nthat you could do that you\ncould turn a needle around.\nAnd in so doing it would pass\nthrough every intermediate direction.\n- Is this in the two-dimensional plane?\n- This is in the two-dimensional plane.\nAnd yeah, so we understand\neverything in two dimensions.\nSo the next question is, what\nhappens in three dimensions?\nSo suppose like the Hubble Space Telescope\nis a tube in space\nand you want to observe every\nsingle star in the universe,\nso you want to rotate the telescope\nto reach every single direction.\nAnd here's the unrealistic part.\nSuppose that space is at a\npremium, which it totally is not.\nYou want to occupy\nas little volume as possible\nin order to rotate your needle\naround in order to see every\nsingle star in the sky.\nHow small a volume do you need to do that?\nAnd so you can modify\nBesicovitch's construction.\nAnd so if your telescope\nhas zero thickness,\nthen you can use as\nlittle volume as you need.\nThat's a simple modification\nof the two dimensional construction.\nBut the question is that\nif your telescope is not zero\nthickness, but just very,\nvery thin, some thickness delta,\nwhat is the minimum volume needed\nto be able to see every single direction\nas a function of delta?\nSo as delta gets smaller,\nas your needle gets thinner,\nthe volume should go down.\nBut how fast does it go down?\nAnd the conjecture was that it\ngoes down very, very slowly,\nlike logarithmically, roughly speaking.\nAnd that was proved after a lot of work.\nSo this seems like a puzzle.\nWhy is it interesting?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5e6d35c-16f8-4b96-9f5b-1fef1b373346": {"__data__": {"id_": "c5e6d35c-16f8-4b96-9f5b-1fef1b373346", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3783bca2-74e3-4b77-8346-b747d3fb0929", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e87d8c24a806b6cbf8dd2f40e3f46c740615e32c5f270b260c29c2d09e3c06de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1206dc93-b5e7-401b-a666-13dec3fc7123", "node_type": "1", "metadata": {}, "hash": "227c87f792b82b4a4ea64c94f90dac277668a0c48489c6ddd4948d082bc9b97d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How small a volume do you need to do that?\nAnd so you can modify\nBesicovitch's construction.\nAnd so if your telescope\nhas zero thickness,\nthen you can use as\nlittle volume as you need.\nThat's a simple modification\nof the two dimensional construction.\nBut the question is that\nif your telescope is not zero\nthickness, but just very,\nvery thin, some thickness delta,\nwhat is the minimum volume needed\nto be able to see every single direction\nas a function of delta?\nSo as delta gets smaller,\nas your needle gets thinner,\nthe volume should go down.\nBut how fast does it go down?\nAnd the conjecture was that it\ngoes down very, very slowly,\nlike logarithmically, roughly speaking.\nAnd that was proved after a lot of work.\nSo this seems like a puzzle.\nWhy is it interesting?\nSo it turns out to be\nsurprisingly connected to a lot\nof problems in partial\ndifferential equations,\nin number theory, in\ngeometry, combinatorics.\nFor example, in wave propagation,\nyou splash some water around,\nyou create water waves\nand they travel in various directions.\nBut waves exhibit both particle\nand wave-type behavior.\nSo you can have what's\ncalled a wave packet,\nwhich is like a very localized\nwave that is localized\nin space and moving at a\ncertain direction in time.\nAnd so if you plot it\nin both space and time,\nit occupies a region\nwhich looks like a tube.\nAnd so what can happen is\nthat you can have a wave\nwhich initially is very dispersed,\nbut it all focuses at a\nsingle point later in time.\nLike you can imagine\ndropping a pebble into a pond\nand ripples spread out.\nBut then if you time reverse that scenario\nand the equations of wave\nmotion are time reversible.\nYou can imagine ripples\nthat are converging\nto a single point, and\nthen a big splash occurs,\nmaybe even a singularity.\nAnd so it's possible to do that.\nAnd geometrically what's going\non is that there's always\nsort of light rays.\nSo like if this wave\nrepresents light, for example,\nyou can imagine this wave\nas the superposition of\nphotons all traveling\nat the speed of light.\nThey all travel on these light rays\nand they're all focusing\nat this one point.\nSo you can have a very\ndispersed wave focus\ninto a very concentrated\nwave at one point in space\nand time, but then it defocuses\nagain and it separates.\nBut potentially, if the conjecture\nhad a negative solution,\nso what that meant is that\nthere's a very efficient way\nto pack tubes pointing in\ndifferent directions into a very,\nvery narrow region of very narrow volume,\nthen you would also be able to\ncreate waves that start out,\nthere'll be some arrangement of waves\nthat start out very, very dispersed,\nbut they would concentrate\nnot just at a single point,\nbut there'll be a large,\nthere'll be a lot of\nconcentrations in space and time.\nAnd you could create\nwhat's called a blowup,\nwhere these waves, their amplitude becomes\nso great that the laws of\nphysics that they're governed by\nare no longer wave equations,\nbut something more\ncomplicated and nonlinear.\nAnd so in mathematical physics,\nwe care a lot about\nwhether certain equations\nin wave equations are stable or not,\nwhether they can create\nthese singularities.\nThere's a famous unsolved problem\ncalled the Navier-Stokes\nregularity problem.\nSo the Navier-Stokes equations\nthat govern a fluid flow\nor incompressible fluids like water.\nThe question asks,\nif you start with a smooth\nvelocity field of water,\ncan it ever concentrate\nso much that the velocity\nbecomes infinite at some point?\nThat's called a singularity.\nWe don't see that in real life.\nIf you splash around water in the bathtub,\nit won't explode on you,\nor have water leaving\nat the speed of light.\nBut potentially it is possible.\nAnd in fact, in recent years,\nthe consensus has drifted\ntowards the belief that in fact,\nfor certain very special\ninitial configurations of, say,\nwater, that singularities can form,\nbut people have not\nyet been able to actually establish this.\nThe Clay Foundation\nhas these seven Millennium Prize Problems,\nhas a million dollar prize\nfor solving one of these problems.\nThis is one of them.\nOf these seven, only one\nof them has been solved.\nThe Poincare conjecture by Perelman.", "mimetype": "text/plain", "start_char_idx": 3292, "end_char_idx": 7429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1206dc93-b5e7-401b-a666-13dec3fc7123": {"__data__": {"id_": "1206dc93-b5e7-401b-a666-13dec3fc7123", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5e6d35c-16f8-4b96-9f5b-1fef1b373346", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cfabc7939e93ab8569a8a3c35a5d8c5d4579ca24aa6d1973c72f866b190dbe52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bce9b038-2c86-44e0-a81a-9f1b0982fcda", "node_type": "1", "metadata": {}, "hash": "5f0c874fe99896fdef3160729fdb152ba37b49ed7c562438814d1cea81a57b3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The question asks,\nif you start with a smooth\nvelocity field of water,\ncan it ever concentrate\nso much that the velocity\nbecomes infinite at some point?\nThat's called a singularity.\nWe don't see that in real life.\nIf you splash around water in the bathtub,\nit won't explode on you,\nor have water leaving\nat the speed of light.\nBut potentially it is possible.\nAnd in fact, in recent years,\nthe consensus has drifted\ntowards the belief that in fact,\nfor certain very special\ninitial configurations of, say,\nwater, that singularities can form,\nbut people have not\nyet been able to actually establish this.\nThe Clay Foundation\nhas these seven Millennium Prize Problems,\nhas a million dollar prize\nfor solving one of these problems.\nThis is one of them.\nOf these seven, only one\nof them has been solved.\nThe Poincare conjecture by Perelman.\nSo the Kakeya conjecture is\nnot directly, directly related\nto the Navier-Stokes problem,\nbut understanding it would\nhelp us understand some aspects\nof things like wave concentration,\nwhich would indirectly\nprobably help us understand\nthe Navier-Stokes problem better.\n- Can you speak to the Navier-Stokes?\nSo the existence and\nsmoothness, like you said,\nMillennium Prize problem.\nYou've made a lot of progress on this one.\nIn 2016, you published a\npaper, \"Finite Time Blowup\nfor an Averaged Three-Dimensional\nNavier-Stokes Equation.\"\n- [Terence] Right.\n- So we're trying\nto figure out if this thing\nusually doesn't blow up.\n- [Terence] Right.\n- But can we say for\nsure it never blows up?\n- Right, yeah.\nSo, yeah, that is literally\nthe million-dollar question.\nSo this is what\ndistinguishes mathematicians\nfrom pretty much everybody else.\nLike, if something holds\n99.99% of the time,\nthat's good enough for most things.\nBut mathematicians are one\nof the few people who really\ncare about whether really 100%\nof all situations are covered by.\nYeah.\nSo most of the time,\nwater does not blow up.\nBut could you design a\nvery special initial state\nthat does this?\n- And maybe we should\nsay that this is a set\nof equations that govern in\nthe field of fluid dynamics.\nTrying to understand how fluid behaves\nand it actually turns out\nto be a really complicated fluid\nis extremely complicated\nthing to try to model.\n- Yeah, so it has practical importance.\nSo this Clay prize problem concerns\nwhat's called the\nIncompressible Navier-Stokes,\nwhich governs things like water.\nThere's something called the\nCompressible Navier-Stokes,\nwhich governs things like air.\nAnd that's particularly\nimportant for weather prediction.\nWeather prediction does a lot\nof computational fluid dynamics.\nA lot of it is actually just trying\nto solve the Navier-Stokes\nequations as best they can.\nAlso gathering a lot of data\nso that they can initialize the equation.\nThere's a lot of moving parts.\nSo it's a very important\nproblem, practically.\n- Why is it difficult to prove\ngeneral things about the set\nof equations like it not blowing up?\n- The short answer is Maxwell's demon.\nSo Maxwell's demon is a\nconcept in thermodynamics,\nlike if you have a box of two\ngases, oxygen and nitrogen,\nand maybe you started with\nall the oxygen on one side\nand nitrogen on the other side,\nbut there's no barrier between\nthem, then they will mix,\nand they should stay mixed.\nThere's no reason why they should unmix.\nBut in principle,\nbecause of all the\ncollisions between them,\nthere could be some sort\nof weird conspiracy.\nMaybe there's a microscopic demon\ncalled Maxwell's demon that will,\nevery time an oxygen and\nnitrogen atom collide,\nthey will bounce off\nin such a way that the oxygen\nsort of drifts onto one side\nand then nitrogen goes to the other,\nand you could have\nan extremely improbable\nconfiguration emerge,\nwhich we never see.\nAnd statistically, it's\nextremely unlikely.\nBut mathematically, it's\npossible that this can happen\nand we can't rule that out.\nAnd this is a situation that\nshows up a lot in mathematics.\nA basic example is the digits of pi.\n3.14159 and so forth.\nThe digits look like they have no pattern,\nand we believe they have no pattern.", "mimetype": "text/plain", "start_char_idx": 6594, "end_char_idx": 10649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bce9b038-2c86-44e0-a81a-9f1b0982fcda": {"__data__": {"id_": "bce9b038-2c86-44e0-a81a-9f1b0982fcda", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1206dc93-b5e7-401b-a666-13dec3fc7123", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2ad4e7d2030238050ee93557b8338fd9d50f17da0a74532aab2d0d55c5e6dcc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b97733a8-a172-4e51-9650-5224170cef27", "node_type": "1", "metadata": {}, "hash": "8bf4a0c5a94bc09d1abca109958a3810d793efd3490029c9c40bebf684206d1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There's no reason why they should unmix.\nBut in principle,\nbecause of all the\ncollisions between them,\nthere could be some sort\nof weird conspiracy.\nMaybe there's a microscopic demon\ncalled Maxwell's demon that will,\nevery time an oxygen and\nnitrogen atom collide,\nthey will bounce off\nin such a way that the oxygen\nsort of drifts onto one side\nand then nitrogen goes to the other,\nand you could have\nan extremely improbable\nconfiguration emerge,\nwhich we never see.\nAnd statistically, it's\nextremely unlikely.\nBut mathematically, it's\npossible that this can happen\nand we can't rule that out.\nAnd this is a situation that\nshows up a lot in mathematics.\nA basic example is the digits of pi.\n3.14159 and so forth.\nThe digits look like they have no pattern,\nand we believe they have no pattern.\nOn the long term, you\nshould see as many ones\nand twos and threes as\nfours and fives and sixes.\nThere should be no preference\nin the digits of pi to favor\nlet's say seven over eight.\nBut maybe there's some demon\nin the digits of pi that\nevery time you compute more\nand more digits, it's a\nbiases one digit to another.\nAnd this is a conspiracy\nthat should not happen.\nThere's no reason it should happen,\nbut there's no way to prove it\nwith our current technology.\nOkay, so getting back to Navier-Stokes,\na fluid has a certain amount of energy,\nand because the fluid is in motion,\nthe energy gets transported around.\nAnd water is also viscous.\nSo if the energy is spread out\nover many different locations,\nthe natural viscosity\nof the fluid will just damp out the energy\nand it will go to zero.\nAnd this is what happens\nwhen we actually experiment with water.\nYou splash around,\nthere's some turbulence\nand waves and so forth,\nbut eventually it settles down.\nAnd the lower the amplitude,\nthe smaller the velocity,\nthe more calm it gets.\nBut potentially there is some\nsort of demon that keeps\npushing the energy of the fluid\ninto a smaller and smaller scale.\nAnd it will move faster and\nfaster, and at faster speeds,\nthe effective viscosity\nis relatively less.\nAnd so it could happen\nthat it creates some\nsort of what's called a\nself-similar blob scenario\nwhere the energy of the fluid\nstarts off at some large scale\nand then it all sort\nof transfers its energy\ninto a smaller region of the fluid,\nwhich then at a much faster rate moves\ninto an even smaller region and so forth.\nAnd each time it does this,\nit takes maybe half as\nlong as the previous one.\nAnd then you could actually converge\nto all the energy concentrating\nin one point in a finite amount of time.\nAnd that scenario is\ncalled finite time blow up.\nSo in practice, this doesn't happen.\nSo water is what's called turbulent.\nSo it is true that if you\nhave a big eddy of water,\nit will tend to break\nup into smaller eddies,\nbut it won't transfer all the energy\nfrom one big eddy into one smaller eddy.\nIt will transfer into maybe three or four,\nand then those must\nsplit up into maybe three\nor four small eddies of their own.\nAnd so the energy gets dispersed\nto the point where the viscosity\ncan then keep everything under control.\nBut if it can somehow\nconcentrate all the energy,\nkeep it all together,\nand do it fast enough\nthat the viscous effects\ndon't have enough time\nto calm everything down,\nthen this blob can occur.\nSo there are papers who\nhad claimed that, oh,\nyou just need to take into\naccount conservation of energy\nand just carefully use the viscosity\nand you can keep everything under control\nfor not just the\nNavier-Stokes, but for many,\nmany types of equations like this.\nAnd so in the past, there\nhave been many attempts\nto try to obtain what's\ncalled global regularity\nfor Navier-Stokes, which is\nopposite of finite-time blowup,\nthat velocity stays smooth.\nAnd it all failed.\nThere was always some sign\nerror or some subtle mistake\nand it couldn't be salvaged.\nSo what I was interested\nin doing was trying\nto explain why we were not able\nto disprove finite-time blowup.\nI couldn't do it for the\nactual equations of fluids,\nwhich were too complicated.", "mimetype": "text/plain", "start_char_idx": 9857, "end_char_idx": 13879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b97733a8-a172-4e51-9650-5224170cef27": {"__data__": {"id_": "b97733a8-a172-4e51-9650-5224170cef27", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bce9b038-2c86-44e0-a81a-9f1b0982fcda", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e6d21db02cfd8ca3ceb33e936db60473b552f82974c998cba940b7de29c28dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72d58255-c070-451c-bfe6-30b2b09ac9ff", "node_type": "1", "metadata": {}, "hash": "e9c7962468cd02350fa3fb234e6eab429b10716cac0f7f9b410c4cea9404f497", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So there are papers who\nhad claimed that, oh,\nyou just need to take into\naccount conservation of energy\nand just carefully use the viscosity\nand you can keep everything under control\nfor not just the\nNavier-Stokes, but for many,\nmany types of equations like this.\nAnd so in the past, there\nhave been many attempts\nto try to obtain what's\ncalled global regularity\nfor Navier-Stokes, which is\nopposite of finite-time blowup,\nthat velocity stays smooth.\nAnd it all failed.\nThere was always some sign\nerror or some subtle mistake\nand it couldn't be salvaged.\nSo what I was interested\nin doing was trying\nto explain why we were not able\nto disprove finite-time blowup.\nI couldn't do it for the\nactual equations of fluids,\nwhich were too complicated.\nBut if I could average the equations\nof motion of Navier-Stokes, basically,\nif I could turn off certain types of ways\nin which water interacts and\nonly keep the ones that I want.\nSo in particular, if there's a fluid\nand it could transfer its energy\nfrom a large eddy into this small eddy\nor this other small eddy,\nI would turn off the energy channel\nthat would transfer energy to this one\nand direct it only\ninto this smaller eddy while\nstill preserving the law\nof concentration of energy.\n- So you're trying to make it blow up.\n- Yeah.\nSo I basically engineer a blowup\nby changing laws of physics,\nwhich is one thing that\nmathematicians are allowed to do,\nwe can change the equation.\n- How does that help you get closer\nto the proof of something?\nSo it provides what's called\nan obstruction in mathematics.\nSo what I did was that basically\nif I turned off certain\nparts of the equation,\nwhich usually when you turn\noff certain interactions,\nmake it less nonlinear,\nit makes it more regular\nand less likely to blow up.\nBut I found that\nby turning off a very well\ndesigned set of interactions,\nI could force all the energy\nto blow in finite time.\nSo what that means is that if you wanted\nto prove global regularity\nfor Navier-Stokes,\nfor the actual equation,\nyou must use some feature\nof the true equation,\nwhich my artificial\nequation does not satisfy.\nSo it rules out certain approaches.\nSo the thing about math is\nit's not just about finding,\ntaking a technique that is\ngoing to work and applying it,\nbut you need to not take the\ntechniques that don't work.\nAnd for the problems that are really hard,\noften there are dozens\nof ways that you might think might apply\nto solve the problem.\nBut it's only after a lot\nof experience that you\nrealize there's no way\nthat these methods are going to work.\nSo having these counter-examples\nfor nearby problems kind of rules out.\nIt saves you a lot of time,\nbecause you're not wasting energy\non things that you now know\ncannot possibly ever work.\n- How deeply connected is\nit to that specific problem\nof fluid dynamics?\nOr just some more general\nintuition you build up\nabout mathematics?\n- Right, yeah.\nSo the key phenomenon\nthat my technique exploits\nis what's called supercriticality.\nSo in partial differential equations,\noften these equations\nare like a tug of war\nbetween different forces.\nSo in Navier-Stokes,\nthere's the dissipation\nforce coming from viscosity,\nand it's very well\nunderstood, it's linear,\nit calms things down.\nIf viscosity was all there was,\nthen nothing bad would ever happen.\nBut there's also transport that energy\nfrom in one location of\nspace can get transported\nbecause the fluid is in\nmotion to other locations.\nAnd that's a nonlinear effect,\nand that causes all the problems.\nSo there are these two competing terms\nin the Navier-Stokes equation,\nthe dissipation term\nand the transport term.\nIf the dissipation term\ndominates, if it's large,\nthen basically you get regularity.\nAnd if the transport term dominates,\nthen we don't know what's going on.\nIt's a very nonlinear situation.\nIt's unpredictable, it's turbulent.\nSo sometimes these forces are\nunbalanced at small scales,\nbut not in balance at large\nscales, or vice versa.\nSo Navier-Stokes is what's\ncalled supercritical.\nSo at smaller and smaller scales,\nthe transport terms are much stronger\nthan the viscosity terms.\nSo the viscosity of the\nthings that calm things down.\nAnd so this is why the problem is hard.", "mimetype": "text/plain", "start_char_idx": 13135, "end_char_idx": 17320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "72d58255-c070-451c-bfe6-30b2b09ac9ff": {"__data__": {"id_": "72d58255-c070-451c-bfe6-30b2b09ac9ff", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b97733a8-a172-4e51-9650-5224170cef27", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "95df0066cf80a3e4133bfbdfc6201c20e5d609cdbfffab6bdec7dfdf5a2cf591", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f", "node_type": "1", "metadata": {}, "hash": "2260529bae13ab39c220b19cb2ad3f4b71068820a78f1e3a32a370ba60fdc1de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And that's a nonlinear effect,\nand that causes all the problems.\nSo there are these two competing terms\nin the Navier-Stokes equation,\nthe dissipation term\nand the transport term.\nIf the dissipation term\ndominates, if it's large,\nthen basically you get regularity.\nAnd if the transport term dominates,\nthen we don't know what's going on.\nIt's a very nonlinear situation.\nIt's unpredictable, it's turbulent.\nSo sometimes these forces are\nunbalanced at small scales,\nbut not in balance at large\nscales, or vice versa.\nSo Navier-Stokes is what's\ncalled supercritical.\nSo at smaller and smaller scales,\nthe transport terms are much stronger\nthan the viscosity terms.\nSo the viscosity of the\nthings that calm things down.\nAnd so this is why the problem is hard.\nIn two dimensions,\nso the Soviet mathematician Ladyzhenskaya,\nshe in the 60s showed\nin two dimensions there was no blowup.\nAnd in two dimensions,\nthe Navier-Stokes equations\nis what's called critical.\nThe effect of transport\nand the effect of viscosity are\nabout the same strength even\nat very, very small scales.\nAnd we have a lot of\ntechnology to handle critical\nand also subcritical equations\nand prove regularity.\nBut for supercritical equations,\nit was not clear what was going on.\nAnd I did a lot of work.\nAnd then there's been a lot\nof followup showing that\nfor many other types of\nsupercritical equations,\nyou can create all kinds\nof blow-up examples.\nOnce the nonlinear effects\ndominate the linear effects\nat small scales,\nyou can have all kinds\nof bad things happen.\nSo this is sort\nof one of the main insights\nof this line of work,\nis that supercriticality\nversus criticality\nand subcriticality, this\nmakes a big difference.\nI mean, that's a key qualitative feature\nthat distinguishes some\nequations for being sort of nice\nand predictable.\nAnd like planetary motion.\nI mean, there are certain\nequations that you can predict\nfor millions of years\nor thousands at least.\nAgain, it's not really a problem.\nBut there's a reason why we\ncan't predict the weather\npast two weeks into the future.\nBecause it's a super critical equation.\nLots of really strange things are going on\nat very fine scales.\n- So whenever there is some huge source\nof non-linearity that\ncan create a huge problem\nfor predicting what's going to happen.\n- Yeah.\nAnd if the nonlinearity is somehow more\nand more featured and\ninteresting at small scales.\nI mean, there's many\nequations that are nonlinear,\nbut in many equations you can\napproximate things by bulk.\nSo for example, planetary motion.\nIf you want to understand\nthe orbit of the moon\nor Mars or something,\nyou don't really need the microstructure\nof the seismology of the moon\nor exactly how the mass is distributed.\nYou just basically you can\nalmost approximate these planets\nby point masses.\nAnd just the aggregate\nbehavior is important.\nBut if you want to model\na fluid like the weather,\nyou can't just say in Los\nAngeles, the temperature is this,\nthe wind speed is this.\nFor supercritical equations,\nthe fine-scale information\nis really important.\n- If we can just linger\non the Navier-Stokes\nequations a little bit.\nSo you've suggested maybe\nyou can describe it,\nthat one of the ways to solve it\nor to negatively resolve it would be to\nsort of to construct a liquid,\na kind of liquid computer.\n- Right.\n- And then show that the halting problem\nfrom computation theory has\nconsequences for fluid dynamics.\nSo show it in that way.\nCan you describe this idea?\n- Right, yeah.\nSo this came out\nof this work of constructing\nthis average equation\nthat blew up.\nSo as part of how I had to do this,\nso there's sort of this\nnaive way to do it.\nYou just keep pushing.\nEvery time you get energy at one scale,\nyou push it immediately to the next scale\nas fast as possible.\nThis is sort of the naive\nway to force blowup.\nIn terms of,\nin five and higher dimensions this works,\nbut in three dimensions\nthere was this funny\nphenomenon that I discovered\nthat if you change laws of physics,\nyou just always keep\ntrying to push the energy\ninto smaller scales,\nwhat happens is that the energy\nstarts getting spread out\ninto many scales at once.", "mimetype": "text/plain", "start_char_idx": 16564, "end_char_idx": 20686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f": {"__data__": {"id_": "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72d58255-c070-451c-bfe6-30b2b09ac9ff", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "285b382fad52d82ff9db188ca011c831bae910b66165899571fb96becacaea8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d337958c-0629-48df-a615-d283de117337", "node_type": "1", "metadata": {}, "hash": "a190062e2fb09c5394e75b5fbaae21833de2b1f4487b246a91b7de4061a480e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Right.\n- And then show that the halting problem\nfrom computation theory has\nconsequences for fluid dynamics.\nSo show it in that way.\nCan you describe this idea?\n- Right, yeah.\nSo this came out\nof this work of constructing\nthis average equation\nthat blew up.\nSo as part of how I had to do this,\nso there's sort of this\nnaive way to do it.\nYou just keep pushing.\nEvery time you get energy at one scale,\nyou push it immediately to the next scale\nas fast as possible.\nThis is sort of the naive\nway to force blowup.\nIn terms of,\nin five and higher dimensions this works,\nbut in three dimensions\nthere was this funny\nphenomenon that I discovered\nthat if you change laws of physics,\nyou just always keep\ntrying to push the energy\ninto smaller scales,\nwhat happens is that the energy\nstarts getting spread out\ninto many scales at once.\nSo you have energy at one scale,\nyou're pushing it into the next scale,\nand then as soon as it enters that scale,\nyou also push it to the next scale.\nBut there's still some energy left over\nfrom the previous scale.\nYou're trying to do everything at once,\nand this spreads out the energy too much.\nAnd then it turns out that\nit makes it vulnerable\nfor viscosity to come in\nand actually just damp out everything.\nSo it turns out this directive portion\ndoesn't actually work.\nThere was a separate paper\nby some other authors\nthat actually showed this\nin three dimensions.\nSo what I needed was to program a delay,\nso kind of like airlocks.\nSo I needed an equation which would start\nwith a fluid doing something at one scale.\nIt would push its energy\ninto the next scale,\nbut it would stay there\nuntil all the energy\nfrom the larger scale got transferred.\nAnd only after you\npushed all the energy in,\nthen you sort of open the next gate,\nand then you push that in as well.\nSo by doing that,\nthe energy inches forward, scale by scale,\nin such a way that it's always localized\nat one scale at a time.\nAnd then it can resist\nthe effects of viscosity\nbecause it's not dispersed.\nSo in order to make that happen, yeah,\nI had to construct a rather\ncomplicated nonlinearity.\nAnd it was basically\nlike it was constructing,\nlike an electronic circuit.\nSo I actually thanked my wife for this\nbecause she was trained\nas an electrical engineer,\nand she talked about,\nshe had to design circuits and so forth.\nAnd if you want a circuit\nthat does a certain thing,\nlike maybe have a light that flashes on\nand then turns off, and\nthen on and then off,\nyou can build it from\nmore primitive components,\ncapacitors and resistors and so forth.\nAnd you have to build a diagram,\nand these diagrams you can sort\nof follow with your eyeballs\nand say, \"Oh, yeah, the\ncurrent will build up here,\nand then it will stop,\nand then it will do that.\"\nSo I knew how to build the analog\nof basic electronic\ncomponents like resistors\nand capacitors and so forth.\nAnd I would stack them together\nin such a way that I\nwould create something\nthat would open one gate,\nand then there'll be a clock.\nAnd then once the clock\nhits the certain threshold,\nit would close it.\nIt was kind of a Rube\nGoldberg type machine,\nbut described mathematically,\nand this ended up working.\nSo what I realized is that\nif you could pull the same thing off\nfor the actual equations.\nSo if the equations of\nwater supported computation.\nSo you can imagine kind of a steampunk,\nbut it's really water\npunk type of thing where\nso modern computers are electronic.\nThey're powered by electrons\npassing through very tiny wires\nand interacting with other\nelectrons and so forth.\nBut instead of electrons,\nyou can imagine these pulses\nof water moving a certain\nvelocity and maybe there\nare two different\nconfigurations corresponding\nto a bit being up or down,\nprobably that if you had two\nof these moving bodies of water collide,\nit would come out with\nsome new configuration\nwhich would be something\nlike an AND gate or OR gate.\nThe output would depend\nin a very predictable way on the inputs.\nAnd like you could chain these together\nand maybe create a Turing machine.\nAnd then you have computers\nwhich are made completely out of water.\nAnd if you have computers,\nthen maybe you can do robotics,\nhydraulics and so forth.", "mimetype": "text/plain", "start_char_idx": 19857, "end_char_idx": 24033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d337958c-0629-48df-a615-d283de117337": {"__data__": {"id_": "d337958c-0629-48df-a615-d283de117337", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "771bb4a3-a87a-4f8d-8aeb-d75093d7e95f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1cf4ce16a94df8ce90fa5b77c39fe330d0545f7af35213f5f1d8c70b5b50565c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c88be701-91ca-41ce-aa46-2dd9476dc5df", "node_type": "1", "metadata": {}, "hash": "765f072ba3d3a9d82f9252adcb17b31453ea28572f2f218f48b311f8677837ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So you can imagine kind of a steampunk,\nbut it's really water\npunk type of thing where\nso modern computers are electronic.\nThey're powered by electrons\npassing through very tiny wires\nand interacting with other\nelectrons and so forth.\nBut instead of electrons,\nyou can imagine these pulses\nof water moving a certain\nvelocity and maybe there\nare two different\nconfigurations corresponding\nto a bit being up or down,\nprobably that if you had two\nof these moving bodies of water collide,\nit would come out with\nsome new configuration\nwhich would be something\nlike an AND gate or OR gate.\nThe output would depend\nin a very predictable way on the inputs.\nAnd like you could chain these together\nand maybe create a Turing machine.\nAnd then you have computers\nwhich are made completely out of water.\nAnd if you have computers,\nthen maybe you can do robotics,\nhydraulics and so forth.\nAnd so you could create some machine\nwhich is basically a fluid analog,\nwhat's called a von Neumann machine.\nSo von Neumann proposed if\nyou want to colonize Mars,\nthe sheer cost of transporting people\non machines to Mars is just ridiculous.\nBut if you could transport\none machine to Mars\nand this machine had the\nability to mine the planet,\ncreate some raw materials, smelt them,\nand build more copies of the same machine,\nthen you could colonize\nthe whole planet over time.\nSo if you could build\na fluid machine, which,\nyeah, so it's a fluid robot.\nAnd what it would do, its purpose in life,\nit's programmed\nso that it would create a\nsmaller version of itself\nin some sort of cold state.\nIt wouldn't start just yet.\nOnce it's ready, the big\nrobot configured of water\nwould transfer all its energy\ninto the smaller configuration\nand then power down and\nthen clean itself up.\nAnd then what's left is this newer state\nwhich would then turn on\nand do the same thing,\nbut smaller and faster.\nAnd then the equation has\na certain scaling symmetry.\nOnce you do that, it\ncan just keep iterating.\nSo this in principle would create a blowup\nfor the actual Navier-Stokes.\nAnd this is what I managed to accomplish\nfor this average Navier-Stokes.\nSo it provided this sort of\nroadmap to solve the problem.\nNow this is a pipe dream,\nbecause there are so many\nthings that are missing\nfor this to actually be a reality.\nSo I can't create these basic logic gates.\nI don't have these special\nconfigurations of water.\nI mean, there's candidates,\nthings like vortex rings\nthat might possibly work.\nBut also analog computing is really nasty\ncompared to digital computing\nbecause there's always errors.\nYou have to do a lot of error\ncorrection along the way.\nI don't know how to completely\npower down the big machine\nso that it doesn't\ninterfere with the running\nof the smaller machine.\nBut everything in principle can happen.\nIt doesn't contradict any\nof the laws of physics.\nSo it's sort of evidence\nthat this thing is possible.\nThere are other groups\nwho are now pursuing ways\nto make Navier-Stokes blowup,\nwhich are nowhere near as\nridiculously complicated as this.\nThey actually are pursuing much closer\nto the direct self\nsimilar model which can,\nit doesn't quite work as is,\nbut there could be some simpler scheme\nthan what I just described\nto make this work.\n- There is a real leap\nof genius here to go\nfrom Navier-Stokes to this Turing machine.\nSo it goes from what the\nself-similar blob scenario\nthat you're trying to get\nthe smaller and smaller blob\nto now having a liquid\nTuring machine gets smaller\nand smaller and smaller.\nAnd somehow seeing how that could be used\nto say something about a blowup.\nI mean, that's a big leap.\n- So there's precedent.\nI mean, so the thing about mathematics\nis that it's really good\nat spotting connections\nbetween what you think of,\nwhat you might think of as\ncompletely different problems.\nBut if the mathematical form is the same,\nyou can draw a connection.\nSo there's a lot of work previously\non what are called cellular automata,\nthe most famous of which\nis Conway's Game of Life.\nThere's this infinite discrete grid,\nand at any given time the\ngrid is either occupied\nby a cell or it's empty.\nAnd there's a very simple rule\nthat tells you how these cells evolve.\nSo sometimes cells live\nand sometimes they die.", "mimetype": "text/plain", "start_char_idx": 23157, "end_char_idx": 27370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c88be701-91ca-41ce-aa46-2dd9476dc5df": {"__data__": {"id_": "c88be701-91ca-41ce-aa46-2dd9476dc5df", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d337958c-0629-48df-a615-d283de117337", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "df209acfa59e54f5478f444685af08edeb2b51da45480d04c49ffeafa41edb16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f584879-68ce-4fd4-a180-57df559f707f", "node_type": "1", "metadata": {}, "hash": "33928fa4894f4a65e050f4f98437cb0857626503250926ff775a0d5a75d900cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And somehow seeing how that could be used\nto say something about a blowup.\nI mean, that's a big leap.\n- So there's precedent.\nI mean, so the thing about mathematics\nis that it's really good\nat spotting connections\nbetween what you think of,\nwhat you might think of as\ncompletely different problems.\nBut if the mathematical form is the same,\nyou can draw a connection.\nSo there's a lot of work previously\non what are called cellular automata,\nthe most famous of which\nis Conway's Game of Life.\nThere's this infinite discrete grid,\nand at any given time the\ngrid is either occupied\nby a cell or it's empty.\nAnd there's a very simple rule\nthat tells you how these cells evolve.\nSo sometimes cells live\nand sometimes they die.\nAnd this is when I was a student,\nit was a very popular screensaver\nto actually just have\nthese animations going,\nand they look very chaotic.\nIn fact they look a little bit\nlike turbulent flow sometimes.\nBut at some point people discovered more\nand more interesting structures\nwithin this Game of Life.\nSo for example, they discovered\nthis thing called a glider.\nSo a glider is a very tiny\nconfiguration of like four\nor five cells which evolves\nand it just moves at a certain direction\nand that's like these vortex rings.\nSo this is an analogy.\nThe Game of Life is kind of\nlike a discrete equation.\nAnd the fluid Navier-Stokes\nis a continuous equation.\nBut mathematically they\nhave some similar features.\nAnd so over time, people discovered more\nand more interesting\nthings that you could build\nwithin the Game of Life.\nThe Game of Life is a very simple system.\nIt only has like three\nor four rules to do it,\nbut you can design all kinds\nof interesting configurations inside it.\nThere's something called a\nglider gun that does nothing\nof spit out gliders one and one at a time.\nAnd then after a lot of effort,\npeople managed to create AND\ngates and OR gates for gliders.\nLike there's this massive\nridiculous structure\nwhich if you have a stream\nof gliders coming in here\nand a stream of gliders coming in here,\nthen you may produce a\nstream of gliders coming out.\nMaybe if both of the streams have gliders,\nthen there'll be an output stream.\nBut if only one of them\ndoes, then nothing comes out.\nSo they could build something like that.\nAnd once you could\nbuild these basic gates,\nthen just from software engineering,\nyou can build almost anything.\nYou can build a Turing machine.\nI mean, it's again, enormous\nsteampunk type things,\nthey look ridiculous.\nBut then people also generated\nself-replicating objects\nin the Game of Life.\nA massive machine, a polynomial machine,\nwhich over a huge period of time\nand always look like glider guns\ninside doing these very\nsteampunk calculations.\nIt would create another version\nof itself which could replicate.\n- That's so incredible.\n- A lot of this was community crowdsourced\nby amateur mathematicians, actually.\nSo I knew about that work.\nAnd so that is part of what inspired me\nto propose the same\nthing with Navier-Stokes,\nwhich is a much, as I said,\nanalog is much worse than digital.\nYou can't just directly\ntake the constructions\nin the Game of Life and plunk them in.\nBut again, it just, it\nshows it's possible.\n- You know, there's a kind\nof emergence that happens\nwith these cellular automata, local rules.\nMaybe it's similar to\nfluids, I don't know.\nBut local rules operating\nat scale can create\nthese incredibly complex\ndynamic structures.\nDo you think any\nof that is amenable to\nmathematical analysis?\nDo we have the tools to say\nsomething profound about that?\n- The thing is,\nyou can get this emergent\nvery complicated structures,\nbut only with very carefully\nprepared initial conditions.\nSo these glider guns and\ngates and software machines,\nif you just plunk on randomly some cells,\nand you will not see any of these.\nAnd that's the analogous\nsituation with Navier-Stokes.\nAgain, with typical initial conditions,\nyou will not have any of this\nweird computation going on.\nBut basically through engineering,\nby specially designing\nthings in a very special way,\nyou can pick clever constructions.\n- I wonder if it's possible to prove\nthe sort of the negative of,\nbasically prove that\nonly through engineering\ncan you ever create something interesting.", "mimetype": "text/plain", "start_char_idx": 26648, "end_char_idx": 30876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f584879-68ce-4fd4-a180-57df559f707f": {"__data__": {"id_": "8f584879-68ce-4fd4-a180-57df559f707f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c88be701-91ca-41ce-aa46-2dd9476dc5df", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "db94c159f8ee69379cc5d58d411b06eee6058232b9b5f004e959f00bea3a2303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "884039c6-0f37-4044-a72f-a969f57be5a9", "node_type": "1", "metadata": {}, "hash": "0d2031b9e3e67c7bec77ee6a1e6ec090097763def80ce3f0b0c2afba13df2644", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But local rules operating\nat scale can create\nthese incredibly complex\ndynamic structures.\nDo you think any\nof that is amenable to\nmathematical analysis?\nDo we have the tools to say\nsomething profound about that?\n- The thing is,\nyou can get this emergent\nvery complicated structures,\nbut only with very carefully\nprepared initial conditions.\nSo these glider guns and\ngates and software machines,\nif you just plunk on randomly some cells,\nand you will not see any of these.\nAnd that's the analogous\nsituation with Navier-Stokes.\nAgain, with typical initial conditions,\nyou will not have any of this\nweird computation going on.\nBut basically through engineering,\nby specially designing\nthings in a very special way,\nyou can pick clever constructions.\n- I wonder if it's possible to prove\nthe sort of the negative of,\nbasically prove that\nonly through engineering\ncan you ever create something interesting.\n- This is a recurring challenge\nin mathematics that I\ncall it the dichotomy\nbetween structure and randomness.\nThat most objects that you can generate\nin mathematics are random.\nThey look like random.\nThe digits of pi, well, we\nbelieve is a good example,\nbut there's a very small number\nof things that have patterns.\nBut now you can prove something\nas a pattern by just constructing.\nIf something has a simple\npattern and you have a proof\nthat it does something like\nrepeat itself every so often,\nyou can do that, and you can prove that,\nfor example, you can\nprove that most sequences\nof digits have no pattern.\nSo if you just pick digits randomly,\nthere's something called low large numbers\nthat tells you you're going to get\nas many ones as twos in the long run.\nBut we have a lot fewer tools,\nif I give you a specific\npattern like the digits of pi,\nhow can I show that this\ndoesn't have some weird pattern to it?\nSome other work that I spend a lot\nof time on is to prove what\nare called structure theorems\nor inverse theorems that give tests\nfor when something is very structured.\nSo some functions are\nwhat's called additive,\nlike if you have a function\nthat must say natural numbers,\nthe natural numbers.\nSo maybe two maps to four,\nthree maps to six, and so forth.\nSome functions also additive,\nwhich means that if you\nadd two inputs together,\nthe output gets added as well.\nFor example, multiplying by a constant,\nif you multiply a number by 10,\nif you multiply A plus B by 10,\nthat's the same as multiplying A by 10\nand B by 10 and then adding them together.\nSo some functions are additive.\nSome functions are kind of additive,\nbut not completely additive.\nSo for example, if I take a number\nand I multiply by the square root of 2\nand I take the integer part of that.\nSo 10 by square root of 2\nis like 14 point something.\nSo 10 up to 14.\n21 up to 28.\nSo in that case additively is true then.\nSo 10 plus 10 is 20, and 14 plus 14 is 28.\nBut because of this rounding,\nsometimes there's round off errors,\nand sometimes when you add A plus B,\nthis function doesn't\nquite give you the sum\nof the two individual outputs,\nbut the sum plus minus one.\nSo it's almost additive,\nbut not quite additive.\nSo there's a lot of useful\nresults in mathematics,\nand I've worked a lot on\ndeveloping things like this,\nto the effect that\nif a function exhibits\nsome structure like this,\nthen it's basically there's\na reason for why it's true.\nAnd the reason is\nbecause there's some other nearby function\nwhich is actually completely structured,\nwhich is explaining this\nsort of partial pattern that you have.\nAnd so if you have these inverse theorems,\nit creates this sort of dichotomy\nthat either the objects\nthat you study either\nhave no structure at all,\nor they are somehow related to\nsomething that is structured.\nAnd in either case you can make progress.\nA good example of this is\nthat there's this old theorem\nin mathematics called Szemeredi's Theorem\nproven in the 1970s.\nIt concerns trying to find\na certain type of pattern\nin a set of numbers.\nThe pattern is arithmetic progression,\nthings like 3, 5, and 7,\nor 10, 15, and 20.", "mimetype": "text/plain", "start_char_idx": 29973, "end_char_idx": 33996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "884039c6-0f37-4044-a72f-a969f57be5a9": {"__data__": {"id_": "884039c6-0f37-4044-a72f-a969f57be5a9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f584879-68ce-4fd4-a180-57df559f707f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c7d919357c19d5f0e7a053e7ccebf5e00b9e291e4369db558d46798cff76f37e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ffd685f-5ade-4c0a-af64-cae596918a8c", "node_type": "1", "metadata": {}, "hash": "95754ea7b07940f0807a149e2c0dc4b1a968f6e631cf25758c0903675d89e6be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the reason is\nbecause there's some other nearby function\nwhich is actually completely structured,\nwhich is explaining this\nsort of partial pattern that you have.\nAnd so if you have these inverse theorems,\nit creates this sort of dichotomy\nthat either the objects\nthat you study either\nhave no structure at all,\nor they are somehow related to\nsomething that is structured.\nAnd in either case you can make progress.\nA good example of this is\nthat there's this old theorem\nin mathematics called Szemeredi's Theorem\nproven in the 1970s.\nIt concerns trying to find\na certain type of pattern\nin a set of numbers.\nThe pattern is arithmetic progression,\nthings like 3, 5, and 7,\nor 10, 15, and 20.\nAnd Szemeredi, Endre Szemeredi,\nproved that any set\nof numbers that is sufficiently big,\nwhat's called positive density,\nhas arithmetic progressions\nin it of any length you wish.\nSo, for example,\nthe odd numbers have a\nset of density one half,\nand they contain arithmetic\nprogressions of any length.\nSo in that case,\nit's obvious because the\nodd numbers are really,\nreally structured.\nI can just take 11, 13, 15, 17.\nI can easily find arithmetic\nexpressions in that set.\nBut Szemeredi also applies to random sets.\nIf I take the set of odd numbers\nand I flip a coin for each number,\nand I only keep the numbers\nfor which I got a heads,\nso I just flip coins.\nI just randomly take out half\nthe numbers, I keep one half.\nSo that's a set that\nhas no patterns at all,\nbut just from random fluctuations,\nyou will still get a lot\nof arithmetic progressions in that set.\n- Can you prove that there's\narithmetic progressions\nof arbitrary length within a random?\n- Yes.\nHave you heard of the\ninfinite monkey theorem?\nUsually mathematicians give\nboring names to theories,\nbut occasionally they give colorful names.\nThe popular version\nof the infinite monkey theorem is that\nif you have an infinite number of monkeys\nin a room with each with a typewriter,\nthey type out text randomly,\nalmost surely one\nof them is going to generate\nthe entire script of Hamlet\nor any other finite string of text.\nIt will just take some time,\nquite a lot of time, actually.\nBut if you have an infinite\nnumber, then it happens.\nSo basically, the theorem says that\nif you take an infinite\nstring of digits or whatever,\neventually any finite\npattern you wish will emerge.\nIt may take a long time, but\nit will eventually happen.\nIn particular, arithmetic progressions\nof any length will eventually happen.\nBut you need an extremely\nlong random sequence\nfor this to happen.\n- I suppose that's intuitive.\nIt's just infinity.\n- Yeah, infinity absorbs a lot of sins.\n- Yeah.\nHow are we humans supposed\nto deal with infinity?\n- Well, you can think of infinity\nas just an abstraction of a finite number\nfor which you do not have a bound for.\nI mean, so nothing in real\nlife is truly infinite.\nBut you can ask yourself questions like,\nwhat if I had as much money as I wanted?\nOr what if I could go as fast as I wanted?\nAnd a way in which\nmathematicians formalize that\nis mathematics has found\na formalism to idealize.\nInstead of something being extremely large\nor extremely small, to actually\nbe exactly infinite or zero.\nAnd often the mathematics\nbecomes a lot cleaner\nwhen you do that.\nI mean, in physics we joke\nabout assuming spherical cows.\nReal world problems have got all kinds\nof real world effects,\nbut you can idealize,\nsend things to infinity,\nsend something to zero,\nand the mathematics becomes a lot simpler\nto work with there.\n- I wonder how often using\ninfinity forces us to deviate\nfrom the physics of reality.\n- Yeah, so there's a lot of pitfalls.\nSo we spend a lot of time\nin undergraduate math\nclasses teaching analysis.\nAnd analysis is often\nabout how to take limits\nand whether you,\nso for example, A plus\nB is always B plus A.\nSo when you have a finite number\nof terms and you add them,\nyou can swap them and there's no problem.", "mimetype": "text/plain", "start_char_idx": 33303, "end_char_idx": 37205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ffd685f-5ade-4c0a-af64-cae596918a8c": {"__data__": {"id_": "5ffd685f-5ade-4c0a-af64-cae596918a8c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "884039c6-0f37-4044-a72f-a969f57be5a9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c4c4da6066cb72a722c33cee868cfcd5080390b8b73048742477a69ef49bee80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3e00c42-805a-4a29-89ab-4052f58a07b6", "node_type": "1", "metadata": {}, "hash": "eabad0491dee6b51860a0e1e481d0f914ada8e873a3939ecafaae20af533f0c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of something being extremely large\nor extremely small, to actually\nbe exactly infinite or zero.\nAnd often the mathematics\nbecomes a lot cleaner\nwhen you do that.\nI mean, in physics we joke\nabout assuming spherical cows.\nReal world problems have got all kinds\nof real world effects,\nbut you can idealize,\nsend things to infinity,\nsend something to zero,\nand the mathematics becomes a lot simpler\nto work with there.\n- I wonder how often using\ninfinity forces us to deviate\nfrom the physics of reality.\n- Yeah, so there's a lot of pitfalls.\nSo we spend a lot of time\nin undergraduate math\nclasses teaching analysis.\nAnd analysis is often\nabout how to take limits\nand whether you,\nso for example, A plus\nB is always B plus A.\nSo when you have a finite number\nof terms and you add them,\nyou can swap them and there's no problem.\nBut when you have an\ninfinite number of terms,\nthere are these sort of\nshell games you can play\nwhere you can have a series\nwhich converges to one\nvalue, but you rearrange it\nand it suddenly converges\nto another value.\nAnd so you can make mistakes.\nYou have to know what you're doing\nwhen you allow infinity.\nYou have to introduce\nthese epsilons and deltas.\nAnd there's a certain type\nof way of reasoning that\nhelps you avoid mistakes.\nIn more recent years,\npeople have started taking\nresults that are true\nin infinite limits and what's\ncalled finitizing them.\nSo you know that\nsomething's true eventually,\nbut you don't know when.\nNow give me a rate.\nOkay, so such that if I\ndon't have an infinite number\nof monkeys, but a large\nfinite number of monkeys,\nhow long do I have to wait\nfor Hamlet to come out?\nAnd that's a more quantitative question.\nAnd this is something that you can attack\nby purely finite methods.\nAnd you can use your finite intuition,\nand in this case it turns\nout to be exponential\nin the length of the text that\nyou're trying to generate.\nAnd so this is why you never\nsee the monkeys create Hamlet.\nYou can maybe see them\ncreate a four-letter word,\nbut nothing that big.\nAnd so I personally find\nonce you finitize an infinite statement,\nit does become much more intuitive\nand it's no longer so weird.\n- So even if you're working with infinity,\nit's good to finitize so that\nyou can have some intuition.\n- Yeah.\nThe downside is\nthat the finitized proofs\nare just much, much messier.\nSo the infinite ones are found first,\nusually, like decades earlier,\nand then later on people finalize them.\n- So since we mentioned a lot\nof math and a lot of physics,\nwhat to use the difference\nbetween mathematics\nand physics as disciplines,\nas ways of understanding\nof seeing the world?\nMaybe we can throw in\nengineering in there.\nYou mentioned your wife is an engineer.\nGive it new perspective on circuits.\nSo this different way\nof looking at the world,\ngiven that you've done\nmathematical physics,\nyou've worn all the hats.\n- Right, so I think science\nin general is interaction\nbetween three things.\nThere's the real world,\nthere's what we observe of the\nreal world, our observations,\nand then our mental models\nas to how we think the world works.\nSo we can't directly access reality.\nOkay.\nAll we have are the observations,\nwhich are incomplete and they have errors.\nAnd there are many cases\nwhere we want to know,\nfor example, what is the\nweather like tomorrow?\nAnd we don't yet have the observation\nand we'd like to predict.\nAnd then we have these simplified models,\nsometimes making unrealistic assumptions,\nspherical cow type things.\nThose are the mathematical models.\nMathematics is concerned with the models.\nScience collects the observations\nand it proposes the models\nthat might explain these observations.\nWhat mathematics does,\nwe stay within the model, and we ask,\nwhat are the consequences of that model?\nWhat predictions would the model\nmake of future observations\nor past observations?\nDoes it fit observed data?\nSo there's definitely a symbiosis.\nI guess mathematics is unusual\namong other disciplines,\nis that we start from hypotheses,\nlike the axioms of a model,\nand ask what conclusions\ncome up from that model.\nIn almost any other discipline,\nyou start with the conclusions.\nI want to do this.\nI want to build a bridge,\nI want to make money,\nI want to do this.\nAnd then you find the paths to get there.", "mimetype": "text/plain", "start_char_idx": 36373, "end_char_idx": 40637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3e00c42-805a-4a29-89ab-4052f58a07b6": {"__data__": {"id_": "e3e00c42-805a-4a29-89ab-4052f58a07b6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ffd685f-5ade-4c0a-af64-cae596918a8c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "581cb92dc20064dc5d3fa73d9dae7f672d471969dff9fc5c0d10fdee0a12409d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23c90370-f267-49b8-95dd-e98132ab8228", "node_type": "1", "metadata": {}, "hash": "533b6fa8848b9470f6c45fee53307acde819016ec89e386fb90b994e553d40f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we don't yet have the observation\nand we'd like to predict.\nAnd then we have these simplified models,\nsometimes making unrealistic assumptions,\nspherical cow type things.\nThose are the mathematical models.\nMathematics is concerned with the models.\nScience collects the observations\nand it proposes the models\nthat might explain these observations.\nWhat mathematics does,\nwe stay within the model, and we ask,\nwhat are the consequences of that model?\nWhat predictions would the model\nmake of future observations\nor past observations?\nDoes it fit observed data?\nSo there's definitely a symbiosis.\nI guess mathematics is unusual\namong other disciplines,\nis that we start from hypotheses,\nlike the axioms of a model,\nand ask what conclusions\ncome up from that model.\nIn almost any other discipline,\nyou start with the conclusions.\nI want to do this.\nI want to build a bridge,\nI want to make money,\nI want to do this.\nAnd then you find the paths to get there.\nThere's a lot less sort\nof speculation about,\nsuppose I did this, what would happen?\nPlanning and modeling.\nSpeculative fiction\nmaybe is one other place,\nbut that's about it, actually.\nMost of the things we do in\nlife is conclusions driven,\nincluding physics and science.\nI mean, they want to know,\nwhere is this asteroid going to go?\nWhat is the weather going to be tomorrow?\nBut mathematics also has\nthis other direction of going\nfrom the axioms.\n- What do you think?\nThere is this tension in\nphysics between theory\nand experiment.\nWhat do you think is a more powerful way\nof discovering truly\nnovel ideas about reality?\n- Well, you need both\ntop-down and bottom-up.\nYeah, it's a really interaction\nbetween all these things.\nSo over time, the\nobservations and the theory\nand the modeling should\nboth get closer to reality.\nBut initially, this is always the case,\nthey're always far apart to begin with.\nBut you need one to figure\nout where to push the other.\nSo if your model is predicting anomalies\nthat are not picked up by experiment,\nthat tells experimenters where\nto look to find more data,\nto refine the models.\nSo it goes back and forth.\nWithin mathematics itself,\nthere's also a theory and\nexperimental component.\nIt's just that until very recently,\ntheory has dominated almost completely.\nLike 99% of mathematics is\ntheoretical mathematics.\nAnd there's a very tiny amount\nof experimental mathematics.\nI mean, people do do it if they\nwant to study prime numbers\nor whatever, they can just\ngenerate large data sets.\nSo once we had a computers, we\nbegan to do it a little bit.\nAlthough even before,\nwell, like Gauss, for example,\nhe discovered, he conjectured\nthe most basic theorem\nin number theory to call\nthe prime number theorem,\nwhich predicts how many\nprimes that up to a million,\nup to a trillion.\nIt's not an obvious question.\nAnd basically what he did\nwas that he computed, I mean,\nmostly by himself,\nbut also hired human computers,\npeople whose professional\njob it was to do arithmetic,\nto compute the first hundred\nthousand primes or something\nand made tables and made a prediction.\nThat was an early example\nof experimental mathematics,\nbut until very recently it was not,\nI mean, theoretical mathematics\nwas just much more successful.\nI mean, of course,\ndoing complicated\nmathematical computations\nwas just not feasible until very recently.\nAnd even nowadays, even though\nwe have powerful computers,\nonly some mathematical things\ncan be explored numerically.\nThere's something called\nthe combinatorial explosion.\nIf you want to study, for\nexample, Zsigmondy's theorem,\nyou want to study all possible subsets\nof the numbers 1 to 1000.\nThere's only 1000 numbers.\nHow bad could it be?\nIt turns out the number\nof different subsets of 1 to\n1000 is 2 to the power 1000,\nwhich is way bigger than\nany computer can currently,\nin fact, any computer\never will ever enumerate.\nSo there are certain math problems\nthat very quickly become\njust intractable to attack\nby direct brute force computation.\nChess is another famous example.\nThe number of chess positions,\nwe can't get a computer to fully explore.\nBut now we have AI,\nwe have tools to explore this space,\nnot with 100% guarantees of\nsuccess, but with experiment.\nSo we can empirically solve\nchess now, for example.", "mimetype": "text/plain", "start_char_idx": 39679, "end_char_idx": 43918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "23c90370-f267-49b8-95dd-e98132ab8228": {"__data__": {"id_": "23c90370-f267-49b8-95dd-e98132ab8228", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3e00c42-805a-4a29-89ab-4052f58a07b6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "644a2add5d8d7a6fd8d214660c70bc0e3fb59accbe1623763698ad7d83556595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b71e743-c993-4693-975d-9cee2c7b23c6", "node_type": "1", "metadata": {}, "hash": "27c6a66edc50a016f9f05fda57bdb4cf2305421964e6114d8060f808e87ae4e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There's something called\nthe combinatorial explosion.\nIf you want to study, for\nexample, Zsigmondy's theorem,\nyou want to study all possible subsets\nof the numbers 1 to 1000.\nThere's only 1000 numbers.\nHow bad could it be?\nIt turns out the number\nof different subsets of 1 to\n1000 is 2 to the power 1000,\nwhich is way bigger than\nany computer can currently,\nin fact, any computer\never will ever enumerate.\nSo there are certain math problems\nthat very quickly become\njust intractable to attack\nby direct brute force computation.\nChess is another famous example.\nThe number of chess positions,\nwe can't get a computer to fully explore.\nBut now we have AI,\nwe have tools to explore this space,\nnot with 100% guarantees of\nsuccess, but with experiment.\nSo we can empirically solve\nchess now, for example.\nWe have very, very good AIs that can,\nthey don't explore every single\nposition in the game tree,\nbut they have found some\nvery good approximation.\nAnd people are using\nactually these chess engines\nto do experimental chess\nthat they're revisiting old\nchess theories about, oh,\nthis type of opening, this is a good type\nof move, this is not.\nAnd they can use these chess\nengines to actually refine,\nin some cases overturn,\nconventional wisdom about chess.\nAnd I do hope that mathematics\nwill have a larger experimental\ncomponent in the future,\nperhaps powered by AI.\n- We'll of course talk about that.\nBut in the case of chess,\nand there's a similar\nthing in mathematics,\nI don't believe it's providing\na kind of formal explanation\nof the different positions.\nIt's just saying which\nposition is better or not.\nAnd you can intuit as a human being,\nand then from that we humans\ncan construct a theory of the matter.\nYou've mentioned the\nPlato's cave allegory.\nSo in case people don't know,\nit's where people are\nobserving shadows of reality,\nnot reality itself.\nAnd they believe what they're\nobserving to be reality.\nIs that in some sense what mathematicians\nand maybe all humans are doing is looking\nat shadows of reality?\nIs it possible for us\nto truly access reality?\n- Well, there are these\nthree ontological things.\nThere's actual reality,\nthere's our observations, and our models.\nAnd technically they are distinct,\nand I think they will always be distinct.\nBut they can get closer over time.\nAnd the process\nof getting closer often\nmeans that you have\nto discard your initial intuitions.\nSo astronomy provides great examples.\nAn initial model of the world is flat\nbecause it looks flat, and it's big.\nAnd the rest of the\nuniverse, the sky is not,\nlike the sun,\nfor example, looks really tiny.\nAnd so you start off\nwith a model which is actually\nreally far from reality,\nbut it fits kind of the\nobservations that you have.\nSo things look good.\nBut over time, as you make more\nand more observations\nbring it closer to reality,\nthe model gets dragged along with it.\nAnd so over time,\nwe had to realize that the\nEarth was round, that it spins,\nit goes around the solar system,\nsolar system goes around the\ngalaxy, and so on and so forth.\nAnd the galaxy universe is expanding.\nThe expansion is itself\nexpanding, accelerating.\nAnd in fact, very\nrecently this year or so,\neven the acceleration\nof the universe itself,\nthere's evidence that it's non-constant.\n- And the explanation behind why that is.\n- [Terence] It's catching up.\n- It's catching up.\nI mean, it's still the\ndark matter, dark energy,\nthis kind of thing.\n- We have a model that sort of explains,\nthat fits the data really well.\nIt just has a few parameters\nthat you have to specify.\nSo people say, oh, that's fudge factors.\nWith enough fudge factors,\nyou can explain anything.\nBut the mathematical point\nof the model is that you\nwant to have fewer parameters\nin your model than data points\nin your observational set.\nSo if you have a model\nwith 10 parameters that\nexplains 10 observations,\nthat is a completely useless model.\nIt's what's called overfitted.\nBut if you have a model\nwith two parameters\nand it explains a trillion observations,\nwhich is basically so,\nyeah, the dark matter model,\nI think has like 14 parameters\nand it explains petabytes of\ndata that the astronomers have.", "mimetype": "text/plain", "start_char_idx": 43118, "end_char_idx": 47265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b71e743-c993-4693-975d-9cee2c7b23c6": {"__data__": {"id_": "4b71e743-c993-4693-975d-9cee2c7b23c6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23c90370-f267-49b8-95dd-e98132ab8228", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8099fe6a49b2bc37cc14cde2339f34a171076da9e43a4ee5522038ad356053d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "339504c9-91ec-44b5-900f-23aab09e5183", "node_type": "1", "metadata": {}, "hash": "f48a3fb18e693934e6cf64d35a394e158bf0dcc4311c90062404f8978613f56f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- It's catching up.\nI mean, it's still the\ndark matter, dark energy,\nthis kind of thing.\n- We have a model that sort of explains,\nthat fits the data really well.\nIt just has a few parameters\nthat you have to specify.\nSo people say, oh, that's fudge factors.\nWith enough fudge factors,\nyou can explain anything.\nBut the mathematical point\nof the model is that you\nwant to have fewer parameters\nin your model than data points\nin your observational set.\nSo if you have a model\nwith 10 parameters that\nexplains 10 observations,\nthat is a completely useless model.\nIt's what's called overfitted.\nBut if you have a model\nwith two parameters\nand it explains a trillion observations,\nwhich is basically so,\nyeah, the dark matter model,\nI think has like 14 parameters\nand it explains petabytes of\ndata that the astronomers have.\nYou can think of a theory.\nLike one way to think\nabout physical mathematical\ntheory is it's a compression\nof the universe and a data compression.\nSo you have these\npetabytes of observations.\nYou like to compress it to a\nmodel which you can describe\nin five pages and specify a\ncertain number of parameters.\nAnd if it can fit to reasonable accuracy,\nalmost all of your observations.\nI mean, the more\ncompression that you make,\nthe better your theory.\n- In fact, one of the great\nsurprises of our universe\nand of everything in it is\nthat it's compressible at all.\nIt's the unreasonable\neffectiveness of mathematics.\n- Yeah, Einstein had a quote like that.\nThe most incomprehensible thing\nabout the universe is\nthat it is comprehensible.\n- [Lex] And not just comprehensible.\nYou can do an equation\nlike E equals MC squared.\n- There is actually\nsome mathematical possible\nexplanation for that.\nSo there's this phenomenon\nin mathematics called universality.\nSo many complex systems\nat the macro scale are coming out\nof lots of tiny interactions\nat the micro scale.\nAnd normally, because of the\ncommon form of explosion,\nyou would think\nthat the macroscale\nequations must be infinitely\nexponentially more complicated\nthan the microscale ones.\nAnd they are if you want to\nsolve them completely exactly.\nLike if you want to model all\nthe atoms in a box of air.\nThat's like Avogadro's\nnumber is humongous.\nThere's a huge number of particles.\nIf you actually have to track\neach one, it'll be ridiculous.\nBut certain laws emerge\nat the microscopic scale\nthat almost don't depend\non what's going on at the macroscale,\nor only depend on a very\nsmall number of parameters.\nSo if you want to model a\ngas of quintillion particles\nin a box, you just need\nto know its temperature\nand pressure and volume\nand a few parameters like five or six.\nAnd it models almost everything you need\nto know about these 10 to\nthe 23 or whatever particles.\nSo we don't understand\nuniversality anywhere near\nas we would like mathematically.\nBut there are much simpler toy models\nwhere we do have a good understanding\nof why universality occurs.\nMost basic one is the\ncentral limit theorem\nthat explains why the bell\ncurve shows up everywhere\nin nature, that so many\nthings are distributed\nby what's called a Gaussian distribution.\nFamous bell curve.\nThere's now even a meme\nbut wit this curve.\n- [Lex] And even the meme applies broadly,\nthere's universality to the meme.\n- Yes, you can call it meta if you like,\nbut there are many, many processes.\nFor example, you can take lots\nand lots of independent random variables\nand average them together in various ways,\nyou can take a simple average\nor more complicated average.\nAnd we can prove in various\ncases that these bell curves,\nthese Gaussians emerge.\nAnd it is a satisfying explanation.\nSometimes they don't.\nSo if you have many different inputs\nand they're all correlated\nin some systemic way,\nthen you can get something very far\nfrom a bell curve show up.\nAnd this is also important\nto know when a system fails.\nSo universality is not a 100%\nreliable thing to rely on.\nThe global financial crisis\nwas a famous example of this.\nPeople thought that\nmortgage defaults had this\nsort of Gaussian type behavior that\nif you ask if you have a population\nof 100,000 Americans with mortgages,\nthey ask what proportion\nof them will default on their mortgages.", "mimetype": "text/plain", "start_char_idx": 46446, "end_char_idx": 50633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "339504c9-91ec-44b5-900f-23aab09e5183": {"__data__": {"id_": "339504c9-91ec-44b5-900f-23aab09e5183", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b71e743-c993-4693-975d-9cee2c7b23c6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2586d28954b9eebd616a1e5974cea49e2abafe12139564e0c49bf85f68c477c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f", "node_type": "1", "metadata": {}, "hash": "53ad41e7d8bb3c25d4ec51f17893f8c1f6fc15da2b8782b9f94029498f5fc2c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, you can take lots\nand lots of independent random variables\nand average them together in various ways,\nyou can take a simple average\nor more complicated average.\nAnd we can prove in various\ncases that these bell curves,\nthese Gaussians emerge.\nAnd it is a satisfying explanation.\nSometimes they don't.\nSo if you have many different inputs\nand they're all correlated\nin some systemic way,\nthen you can get something very far\nfrom a bell curve show up.\nAnd this is also important\nto know when a system fails.\nSo universality is not a 100%\nreliable thing to rely on.\nThe global financial crisis\nwas a famous example of this.\nPeople thought that\nmortgage defaults had this\nsort of Gaussian type behavior that\nif you ask if you have a population\nof 100,000 Americans with mortgages,\nthey ask what proportion\nof them will default on their mortgages.\nIf everything was decorrelated,\nthere would be a nice bell curve,\nand you can manage risk with options\nand derivatives and so forth.\nAnd it is a very beautiful theory.\nBut if there are systemic shocks\nin the economy that can\npush everybody to default\nat the same time, that's\nvery non-Gaussian behavior.\nAnd this wasn't fully\naccounted for in 2008.\nNow I think there's some more awareness\nthat this is a systemic risk\nis actually a much bigger issue.\nAnd just because the\nmodel is pretty and nice,\nit may not match reality.\nThe mathematics of working out\nwhat models do is really important.\nBut also the science\nof validating when the models fit reality\nand when they don't, you need both.\nBut mathematics can help\nbecause, for example,\nthese central limit\ntheorems, it tells you that\nif you have certain axioms\nlike type non-correlation,\nthat if all the inputs were\nnot correlated to each other,\nthen you have these Gaussian\nbehaviors that things are fine.\nIt tells you where to look\nfor weaknesses in the model.\nSo if you have a\nmathematical understanding\nof the central limit theorem\nand someone proposes to\nuse these Gaussian copulas\nor whatever to model default risk,\nif you're mathematically\ntrained, you would say,\n\"Okay, but what are the\nsystemic correlation\nbetween all your inputs?\"\nAnd so then you can ask the economists,\n\"How much of a risk is that?\"\nAnd then you can go look for that.\nSo there's always this synergy\nbetween science and mathematics.\n- A little bit on the\ntopic of universality,\nyou're known and celebrated\nfor working across an incredible\nbreadth of mathematics.\nReminiscent of Hilbert a century ago.\nIn fact, the great Fields\nMedal winning mathematician\nTim Gowers has said\nthat you are the closest\nthing we get to Hilbert.\n- Ha!\n(Lex chuckles)\n- [Lex] He's a colleague of yours?\n- [Terence] Oh yeah, good friend.\n- But anyway, so you are known\nfor this ability to go both\ndeep and broad in mathematics.\nSo you're the perfect person to ask.\nDo you think there are threads\nthat connect all the disparate\nareas of mathematics?\nIs there a kind\nof deep underlying structure\nto all of mathematics?\n- There's certainly a lot\nof connecting threads,\nand a lot of the progress of\nmathematics can be represented\nby taking by stories of\ntwo fields of mathematics\nthat were previously not\nconnected and finding connections.\nAn ancient example is\ngeometry and number theory.\nSo in the times of the ancient Greeks,\nthese were considered different subjects.\nI mean mathematicians worked on both.\nEuclid worked both on\ngeometry most famously,\nbut also on numbers.\nBut they were not really\nconsidered related.\nI mean a little bit like you could say\nthat this length was\nfive times this length\nbecause you could take\nfive copies of this length\nand so forth.\nBut it wasn't until\nDescartes who really realized\nthat develop analytic geometry,\nyou can parameterize the\nplane, a geometric object,\nby two real numbers.\nEvery point can be.\nAnd so geometric problems can be turned\ninto problems about numbers.\nAnd today this feels almost trivial.\nThere's no content to list.\nOf course a plane is X and Y,\nbecause that's what we\nteach and it's internalized.\nBut it was an important development\nthat these two fields were unified,\nand this process has just gone on\nthroughout mathematics\nover and over again.\nAlgebra and geometry were separated\nand now we have a split algebraic geometry\nthat connects them and\nover and over again.", "mimetype": "text/plain", "start_char_idx": 49778, "end_char_idx": 54067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f": {"__data__": {"id_": "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "339504c9-91ec-44b5-900f-23aab09e5183", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "fe324be651a374272961ef39803e754c5c5a19ecb5d38327cd45fe9c60049499", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "249ebcdc-70e4-416e-b102-2c39abe89ac1", "node_type": "1", "metadata": {}, "hash": "f14a147cddb3dcf126ee991dae8a7f104bd35a8d2a2a033bcb276fb51a244a91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But they were not really\nconsidered related.\nI mean a little bit like you could say\nthat this length was\nfive times this length\nbecause you could take\nfive copies of this length\nand so forth.\nBut it wasn't until\nDescartes who really realized\nthat develop analytic geometry,\nyou can parameterize the\nplane, a geometric object,\nby two real numbers.\nEvery point can be.\nAnd so geometric problems can be turned\ninto problems about numbers.\nAnd today this feels almost trivial.\nThere's no content to list.\nOf course a plane is X and Y,\nbecause that's what we\nteach and it's internalized.\nBut it was an important development\nthat these two fields were unified,\nand this process has just gone on\nthroughout mathematics\nover and over again.\nAlgebra and geometry were separated\nand now we have a split algebraic geometry\nthat connects them and\nover and over again.\nAnd that's certainly the type\nof mathematics that I enjoy the most.\nSo I think there's\nsort of different styles\nto being a mathematician.\nI think hedgehogs and fox.\nA fox knows many things a little bit,\nbut a hedgehog knows one\nthing very, very well.\nAnd in mathematics there's\ndefinitely both hedgehogs\nand foxes, and then there's\npeople who are kind of,\nwho can play both roles.\nAnd I think ideal collaboration\nbetween mathematicians involves,\nyou need some diversity,\nlike a fox working with many\nhedgehogs or vice versa.\nBut I identify mostly as a fox.\nCertainly I like arbitrage,\nsomehow learning how one field works,\nlearning the tricks of that field\nand then going to another field\nwhich people don't think is related,\nbut I can adapt the tricks.\n- So see the connections\nbetween the fields.\n- Yeah.\nSo there are other\nmathematicians who are far deeper\nthan I am, they're really hedgehogs.\nThey know everything about one field,\nand they're much faster and\nmore effective in that field.\nBut I can give them these extra tools.\n- I mean, you've said that\nyou can be both the hedgehog\nand the fox, depending on the context,\ndepending on the collaboration.\nSo what can you, if it's at all possible,\nspeak to the difference\nbetween those two ways of\nthinking about a problem?\nSay you're encountering a new problem,\nsearching for the connections\nversus like very singular focus.\n- I'm much more comfortable\nwith the fox paradigm.\nI like looking for analogies, narratives.\nI spend a lot of time,\nif there's a result,\nI see it in one field,\nand I like the result,\nit's a cool result.\nBut I don't like the proof.\nIt uses types of mathematics\nthat I'm not super familiar with.\nI often try to reprove it myself\nusing the tools that I favor.\nOften my proof is worse,\nbut by the exercise of doing so I can say,\n\"Oh, now I can see what the\nother proof was trying to do.\"\nAnd from that I can get some understanding\nof the tools that are used in that field.\nSo it's very exploratory,\ndoing crazy things\nand crazy builds and\nreinventing the wheel a lot.\nWhereas the hedgehog style is,\nI think, much more scholarly.\nYou're very knowledge-based.\nYou stay up to speed\non all the developments\nin this field.\nYou know all the history.\nYou have a very good understanding\nof exactly the strengths\nand weaknesses of each\nparticular technique.\nYeah, I think you'd rely a lot more on\nsort of calculation than sort\nof trying to find narratives.\nSo, yeah, I mean, I could do that too,\nbut there are other people who\nare extremely good at that.\n- Let's step back and maybe look at a bit\nof a romanticized version of mathematics.\nSo I think you've said\nthat early on in your life,\nmath was more like a\npuzzle solving activity\nwhen you were young.\nWhen did you first encounter a problem\nor proof where you realized\nmath can have a kind of elegance\nand beauty to it?\n- That's a good question.\nWhen I came to graduate\nschool in Princeton.\nSo John Conway was there at the time.\nHe passed away a few years ago.\nBut I remember one\nof the very first research\ntalks I went to was a talk\nby Conway on what he called extreme proof.\nSo Conway just had this amazing way\nof thinking about all kinds of things\nin a way that you would normally think of.\nSo he thought of proofs themselves\nas occupying some sort of space.", "mimetype": "text/plain", "start_char_idx": 53212, "end_char_idx": 57349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "249ebcdc-70e4-416e-b102-2c39abe89ac1": {"__data__": {"id_": "249ebcdc-70e4-416e-b102-2c39abe89ac1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddbfaabb-7cfd-4eaa-a4a0-658bd763cb7f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "609c5764cf0b7c2f990a412052a73a6832e7f4b9f182acdec5e273337bb712a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bea65ee-fd19-459b-979b-a51f73888ec8", "node_type": "1", "metadata": {}, "hash": "59267535225d6a0f85aa8fc8b13ed0bbdf8fe343a4ce2614018860bb0279b4c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Let's step back and maybe look at a bit\nof a romanticized version of mathematics.\nSo I think you've said\nthat early on in your life,\nmath was more like a\npuzzle solving activity\nwhen you were young.\nWhen did you first encounter a problem\nor proof where you realized\nmath can have a kind of elegance\nand beauty to it?\n- That's a good question.\nWhen I came to graduate\nschool in Princeton.\nSo John Conway was there at the time.\nHe passed away a few years ago.\nBut I remember one\nof the very first research\ntalks I went to was a talk\nby Conway on what he called extreme proof.\nSo Conway just had this amazing way\nof thinking about all kinds of things\nin a way that you would normally think of.\nSo he thought of proofs themselves\nas occupying some sort of space.\nSo if you want put to prove something,\nlet's say that there's\ninfinitely many primes, okay,\nyou have all different proofs,\nbut you could rank them in different axes.\nLike some proofs are elegant,\nsome proofs are long,\nsome proofs are elementary and so forth.\nAnd so this is cloud.\nSo the space of all proofs\nitself has some sort of shape.\nAnd so he was interested in\nextreme points of this shape.\nLike out of all these proofs,\nwhat is one that is the shortest\nat the expense of everything else\nor the most elementary or whatever.\nAnd so he gave some examples\nof well-known theorems\nand then he would give what he\nthought was the extreme proof\nin these different aspects.\nI just found that really eye-opening\nthat it's not just getting a proof\nfor a result was interesting,\nbut once you have that\nproof trying to optimize it\nin various ways that,\nbut proofing itself had\nsome craftsmanship to it.\nIt certainly informed\nmy writing style that\nwhen you do your math assignments\nand undergraduate your\nhomework and so forth,\nyou're sort of encouraged\nto just write down any proof\nthat works and hand it in.\nAs long as it gets a\ntick mark, you move on.\nBut if you want your results\nto actually be influential\nand be read by people,\nit can't just be correct.\nIt should also be a\npleasure to read, motivated,\nbe adaptable, to\ngeneralize to other things.\nIt's the same in many other\ndisciplines like coding.\nThere's a lot of analogies\nbetween math and coding.\nI like analogies if you haven't noticed.\n(Lex laughs)\nBut you can code something\nspaghetti code that works\nfor a certain task,\nand it's quick and dirty, and it works.\nBut there's lots of good principles\nfor writing code well so\nthat other people can use it,\nbuild upon it, and so then\nhas fewer bugs and whatever.\nAnd there's similar\nthings with mathematics.\n- Yeah, first of all there's\nso many beautiful things there.\nAnd Conway is one of the great\nminds in mathematics ever\nand computer science.\nJust even considering the\nspace of proofs, and saying,\n\"Okay, what does this space look like\nand what are the extremes?\"\nLike you mentioned, coding\nis an analogy is interesting\nbecause there's also this\nactivity called Code Golf.\n- [Terence] Oh, yeah, yeah, yeah.\n- Which I also find beautiful\nand fun where people use\ndifferent programming languages\nto try to write the\nshortest possible program\nthat accomplishes a particular task.\n- [Terence] Yeah.\n- Then I believe there's\neven competitions on this.\nAnd it's also a nice way\nto stress test not just the\nsort of the programs or\nin this case the proofs,\nbut also the different languages.\nMaybe that's a different notation\nor whatever to use to\naccomplish a different task.\n- Yeah, you learn a lot.\nI mean it may seem like\na frivolous exercise,\nbut it can generate all\nthese insights which\nif you didn't have this\nartificial objective\nto pursue, you might not see.\n- What to use the most beautiful\nor elegant equation in mathematics?\nI mean, one of the things\nthat people often look to\nin beauty is the simplicity.\nSo if you look at E equals MC squared.\nSo when a few concepts come together,\nthat's why the Euler identity\nis often considered the\nmost beautiful equation\nin mathematics.\nDo you find beauty in that\none in the Euler identity?\n- Yeah, well, as I said, I mean,\nwhat I find most appealing is connections\nbetween different things that,\nso E to the pi I equals minus one.", "mimetype": "text/plain", "start_char_idx": 56589, "end_char_idx": 60731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bea65ee-fd19-459b-979b-a51f73888ec8": {"__data__": {"id_": "1bea65ee-fd19-459b-979b-a51f73888ec8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "249ebcdc-70e4-416e-b102-2c39abe89ac1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8f10702c4b4ece9bf3fec31ba6cda01ef2fef245aec65ea233a32f6020abff50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81", "node_type": "1", "metadata": {}, "hash": "28fae06a333da9aa967c0b55086374de347e614658de83de93b3c1c29b18098f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Maybe that's a different notation\nor whatever to use to\naccomplish a different task.\n- Yeah, you learn a lot.\nI mean it may seem like\na frivolous exercise,\nbut it can generate all\nthese insights which\nif you didn't have this\nartificial objective\nto pursue, you might not see.\n- What to use the most beautiful\nor elegant equation in mathematics?\nI mean, one of the things\nthat people often look to\nin beauty is the simplicity.\nSo if you look at E equals MC squared.\nSo when a few concepts come together,\nthat's why the Euler identity\nis often considered the\nmost beautiful equation\nin mathematics.\nDo you find beauty in that\none in the Euler identity?\n- Yeah, well, as I said, I mean,\nwhat I find most appealing is connections\nbetween different things that,\nso E to the pi I equals minus one.\nSo yeah, people always use\nall the fundamental constants.\nOkay, that's cute,\nbut to me.\n(Lex laughing)\nSo the exponential function was introduced\nby Euler to measure exponential growth.\nSo compound interest or decay.\nAnything which is continuously growing,\ncontinuously decreasing growth and decay,\nor dilation or contraction is modeled\nby the exponential function.\nWhereas pi comes around\nfrom circles and rotation.\nIf you want to rotate a needle,\nfor example, 180 degrees,\nyou need to rotate by pi radians.\nAnd i, complex numbers,\nrepresents this whole imaginary axes\nof a 90-degree rotation.\nSo a change in direction.\nSo the X metric function represents growth\nand decay in the direction\nthat you really are.\nWhen you stick an i in the exponential,\nnow, instead of motion\nin the same direction\nas your current position,\nit's motion right angles\nto your current position,\nso rotation.\nAnd then so E to the pi equals minus one\ntells you that if you rotate\nfor time pi, you end up\nat the other direction.\nSo it unifies geometry through dilation\nand exponential growth or\ndynamics through this act\nof complexification, rotation by i.\nSo it connects together all\nthese tools in mathematics,\ndynamics, geometry, and\nthe complex numbers,\nthey're all considered almost,\nthey're all next door\nneighbors in mathematics\nbecause of this identity.\n- Do you think the thing\nyou mentioned is cute,\nthe collision of notations\nfrom these disparate fields is\njust a frivolous side effect?\nOr do you think there is\nlegitimate value in one notation?\nAll our old friends\ncome together at night.\n- Well, it's confirmation that\nyou have the right concepts.\nSo when you first study anything,\nyou have to measure things\nand give them names.\nAnd initially, sometimes,\nbecause your model is again\ntoo far off from reality,\nyou give the wrong things the best names,\nand you only find out later\nwhat's really important.\n- Physicists can do this\nsometimes, but it turns out okay.\n- So actually with physics,\nso E equals MC squared.\nOkay, so one of the big things was the E.\nSo when Aristotle first came\nup with his laws of motion,\nand then Galileo and Newton and so forth,\nthey saw the things they could measure.\nThey could measure mass\nand acceleration and force and so forth.\nAnd so Newtonian mechanics, for example,\nF = ma was the famous\nNewton's second law of motion.\nSo those were the primary objects.\nSo they gave them the central\nbilling in the theory.\nIt was only later,\nafter people started\nanalyzing these equations\nthat there always seemed\nto be these quantities\nthat were conserved.\nSo in particular momentum and energy.\nAnd it's not obvious that\nthings have an energy.\nIt's not something you\ncan directly measure\nthe same way you can measure mass\nand velocity and so forth.\nBut over time people realized\nthat this was actually a\nreally fundamental concept.\nHamilton eventually in 19th century,\nreformulated Newton's laws of physics\ninto what's called Hamiltonian\nmechanics, where the energy,\nwhich is now called the Hamiltonian,\nwas the dominant object.\nOnce you know how to measure\nthe Hamiltonian of any system,\nyou can describe completely the dynamics,\nlike what happens to all the states.\nIt really was a central actor\nwhich was not obvious initially.\nAnd this helped.\nActually this change of\nperspective really helped\nwhen quantum mechanics came along,\nbecause the early physicists\nwho studied quantum mechanics,\nthey had a lot of trouble trying\nto adapt their Newtonian thinking\nbecause everything was a particle\nand so forth to quantum mechanics,\nbecause now everything was a wave.\nIt just looks really, really weird.", "mimetype": "text/plain", "start_char_idx": 59940, "end_char_idx": 64342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81": {"__data__": {"id_": "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bea65ee-fd19-459b-979b-a51f73888ec8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "999b96f5cb266c16abcf3472202385e09a6738aaeb374acd8a0c769b2a1c3d17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a9932d5-abe6-4ac1-ab09-7e2a5be956da", "node_type": "1", "metadata": {}, "hash": "7718e2360a34e9026dd3540834d0257e7a50d6831c6aad4cc3cc56d3e9b38320", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's not something you\ncan directly measure\nthe same way you can measure mass\nand velocity and so forth.\nBut over time people realized\nthat this was actually a\nreally fundamental concept.\nHamilton eventually in 19th century,\nreformulated Newton's laws of physics\ninto what's called Hamiltonian\nmechanics, where the energy,\nwhich is now called the Hamiltonian,\nwas the dominant object.\nOnce you know how to measure\nthe Hamiltonian of any system,\nyou can describe completely the dynamics,\nlike what happens to all the states.\nIt really was a central actor\nwhich was not obvious initially.\nAnd this helped.\nActually this change of\nperspective really helped\nwhen quantum mechanics came along,\nbecause the early physicists\nwho studied quantum mechanics,\nthey had a lot of trouble trying\nto adapt their Newtonian thinking\nbecause everything was a particle\nand so forth to quantum mechanics,\nbecause now everything was a wave.\nIt just looks really, really weird.\nYou ask what is the\nquantum version of F = ma?\nAnd it's really, really hard\nto give an answer to that.\nBut it turns out that the Hamiltonian,\nwhich was so secretly\nbehind the scenes in classical mechanics,\nalso is the key object\nin quantum mechanics,\nthat there's also an object\ncalled the Hamiltonian.\nIt's a different type of object.\nIt's what's called an operator\nrather than a function.\nBut again, once you specify it,\nyou specify the entire dynamics.\nSo there's something called\nSchrodinger's equation\nthat tells you exactly\nhow quantum systems evolve\nonce you have a Hamiltonian.\nSo side by side,\nthey look completely different objects.\nOne involves particles, one\ninvolves waves, and so forth.\nBut with this centrality,\nyou could start actually\ntransferring a lot of intuition\nand facts from classical\nmechanics to quantum mechanics.\nSo for example,\nin classical mechanics there's this thing\ncalled Noether's theorem.\nEvery time there's a symmetry\nin a physical system,\nthere is a conservation law.\nSo the laws of physics\nare translation invariant.\nLike if I move 10 steps to the left,\nI experience the same laws\nof physics as if I was here.\nAnd that corresponds to\nconservation momentum.\nIf I turn around by some angle,\nagain I experience the\nsame laws of physics.\nThis corresponds to the\nconservation of angular momentum.\nIf I wait for 10 minutes,\nI still have the same laws of physics.\nSo this time transition invariance,\nthis corresponds to the law\nof concentration of energy.\nSo there's this fundamental\nconnection between symmetry\nand conservation.\nAnd that's also true in quantum mechanics,\neven though the equations\nare completely different.\nBut because they're both\ncoming from the Hamiltonian,\nthe Hamiltonian controls everything.\nEvery time the Hamiltonian has a symmetry,\nthe equations will have\na conservation law.\nSo once you have the right language,\nit actually makes things a lot cleaner.\nOne of the problems why we\ncan't unify quantum mechanics\nand general relativity yet,\nwe haven't figured out\nwhat the fundamental objects are like.\nFor example, we have to\ngive up the notion of space\nand time being these almost\nEuclidean type spaces.\nAnd we kind of know that\nat very tiny scales there's going\nto be quantum fluctuations,\nthere's spacetime foam,\nand trying to use\nCartesian coordinates, XYZ,\nit's a non-starter,\nbut we don't know how to\nwhat to replace it with.\nWe don't actually have\nthe mathematical concepts.\nThe analog got a Hamiltonian\nthat sort of organized everything.\n- Does your gut say that there\nis a theory of everything,\nso this is even possible to unify,\nto find this language that\nunifies general relativity\nand quantum mechanics?\n- I believe so.\nI mean, the history of physics\nhas been that of unification,\nmuch like mathematics.\nOver the years,\nelectricity and magnetism\nwere separate theories\nand then Maxwell unified them.\nNewton unified the motions\nof heavens with the motions\nof objects on the earth and so forth.\nSo it should happen.\nIt's just that again,\nto go back to this model\nfor observations and theory,\npart of our problem is\nthat physics is a victim\nof its own success,\nthat our two big theories of physics,\ngeneral relativity and quantum\nmechanics, are so good now.\nTogether they cover 99.9% of\nsort of all the observations we can make.", "mimetype": "text/plain", "start_char_idx": 63387, "end_char_idx": 67637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a9932d5-abe6-4ac1-ab09-7e2a5be956da": {"__data__": {"id_": "7a9932d5-abe6-4ac1-ab09-7e2a5be956da", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e7102dc-6c41-4ad9-a7ed-bc04e7b3ab81", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6d6bbc7cf3a8869772e809e44804a16414501e2743080f9da3fb517a878267ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f741db7b-4c76-42ba-9231-802a876e403f", "node_type": "1", "metadata": {}, "hash": "6c36dc5cb1ae76473668163e44c9b788eba61e7c4a51c2d22ba34c98e89d5d20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The analog got a Hamiltonian\nthat sort of organized everything.\n- Does your gut say that there\nis a theory of everything,\nso this is even possible to unify,\nto find this language that\nunifies general relativity\nand quantum mechanics?\n- I believe so.\nI mean, the history of physics\nhas been that of unification,\nmuch like mathematics.\nOver the years,\nelectricity and magnetism\nwere separate theories\nand then Maxwell unified them.\nNewton unified the motions\nof heavens with the motions\nof objects on the earth and so forth.\nSo it should happen.\nIt's just that again,\nto go back to this model\nfor observations and theory,\npart of our problem is\nthat physics is a victim\nof its own success,\nthat our two big theories of physics,\ngeneral relativity and quantum\nmechanics, are so good now.\nTogether they cover 99.9% of\nsort of all the observations we can make.\nAnd you have to either go\nto extremely insane particle accelerations\nor the early universe\nor things that are really hard to measure\nin order to get any deviation\nfrom either of these two theories\nto the point where you\ncan actually figure out\nhow to combine them together.\nBut I have faith that we've\nbeen doing this for centuries\nand we've made progress before.\nThere's no reason why we should stop.\n- Do you think you'll be a mathematician\nthat develops theory of everything?\n- What often happens is that\nwhen the physicists need\nsome theory of mathematics,\nthere's often some precursor\nthat the mathematicians\nworked out earlier.\nSo when Einstein started\nrealizing that space was curved,\nhe went to some mathematician and asked,\n\"Is there some theory\nof curved space that\nmathematicians already came up\nwith that could be useful?\"\nAnd he said, \"Oh, yeah,\"\nI think Riemann came up with something.\nAnd so, yeah, Riemann had\ndeveloped Riemannian geometry,\nwhich is precisely a theory\nof spaces that are curved\nin various general ways,\nwhich turned out to be almost\nexactly what was needed\nfor Einstein's theory.\nThis is going back to Wigner's\nunreasonable effectiveness\nof mathematics.\nI think the theories that work well\nto explain the universe tend\nto also involve the same\nmathematical objects that work well\nto solve mathematical problems.\nUltimately, they're just\nsort of both ways of\norganizing data in useful ways.\n- It just feels like you might need\nto go some weird land\nthat's very hard to intuit.\nLike you have like string theory.\n- Yeah, that was a leading\ncandidate for many decades.\nI think it's slowly\nfalling out of fashion,\nbecause it's not matching experiment.\n- So one of the big challenges,\nof course, like you said,\nis experiment is very tough.\n- [Terence] Yes.\n- Because of how effective\nboth theories are.\nBut the other is like\njust you're talking about,\nyou're not just deviating from space time,\nyou're going into like some\ncrazy number of dimensions.\nYou're doing all kinds of weird stuff.\nThat to us, we've gone\nso far from this flat\nEarth that we started at,\nlike you mentioned.\n- [Terence] Yeah, yeah, yeah.\n- Now we're just,\nit's very hard to use our\nlimited ape descendants\nof a cognition to intuit what\nthat reality really is like.\n- This is why analogies are so important.\nI mean, so, yeah,\nthe round Earth is not intuitive,\nbecause we're stuck on it.\nBut round objects in general,\nwe have pretty good\nintuition a little bit.\nAnd we have intuition about\nlight works and so forth.\nAnd it's actually a good exercise\nto actually work out how\neclipses and phases of the sun\nand the moon and so forth\ncan be really easily\nexplained by round Earth\nand round moon models.\nAnd you can just take a\nbasketball and a golf ball\nand a light source and actually\ndo these things yourself.\nSo the intuition is there,\nbut you have to transfer it.\n- That is a big leap intellectually\nfor us to go from flat to round Earth,\nbecause our life is\nmostly lived in flatland,\nto load that information.\nAnd we're all like, take it for granted.\nWe take so many things for granted\nbecause science has\nestablished a lot of evidence\nfor this kind of thing.\nBut we're around rock\nflying through space.", "mimetype": "text/plain", "start_char_idx": 66782, "end_char_idx": 70842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f741db7b-4c76-42ba-9231-802a876e403f": {"__data__": {"id_": "f741db7b-4c76-42ba-9231-802a876e403f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a9932d5-abe6-4ac1-ab09-7e2a5be956da", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a03ccab10bf81b80e798bf0155357441e9eca892d680863174a4c810aef4adb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "157aa93f-aa5a-4e96-98db-960647b9095c", "node_type": "1", "metadata": {}, "hash": "20a61d041b12b320a0181a1d045a8ca2308d4095475a0b01622fae36063eac44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But round objects in general,\nwe have pretty good\nintuition a little bit.\nAnd we have intuition about\nlight works and so forth.\nAnd it's actually a good exercise\nto actually work out how\neclipses and phases of the sun\nand the moon and so forth\ncan be really easily\nexplained by round Earth\nand round moon models.\nAnd you can just take a\nbasketball and a golf ball\nand a light source and actually\ndo these things yourself.\nSo the intuition is there,\nbut you have to transfer it.\n- That is a big leap intellectually\nfor us to go from flat to round Earth,\nbecause our life is\nmostly lived in flatland,\nto load that information.\nAnd we're all like, take it for granted.\nWe take so many things for granted\nbecause science has\nestablished a lot of evidence\nfor this kind of thing.\nBut we're around rock\nflying through space.\nThat's a big leap,\nand you have to take\na chain of those leaps\nthe more and more and more we progress.\n- Right, yeah.\nSo modern science is maybe, again,\na victim of its own success,\nis that in order to be more accurate,\nit has to move further\nand further away from\nyour initial intuition.\nAnd so for someone who hasn't\ngone through the whole process\nof science education,\nit looks more and more\nsuspicious because of that.\nSo we need more grounding, I think.\nI mean, there are scientists\nwho do excellent outreach,\nbut there's lots of science\nthings that you can do at home.\nThere's lots of YouTube videos.\nI did a YouTube video\nrecently with Grant Sanderson.\nWe talked about this earlier,\nhow the ancient Greeks were able\nto measure things like\nthe distance of the moon,\ndistance to the Earth,\nand using techniques that you\ncould also replicate yourself.\nIt doesn't all have to\nbe fancy space telescopes\nand really intimidating mathematics.\n- Yeah, I highly recommend that.\nI believe you gave a lecture\nand you also did an\nincredible video with Grant.\nIt's a beautiful experience\nto try to put yourself\nin the mind of a person from\nthat time, shrouded in mystery.\nYou're on this planet, you\ndon't know the shape of it,\nthe size of it.\nYou see some stars, you see some things,\nand you try to localize\nyourself in this world\nand try to make some\nkind of general statements\nabout distance to places.\n- Change of perspective\nis really important.\nYou say travel broadens the mind.\nThis is intellectual travel.\nYou know, put yourself in the\nmind of the ancient Greeks\nor some other person,\nsome other time period,\nmake hypotheses, spherical\ncows, whatever, speculate.\nAnd this is what mathematicians do\nand some artists do, actually.\n- It's just incredible that\ngiven the extreme constraints,\nyou could still say very powerful things.\nThat's why it's inspiring,\nlooking back in history,\nhow much can be figured out\nwhen you don't have much\nfigure out stuff work.\n- If you propose axioms,\nthen the mathematics lets\nyou follow those axioms\nto their conclusions.\nAnd sometimes you can get quite a long way\nfrom initial hypotheses.\n- If we can stay in the land of the weird.\nYou mentioned general relativity.\nYou've contributed to the\nmathematical understanding\nof Einstein's field equations.\nCan you explain this work?\nAnd from a sort of\nmathematical standpoint,\nwhat aspects of general\nrelativity are intriguing to you,\nchallenging to you?\n- I have worked on some equations.\nThere's something called\nthe wave maps equation,\nor the sigma field model,\nwhich is not quite the equation\nof spacetime gravity itself,\nbut of certain fields that\nmight exist on top of spacetime.\nSo Einstein's equations of\nrelativity just describe space\nand time itself.\nBut then there's other fields\nthat live on top of that.\nThere's the electromagnetic field,\nthere's things called Yang-Mills fields,\nand there's this whole\nhierarchy of different equations\nof which Einstein is considered one\nof the most nonlinear and difficult.\nBut relatively low on the hierarchy\nwas this thing called\nthe wave maps equation.\nSo it's a wave which at any given point\nis fixed to be like on a sphere.\nSo I can think of a bunch\nof arrows in space and time,\nso it's pointing in different directions,\nbut they propagate like waves.\nIf you wiggle an arrow,\nit will propagate and\nmake all the arrows move,\nkind of like sheaves of\nwheat in the wheat field.", "mimetype": "text/plain", "start_char_idx": 70024, "end_char_idx": 74247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "157aa93f-aa5a-4e96-98db-960647b9095c": {"__data__": {"id_": "157aa93f-aa5a-4e96-98db-960647b9095c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f741db7b-4c76-42ba-9231-802a876e403f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ae50ba05ddbd93c09caf2875ffa144bbe3fefb70382cc3ea392f23393588fa75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89c5fb97-82f0-402d-b968-c07aa9875527", "node_type": "1", "metadata": {}, "hash": "b1a7d7108589579bde0acc64c2abd08af50ce1cb67ba0f6b385010dd925ab594", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So Einstein's equations of\nrelativity just describe space\nand time itself.\nBut then there's other fields\nthat live on top of that.\nThere's the electromagnetic field,\nthere's things called Yang-Mills fields,\nand there's this whole\nhierarchy of different equations\nof which Einstein is considered one\nof the most nonlinear and difficult.\nBut relatively low on the hierarchy\nwas this thing called\nthe wave maps equation.\nSo it's a wave which at any given point\nis fixed to be like on a sphere.\nSo I can think of a bunch\nof arrows in space and time,\nso it's pointing in different directions,\nbut they propagate like waves.\nIf you wiggle an arrow,\nit will propagate and\nmake all the arrows move,\nkind of like sheaves of\nwheat in the wheat field.\nAnd I was interested in the\nglobal regularity problem again\nfor this question.\nIs it possible for all the energy here\nto collect at a point?\nSo the equation I considered\nwas actually what's called\na critical equation,\nwhere the behavior at all\nscales is roughly the same.\nAnd I was able,\nbarely to show that you couldn't\nactually force a scenario\nwhere all the energy\nconcentrated at one point,\nthat the energy had to\ndisperse a little bit,\nand the moment it dispersed a little bit,\nit would stay regular.\nYeah, this was back in 2000.\nThat was part of why I got interested\nin Navier-Stokes afterwards, actually.\nYeah, so I developed some\ntechniques to solve that problem.\nSo part of it is this\nproblem is really nonlinear\nbecause of the curvature of the sphere,\nThere was a certain nonlinear effect,\nwhich was a non-perturbative effect.\nWhen you sort of looked at it normally,\nit looked larger than the linear effects\nof the wave equation.\nAnd so it was hard to\nkeep things under control,\neven when your energy was small.\nBut I developed what's called\na gauge transformation.\nSo the equation is\nkind of like an evolution\nof sheaves of wheat\nand they're all bending back and forth.\nAnd so there's a lot of motion.\nBut if you imagine stabilizing the flow\nby attaching little cameras\nat different points in space,\nwhich are trying to move\nin a way that captures most of the motion,\nand under this stabilized flow,\nthe flow becomes a lot more linear.\nI discovered a way\nto transform the equation\nto reduce the amount\nof nonlinear effects.\nAnd then I was able to solve the equation.\nI found this transformation\nwhile visiting my aunt in Australia.\nAnd I was trying to\nunderstand the dynamics\nof all these fields.\nAnd I couldn't do it with pen and paper.\nAnd I had not enough facility\nof computers to do any\ncomputer simulations.\nSo I ended up closing my\neyes, being on the floor,\nand just imagining myself to\nactually be this vector field\nand rolling around\nand to try to see how\nto change coordinates\nin such a way that somehow things\nin all directions would behave\nin a reasonably linear fashion.\nAnd yeah, my aunt walked in\non me while I was doing that\nand she was asking, what\nam I doing doing this?\n- It's complicated, is the answer.\n- Yeah, yeah.\nAnd she goes, \"Okay,\nfine you're a young man.\nI don't ask questions.\"\n- I have to ask about the,\nhow do you approach\nsolving difficult problems?\nIf it's possible to go\ninside your mind when you're thinking,\nare you visualizing\nin your mind the mathematical\nobjects, symbols, maybe?\nWhat are you visualizing\nin your mind usually\nwhen you're thinking?\n- A lot of pen and paper.\nOne thing you pick up as a\nmathematician is sort of,\nI call it cheating strategically.\nSo the beauty of\nmathematics is that you get\nto change the problem,\nchange the rules as you wish.\nYou don't get to do this\nfor any other field.\nIf you're an engineer and someone says,\n\"Build a bridge over this\nriver,\" you can't say,\n\"I want to build this\nbridge over here instead,\"\nor, \"I want to put it out\nof paper instead of steel.\"\nBut a mathematician, you\ncan do whatever you want.\nIt's like trying to solve a computer game\nwhere there's unlimited\ncheat codes available.\nAnd so you can set this\ndimension that's large,\nI'll set it to one.\nI'd solve the one\ndimensional problem first.\nThere's a main term and an error term.\nI'm going to make a\nspherical cow assumption.", "mimetype": "text/plain", "start_char_idx": 73507, "end_char_idx": 77632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89c5fb97-82f0-402d-b968-c07aa9875527": {"__data__": {"id_": "89c5fb97-82f0-402d-b968-c07aa9875527", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "157aa93f-aa5a-4e96-98db-960647b9095c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "effd82a0036037084d0ab79acb6d2c9cb2d1d428f9dee27f39ead0686740a015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b30d152-d86d-4389-b67b-07c19c906bfa", "node_type": "1", "metadata": {}, "hash": "bacd69da8fe59a8b5577cce30d96be7700c395944f0cf256d73329a587fb43ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One thing you pick up as a\nmathematician is sort of,\nI call it cheating strategically.\nSo the beauty of\nmathematics is that you get\nto change the problem,\nchange the rules as you wish.\nYou don't get to do this\nfor any other field.\nIf you're an engineer and someone says,\n\"Build a bridge over this\nriver,\" you can't say,\n\"I want to build this\nbridge over here instead,\"\nor, \"I want to put it out\nof paper instead of steel.\"\nBut a mathematician, you\ncan do whatever you want.\nIt's like trying to solve a computer game\nwhere there's unlimited\ncheat codes available.\nAnd so you can set this\ndimension that's large,\nI'll set it to one.\nI'd solve the one\ndimensional problem first.\nThere's a main term and an error term.\nI'm going to make a\nspherical cow assumption.\nI assume the error term is zero.\nAnd so the way you should\nsolve these problems is not\nin sort of this Iron Man mode\nwhere you make things maximally difficult,\nbut actually the way you should approach\nany reasonable math problem is that\nif there are 10 things that\nare making your life difficult,\nfind a version of the\nproblem that turns off nine\nof the difficulties,\nbut only keeps one of\nthem, and solve that.\nSo you install nine cheats,\nokay, if you solve 10 cheats,\nthen the game is trivial,\nbut you install nine cheats,\nyou solve one problem,\nthat teaches you how\nto deal with that particular difficulty,\nand then you turn that one off,\nand you turn something else on,\nand then you solve that one.\nAnd after you know how\nto solve the 10 problems,\n10 difficulties separately,\nthen you have to start\nmerging them a few at a time.\nAs a kid, I watched a lot of\nthese Hong Kong action movies\nfrom my culture.\nAnd one thing is, every time\nit's a fight scene, some,\nmaybe the hero gets swarmed\nby 100 bad guy goons\nor whatever, but it'll\nalways be choreographed\nso that he'd always be only\nfighting one person at a time.\nAnd then it would defeat\nthat person and move on.\nAnd because of that, he\ncould defeat all of them.\nBut whereas if they had fought\na bit more intelligently\nand just swarmed the guy at once,\nit would make for much worse\ncinema, but they would win.\n- Are you usually pen and paper?\nAre you working with computer and LaTeX?\n- I'm mostly pen and paper, actually.\nSo in my office I have\nfour giant blackboards\nand sometimes I just have\nto write everything I\nknow about the problem\non the four blackboards\nand then sit on my couch\nand just sort of see the whole thing.\n- Is it all symbols like notation,\nor is there some drawings?\n- Oh, there's a lot of drawing\nand a lot of bespoke doodles\nthat only make sense to me.\nAnd that's the beauty of a blackboard.\nYou erase.\nAnd it's a very organic thing.\nI'm beginning to use\nmore and more computers,\npartly because AI makes it much easier\nto do simple coding things.\nIf I wanted to plot a function before,\nwhich is moderately\ncomplicated as some iteration\nor something, I'd have\nto remember how to set up a Python program\nand how does a for loop work and debug it.\nAnd it would take two hours and so forth.\nAnd now I can do it in 10, 15 minutes.\nI'm using more and more computers\nto do simple explorations.\n- Let's talk about AI a\nlittle bit, if we could.\nSo maybe a good entry\npoint is just talking\nabout computer-assisted proofs in general.\nCan you describe the Lean formal\nproof programming language\nand how it can help as a proof assistant,\nand maybe how you started using it\nand how it has helped you?\n- So Lean is a computer language much like\nsort of standard languages\nlike Python and C and so forth,\nexcept that in most languages the focus is\non using executable code.\nLines of code do things,\nthey flip bits or they make a robot move,\nor they deliver you text on\nthe Internet or something.\nSo Lean is a language\nthat can also do that.\nIt can also be run as a\nstandard traditional language,\nbut it can also produce certificates.\nSo a software language like\nPython might do a computation\nand give you that the answer is seven.\nDoes the sum of three plus\nfour is equal to seven?", "mimetype": "text/plain", "start_char_idx": 76872, "end_char_idx": 80877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b30d152-d86d-4389-b67b-07c19c906bfa": {"__data__": {"id_": "0b30d152-d86d-4389-b67b-07c19c906bfa", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89c5fb97-82f0-402d-b968-c07aa9875527", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1b85d4f8541a0719930d59296c7c02f55ac22043b14e1e09b7a52b812a50f35c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c191653-c6f6-4933-8640-cbcac4171d00", "node_type": "1", "metadata": {}, "hash": "247e7240fe2a77d71a7ca7a390f97f3459b51bf22b59084f8d21bb2353bde205", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Let's talk about AI a\nlittle bit, if we could.\nSo maybe a good entry\npoint is just talking\nabout computer-assisted proofs in general.\nCan you describe the Lean formal\nproof programming language\nand how it can help as a proof assistant,\nand maybe how you started using it\nand how it has helped you?\n- So Lean is a computer language much like\nsort of standard languages\nlike Python and C and so forth,\nexcept that in most languages the focus is\non using executable code.\nLines of code do things,\nthey flip bits or they make a robot move,\nor they deliver you text on\nthe Internet or something.\nSo Lean is a language\nthat can also do that.\nIt can also be run as a\nstandard traditional language,\nbut it can also produce certificates.\nSo a software language like\nPython might do a computation\nand give you that the answer is seven.\nDoes the sum of three plus\nfour is equal to seven?\nBut Lean can produce not just the answer,\nbut a proof that how it\ngot the answer of seven\nas three plus four, and\nall the steps involved.\nIt creates these more complicated objects,\nnot just statements,\nbut statements with\nproofs attached to them.\nAnd every line of code is just a way\nof piecing together previous\nstatements to create new ones.\nSo the idea is not new.\nThese things are called proof assistants,\nand so they provide languages\nfor which you can create\nquite complicated,\nintricate mathematical proofs.\nAnd they produce these certificates\nthat give it 100% guarantee\nthat your arguments are correct,\nif you trust the compiler, obviously.\nBut they made the compiler really small\nand there are several\ndifferent compilers available\nfor the same level.\n- Can you give people some\nintuition about the difference\nbetween writing on pen and paper\nversus using Lean programming language?\nHow hard is it to formalize statement?\n- So Lean, a lot of\nmathematicians were involved\nin the design of Lean, so it's designed\nso that individual lines of\ncode resemble individual lines\nin a mathematical argument.\nYou might want to introduce a variable,\nyou might want to prove our contradiction.\nThere are various standard\nthings that you can do,\nand it's written\nso ideally it should be like\na one-to-one correspondence.\nIn practice it isn't,\nbecause Lean is like explaining a proof\nto an extremely pedantic colleague\nwho will point out, \"Okay,\ndid you really mean this?\nWhat happens if this is zero?\nOkay, how do you justify this?\"\nSo Lean has a lot of automation in it\nto try to be less annoying.\nSo, for example,\nevery mathematical object\nhas to come of a type.\nLike if I talk about X, is X a real number\nor a natural number or\na function or something?\nIf you write things informally,\nit's often in terms of context.\nYou say, \"Clearly X is equal to,\"\n\"Let X be the sum of Y and Z,\"\nand Y and Z were already real numbers,\nso X should also be a real number.\nSo Lean can do a lot of that,\nbut every so often it says,\n\"Wait a minute, can you tell me more\nabout what this object is?\nWhat type of object it is?\"\nYou have to think more\nat a philosophical level,\nnot just sort of computations\nthat you're doing,\nbut sort of what each object\nactually is in some sense.\n- Is it using something like\nLLMs to do the type inference?\nOr like you mentioned, with the LLMs?\n- It's using much more traditional,\nwhat's called good old-fashioned AI.\nYou can represent all\nthese things as trees\nand there's always algorithms\nto match one tree to another tree.\n- So it's actually doable\nto figure out if\nsomething is a real number\nor a natural number?\n- Every object stuff comes\nwith a history of where it came from\nand you can kind of trace.\n- Oh, I see.\n- Yeah, so it's designed for reliability.\nSo modern AIs are not used in,\nit's a disjoint technology.\nPeople are beginning to\nuse AIs on top of Lean.\nSo when a mathematician tries\nto program proof in Lean,\noften there's a step.\nOkay, now I want\nto use the fundamental\ncalculus to do the next step.\nSo the Lean developers have\nbuilt this massive project\ncalled Mathlib, a collection of tens\nof thousands of useful facts\nabout mathematical objects.\nAnd somewhere in there is the\nfundamental theme of calculus,\nbut you need to find it.\nSo a lot, the bottle neck\nnow is actually Lemma Search.", "mimetype": "text/plain", "start_char_idx": 79999, "end_char_idx": 84199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c191653-c6f6-4933-8640-cbcac4171d00": {"__data__": {"id_": "0c191653-c6f6-4933-8640-cbcac4171d00", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b30d152-d86d-4389-b67b-07c19c906bfa", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8a6e0b13603fe7db10d855e0918ef54dedc033540389ccbcc313d2387179edd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5698edce-ce59-47c5-8505-22e1c59df110", "node_type": "1", "metadata": {}, "hash": "81a27a5dcdaeca41e083f682993de2f04dcb389de8ddf09f343323636f4d706b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So it's actually doable\nto figure out if\nsomething is a real number\nor a natural number?\n- Every object stuff comes\nwith a history of where it came from\nand you can kind of trace.\n- Oh, I see.\n- Yeah, so it's designed for reliability.\nSo modern AIs are not used in,\nit's a disjoint technology.\nPeople are beginning to\nuse AIs on top of Lean.\nSo when a mathematician tries\nto program proof in Lean,\noften there's a step.\nOkay, now I want\nto use the fundamental\ncalculus to do the next step.\nSo the Lean developers have\nbuilt this massive project\ncalled Mathlib, a collection of tens\nof thousands of useful facts\nabout mathematical objects.\nAnd somewhere in there is the\nfundamental theme of calculus,\nbut you need to find it.\nSo a lot, the bottle neck\nnow is actually Lemma Search.\nThere's a tool that you\nknow is in there somewhere\nand you need to find it.\nAnd so there are various\nsearch engines specialized\nfor Mathlib that you can do,\nbut there's now these\nlarge language models\nthat you can say,\n\"I need the fundamental\ncalculus at this point.\"\nAnd it was like, okay,\nfor example, when I code,\nI have GitHub Copilot\ninstalled as a plugin to my IDE\nand it scans my text, and\nit sees what I need, says,\nI might even type,\nnow I need to use the\nfundamental theorem of calculus.\nAnd then it might\nsuggest, \"Okay, try this,\"\nand maybe 25% of the\ntime it works exactly.\nAnd then another 10, 15% of\nthe time it doesn't quite work,\nbut it's close enough\nthat I can say, \"Oh yeah,\nif I just change it here\nand here, it'll work.\"\nAnd then like half the time\nit gives me complete rubbish.\nBut people are beginning to\nuse AIs a little bit on top,\nmostly on the level of\nbasically fancy autocomplete,\nthat you can type half\nof one line of a proof\nand it will find.\nIt will tell you.\n- Yeah, but a fancy, especially fancy\nwith the sort of capital\nletter F is remove some\nof the friction mathematician\nmight feel when they move\nfrom pen and paper to formalizing.\n- Yes, yeah.\nSo right now I estimate that the effort,\ntime and effort taken\nto formalize a proof is about\n10 times the amount taken\nto write it out.\nYeah, so it's doable, but it's annoying.\n- But doesn't it kill the whole vibe\nof being a mathematician\nhaving a pedantic coworker?\n- Right, yeah, if that was the\nonly aspect of it, okay, but,\nokay, there are some cases where\nit's actually more pleasant\nto do things formally.\nSo there was a theorem I formalized\nand there was a certain\nconstant 12 that came out\nin the final statement.\nAnd so this 12 had to be\ncarried all through the proof\nand everything had to be checked,\nthat all these other numbers had\nto be consistent with\nthis final number 12.\nAnd then so we wrote a paper\nthrough this theorem with this number 12,\nand then a few weeks\nlater, someone said, \"Oh,\nwe can actually improve this 12 to an 11\nby reworking some of these steps.\"\nAnd when this happens with pen and paper,\nevery time you change a parameter,\nyou have to check line\nby line that every single line\nof your proof still works.\nAnd there can be subtle things\nthat you didn't quite\nrealize, some properties\non number 12 that you didn't even realize\nthat you were taking advantage of.\nSo a proof can break\ndown at a subtle place.\nSo we had formalized the\nproof with this constant 12.\nAnd then when this new\npaper came out, we said,\n\"Oh, so that took like\nthree weeks to formalize\nand like 20 people to\nformalize this original proof.\"\nI said, \"Oh, but now let's\nupdate the 12 to 11.\"\nAnd what you can do with Lean is that\nin your headline theorem,\nyou change a 12 to 11.\nYou run the compiler,\nand of the thousands of\nlines of code you have,\n90% of them still work.\nAnd there's a couple\nthat are lined in red.\nNow, I can't justify these steps,\nbut it immediately isolates\nwhich steps you need to change.\nBut you can skip over everything,\nwhich works just fine.", "mimetype": "text/plain", "start_char_idx": 83417, "end_char_idx": 87252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5698edce-ce59-47c5-8505-22e1c59df110": {"__data__": {"id_": "5698edce-ce59-47c5-8505-22e1c59df110", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c191653-c6f6-4933-8640-cbcac4171d00", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7b0b3760ccba2be6008436634f8b326b2e5576287ef67037a9d6514643dba0cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccef3225-ca0a-47a2-a09e-a38382d0c333", "node_type": "1", "metadata": {}, "hash": "3cb9ae5d68ab715652cd034d154406a5f6013c00efa7c4f29adac4df4f3fddbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So a proof can break\ndown at a subtle place.\nSo we had formalized the\nproof with this constant 12.\nAnd then when this new\npaper came out, we said,\n\"Oh, so that took like\nthree weeks to formalize\nand like 20 people to\nformalize this original proof.\"\nI said, \"Oh, but now let's\nupdate the 12 to 11.\"\nAnd what you can do with Lean is that\nin your headline theorem,\nyou change a 12 to 11.\nYou run the compiler,\nand of the thousands of\nlines of code you have,\n90% of them still work.\nAnd there's a couple\nthat are lined in red.\nNow, I can't justify these steps,\nbut it immediately isolates\nwhich steps you need to change.\nBut you can skip over everything,\nwhich works just fine.\nAnd if you program things correctly\nwith good programming practices,\nmost of your lines will not be red.\nAnd there'll just be a\nfew places where you,\nI mean, if you don't\nhard code your constants,\nbut you sort of use smart\ntactics and so forth,\nyou can localize the things you need\nto change to a very small period of time.\nSo it's like within a day or two,\nwe had updated our proof\nbecause this is a very quick process.\nYou make a change.\nThere are 10 things now that don't work.\nFor each one, you make a change.\nAnd now there's five more\nthings that don't work.\nBut the process converges\nmuch more smoothly\nthan with pen and paper.\n- So that's for writing.\nAre you able to read it?\nLike, if somebody else sends\na proof, are you able to,\nwhat's the, versus paper?\n- Yeah, so the proofs are longer,\nbut each individual\npiece is easier to read.\nSo if you take a math paper\nand you jump to page 27\nand you look at paragraph six\nand you have a line of text of math,\nI often can't read it immediately\nbecause it assumes various definitions,\nwhich I have to go back,\nand maybe 10 pages\nearlier this was defined,\nand the proof is scattered\nall over the place,\nand you basically are forced\nto read fairly sequentially.\nIt's not like, say, a novel,\nwhere in theory you could open\nup a novel halfway through\nand start reading.\nThere's a lot of context.\nBut with a proof in Lean,\nif you put your cursor on a line of code,\nevery single object there,\nyou can hover over it and\nit will say what it is,\nwhere it came from, where\nthe stuff is justified.\nYou can trace things back much easier\nthan of flipping through a math paper.\nSo one thing that Lean really enables\nis actually collaborating on proofs\nat a really atomic scale\nthat you really couldn't do in the past.\nSo traditionally with pen and paper,\nwhen you want to collaborate\nwith another mathematician,\neither you do it\nat a blackboard where\nyou can really interact,\nbut if you're doing it sort\nof by email or something,\nbasically you have to segment it.\nI'm going to finish section\nthree, you do section four,\nbut you can't really\nsort of work on the same\nthing collaboratively\nat the same time.\nBut with Lean, you can be\ntrying to formalize some portion\nof the proof and say, \"I\ngot stuck at line 67 here,\nI need to prove this thing,\nbut it doesn't quite work.\nHere's the three lines of\ncode I'm having trouble with.\"\nBut because all the context is\nthere, someone else can say,\n\"Oh, okay, I recognize\nwhat you need to do.\nYou need to apply this\ntrick or this tool,\"\nand you can do extremely\natomic-level conversations.\nSo because of Lean,\nI can collaborate with dozens of people\nacross the world,\nmost of whom I don't\nhave never met in person,\nand I may not know actually\neven whether how reliable they are\nin the proofs they give me,\nbut Lean gives me a certificate of trust,\nso I can do trustless mathematics.\n- So there's so many\ninteresting questions.\nSo one, you're known for\nbeing a great collaborator.\nSo what is the right way\nto approach solving a difficult\nproblem in mathematics\nwhen you're collaborating?\nAre you doing a divide\nand conquer type of thing,\nor are you focusing on a particular part\nand you're brainstorming?\n- There's always a\nbrainstorming process first.\nYeah.", "mimetype": "text/plain", "start_char_idx": 86579, "end_char_idx": 90488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccef3225-ca0a-47a2-a09e-a38382d0c333": {"__data__": {"id_": "ccef3225-ca0a-47a2-a09e-a38382d0c333", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5698edce-ce59-47c5-8505-22e1c59df110", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b2ad004a7622f2187d8fc00929b672a95c704e55f207a19186280cbf37da881b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6d2e27b-768f-46c0-a0df-acefb430ecf1", "node_type": "1", "metadata": {}, "hash": "feb877f1883a18374c78b02f5ddf49af3895697fea4c63eca0562c513c27f2b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You need to apply this\ntrick or this tool,\"\nand you can do extremely\natomic-level conversations.\nSo because of Lean,\nI can collaborate with dozens of people\nacross the world,\nmost of whom I don't\nhave never met in person,\nand I may not know actually\neven whether how reliable they are\nin the proofs they give me,\nbut Lean gives me a certificate of trust,\nso I can do trustless mathematics.\n- So there's so many\ninteresting questions.\nSo one, you're known for\nbeing a great collaborator.\nSo what is the right way\nto approach solving a difficult\nproblem in mathematics\nwhen you're collaborating?\nAre you doing a divide\nand conquer type of thing,\nor are you focusing on a particular part\nand you're brainstorming?\n- There's always a\nbrainstorming process first.\nYeah.\nSo math research projects, by\ntheir nature, when you start,\nyou don't really know\nhow to do the problem.\nIt's not like an engineering project\nwhere somehow the theory has\nbeen established for decades\nand its implementation is\nthe main difficulty body.\nYou have to figure out even\nwhat is the right path.\nSo this is what I said\nabout cheating first.\nIt's like, to go back to\nthe bridge building analogy,\nfirst, assume you have infinite budget\nand unlimited amounts of\nworkforce and so forth.\nNow can you build this bridge?\nOkay, now have an infinite\nbudget but only finite workforce.\nNow can you do that, and so forth.\nOf course, no engineer\ncan actually do this.\nThey have fixed requirements.\nYes, there's this sort\nof jam sessions always\nat the beginning,\nwhere you try all kinds of crazy things,\nand you make all these\nassumptions that are unrealistic\nbut you plan to fix later.\nAnd you try to see if\nthere's even some skeleton\nof an approach that might work,\nand then hopefully that\nbreaks up the problem\ninto smaller sub-problems,\nwhich you don't know how to do,\nbut then you focus on the sub ones,\nand sometimes different\ncollaborators are better\nat working on certain things.\nSo one of my theorems I'm known for\nis a theorem of Ben Green,\nwhich is now called the Green-Tao theorem.\nIt's a statement\nthat the primes contain\narithmetic progressions\nof any length.\nSo it's a modification of\nthis theorem of Szemeredi.\nAnd the way we collaborated was\nthat Ben had already\nproven a similar result\nfor progressions of length three.\nHe showed that sets like\nthe primes contain lots\nand lots of progressions of length three.\nAnd even subsets of the primes,\ncertain subsets do.\nBut his techniques only worked\nfor length three progressions.\nThey didn't work for longer progressions.\nBut I had these techniques\ncoming from ergodic theory,\nwhich is something that\nI had been playing with,\nand I knew better than Ben at the time.\nAnd so if I could justify\ncertain randomness properties\nof some set relating to the primes,\nlike there's a certain\ntechnical condition,\nwhich if I could have it,\nif Ben could supply me to this fact,\nI could conclude the theorem.\nBut what I asked was a\nreally difficult question\nin number theory, which he said,\n\"There's no way we can prove this.\"\nSo he said, \"Can you prove your part\nof the theorem using a weaker hypothesis,\nthat I have a chance to prove?\"\nAnd he proposed something\nwhich he could prove,\nbut it was too weak for me.\nI can't use this.\nSo there was this conversation\ngoing back and forth.\n- Different cheats too.\n- Yeah, I want to cheat\nmore, he wants to cheat less.\nBut eventually we found a property which,\nA, he could prove,\nand B, I could use,\nand then we could prove our view.\nThere are all kinds of dynamics.\nI mean, every collaboration\nhas some story.\nNo two are the same.\n- And then on the flip side of that,\nlike you mentioned with Lean programming,\nnow that's almost like a different story,\nbecause you can create,\nI think you've mentioned\na kind of a blueprint\nfor a problem, and then\nyou can really do a divide\nand conquer with Lean,\nwhere you're working on separate parts\nand they're using the\ncomputer system proof checker,\nessentially to make sure\nthat everything is correct along the way.\n- Yeah.\nSo it makes everything\ncompatible and trustable.\nSo currently only a few\nmathematical projects\ncan be cut up in this way.", "mimetype": "text/plain", "start_char_idx": 89724, "end_char_idx": 93862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6d2e27b-768f-46c0-a0df-acefb430ecf1": {"__data__": {"id_": "a6d2e27b-768f-46c0-a0df-acefb430ecf1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccef3225-ca0a-47a2-a09e-a38382d0c333", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6007a0b9e7601d536927a749f62737d2248b3ce8fd6c44cec62f4bb85e4101bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b", "node_type": "1", "metadata": {}, "hash": "ffc01844435d7e259d9e65b925adadca463a08c9570ef4a7d603185eecae4df2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Different cheats too.\n- Yeah, I want to cheat\nmore, he wants to cheat less.\nBut eventually we found a property which,\nA, he could prove,\nand B, I could use,\nand then we could prove our view.\nThere are all kinds of dynamics.\nI mean, every collaboration\nhas some story.\nNo two are the same.\n- And then on the flip side of that,\nlike you mentioned with Lean programming,\nnow that's almost like a different story,\nbecause you can create,\nI think you've mentioned\na kind of a blueprint\nfor a problem, and then\nyou can really do a divide\nand conquer with Lean,\nwhere you're working on separate parts\nand they're using the\ncomputer system proof checker,\nessentially to make sure\nthat everything is correct along the way.\n- Yeah.\nSo it makes everything\ncompatible and trustable.\nSo currently only a few\nmathematical projects\ncan be cut up in this way.\nAt the current state of the art,\nmost of the Lean activity\nis on formalizing proofs\nthat have already been proven by humans.\nA math paper basically is\na blueprint in a sense.\nIt is taking a difficult\nstatement like big theorem\nand breaking it up into\n100 little numbers is,\nbut often not all written\nwith enough detail that each one can be\nsort of directly formalized.\nA blueprint is like a really\npedantically written version\nof a paper where every step is explained\nto as much detail as possible\nand trying to make each\nstep kind of self-contained\nor depending on only\na very specific number\nof previous statements\nthat have been proven,\nso that each node of this blueprint graph\nthat gets generated can\nbe tackled independently\nof the others, and you don't even need\nto know how the whole thing works.\nSo it's like a modern supply chain.\nIf you want to create an iPhone\nor some other complicated object,\nno one person can build a single object,\nbut you can have specialists who just\nif they're given some widgets\nfrom some other company,\nthey can combine them together\nto form a slightly bigger widget.\n- I think that's a really\nexciting possibility\nbecause if you can find problems\nthat could be broken down this way,\nthen you could have thousands\nof contributors, right?\nCompletely distributed.\n- Yes, yes, yes.\nSo I told you before about\nthe split between theoretical\nand experimental mathematics.\nAnd right now most\nmathematics is theoretical\nand only a tiny bit is experimental.\nI think the platform that\nLean and other software tools,\nso GitHub and things like that\nwill allow experimental mathematics\nto scale up to a much greater\ndegree than we can do now.\nSo right now, if you want to\ndo any mathematical exploration\nof some mathematical pattern or something,\nyou need some code to\nwrite out the pattern.\nAnd I mean, sometimes there\nare some computer algebra\npackages that help,\nbut often it's just one\nmathematician coding lots\nand lots of Python or whatever.\nAnd because coding is such\nan error prone activity,\nit's not practical\nto allow other people\nto collaborate with you\non writing modules for your code,\nbecause if one of the\nmodules has a bug in it,\nthe whole thing is unreliable.\nSo you get these bespoke\nspaghetti code written\nby not professional\nprogrammers, but mathematicians,\nand they're clunky and slow.\nAnd so because of that,\nit's hard to really mass\nproduce experimental results.\nBut I think with Lean, I mean,\nI'm already starting some projects\nwhere we are not just\nexperimenting with data,\nbut experimenting with proofs.\nSo I have this project\ncalled the Equational Theories Project.\nBasically, we generated about\n22 million little problems\nin abstract algebra.\nMaybe I should back up and\ntell you what the project is.\nOkay, so abstract algebra\nstudies operations\nlike multiplication\nand addition and their\nabstract properties.\nSo multiplication, for\nexample, is commutative.\nX times Y is always Y times\nX, at least for numbers.\nAnd it's also associative.\nX times Y times Z\nis the same as X times Y times Z.\nSo these operations obey some\nlaws that don't obey others.\nFor example, X times X\nis not always equal to X,\nso that law is not always true.\nSo given any operation, it\nobeys some laws and not others.\nAnd so we generated about\n4,000 of these possible laws\nof algebra that certain\noperations can satisfy.\nAnd our question is, which\nlaws imply which other ones?\nSo, for example, does\ncommutativity imply associativity?", "mimetype": "text/plain", "start_char_idx": 93017, "end_char_idx": 97319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b": {"__data__": {"id_": "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6d2e27b-768f-46c0-a0df-acefb430ecf1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a7d302d52d4d9c394f4f6b19f7dd56bc08e0fcea86c399f72aaa56cdecd116f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b", "node_type": "1", "metadata": {}, "hash": "92589f248e6f47a2f3a99453f1227b74f6e75392c4c63f1a3c8b2d57835bab2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Basically, we generated about\n22 million little problems\nin abstract algebra.\nMaybe I should back up and\ntell you what the project is.\nOkay, so abstract algebra\nstudies operations\nlike multiplication\nand addition and their\nabstract properties.\nSo multiplication, for\nexample, is commutative.\nX times Y is always Y times\nX, at least for numbers.\nAnd it's also associative.\nX times Y times Z\nis the same as X times Y times Z.\nSo these operations obey some\nlaws that don't obey others.\nFor example, X times X\nis not always equal to X,\nso that law is not always true.\nSo given any operation, it\nobeys some laws and not others.\nAnd so we generated about\n4,000 of these possible laws\nof algebra that certain\noperations can satisfy.\nAnd our question is, which\nlaws imply which other ones?\nSo, for example, does\ncommutativity imply associativity?\nAnd the answer is no,\nbecause it turns out you\ncan describe an operation\nwhich obeys the commutative law\nbut doesn't obey the associative of law.\nSo by producing an example,\nyou can show that commutativity\ndoes not imply associativity,\nbut some other laws do imply other laws\nby substitution and so forth.\nAnd you can write down\nsome algebraic proof.\nSo we look at all the pairs\nbetween these 4,000 laws,\nand there's about 22\nmillion of these pairs.\nAnd for each pair we ask,\n\"Does this law imply this law?\nIf so, give a proof.\nIf not, give a counterexample.\"\nSo 22 million problems, each\none of which you could give\nto like an undergraduate algebra student,\nand they had a decent chance\nof solving the problem.\nAlthough there are a\nfew, at least 22 million,\nthere are like 100 or so\nthat are really quite hard,\nbut a lot are easy.\nAnd the project was just\nto work out to determine the entire graph,\nlike which ones imply which other ones.\n- That's an incredible\nproject, by the way.\nSuch a good idea, such a good test\nof the very thing we've been\ntalking about at a scale.\nThat's remarkable.\n- Yeah, so it would\nnot have been feasible.\nI mean, the state of the art\nin the literature was like 15 equations\nand sort of how they applied.\nThat's sort of at the limit\nof what a human repentant paper can do.\nSo you need to scale that up.\nSo you need to crowdsource,\nbut you also need to trust all the,\nI mean, no one person can check\n22 million of these proofs.\nYou needed to be computerized.\nAnd so it only became possible with Lean.\nWe Were hoping to use a lot of AI as well.\nSo the project is almost complete.\nSo of these 22 million, all\nbut two have been settled.\n- Wow.\n- Actually, and of those two,\nwe have a pen and paper proof for two,\nand we're formalizing it.\nIn fact, this morning I was\nworking on finishing it.\nSo we're almost done on this.\n- [Lex] It's incredible.\n- Yeah, fantastic.\n- How many people were you able to get?\n- About 50, which in mathematics\nis considered a huge number.\n- It's a huge number.\nThat's crazy.\n- Yeah.\nSo we're going to have\na paper of 50 authors\nand a big appendix of\nwho contributor what.\n- Here's an interesting question.\nNot to maybe speak even\nmore generally about it.\nWhen you have this pool of people,\nis there a way to\norganize the contributions\nby level of expertise of the\npeople of the contributors?\nNow, okay, I'm asking a lot\nof pothead questions here,\nbut I'm imagining a bunch of humans\nand maybe in the future some AIs,\ncan there be like an ELO rating type\nof situation where like\na gamification of this?\n- The beauty of these Lean projects\nis that automatically\nyou get all this data.\nSo everything's been\nuploaded to this GitHub\nand GitHub tracks who contributed what,\nso you could generate statistics\nat any later point in time.\nYou could say, \"Oh,\nthis person contributed\nthis many lines of code or whatever.\"\nThese are very crude metrics.\nI would definitely not\nwant this to become part\nof your tenure review or something.\nBut I think already in\nenterprise computing,\npeople do use some of these metrics\nas part of the assessment of\nperformance of an employee.", "mimetype": "text/plain", "start_char_idx": 96481, "end_char_idx": 100438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b": {"__data__": {"id_": "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "577ab3c0-1464-4c3c-a8cc-3c0dbbeae50b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4cc0fd15561fc07b1464c0493a8bfac1f79be29b230f33358ccbc9a990fa0511", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eed5f86e-a7b6-48b9-8f62-abf533a9108b", "node_type": "1", "metadata": {}, "hash": "5833e13cb172a9ca259ab7326f6bf5deb7d7a07a6567a4cab326bc0770c9b565", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, okay, I'm asking a lot\nof pothead questions here,\nbut I'm imagining a bunch of humans\nand maybe in the future some AIs,\ncan there be like an ELO rating type\nof situation where like\na gamification of this?\n- The beauty of these Lean projects\nis that automatically\nyou get all this data.\nSo everything's been\nuploaded to this GitHub\nand GitHub tracks who contributed what,\nso you could generate statistics\nat any later point in time.\nYou could say, \"Oh,\nthis person contributed\nthis many lines of code or whatever.\"\nThese are very crude metrics.\nI would definitely not\nwant this to become part\nof your tenure review or something.\nBut I think already in\nenterprise computing,\npeople do use some of these metrics\nas part of the assessment of\nperformance of an employee.\nAgain, this is a direction\nwhich is a bit scary\nfor academics to go down.\nWe don't like metrics so much.\n- And yet academics use metrics.\nThey just use old ones.\nNumber of papers.\n- Yeah, yeah, it's true.\nIt's true that, yeah, I mean.\n- It feels like this is\na metric, while flawed,\nis going more in the\nright direction, right?\n- [Terence] Yeah.\n- It's interesting.\nAt least it's a very interesting metric.\n- Yeah, I think it's interesting to study.\nI think you can do studies\nof whether these are better predictors.\nThere's this problem\ncalled Goodhart's Law.\nIf a statistic is actually used\nto incentivize performance,\nit becomes gamed, and then it\nis no longer a useful measure.\n- Oh, humans always game.\n- Yeah, yeah.\nNo, it's rational.\nSo what we've done for this\nproject is self-report.\nSo there are actually standard categories\nfrom the sciences\nof what types of\ncontributions people give.\nSo there's this concept\nand validation and resources\nand coding and so forth.\nSo there's a standard list\nof 12 or so categories,\nand we just ask each contributor to,\nthere's a big matrix of all the authors\nand all the categories just\nto tick the boxes where they\nthink that they contributed,\nand just give a rough idea.\nSo you did some coding and\nyou provided some compute,\nbut you didn't do any of the pen\nand paper verification or whatever.\nAnd I think that works out.\nTraditionally, mathematicians\njust order alphabetically\nby certain name.\nSo we don't have this\ntradition, as in the sciences,\nof lead author and second\nauthor and so forth,\nwhich we're proud of.\nWe make all the authors equal status,\nbut it doesn't quite scale to this size.\nSo a decade ago I was involved\nin these things called Polymath Projects.\nIt was the crowdsourcing mathematics,\nbut without the Lean component.\nSo it was limited by you\nneeded a human moderator\nto actually check that all\nthe contributions coming in\nwere actually valid.\nAnd this was a huge bottleneck, actually.\nBut still we had projects\nthat were 10 authors or so,\nbut we had decided\nat the time not to try\nto decide who did what,\nbut to have a single pseudonym.\nSo we created this fictional\ncharacter called DHJ Polymath.\nIn the spirit of Bourbaki.\nBourbaki is the pseudonym\nfor a famous group of\nmathematicians in the 20th century.\nAnd so the paper was\nauthored under pseudonym,\nso none of us got the author credit.\nThis actually turned\nout to be not so great\nfor a couple of reasons.\nSo one is that if you actually\nwanted to be considered\nfor tenure or whatever,\nyou could not use this paper\nas you submitted on your publications\nbecause you didn't have\nthe formal author credit.\nBut the other thing that we've recognized\nuntil much later is that\nwhen people referred\nto these projects, they naturally refer\nto the most famous person who\nwas involved in the project.\nOh, so this was Tim\nGowers' Polymath Project,\nthis was Terence Tao's Polymath Project,\nand not mention the other 19\nor whatever people that were involved.\nSo we're trying something\ndifferent this time around,\nwhere everyone's an author,\nbut we will have an\nappendix with this matrix\nand we'll see how that works.\n- So both projects are incredible.\nJust the fact that you're involved\nin such huge collaborations.", "mimetype": "text/plain", "start_char_idx": 99668, "end_char_idx": 103662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eed5f86e-a7b6-48b9-8f62-abf533a9108b": {"__data__": {"id_": "eed5f86e-a7b6-48b9-8f62-abf533a9108b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dcd2897-9045-4f56-a5a4-b2873b0b7a1b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "db1d1a3ce4b4c7ac2897fe056a65357c4773c64abf267a34d8f9829c4df3b65a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d64675ea-28b2-4067-aaf2-6e6b58038044", "node_type": "1", "metadata": {}, "hash": "929dac861169cc22e9b81f51e4288edc1d0e08cab08f1ceb5a7a38a2098df59a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This actually turned\nout to be not so great\nfor a couple of reasons.\nSo one is that if you actually\nwanted to be considered\nfor tenure or whatever,\nyou could not use this paper\nas you submitted on your publications\nbecause you didn't have\nthe formal author credit.\nBut the other thing that we've recognized\nuntil much later is that\nwhen people referred\nto these projects, they naturally refer\nto the most famous person who\nwas involved in the project.\nOh, so this was Tim\nGowers' Polymath Project,\nthis was Terence Tao's Polymath Project,\nand not mention the other 19\nor whatever people that were involved.\nSo we're trying something\ndifferent this time around,\nwhere everyone's an author,\nbut we will have an\nappendix with this matrix\nand we'll see how that works.\n- So both projects are incredible.\nJust the fact that you're involved\nin such huge collaborations.\nBut I think I saw a\ntalk from Kevin Buzzard\nabout the Lean programming\nlanguage just a few years ago,\nand you're saying that\nthis might be the future\nof mathematics.\nAnd so it's also exciting\nthat you're embracing,\none of the greatest\nmathematicians in world,\nembracing this what seems like the paving\nof the future of mathematics.\nSo I have to ask you here\nabout the integration of AI\ninto this whole process.\nSo DeepMind's AlphaProof\nwas trained using reinforcement\nlearning on both failed\nand successful formal Lean\nproofs of IMO problems.\nSo this is sort of high-level high school.\n- Oh, very high level.\n- Yes, very high-level high school level\nmathematics problems.\nWhat do you think about the system\nand maybe what is the gap\nbetween this system that is able\nto prove the high school level problems\nversus graduate level problems?\n- Yeah, the difficulty\nincreases exponentially\nwith the number of steps\ninvolved in the proof.\nIt's a combinatorial explosion.\nSo the thing of large language models\nis that they make mistakes.\nAnd so if a proof has got 20 steps\nand your has a 10% failure rate\nat each step of going\nin the wrong direction,\nit's extremely unlikely\nto actually reach the end.\n- Actually, just to take\na small tangent here,\nhow hard is the problem of mapping\nfrom natural language\nto the formal program?\n- Oh yeah, it's extremely hard actually.\nNatural language, it's\nvery fault tolerant.\nLike you can make a few\nminor grammatical errors\nand a speaker in the second\nlanguage can get some idea\nof what you're saying.\nBut formal language, if you\nget one little thing wrong,\nthe whole thing is nonsense.\nEven formal to formal is very hard.\nThere are different incompatible\nprefaces in languages.\nThere's Lean, but also Coq\nand Isabelle and so forth.\nEven converting from a formal language\nto formal language\nis an unsolved, basically\nunsolved problem.\n- That is fascinating.\nOkay, so but once you\nhave an informal language,\nthey're using their RL-trained model.\nSo something akin to\nAlphaZero that they used\nto go to then try to come up with proofs.\nThey also have a model.\nI believe it's a separate\nmodel for geometric problems.\nSo what impresses you about this system,\nand what do you think is the gap?\n- Yeah, yeah, we talked earlier\nabout things that are\namazing over time become\nkind of normalized.\nSo now somehow,\nof course geometry is a solvable problem.\n- Right, that's true, that's true.\nI mean it's still beautiful.\n- Yeah, yeah, no,\nthese are great work that\nshows what's possible.\nThe approach doesn't scale currently.\nThere are three days of Google server time\nto solve one high school\nmath problem there.\nThis is not a scalable prospect,\nespecially with the exponential increase\nas the complexity increases.\n- We should mention\nthat they got a silver medal performance.\n- The equivalent of.\n- [Lex] Equivalent of a\nsilver medal performance.\n- So first of all,\nthey took way more time than was allotted,\nand they had this assistance\nwhere the humans helped\nby formalizing, but also they're\ngiving us those full marks\nfor the solution, which I\nguess is formally verified.\nSo I guess that's fair.", "mimetype": "text/plain", "start_char_idx": 102799, "end_char_idx": 106785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d64675ea-28b2-4067-aaf2-6e6b58038044": {"__data__": {"id_": "d64675ea-28b2-4067-aaf2-6e6b58038044", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eed5f86e-a7b6-48b9-8f62-abf533a9108b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "2c8ee0151a2e3c210621c42207458251232c5b717c8ac0458ff257cc657c99dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "300806ea-49cf-4f0e-acd9-49abc6d7280b", "node_type": "1", "metadata": {}, "hash": "63fab8d3f20f025f31bff942ac81c2031a52b92864e99951431bc1be3fe68a9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So now somehow,\nof course geometry is a solvable problem.\n- Right, that's true, that's true.\nI mean it's still beautiful.\n- Yeah, yeah, no,\nthese are great work that\nshows what's possible.\nThe approach doesn't scale currently.\nThere are three days of Google server time\nto solve one high school\nmath problem there.\nThis is not a scalable prospect,\nespecially with the exponential increase\nas the complexity increases.\n- We should mention\nthat they got a silver medal performance.\n- The equivalent of.\n- [Lex] Equivalent of a\nsilver medal performance.\n- So first of all,\nthey took way more time than was allotted,\nand they had this assistance\nwhere the humans helped\nby formalizing, but also they're\ngiving us those full marks\nfor the solution, which I\nguess is formally verified.\nSo I guess that's fair.\nThere are efforts,\nthere will be a proposal\nat some point to actually\nhave an AI math olympiad\nwhere at the same time\nas the human contestants get\nthe actual olympiad problems,\nAIs will also be given the same problems\nwith the same time period.\nAnd the outputs will have to\nbe graded by the same judges,\nwhich means that it\nwill have to be written\nin natural language rather\nthan formal language.\n- Oh, I hope that happens.\nI hope that this IMO it happens.\nI hope the next one.\n- It won't happen this IMO.\nThe performance is not good enough\nin the time period.\nBut there are smaller competitions.\nThere are competitions\nwhere the answer is a number\nrather than a long form proof.\nAnd AIs are actually a lot better\nat problems where there's a\nspecific numerical answer,\nbecause it's easy to do\nreinforcement learning on it.\nYou got the right answer,\nyou got the wrong answer,\nit's a very clear signal.\nBut a long form proof\neither has to be formal\nand then the lean can give\nit a thumbs up, thumbs down,\nor it's informal.\nBut then you need a human to grade it.\nAnd if you're trying to do billions\nof reinforcement learning runs,\nyou can't hire enough\nhumans to grade those.\nIt's already hard enough\nfor the last language models\nto do reinforcement learning\non just the regular text that people get.\nBut now if you actually hire people,\nnot just give thumbs up, thumbs down,\nbut actually check the\noutput mathematically, yeah,\nthat's too expensive.\n- So if we just explore\nthis possible future,\nwhat is the thing that\nhumans do that's most special\nin mathematics,\nso that you could see AI\nnot cracking for a while?\nSo inventing new theories,\nso coming up with new conjectures\nversus proving the conjectures,\nbuilding new abstractions,\nnew representations,\nmaybe an AI Terence style\nwith seeing new connections\nbetween disparate fields?\n- That's a good question.\nI think the nature\nof what mathematicians do\nover time has changed a lot.\nSo 1,000 years ago,\nmathematicians had to\ncompute the date of Easter.\nAnd there's really\ncomplicated calculations,\nbut it's all automated, been\nautomated for centuries.\nWe don't need that anymore.\nThey used to navigate, to\ndo spherical navigation,\nspherical trigonometry,\nto navigate how to get from\nthe old world to the new.\nSo I think a very complicated\ncalculation, again,\nbeing automated.\nEven a lot of undergraduate mathematics,\neven before AI, like\nWolfram Alpha, for example,\nis not a language model,\nbut it can solve a lot of\nundergraduate level math tasks.\nSo on the computational side,\nverifying routine things\nlike having a problem, and I say,\n\"Here's a problem in partial\ndifferential equations,\ncould you solve it using any\nof the 20 standard techniques?\"\nAnd the AI will say,\n\"Yes, I've tried all 20\nand here are the 100\ndifferent permutations,\nand here's my results.\"\nAnd that type of thing, I\nthink it will work very well.\nType of scaling to once\nyou solve one problem,\nto make the AI attack\n100 adjacent problems.\nThe things that humans do still.\nSo where the AI really\nstruggles right now is knowing\nwhen it's made a wrong\nturn that it can say, \"Oh,\nI'm going to solve this problem.\nI'm going to split up this\nproblem into these two cases.\nI'm going to try this technique.\"\nAnd sometimes if you're lucky\nand it's a simple problem,\nit's the right technique\nand you solve the problem.", "mimetype": "text/plain", "start_char_idx": 105982, "end_char_idx": 110120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "300806ea-49cf-4f0e-acd9-49abc6d7280b": {"__data__": {"id_": "300806ea-49cf-4f0e-acd9-49abc6d7280b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d64675ea-28b2-4067-aaf2-6e6b58038044", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5897b2d42803dff695a01d2df8f74282679f2255eea92a6a9e752f3ffa453e6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f3e6539-3fb5-4e10-b79f-48db9e00335b", "node_type": "1", "metadata": {}, "hash": "094fabc647c9e3436dbf6a54655f8bf1e0dd8e9df3a4794f3cccb8822306d94f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the AI will say,\n\"Yes, I've tried all 20\nand here are the 100\ndifferent permutations,\nand here's my results.\"\nAnd that type of thing, I\nthink it will work very well.\nType of scaling to once\nyou solve one problem,\nto make the AI attack\n100 adjacent problems.\nThe things that humans do still.\nSo where the AI really\nstruggles right now is knowing\nwhen it's made a wrong\nturn that it can say, \"Oh,\nI'm going to solve this problem.\nI'm going to split up this\nproblem into these two cases.\nI'm going to try this technique.\"\nAnd sometimes if you're lucky\nand it's a simple problem,\nit's the right technique\nand you solve the problem.\nAnd sometimes it will have a problem.\nIt would propose an approach\nwhich is just complete nonsense,\nbut it looks like a proof.\nSo this is one annoying thing\nabout LLM-generated mathematics.\nWe've had human generated mathematics\nthat's very low-quality.\nLike submissions from people\nwho don't have the formal\ntraining and so forth.\nBut if a human proof is bad,\nyou can tell there's bad\nproof pretty quickly.\nIt makes really basic mistakes.\nBut the AI-generated proofs,\nthey can look superficially flawless.\nAnd that's partly\nbecause that's what the\nreinforcement learning\nhas actually trained them to do,\nto produce text that looks\nlike what is correct,\nwhich for many applications\nis good enough.\nSo the errors are often really subtle,\nand then when you spot\nthem, they're really stupid.\nNo human would have\nactually made that mistake.\n- Yeah, it's actually really frustrating\nin the programming context\nbecause I program a lot.\nAnd yeah, when a human makes\nwhen a low-quality code,\nthere's something called code smell.\nYou can tell.\nYou can tell immediately,\nlike, okay, there's signs,\nbut with AI generate code.\n- Odorless.\n- And then you're right,\neventually you find an obvious dumb thing\nthat just looks like good code.\n- [Terence] Yeah.\n- It's very tricky too,\nand frustrating for some reason to work.\n- Yeah.\nSo the sense of smell.\nThere you go.\nThis is one thing that humans have\nand there's a metaphorical\nmathematical smell.\nIt's not clear how\nto get the AIs to duplicate that.\nEventually, I mean, so the\nway AlphaZero and so forth,\nthey make progress on go\nand chess and so forth,\nis in some sense they have\ndeveloped a sense of smell for go\nand chess positions, that this\nposition is good for white,\nthat's good for black.\nThey can't enunciate why,\nbut just having that sense of\nsmell lets them strategize.\nSo if AIs gain that ability\nto sort of assess the viability\nof certain proof strategies.\nSo you can say,\nI'm going to try to break up this problem\ninto two smaller sub-tasks,\nand they can say,\n\"Oh, this looks good.\"\nTwo tasks look like they're simpler tasks\nthan your main task\nand they still got a good\nchance of being true.\nSo this is good to try.\nOr, no, you made the problem worse\nbecause each of the two\nsub-problems is actually harder\nthan your original problem,\nwhich is actually what normally happens\nif you try a random thing to try.\nNormally it's very easy\nto transform a problem\ninto an even harder problem.\nVery rarely do you transform\nto a simpler problem.\nSo if they can pick up a sense of smell,\nthen they could maybe start competing\nwith human level mathematicians.\n- So this is a hard\nquestion, but not competing,\nbut collaborating.\nOkay, hypothetical.\nIf I gave you an oracle that\nwas able to do some aspect\nof what you do and you could\njust collaborate with it.\n- [Terence] Yeah, yeah, yeah.\n- What would that oracle,\nwhat would you like that\noracle to be able to do?\nWould you like it to maybe\nbe a verifier, like, check.\nDo the code smell, like you're,\nyes, Professor Tao, this is the correct,\nthis is a promising, fruitful direction.\n- [Terence] Yeah, yeah, yeah.\n- Or would you like it to\ngenerate possible proofs\nand then you see which\none is the right one?\nOr would you like it\nto maybe generate\ndifferent representation,\ntotally different ways\nof seeing this problem?\n- Yeah, I think all of the above.", "mimetype": "text/plain", "start_char_idx": 109489, "end_char_idx": 113467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f3e6539-3fb5-4e10-b79f-48db9e00335b": {"__data__": {"id_": "2f3e6539-3fb5-4e10-b79f-48db9e00335b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "300806ea-49cf-4f0e-acd9-49abc6d7280b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "340763d845e49b26165cb703303d686c8223a148debb9ba540abfbcb926d6720", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff", "node_type": "1", "metadata": {}, "hash": "6898eaba9c6dc0bc693f0bfc44702a47771d7671aa917f928d4839eb10ca94f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So this is a hard\nquestion, but not competing,\nbut collaborating.\nOkay, hypothetical.\nIf I gave you an oracle that\nwas able to do some aspect\nof what you do and you could\njust collaborate with it.\n- [Terence] Yeah, yeah, yeah.\n- What would that oracle,\nwhat would you like that\noracle to be able to do?\nWould you like it to maybe\nbe a verifier, like, check.\nDo the code smell, like you're,\nyes, Professor Tao, this is the correct,\nthis is a promising, fruitful direction.\n- [Terence] Yeah, yeah, yeah.\n- Or would you like it to\ngenerate possible proofs\nand then you see which\none is the right one?\nOr would you like it\nto maybe generate\ndifferent representation,\ntotally different ways\nof seeing this problem?\n- Yeah, I think all of the above.\nA lot of it is we don't\nknow how to use these tools\nbecause it's a paradigm, that it's not,\nwe have not had\nin the past assistants\nthat are competent enough\nto understand complex\ninstructions that can work\nat massive scale, but are also unreliable.\nIt's interesting,\nunreliable in subtle ways\nwhilst providing sufficiently good output.\nIt's an interesting combination.\nYou have graduate students\nthat you work with\nwho kind of like this, but not at scale.\nAnd we had previous software\ntools that can work at scale,\nbut very narrow.\nSo we have to figure out how to use them.\nI mean, so Tim Gowers actually,\nhe actually foresaw\nin 2000 he was envisioning what\nmathematics would look like\nin actually two and a half decades.\n(Lex and Terence laughing)\n- That's funny.\n- Yeah.\nHe wrote in his article,\nlike a hypothetical conversation\nbetween a mathematical assistant\nof the future and himself.\nHe's trying to solve a problem\nand they would have a conversation.\nSometimes the human would propose an idea\nand the AI would evaluate it,\nand sometimes the AI would pose an idea\nand sometimes a computation was required\nand AI would just go and say, \"Okay,\nI've checked the 100 cases needed here.\"\nOr, \"You said this is true for all N.\nI've checked to put N up to\n100 and it looks good so far.\"\nOr, \"Hang on, there's a\nproblem at N equals 46.\"\nSo just a free form conversation\nwhere you don't know\nin advance where things are going to go.\nBut just based on,\nI think ideas are good\nproposed on both sides.\nCalculations get proposed on both sides.\nI've had conversations\nwith AI where I say, \"Okay,\nwe're going to collaborate\nto solve this math problem,\"\nand it's a problem that I\nalready know the solution to,\nso I try to prompt it.\nOkay, so here's the problem.\nI suggest using this tool\nand it'll find this lovely argument\nusing a completely different\ntool which eventually goes\ninto the weeds and say, \"No, no, no,\ntry using this,\" and it\nmight start using this\nand then it'll go back to the tool\nthat it wanted to do before,\nand you have to keep railroading\nit onto the path you want.\nAnd I could eventually force\nit to give the proof I wanted,\nbut it was like herding cats.\nAnd the amount of personal\neffort I had to take to not just\nsort of prompt it, but\nalso check its output\nbecause a lot of what it\nlooked like is going to work.\nAnd I know there's a problem on line 17\nand basically arguing\nwith was more exhausting\nthan doing it unassisted,\nbut that's the current\nstate the of the art.\n- I wonder if there's a\nphase shift that happens\nto where it no longer\nfeels like herding cats,\nand maybe it'll surprise\nus how quickly that comes.\n- I believe so.\nIn formalization,\nI mentioned before that\nit takes 10 times longer\nto formalize a proof\nthan to write it by hand.\nWith these modern AI tools\nand also just better tooling,\nthe Lean developers are\ndoing a great job adding more\nand more features and\nmaking it user friendly.\nIt's going from nine to eight to seven.\nOkay, no big deal.", "mimetype": "text/plain", "start_char_idx": 112722, "end_char_idx": 116448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff": {"__data__": {"id_": "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f3e6539-3fb5-4e10-b79f-48db9e00335b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b26a0221a7d6e4242344ac037db1b5ca6c6210c6e3178c30d982add531451368", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54429056-0727-4e3c-a058-a639da294d79", "node_type": "1", "metadata": {}, "hash": "d0fc3de836c74e356ec7e3521757dfa0dd31d5e9feec339581b82ca36cccbfc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And I know there's a problem on line 17\nand basically arguing\nwith was more exhausting\nthan doing it unassisted,\nbut that's the current\nstate the of the art.\n- I wonder if there's a\nphase shift that happens\nto where it no longer\nfeels like herding cats,\nand maybe it'll surprise\nus how quickly that comes.\n- I believe so.\nIn formalization,\nI mentioned before that\nit takes 10 times longer\nto formalize a proof\nthan to write it by hand.\nWith these modern AI tools\nand also just better tooling,\nthe Lean developers are\ndoing a great job adding more\nand more features and\nmaking it user friendly.\nIt's going from nine to eight to seven.\nOkay, no big deal.\nBut one day it will drop below one,\nand that's the phase shift,\nbecause suddenly it makes\nsense when you write a paper\nto write it in Lean first\nor through a conversational\nAI who is generating Lean\non the fly with you,\nand it becomes natural\nfor journals to accept.\nMaybe they'll offer expedite refereeing.\nIf a paper has already\nbeen formalized in Lean,\nthey would just ask the referee to comment\non the significance of the results\nand how it connects to literature\nand not worry so much\nabout the correctness\nbecause that's been certified.\nPapers are getting longer\nand longer in mathematics\nand actually it's harder\nand harder to get good refereeing\nfor the really long ones unless\nthey're really important.\nIt is actually an issue which\nthe formalization is coming in\nat just the right time for this to be.\n- The easier and easier it\ngets because of the tooling\nand all the other factors,\nthen you're going to see much more,\nlike Mathlib will grow\npotentially exponentially.\nIt's a virtuous cycle, okay.\n- I mean one face shift\nof this type that happened\nin the past was the adoption of LaTeX.\nSo LaTeX is this typesetting language\nthat all mathematicians use now.\nSo in the past people used\nall kinds of word processors\nand typewriters and whatever,\nbut at some point LaTeX\nbecame easier to use\nthan all other competitors\nand people would switch\nwithin a few years.\nIt was just a dramatic phase shift.\n- It's a wild out-there\nquestion, but what year?\nHow far away are we\nfrom a AI system being a collaborator\non a proof that wins the Fields Medal,\nso that level?\n- Okay.\nWell it depends on the\nlevel of collaboration.\n- No, like it deserves\nto be to get the Fields Medal,\nso half and half.\n- Already, I could imagine\nif it was medal-winning paper\nhaving some AI assistance\nin writing it.\nThe autocomplete alone, I use it.\nIt speeds up my own writing.\nYou can have a theorem, you have a proof,\nand the proof has three cases,\nand I write down the\nproof of the first case,\nand the autocomplete just\nsuggests now here's how the proof\nof the second case could work.\nAnd it was exactly correct.\nThat was great.\nSaved me like 5, 10 minutes of typing.\n- But in that case the AI system\ndoesn't get the Fields Medal. (chuckles)\n- [Terence] No.\n- Are we talking 20 years,\n50 years, 100 years?\nWhat do you think?\n- Okay, so I gave a prediction in print.\nSo by 2026, which is now next year\nthere will be math collaborations with AI.\nSo not Fields Medal winning,\nbut actual research level papers.\n- Like published ideas that\nare in part generated by AI.\n- Maybe not the ideas,\nbut at least some of the\ncomputations, verifications.\n- Has that already happened?\n- That's already happened, yeah.\nThere are problems that were solved\nby a complicated process\nconversing with AI\nto propose things.\nAnd the human goes and tries it,\nand comes back, doesn't work.\nBut it might propose a different idea.\nIt's hard to disentangle exactly.\nThere are certainly math results\nwhich could only have been accomplished\nbecause there was a human\nmathematician and an AI involved,\nbut it's hard to sort\nof disentangle credit.\nI mean these tools,\nthey do not replicate\nall the skills needed\nto do mathematics, but they can replicate\nsort of some non-trivial\npercentage of them, 30,\n40% so they can fill in gaps.\nSo coding is a good example.\nIt's annoying for me to code in Python.", "mimetype": "text/plain", "start_char_idx": 115796, "end_char_idx": 119800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54429056-0727-4e3c-a058-a639da294d79": {"__data__": {"id_": "54429056-0727-4e3c-a058-a639da294d79", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9896f2fe-c2e2-49f2-a786-a3f2ceb641ff", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "286fbea625bbc6f1807d4861def679d203b36103d0ed4cafb204a87412388a68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ea3c278-0d99-44ec-ac77-cd372b863cca", "node_type": "1", "metadata": {}, "hash": "25dffe0ecebe5696cb6fc183a312ec1c9c1ccd6363e7486aa9927bcbea22c3ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Maybe not the ideas,\nbut at least some of the\ncomputations, verifications.\n- Has that already happened?\n- That's already happened, yeah.\nThere are problems that were solved\nby a complicated process\nconversing with AI\nto propose things.\nAnd the human goes and tries it,\nand comes back, doesn't work.\nBut it might propose a different idea.\nIt's hard to disentangle exactly.\nThere are certainly math results\nwhich could only have been accomplished\nbecause there was a human\nmathematician and an AI involved,\nbut it's hard to sort\nof disentangle credit.\nI mean these tools,\nthey do not replicate\nall the skills needed\nto do mathematics, but they can replicate\nsort of some non-trivial\npercentage of them, 30,\n40% so they can fill in gaps.\nSo coding is a good example.\nIt's annoying for me to code in Python.\nI'm not a native,\nI'm not a professional programmer,\nbut with AI the friction cost\nof doing it is much reduced,\nso it fills in that gap for me.\nAI is getting quite good\nat literature review.\nI mean there's still a problem\nwith hallucinating\nreferences that don't exist,\nbut this I think is a solvable problem.\nIf you train in the right way\nand so forth, and verify\nusing the Internet,\nyou should in a few years get the point\nwhere you have a lemma that you need\nand say, \"Has anyone\nproven this lemma before?\"\nAnd it will do basically a\nfancy web search AI assistant\nand say, \"Yeah, there are these six papers\nwhere something similar has happened.\"\nAnd I mean you can ask it right now\nand it will give you six papers\nof which maybe one is\nlegitimate and relevant,\none exists but is not relevant\nand four are hallucinated.\nIt has a non-zero success rate right now,\nbut there's so much garbage,\nthe signal to noise ratio is so poor\nthat it's most helpful\nwhen you already somewhat\nknow the literature\nand you just need to be\nprompted to be reminded\nof a paper that was really\nsubconsciously in your memory.\n- Versus helping you discover new\nyou were not even aware of\nbut is the correct citation.\n- Yeah, that's, yeah,\nthat it can sometimes do,\nbut when it does, it's\nburied in a list of options\nfor which the other.\n- They're bad.\n- [Terence] Yeah.\n- I mean being able\nto automatically generate\na related work section\nthat is correct, that's\nactually a beautiful thing\nthat might be another phase shift,\nbecause it assigns credit correctly.\n- [Terence] Yeah.\n- It breaks you out of\nthe silos of thought.\n- Yeah, no, there's a big\nhump to overcome right now.\nI mean it's like self-driving cars.\nThe safety margin has to be really high\nfor it to be feasible.\nSo yeah, so there's a last\nmile problem with a lot\nof AI applications\nthat they can develop tools\nthat work 20%, 80% of the time,\nbut it's still not good enough.\nAnd in fact even worse\nthan good in some ways.\n- I mean another way of asking\nthe Fields Medal question\nis what year do you think you'll wake up\nand be like real surprised\nyou read the headline the news\nof something happened that AI did,\nlike a real breakthrough, something.\nIt doesn't like Fields\nMedal, Riemann hypothesis,\nit could be like really\njust this AlphaZero moment\nwould go, that kind of thing.\n- Yeah.\nThis decade I can see\nit making a conjecture\nbetween two things that\npeople thought was unrelated.\n- Oh, interesting.\nGenerating a conjecture\nthat's a beautiful conjecture.\n- Yeah.\nAnd actually has a real\nchance of being correct\nand meaningful.\n- Because that's actually\nkind of doable, I suppose.\nBut where the data is.\nYeah, no, that would be truly amazing.\n- The current models struggle a lot.\nI mean, so a version of this is,\nthe physicists have a dream\nof getting the AIs to\ndiscover new laws of physics.\nThe dream is you just\nfeed it all this data,\nand here is a new pattern\nthat we didn't see before.\nBut it actually even\nstruggles, the current state\nof the art even struggles\nto discover old laws\nof physics from the data.\nOr if it does, there's a big\nconcern of contamination.", "mimetype": "text/plain", "start_char_idx": 118995, "end_char_idx": 122918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ea3c278-0d99-44ec-ac77-cd372b863cca": {"__data__": {"id_": "7ea3c278-0d99-44ec-ac77-cd372b863cca", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54429056-0727-4e3c-a058-a639da294d79", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0ed8a9866f84b66e959d53f7c4cf9a425071c02963faada0670b7675a232fb68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0e550d0-9ea1-4817-9865-d509e16ceece", "node_type": "1", "metadata": {}, "hash": "5c8ae56fdcb2e5e3b6f02728b900e7df295927327892c66595a7c295e5076076", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\nThis decade I can see\nit making a conjecture\nbetween two things that\npeople thought was unrelated.\n- Oh, interesting.\nGenerating a conjecture\nthat's a beautiful conjecture.\n- Yeah.\nAnd actually has a real\nchance of being correct\nand meaningful.\n- Because that's actually\nkind of doable, I suppose.\nBut where the data is.\nYeah, no, that would be truly amazing.\n- The current models struggle a lot.\nI mean, so a version of this is,\nthe physicists have a dream\nof getting the AIs to\ndiscover new laws of physics.\nThe dream is you just\nfeed it all this data,\nand here is a new pattern\nthat we didn't see before.\nBut it actually even\nstruggles, the current state\nof the art even struggles\nto discover old laws\nof physics from the data.\nOr if it does, there's a big\nconcern of contamination.\nThat it did it only because somewhere\nin this training did it\nsomehow knew Boyle's Law\nor whatever law that you're\ntrying to reconstruct.\nPart of it is that we\ndon't have the right type\nof training data for this.\nSo for laws of physics,\nwe don't have a million\ndifferent universes\nwith a million different laws of nature.\nA lot of what we're missing\nin math is actually the negative space of,\nso we have published things\nof things that people\nhave been able to prove\nand conjectures that\nended up being verified\nor maybe counterexamples produced.\nBut we don't have data on\nthings that were proposed\nand they're kind of a good thing to try,\nbut then people quickly realized\nthat it was the wrong conjecture\nand then they said, \"Oh,\nbut we should actually\nchange our claim to modify it\nin this way to actually\nmake it more plausible.\"\nThere's a trial\nand error process which\nis a real integral part\nof human mathematical discovery,\nwhich we don't record\nbecause it's embarrassing.\nWe make mistakes and we only\nlike to publish our wins.\nAnd the AI has no access\nto this data to train on.\nI sometimes joke that\nbasically AI has to go\nthrough grad school and\nactually go to grad courses,\ndo the assignments, go to\noffice hours, make mistakes,\nget advice on how to correct the mistakes\nand learn from that.\n- Let me ask you, if I may,\nabout Grigori Perelman.\nYou mentioned that you try\nto be careful in your work\nand not let a problem\ncompletely consume you.\nJust you've really fallen\nin love with the problem\nand really cannot rest until you solve it.\nBut you also hasted\nto add that sometimes\nthis approach actually\ncan be very successful.\nAn example you gave is Grigori Perelman,\nwho proved the Poincare conjecture\nand did so by working\nalone for seven years\nwith basically little contact\nwith the outside world.\nCan you explain this one\nMillennium Prize Problem\nthat's been solved,\nPoincare conjecture, and maybe speak\nto the journey that\nGrigori Perelman's been on?\n- All right, so it's a\nquestion about curved spaces.\nEarth is a good example.\nSo Earth you can think of as a 2D surface.\nAnd just moving around the Earth,\nit could maybe be a\ntorus with a hole in it,\nor it could have many holes.\nAnd there are many different\ntopologies a priori\nthat a surface could have,\neven if you assume that it's bounded\nand smooth, and so forth.\nSo we have figured out\nhow to classify surfaces.\nAs a first approximation,\neverything's determined\nby something called the\ngenus, how many holes it has.\nSo a sphere has genus zero,\na donut has genus one, and so forth.\nAnd one way you\ncan tell these surfaces apart\nproperty the sphere has,\nwhich is called simply connected.\nIf you take any closed loop on the sphere,\nlike a big closed sort of rope,\nyou can contract it to a point\nwhile staying on the surface.\nAnd the sphere has this\nproperty, but a torus doesn't.\nIf you're on a torus and\nyou take a rope that goes\naround say the outer diameter torus,\nthere's no way it can't\nget through the hole.\nThere is no way to contract it to a point.\nSo it turns out that the\nsphere is the only surface\nwith this property of contractibility up\nto like continuous\ndeformations of the sphere.", "mimetype": "text/plain", "start_char_idx": 122125, "end_char_idx": 126082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0e550d0-9ea1-4817-9865-d509e16ceece": {"__data__": {"id_": "e0e550d0-9ea1-4817-9865-d509e16ceece", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ea3c278-0d99-44ec-ac77-cd372b863cca", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4fd4c402bb8573d9b7898b7a17969b60b1a7b557ab394e1fc08fcf990f6e810a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330a8a26-65a9-4f35-869e-89610e3a1d2b", "node_type": "1", "metadata": {}, "hash": "d76a5685ee01b8ecea7c93f2aab54e842784ebcc0d3cba5658a818804078146c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a first approximation,\neverything's determined\nby something called the\ngenus, how many holes it has.\nSo a sphere has genus zero,\na donut has genus one, and so forth.\nAnd one way you\ncan tell these surfaces apart\nproperty the sphere has,\nwhich is called simply connected.\nIf you take any closed loop on the sphere,\nlike a big closed sort of rope,\nyou can contract it to a point\nwhile staying on the surface.\nAnd the sphere has this\nproperty, but a torus doesn't.\nIf you're on a torus and\nyou take a rope that goes\naround say the outer diameter torus,\nthere's no way it can't\nget through the hole.\nThere is no way to contract it to a point.\nSo it turns out that the\nsphere is the only surface\nwith this property of contractibility up\nto like continuous\ndeformations of the sphere.\nSo things that are what\nare called topologically\nequivalent of the sphere.\nSo Poincare asks the same\nquestion in higher dimensions.\nSo this it becomes hard to visualize,\nbecause surface you can think of\nas embedded in three dimensions.\nBut a curved free space,\nwe don't have good intuition\nof 4D space to live it.\nAnd there are also 3D space\nspaces that can't even fit\ninto four dimensions.\nYou need five or six or higher.\nBut anyway, mathematically you\ncan still pose this question\nthat if you have a bounded\nthree dimensional space now,\nwhich also has this\nsimply connective property\nthat every loop can be\ncontracted, can you turn it\ninto a three dimensional\nversion of a sphere?\nAnd so this is the point where conjecture,\nweirdly in higher dimensions,\n4 and 5 was actually easier,\nso it was solved first\nin higher dimensions.\nThere's somehow more room\nto do the deformation,\nit's easier to move\nthings around a sphere.\nBut three was really hard.\nSo people tried many approaches.\nThere's sort of commentary approaches\nwhere you chop up the\nsurface into little triangles\nor tetrahedron, and you just try to argue\nbased on how the faces\ninteract each other.\nThere were algebraic approaches.\nThere's various algebraic objects,\nlike things called the fundamental\ngroup that you can attach\nto these homology and cohomology\nand all these very fancy tools.\nThey also didn't quite work.\nBut Richard Hamilton proposed\na partial differential equations approach.\nSo the problem is that\nyou have this object\nwhich is sort of secretly the sphere,\nbut it's given to you in a weird way.\nSo think of a ball that's\nbeing kind of crumpled up\nand twisted, and it's not\nobvious that it's a ball,\nbut if you have some sort of surface,\nwhich is a deformed sphere, you could,\nfor example, think of it as\nthe surface of a balloon.\nYou could try to inflate it.\nYou blow it up and naturally,\nas you fill it with air,\nthe wrinkles will sort of smooth out\nand it will turn into\na nice, round sphere.\nUnless of course, it was\na torus or something,\nin which case it would\nget stuck at some point.\nLike if you inflate a\ntorus, there'll be a point\nin the middle when the\ninner ring shrinks to zero.\nYou get a singularity, and you\ncan't blow up a any further,\nyou can't flow any further.\nSo he created this flow,\nwhich is now called Ricci flow,\nwhich is a way of taking an\narbitrary surface or space\nand smoothing it out to\nmake it rounder and rounder,\nto make it look like a sphere.\nAnd he wanted to show\nthat either this process\nwould give you a sphere\nor it would create a singularity.\nVery much like how PDEs,\neither they have global\nregularity or finite time blowup.\nBasically, it's almost\nexactly the same thing.\nIt's all connected.\nAnd he showed that for two dimensions,\ntwo-dimensional surfaces,\nif you started something like\nno singularities ever form,\nyou never ran into trouble,\nand you could flow, and\nit will give you a sphere.\nSo he got a new proof of\nthe two dimensional result.\n- But by the way, that's\na beautiful explanation\nof Ricci flow and its\napplication in this context.\nHow difficult is the mathematics\nhere for the 2D case?\n- Yeah, these are quite\nsophisticated equations,\non par with the Einstein equations.", "mimetype": "text/plain", "start_char_idx": 125301, "end_char_idx": 129286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "330a8a26-65a9-4f35-869e-89610e3a1d2b": {"__data__": {"id_": "330a8a26-65a9-4f35-869e-89610e3a1d2b", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0e550d0-9ea1-4817-9865-d509e16ceece", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "cb81760d7bcc164f19eca3d0c9cc2d58ba1f5f61b4c2669612798069a03491bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f6eaee-c38e-4bac-a715-511265f91f25", "node_type": "1", "metadata": {}, "hash": "9f508843f79ecb627b4aa2c0748c07aaee0a3801e44c1646f715860d0c5542da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And he wanted to show\nthat either this process\nwould give you a sphere\nor it would create a singularity.\nVery much like how PDEs,\neither they have global\nregularity or finite time blowup.\nBasically, it's almost\nexactly the same thing.\nIt's all connected.\nAnd he showed that for two dimensions,\ntwo-dimensional surfaces,\nif you started something like\nno singularities ever form,\nyou never ran into trouble,\nand you could flow, and\nit will give you a sphere.\nSo he got a new proof of\nthe two dimensional result.\n- But by the way, that's\na beautiful explanation\nof Ricci flow and its\napplication in this context.\nHow difficult is the mathematics\nhere for the 2D case?\n- Yeah, these are quite\nsophisticated equations,\non par with the Einstein equations.\nSlightly simpler, but yeah,\nbut they were considered hard\nnonlinear equations to solve.\nAnd there's lots of special\ntricks in 2D that helped.\nBut in 3D, the problem was\nthat this equation was\nactually supercritical.\nThe same problems as\nNavier-Stokes, as you blow up,\nmaybe the curvature would\nget concentrated in finer,\nsmaller regions.\nAnd it looked more\nand more nonlinear and things\njust looked worse and worse.\nAnd there could be all\nkinds of singularities\nthat showed up.\nSome singularities,\nthere's these things called neck pinchers,\nwhere the surface sort\nof behaves like a barbell\nand it pinches at a point.\nSome singularities are\nsimple enough that you can\nsort of see what to do next.\nYou just make a snip\nand then you can turn one surface into two\nand evolve them separately.\nBut there was the\nprospect that there's some\nreally nasty knotted\nsingularities showed up\nthat you couldn't see\nhow to resolve in any way\nthat you couldn't do any surgery to.\nSo you need to classify\nall the singularities,\nlike what are all the possible ways\nthat things can go wrong?\nSo what Perelman did was, first of all,\nhe turned the problem\nfrom a supercritical problem\nto a critical problem.\nI said before about how\nthe invention of energy,\nthe Hamiltonian really\nclarified Newtonian mechanics.\nSo he introduced something\nwhich is now called\nPerelman's reduced volume\nand Perelman's entropy.\nAnd he introduced new\nquantities, kind of like energy,\nthat looked the same at every single scale\nand turned the problem into a critical one\nwhere the nonlinearities\nactually suddenly looked\na lot less scary than they did before.\nAnd then he had to solve.\nHe still had to analyze the singularities\nof this critical problem.\nAnd that itself was a problem similar\nto this wave maps thing\nI worked on, actually.\nSo on the level of difficulty of that.\nSo he managed to classify\nall the singularities\nof this problem\nand show how to apply\nsurgery to each of these.\nAnd through that was able\nto resolve the Poincare conjecture.\nSo quite like a lot of\nreally ambitious steps\nand nothing that a large\nlanguage model today,\nfor example, could.\nI mean, at best I could\nimagine proposing this idea\nas one of hundreds of\ndifferent things to try,\nbut the other 99 would\nbe complete dead ends,\nbut you'd only find out\nafter months of work.\nHe must have had some sense\nthat this was the right track\nto pursue, because it takes\nyears to get from A to B.\n- So you've done, like you\nsaid, actually, you see,\neven strictly mathematically,\nbut more broadly,\nin terms of the process,\nyou've done similarly difficult things.\nWhat can you infer from the\nprocess he was going through,\nbecause he was doing it alone.\nWhat are some low points\nin a process like that?\nWhen you start to, like\nyou've mentioned hardship,\nlike AI doesn't know when it's failing.\nWhat happens to you,\nyou're sitting in your office,\nwhen you realize the thing you\ndo did for the last few days,\nmaybe weeks, is a failure?\n- Well, for me, I switch\nto a different problem.\n(Lex laughing)\nSo as I said, I'm a\nfox, I'm not a hedgehog.\n- But you legitimately,\nthat is a break that you\ncan take is to step away\nand look at a different problem?\n- Yeah, you can modify the problem too.", "mimetype": "text/plain", "start_char_idx": 128537, "end_char_idx": 132500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16f6eaee-c38e-4bac-a715-511265f91f25": {"__data__": {"id_": "16f6eaee-c38e-4bac-a715-511265f91f25", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330a8a26-65a9-4f35-869e-89610e3a1d2b", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b883e3fc3823623951152a820c769563b3c6179bb4311fabb6b417c0598f0a2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1439b63d-8b30-4436-82fe-f476ae6d62c0", "node_type": "1", "metadata": {}, "hash": "5fba14f1cc415269087403fa37a50c8ca8c821681ce56083669f3b46ebd0415b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So you've done, like you\nsaid, actually, you see,\neven strictly mathematically,\nbut more broadly,\nin terms of the process,\nyou've done similarly difficult things.\nWhat can you infer from the\nprocess he was going through,\nbecause he was doing it alone.\nWhat are some low points\nin a process like that?\nWhen you start to, like\nyou've mentioned hardship,\nlike AI doesn't know when it's failing.\nWhat happens to you,\nyou're sitting in your office,\nwhen you realize the thing you\ndo did for the last few days,\nmaybe weeks, is a failure?\n- Well, for me, I switch\nto a different problem.\n(Lex laughing)\nSo as I said, I'm a\nfox, I'm not a hedgehog.\n- But you legitimately,\nthat is a break that you\ncan take is to step away\nand look at a different problem?\n- Yeah, you can modify the problem too.\nI mean, you can ask some cheat\nif there's a specific\nthing that's blocking you\nthat some bad case keeps showing up\nfor which your tool doesn't work.\nYou can just assume\nby fiat this bad case doesn't occur.\nSo you do some magical thinking,\nbut strategically, okay,\nto see if the rest of the\nargument goes through.\nIf there's multiple\nproblems with your approach,\nthen maybe you just give up.\nBut if this is the only problem,\nthen everything else checks out,\nthen it's still worth fighting.\nSo yeah, you have\nto do some forward\nreconnaissance sometimes.\n(Lex laughs)\n- And that is sometimes productive.\nTo assume like, okay, we'll\nfigure it out eventually.\n- Oh, yeah.\nSometimes actually it's even\nproductive to make mistakes.\nSo one of the, I mean there was a project\nwhich actually we won some prizes for,\nwith four other people.\nWe worked on this PDE problem again,\nactually this blowup\nregularity type problem.\nAnd it was considered very hard.\nJean Bourgain, who was\nanother Fields Medalist\nwho worked on a special case of this,\nbut he could not solve the general case.\nAnd we worked on this\nproblem for two months\nand we thought we solved it.\nWe had this cute argument\nthat if everything fit\nand we were excited,\nwe were planning celebration\nto all get together\nand have champagne or something,\nand we started writing it up,\nand one of us, not me actually,\nbut another co-author said,\n\"Oh, in this lemma here,\nwe have to estimate these\n13 terms that show up\nin this expansion.\nAnd we estimated 12 of them.\nBut in our notes I can't find\nthe estimation of the 13th.\nCan someone apply that?\"\nAnd I said, \"Sure, I'll look at this.\"\nAnd actually yeah, we didn't cover that.\nWe completely omitted this term.\nAnd this term turned out to be worse\nthan the other 12 terms put together.\nIn fact, we could not estimate this term.\nAnd we tried for a few more months\nand all different permutations\nand there was always this one thing,\none term that we could not control.\nAnd so this was very frustrating.\nBut because we had already invested months\nand months of effort in this already,\nwe stuck at this.\nWe tried increasingly desperate\nthings and crazy things.\nAnd after two years,\nwe found that approach was\nactually somewhat different quite\na bit from our initial strategy\nwhich didn't generate\nthese problematic terms\nand actually solved the problem.\nSo we solved the problem after two years.\nBut if we hadn't had\nthat initial first dawn\nof nearly solving the problem,\nwe would have given up\nby month two or something\nand worked on an easier problem.\nIf we had known it would take two years,\nnot sure we would have\nstarted the project.\nSometimes actually having the incorrect,\nit's like Columbus\ntraveling in the New World.\nIt's a incorrect version\nof measurement of the size of the Earth.\nHe thought he was going to find\na new trade route to India,\nor at least that was how he\nsold it in his prospectus.\nI mean, it could be that\nhe actually secretly knew.\n- Just on the psychological element,\ndo you have emotional\nor self-doubt that just\noverwhelms you most like that?\nBecause this stuff,\nit feels like math is so\nengrossing that it can break you.\nWhen you invest so much\nyourself in the problem\nand then it turns out wrong,\nyou could start to,\nsimilar way chess has broken some people.", "mimetype": "text/plain", "start_char_idx": 131711, "end_char_idx": 135782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1439b63d-8b30-4436-82fe-f476ae6d62c0": {"__data__": {"id_": "1439b63d-8b30-4436-82fe-f476ae6d62c0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16f6eaee-c38e-4bac-a715-511265f91f25", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "61ab3b3a011e249d31431ecc50bc4559a095234452c94f65478e5f98ce7a7252", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98650303-7587-447e-8e8d-e6dac19ef856", "node_type": "1", "metadata": {}, "hash": "442ef31b27e4c1230dc455a2266223ce5687a8e93ffe9eff757517ce9c35f57f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If we had known it would take two years,\nnot sure we would have\nstarted the project.\nSometimes actually having the incorrect,\nit's like Columbus\ntraveling in the New World.\nIt's a incorrect version\nof measurement of the size of the Earth.\nHe thought he was going to find\na new trade route to India,\nor at least that was how he\nsold it in his prospectus.\nI mean, it could be that\nhe actually secretly knew.\n- Just on the psychological element,\ndo you have emotional\nor self-doubt that just\noverwhelms you most like that?\nBecause this stuff,\nit feels like math is so\nengrossing that it can break you.\nWhen you invest so much\nyourself in the problem\nand then it turns out wrong,\nyou could start to,\nsimilar way chess has broken some people.\n- Yeah, I think different\nmathematicians have different\nlevels of emotional\ninvestment in what they do.\nI mean, I think for some\npeople it's just a job.\nYou have a problem and\nif it doesn't work out,\nyou go on the next one.\nYeah, so the fact that\nyou can always move on\nto another problem,\nit reduces the emotional connection.\nI mean there are cases.\nSo there are certain problems\nthat are what are called\nmathematical diseases\nwhere just latch onto that one problem\nand they spend years\nand years thinking about\nnothing but that one problem\nand maybe their career\nsuffers and so forth.\nThey say, \"Okay, but this big win,\nonce I finish this problem,\nthat will make up for all the\nyears of lost opportunity.\"\nOccasionally it works, but\nI really don't recommend it\nfor people without the right fortitude.\nYeah, so I've never been super\ninvested in any one problem.\nOne thing that helps is that we don't need\nto call our problems in advance.\nWell, when we do grant proposals,\nwe say we will study this set of problems,\nbut even though we\ndon't promise definitely\nby five years I will supply\na proof of all these things.\nYou promise to make some progress\nor discover some interesting phenomena\nand maybe you don't solve the problem,\nbut you find some related problem\nthat you can say something new about,\nand that's a much more feasible task.\n- But I'm sure for you\nthere's problems like this.\nYou have made so much progress\ntowards the hardest problems\nin the history of mathematics.\nSo is there a problem\nthat just haunts you,\nit sits there in the dark corners?\nTwin prime conjecture,\nRiemann hypothesis, Goldbach conjecture?\n- Twin prime, that sounds,\n(Lex laughing)\nagain, so, I mean, the problem\nis like the Riemann hypothesis,\nthose are so far out of reach.\n- [Lex] You think so?\n- Yeah.\nThere's no even viable strategy.\nLike, even if I activate all\nthe cheats that I know of\nin this problem, like,\nthere's just still no\nway to get from A to B.\nI think it needs a breakthrough\nin another area of\nmathematics to happen first\nand for someone\nto recognize that it would be\na useful thing to transport\ninto this problem.\n- So we should maybe step\nback for a little bit\nand just talk about prime numbers.\nSo they're often referred to\nas the atoms of mathematics.\nCan you just speak\nto the structure that these atoms provide?\n- So the natural numbers have\ntwo basic operations attached\nto them, addition and multiplication.\nSo if you want to generate\nthe natural numbers,\nyou can do one of two things.\nYou can just start with one\nand add one to itself over and over again,\nand that generates you\nthe natural numbers.\nSo additively, they're\nvery easy to generate.\n1, 2, 3, 4, 5.\nOr you can take the\nprime number if you want\nto generate multiplicatively,\nyou can take all the\nprime numbers, 2, 3, 5, 7,\nand multiply them all together.\nAnd together, that gives\nyou all the natural numbers,\nexcept maybe for one.\nSo there are these two separate ways\nof thinking about the natural numbers\nfrom an additive point of view\nand a multiplicative point of view.\nAnd separately, they're not so bad.\nSo any question that\nonly involves addition\nis relatively easy to solve.\nAnd any question that only\ninvolves multiplication\nis relatively easy to solve.\nBut what has been frustrating\nis that you combine the two together\nand suddenly you get this extremely rich.", "mimetype": "text/plain", "start_char_idx": 135045, "end_char_idx": 139127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98650303-7587-447e-8e8d-e6dac19ef856": {"__data__": {"id_": "98650303-7587-447e-8e8d-e6dac19ef856", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1439b63d-8b30-4436-82fe-f476ae6d62c0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c1bb8374ec83e0808155f580206717497c3b1c57a29520a95f8eaed55b1bfa5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dae400d-c2bd-4497-b007-beac2ea39abd", "node_type": "1", "metadata": {}, "hash": "30b6dacff66b3537937b551f364a86781e69d90610bb4b4ac8d9df193c60309c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So additively, they're\nvery easy to generate.\n1, 2, 3, 4, 5.\nOr you can take the\nprime number if you want\nto generate multiplicatively,\nyou can take all the\nprime numbers, 2, 3, 5, 7,\nand multiply them all together.\nAnd together, that gives\nyou all the natural numbers,\nexcept maybe for one.\nSo there are these two separate ways\nof thinking about the natural numbers\nfrom an additive point of view\nand a multiplicative point of view.\nAnd separately, they're not so bad.\nSo any question that\nonly involves addition\nis relatively easy to solve.\nAnd any question that only\ninvolves multiplication\nis relatively easy to solve.\nBut what has been frustrating\nis that you combine the two together\nand suddenly you get this extremely rich.\nI mean, we know that there are statements\nin number theory that are\nactually as undecidable.\nThere are certain polynomials\nin some number of variables.\nIs there a solution in\nthe natural numbers?\nAnd the answer depends on\nand undecidable statement\nlike whether the axioms\nof mathematics are consistent or not.\nBut even the simplest problems\nthat combine something\nmultiplicative, such as the primes,\nwith something additive,\nsuch as shifting by two,\nseparately, we understand\nboth of them well.\nBut if you ask, when you\nshift the prime by two,\nhow often can you get another prime?\nIt's been amazingly\nhard to relate the two.\n- And we should say\nthat the twin-prime\nconjecture is just that.\nIt posits that there are\ninfinitely many pairs\nof prime numbers that differ by two.\nNow, the interesting thing\nis that you have been very successful\nat pushing forward the field\nin answering these complicated\nquestions of this variety.\nLike you mentioned, the Green-Tao theorem,\nit proves that prime\nnumbers contain arithmetic\nprogressions of any length.\n- [Terence] Right.\n- [Lex] Which is mind-blowing\nthat you could prove something like that.\n- Right, yeah.\nSo what we've realized\nbecause of this type\nof research is that different\npatterns have different levels\nof indestructibility.\nSo what makes the twin-prime\nproblem hard is that\nif you take all the primes\nin the world, 3, 5, 7, 11,\nso forth, there are some twins in there.\n11 and 13 is a twin prime,\npair of twin primes and so forth.\nBut you could easily, if you wanted to,\nredact the primes to\nget rid of these twins.\nThe twins, they show up\nand there are infinitely many of them,\nbut they're actually reasonably sparse.\nInitially there's quite a few,\nbut once you got to the\nmillions, the trillions,\nthey become rarer and rarer.\nAnd you could actually just.\nIf someone was given access\nto the database of primes,\nyou just edit out a few\nprimes here and there.\nThey could make the\ntwin-prime conjecture false\nby just removing like 0.01%\nof the primes or something.\nJust well chosen to do this.\nAnd so you could present\na censored database\nof the primes which passes all\nof the statistical tests of the primes.\nIt obeys things like\nthe prime number theorem\nand other facts about the primes,\nbut doesn't contain any\ntwin primes anymore.\nAnd this is a real obstacle\nfor the twin-prime conjecture.\nIt means that any proof structure strategy\nto actually find twin prime\nin the actual primes\nmust fail when applied\nto these slightly edited primes.\nAnd so it must be some very subtle,\ndelicate feature of the\nprimes that you can't just get\nfrom aggregate statistical analysis.\n- Okay, so that's out. (laughs)\n- Yeah, on the other hand,\narithmetic progressions has turned out\nto be much more robust.\nYou can take the primes\nand you can eliminate 99% of\nthe prime primes actually,\nand you can take any 90\nparticipants you want.\nAnd it turns out, and\nanother thing we proved\nis that you still get\nasthmatic progressions.\nAsthmatic progressions are much,\nthey're like cockroaches.\n- Of arbitrary length.\n- Yes, yes.\n- That's crazy.\nFor people who don't know,\narithmetic progressions is a\nsequence of numbers that differ\nby some fixed amount.\n- Yeah, but it's again like,\nit's an infinite monkey type phenomenon.", "mimetype": "text/plain", "start_char_idx": 138396, "end_char_idx": 142379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2dae400d-c2bd-4497-b007-beac2ea39abd": {"__data__": {"id_": "2dae400d-c2bd-4497-b007-beac2ea39abd", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98650303-7587-447e-8e8d-e6dac19ef856", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "7c4c6d3ac4594112f5d2b0cf6acbd6f046456f329b09538ab17092aa3cf69e47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d4f97c-2ec9-4014-af81-5f265a4492bb", "node_type": "1", "metadata": {}, "hash": "cdfd15d635b385f4326d79fa5af3ad2dc9d2d99c3bb66ba5ccd9f61cce05d163", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so it must be some very subtle,\ndelicate feature of the\nprimes that you can't just get\nfrom aggregate statistical analysis.\n- Okay, so that's out. (laughs)\n- Yeah, on the other hand,\narithmetic progressions has turned out\nto be much more robust.\nYou can take the primes\nand you can eliminate 99% of\nthe prime primes actually,\nand you can take any 90\nparticipants you want.\nAnd it turns out, and\nanother thing we proved\nis that you still get\nasthmatic progressions.\nAsthmatic progressions are much,\nthey're like cockroaches.\n- Of arbitrary length.\n- Yes, yes.\n- That's crazy.\nFor people who don't know,\narithmetic progressions is a\nsequence of numbers that differ\nby some fixed amount.\n- Yeah, but it's again like,\nit's an infinite monkey type phenomenon.\nFor any fixed length of your set,\nyou don't get arbitrary\nlength progressions,\nyou only get quite short progressions.\n- But you're saying twin prime\nis not an infinite monkey phenomena.\nI mean, it's a very subtle monkey.\nIt's still an infinite monkey phenomenon.\n- Right, yeah, if the primes\nwere really genuinely random,\nif the primes were generated\nby monkeys, then yes,\nin fact, the infinite\nmonkey theorem would.\n- Oh, but you're saying that\ntwin prime is, it doesn't.\nYou can't use the same tools.\nIt doesn't appear random almost.\n- Well, we don't know.\nYeah, we believe the primes\nbehave like a random set.\nSo the reason why we care\nabout the twin-prime\nconjecture is it's a test case\nfor whether we can genuinely, confidently,\nsay with 0% chance of error,\nthat the primes behave like a random set.\nRandom versions of the\nprimes we know contain twins,\nat least with 100% probability,\nor probably tending to 100%\nas you go out further and further.\nYeah.\nSo the primes, we believe\nthat they're random.\nThe reason why arithmetic\nprogressions are indestructible\nis that regardless of\nwhether it looks random\nor looks structured like\nperiodic, in both cases,\narithmetic regressions appear,\nbut for different reasons.\nAnd this is basically all the\nways in which the theorem,\nthere are many proofs of these\nsort of arithmetic progression theorems,\nand they're all proven\nby some sort of dichotomy\nwhere your set is either\nstructured or random,\nand in both cases, you\ncan say that something,\nand then you put the two together.\nBut in twin-primes, if\nthe primes are random,\nthen you're happy, you win.\nBut if the primes are structured,\nthey could be structured\nin a specific way that\neliminates the twins.\nAnd we can't rule out that one conspiracy.\n- And yet you were able\nto make, as I understand,\nprogress on the k-tuple version.\n- Right, yeah.\nSo the one funny thing about conspiracies\nis that any one conspiracy theory\nis really hard to disprove.\nThat if you believe the world\nis run by lizards, you say,\n\"Here's some evidence that\nit's not run by lizards.\"\nWell, but that evidence\nwas planted by the lizards.\nYou may have encountered\nthis kind of phenomenon.\nThere's almost no way to\ndefinitively rule out a conspiracy.\nAnd the same is true in mathematics,\nthat a conspiracy is solely devoted\nto eliminating twin primes.\nYou would have to also infiltrate\nother areas of mathematics.\nBut it could be made consistent,\nat least as far as we know.\nBut there's a weird phenomenon\nthat you can make one conspiracy\nrule out other conspiracies.\nSo if the world is run by lizards,\nit can't also be run by aliens.\n- (chuckles) Right.\n- So one unreasonable\nthing is hard to disprove.\nBut more than one, there are tools.\nSo, for example,\nwe know there's infinitely\nmany primes that are,\nno two of which are,\nso there are infinitely pairs\nof primes which differ by\nat most 246 actually is the current.\n- So there's like a bound.\n- Yes, so there's twin primes.\nThere's things called cousin\nprimes that differ by four.\nThere's called sexy\nprimes that differ by six.\n- What are sexy primes?\n- Primes that differ by six.\nThe name is much less,\nthe concept is much less\nexciting than the name suggests.\n- Got it.", "mimetype": "text/plain", "start_char_idx": 141621, "end_char_idx": 145576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3d4f97c-2ec9-4014-af81-5f265a4492bb": {"__data__": {"id_": "a3d4f97c-2ec9-4014-af81-5f265a4492bb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dae400d-c2bd-4497-b007-beac2ea39abd", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4d8c6256b17c2bc6164c8d5e69dce78cccb3a06cb18953dd6d4f22c7f0d21bde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56faf331-26a2-4a00-a188-57dc820d0c1d", "node_type": "1", "metadata": {}, "hash": "e6cd58eadffb0575627855f9711df39b31ede92e3ed66fe5f40cb206f7a7a414", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But there's a weird phenomenon\nthat you can make one conspiracy\nrule out other conspiracies.\nSo if the world is run by lizards,\nit can't also be run by aliens.\n- (chuckles) Right.\n- So one unreasonable\nthing is hard to disprove.\nBut more than one, there are tools.\nSo, for example,\nwe know there's infinitely\nmany primes that are,\nno two of which are,\nso there are infinitely pairs\nof primes which differ by\nat most 246 actually is the current.\n- So there's like a bound.\n- Yes, so there's twin primes.\nThere's things called cousin\nprimes that differ by four.\nThere's called sexy\nprimes that differ by six.\n- What are sexy primes?\n- Primes that differ by six.\nThe name is much less,\nthe concept is much less\nexciting than the name suggests.\n- Got it.\n- So you can make a conspiracy\nrule out one of them these,\nbut once you have 50 of them,\nit turns out that you can't\nrule out all of them at once.\nIt requires too much energy somehow\nin this conspiracy space.\n- How do you do the bound part?\nHow do you develop a bound\nfor the difference between the primes\nthat there's an infinite number of?\n- So it's ultimately\nbased on what's called\nthe pigeonhole principle.\nSo the pigeonhole principle\nis the statement that\nif you have a number of pigeons\nand they all have to go into pigeonholes\nand you have more\npigeons than pigeonholes,\nthen one of the pigeonholes has to have\nat least two pigeons there.\nSo there has to be two pigeons\nthat are close together.\nSo, for instance, if you have 100 numbers\nand they all range from one to a thousand,\ntwo of them have to be at most 10 apart,\nbecause you can divide up the numbers\nfrom 1 to 100 into 100 pigeon holes.\nLet's say if you have\n101 numbers, 101 numbers,\nthen two of them have to be\ndistance less than 10 part\nbecause two of them have to\nbelong to the same pigeonhole.\nSo it's a basic feature of a\nbasic principle in mathematics.\nSo it doesn't quite work\nwith the primes directly\nbecause the primes get sparser\nand sparser as you go out,\nthat fewer and fewer numbers are prime.\nBut it turns out that there's a way\nto assign weights to numbers.\nSo there are numbers that\nare kind of almost prime,\nbut they don't have no factors\nat all other than themselves in one,\nbut they have very few factors.\nAnd it turns out\nthat we understand almost\nprimes a lot better\nthan understand primes.\nAnd so, for example, it\nwas known for a long time\nthat there were twin almost-primes.\nThis has been worked out.\nSo almost primes are something\nwe kind of understand.\nSo you can actually restrict\nattention to a suitable set\nof almost primes.\nAnd whereas the primes are\nvery sparse overall relative\nto the almost primes, they\nactually are much less sparse.\nYou can set up a set of almost primes\nwhere the primes have\ndensity like say one percent.\nAnd that gives you a shot\nat proving by applying some\nsort of original principle\nthat there's pairs\nof primes that are just only 100 about.\nBut in order to prove the\ntwin-prime conjecture,\nyou need to get the density of primes\ninside the almost primes\nup to a threshold of 50%.\nOnce you get up to 50%,\nyou will get twin primes.\nBut unfortunately there are barriers.\nWe know that no matter what\nkind of good set of\nalmost primes you pick,\nthe density primes can\nnever get above 50%.\nIt's called the parity barrier.\nAnd I would love to find.\nYeah, so one of my long-term dreams\nis to find a way to breach that barrier,\nbecause it would open up not\nonly to twin-prime conjecture,\nbut the Goldbach conjecture\nand many other problems\nin number theory are currently blocked\nbecause our current\ntechniques would require going\nbeyond this theoretical parity barrier.\nIt's like going faster\nthan the speed of light.\n- Yeah.\nSo we should say a twin-prime conjecture,\none of the biggest problems\nin the history of mathematics.\nGoldbach conjecture also.\nThey feel like next door neighbors.\nHas there been days when\nyou felt you saw the path?\n- Oh yeah.\nSometimes you try something\nand it works super well.", "mimetype": "text/plain", "start_char_idx": 144826, "end_char_idx": 148795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56faf331-26a2-4a00-a188-57dc820d0c1d": {"__data__": {"id_": "56faf331-26a2-4a00-a188-57dc820d0c1d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d4f97c-2ec9-4014-af81-5f265a4492bb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "24abf4f7833351bf284f8ff91044c71859ed9e487ed858805c8c295604e318ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14f00316-9ae6-4896-b48a-062e8aa3dc97", "node_type": "1", "metadata": {}, "hash": "568508174c2d25c4782faad7ff890e7e45f29b0e5dd68dc38a61b2c730e4dd0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But unfortunately there are barriers.\nWe know that no matter what\nkind of good set of\nalmost primes you pick,\nthe density primes can\nnever get above 50%.\nIt's called the parity barrier.\nAnd I would love to find.\nYeah, so one of my long-term dreams\nis to find a way to breach that barrier,\nbecause it would open up not\nonly to twin-prime conjecture,\nbut the Goldbach conjecture\nand many other problems\nin number theory are currently blocked\nbecause our current\ntechniques would require going\nbeyond this theoretical parity barrier.\nIt's like going faster\nthan the speed of light.\n- Yeah.\nSo we should say a twin-prime conjecture,\none of the biggest problems\nin the history of mathematics.\nGoldbach conjecture also.\nThey feel like next door neighbors.\nHas there been days when\nyou felt you saw the path?\n- Oh yeah.\nSometimes you try something\nand it works super well.\nYou again, the sense\nof mathematical smell\nwe talked about earlier.\nYou learn from experience\nwhen things are going too well\nbecause there are certain\ndifficulties that you\nsort of have to encounter.\nI think the way a colleague\nof mine put it is that if you\nare on the streets of New York\nand you put on a blindfold,\nand you're put in a car,\nand after some hours the blindfold is off\nand you're in Beijing.\nThat was too easy.\nSomehow there was no ocean being crossed.\nEven if you don't know\nexactly what was done,\nyou're suspecting that\nsomething wasn't right.\n- But is that still\nin the back of your head\nto do you return to these.\nDo you return to the\nprime numbers every once\nin a while to see?\n- Yeah, when I have nothing better to do,\nwhich is less and less now.\nI get busy with so many things these days.\nBut yeah, when I have free time,\nand I'm too frustrated to work on my\nsort of real research projects,\nand I also don't want to\ndo my administrative stuff,\nor I don't want to do\nsome errand for my family.\nI can play these things for fun.\nAnd usually you get nowhere.\nYeah, you have to learn to just say,\n\"Okay, fine, once again,\nnothing happened.\nI will move on.\"\nVery occasionally, one of these\nproblems I actually solved,\nor sometimes as you say,\nyou think you solved it,\nand then you forward for maybe 15 minutes,\nand then you think I should check this,\nbecause this is too easy,\ntoo good to be true.\nAnd it usually is.\n- What's your gut say\nabout when these problems would be solved,\ntwin-prime and Goldbach?\n- Prime I think we will keep\ngetting more partial results.\nIt does need at least one.\nThis parity barrier is the\nbiggest remaining obstacle.\nThere are simpler versions\nof the conjecture where we\nare getting really close.\nSo I think we will,\nin 10 years we will have many\nmore much closer results.\nWe may not have the whole thing,\nso twin-primes is somewhat close.\nRiemann hypothesis, I have no clue.\nI mean, it has happened\nby accident, I think.\n- So the Riemann hypothesis is\na kind of more general conjecture\nabout the distribution of prime numbers.\n- Right, yeah.\nIt states that are sort\nof viewed multiplicatively\nfor questions only involving\nmultiplication, no addition,\nthe primes really do behave\nas randomly as you could hope.\nSo there's a phenomenon\nin probability called square\nroot cancellation that\nif you want to poll, say\nAmerica upon some issue\nand you ask one of the two voters,\nyou may have sampled a bad sample\nand then you get a really\nimprecise measurement\nof the full average.\nBut if you sample more and more people,\nthe accuracy gets better and better,\nand it actually improves\nlike the square root\nof the number of people you sampled.\nSo yeah, if you sample 1,000 people,\nyou can get like a 2\nor 3% margin of error.\nSo in the same sense,\nif you measure the primes in a\ncertain multiplicative sense,\nthere's a certain type of\nstatistic you can measure.\nIt's called the Riemann zeta function,\nand it fluctuates up and down.\nBut in some sense, as you\nkeep averaging more and more,\nif you sample more and more,\nthe fluctuation should go\ndown as if they were random.\nAnd there's a very precise\nway to quantify that.", "mimetype": "text/plain", "start_char_idx": 147930, "end_char_idx": 151952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14f00316-9ae6-4896-b48a-062e8aa3dc97": {"__data__": {"id_": "14f00316-9ae6-4896-b48a-062e8aa3dc97", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56faf331-26a2-4a00-a188-57dc820d0c1d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "405c6a3991efe2e95e146afd0ce9fc660b086e499d7b0c828befd3c0b0f53482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3730a016-7e1f-48bd-bddc-9d711be904b0", "node_type": "1", "metadata": {}, "hash": "8ab980506d0414478442eb065b5aa425cc7239c5b0b3e84dfec37a19efd65605", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But if you sample more and more people,\nthe accuracy gets better and better,\nand it actually improves\nlike the square root\nof the number of people you sampled.\nSo yeah, if you sample 1,000 people,\nyou can get like a 2\nor 3% margin of error.\nSo in the same sense,\nif you measure the primes in a\ncertain multiplicative sense,\nthere's a certain type of\nstatistic you can measure.\nIt's called the Riemann zeta function,\nand it fluctuates up and down.\nBut in some sense, as you\nkeep averaging more and more,\nif you sample more and more,\nthe fluctuation should go\ndown as if they were random.\nAnd there's a very precise\nway to quantify that.\nAnd the Riemann hypothesis\nis a very elegant way\nthat captures this.\nBut as with many other\nways in mathematics,\nwe have very few tools\nto show that something\nreally genuinely behaves really random.\nAnd this is actually not\njust a little bit random,\nbut it's asking that it behaves as random\nas an actually random set,\nthis square root cancellation.\nAnd we know because of things\nrelated to the parity problem,\nactually that most\nof the usual techniques cannot\nhope to settle this question.\nThe proof has to come out of left field.\nYeah, but what that is,\nno one has any serious proposal.\nAnd there's various ways\nto sort of, as I said,\nyou can modify the primes a little bit\nand you can destroy\nthe Riemann hypothesis.\nSo it has to be very delicate.\nYou can't apply something that\nhas huge margins of error.\nIt has to be would just barely work.\nAnd there's all these\npitfalls that you have\nto dodge very adeptly to it.\n- The prime numbers are just fascinating.\n- [Terence] Yeah.\n- What to you is most mysterious\nabout the prime numbers?\n- That's a good question.\nSo conjecturally we have\na good model of them.\nAs I said, they have certain\npatterns like the primes\nare usually odd for instance.\nBut apart from these obvious patterns,\nthey behave very randomly.\nAnd just assuming that they behave.\nSo there's something\ncalled the Cramer random\nmodel of the primes,\nthat after a certain point\nprimes just behave like a random set.\nAnd there's various slight\nmodifications to this model,\nbut this has been a very good model.\nIt matches the numerics.\nIt tells us what to predict.\nLike I can tell you\nwith complete certainty the\ntwin-prime conjecture is true.\nThe random model gives\noverwhelming odds that it's true.\nI just can't prove it.\nMost of our mathematics is optimized\nfor solving things with patterns in them.\nAnd the primes have this anti-pattern,\nas do almost everything really.\nBut we can't prove that.\nI guess it's not\nmysterious that the primes,\nit's kind of random,\nbecause there's no reason\nfor them to have any\nkind of secret pattern.\nBut what is mysterious\nis what is the mechanism\nthat really forces the\nrandomness to happen.\nAnd this is just absent.\n- Another incredibly\nsurprisingly difficult problem\nis the Collatz conjecture.\n- Oh, yes.\n- Simple to state,\nbeautiful to visualize in its simplicity,\nand yet extremely difficult to solve.\nAnd yet you have been\nable to make progress.\nPaul Erdos said about\nthe Collatz conjecture\nthat mathematics may not\nbe ready for such problems.\nOthers have stated\nthat it is an extraordinarily\ndifficult problem,\ncompletely out of reach.\nThis is in 2010, out of reach\nof present-day mathematics.\nAnd yet you have made some progress.\nWhy is it so difficult to make?\nCan you actually even explain what it is\nif it's easy to?\n- Yeah, so it's a problem\nthat you can explain.\nIt helps with some visual aids, but yeah.\nSo you take any natural\nnumber, like say 13,\nand you apply the\nfollowing procedure to it.\nSo if it's even you divide it by two,\nand if it's odd,\nyou multiply it by three and add one.\nSo even numbers get smaller,\nodd numbers get bigger.\nSo 13 would become 40,\nbecause 13 times three is 39.\nAdd one, you get 40.\nSo it's a simple process for\nodd numbers and even numbers,\nthey're both very easy operations.\nAnd then you put them together,\nit's still reasonably simple.\nBut then you ask what\nhappens when you iterate it?\nYou take the output that you\njust got and feed it back in.", "mimetype": "text/plain", "start_char_idx": 151317, "end_char_idx": 155393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3730a016-7e1f-48bd-bddc-9d711be904b0": {"__data__": {"id_": "3730a016-7e1f-48bd-bddc-9d711be904b0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14f00316-9ae6-4896-b48a-062e8aa3dc97", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "10f3b0d83ba4e55e23008186fc0b2f2d8b6dd50b8ac41e0fff0a5dc605911a97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af36f640-af93-4630-a6e6-107db2de54cb", "node_type": "1", "metadata": {}, "hash": "b28d68b4ac194e47e2b6fb5b8da65741345210bdf053dda8f9cb743337958fd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Why is it so difficult to make?\nCan you actually even explain what it is\nif it's easy to?\n- Yeah, so it's a problem\nthat you can explain.\nIt helps with some visual aids, but yeah.\nSo you take any natural\nnumber, like say 13,\nand you apply the\nfollowing procedure to it.\nSo if it's even you divide it by two,\nand if it's odd,\nyou multiply it by three and add one.\nSo even numbers get smaller,\nodd numbers get bigger.\nSo 13 would become 40,\nbecause 13 times three is 39.\nAdd one, you get 40.\nSo it's a simple process for\nodd numbers and even numbers,\nthey're both very easy operations.\nAnd then you put them together,\nit's still reasonably simple.\nBut then you ask what\nhappens when you iterate it?\nYou take the output that you\njust got and feed it back in.\nSo 13 becomes 40.\n40 is now even, divided by 2, 20.\n20 still even, divided by 2, 10, 5.\nAnd then 5 times 3 plus 1 is 16,\nand then 8, 4, 2, 1.\nAnd then from 1 it goes 1, 4, 2, 1, 4, 2.\nIt cycles forever.\nSo this sequence I just\ndescribed, 13, 40, 20, 10.\nSo these are also called\nhailstone sequences,\nbecause there's an oversimplified model\nof hailstone formation which\nis not actually quite correct,\nbut it's still somehow taught\nto high school students,\nas the first approximation,\nis that a little nugget\nof ice gets, an ice\ncrystal forms in a cloud,\nand it goes up and down\nbecause of the wind.\nAnd sometimes when it's cold,\nit acquires a bit more mass,\nand maybe it melts a little bit.\nAnd this process of going up\nand down creates this\npartially melted ice,\nwhich eventually creates this hailstone,\nand eventually it falls out the earth.\nSo the conjecture is that no\nmatter how high you start up,\nyou take a number which is\nin the millions or billions,\nthis process that goes up if you're odd,\nand down if you're even,\nit eventually goes down\nto earth all the time.\n- No matter where you start,\nwith this very simple algorithm,\nyou end up at one.\n- [Terence] Right.\n- And you might climb for a while.\n- [Terence] Right, yeah.\n- [Lex] Up and down.\n- Yeah, if you plot it, these sequences,\nthey look like Brownian motion,\nthey look like the stock market.\nThey just go up and down in\na seemingly random pattern.\nAnd in fact, usually that's what happens,\nthat if you plug in a random number,\nyou can actually prove,\nat least initially, that it\nwould look like random walk.\nAnd that's actually a random\nwalk with a downward drift.\nIt's like if you're always\ngambling on a roulette\nat the casino with odds\nslightly weighted against you.\nSo sometimes you win, sometimes you lose,\nbut over in the long run,\nyou lose a bit more than you win.\nAnd so normally your wallet will go\nto zero if you just keep\nplaying over and over again.\n- So statistically it makes sense.\n- Yes.\nSo the result that I\nproved, roughly speaking,\nasserts that statistically,\nlike 99% of all inputs would drift down\nto maybe not all the way\nto one, but to be much,\nmuch smaller than what you started.\nSo it's like if I told you\nthat if you go to a casino,\nmost of the time you end up,\nif you keep playing for long enough,\nyou end up with a smaller amount\nin your wallet than when you started.\nThat's kind of like the\nresult that I proved.\n- So why is that result\nlike can you continue down\nthat thread to prove the full conjecture?\n- Well, the problem is\nthat I used arguments\nfrom probability theory,\nand there's always this exceptional event.\nSo in probability we have this law\nof large numbers which\ntells you things like\nif you play a casino with a game,\nat a casino with a losing\nexpectation, over time,\nyou are guaranteed almost\nsurely with probability\nas close to 100% as you wish,\nyou're guaranteed to lose money.\nBut there's always this\nexceptional outlier.", "mimetype": "text/plain", "start_char_idx": 154638, "end_char_idx": 158329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af36f640-af93-4630-a6e6-107db2de54cb": {"__data__": {"id_": "af36f640-af93-4630-a6e6-107db2de54cb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3730a016-7e1f-48bd-bddc-9d711be904b0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b154a5a6d03810b44e2d89109f0e5aca276a757e16b327264f6508fafe811705", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee", "node_type": "1", "metadata": {}, "hash": "4863f275f85e406a80f6de5b2a99ab9faf9103c0d3c056d2bd7670c888a44bb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So it's like if I told you\nthat if you go to a casino,\nmost of the time you end up,\nif you keep playing for long enough,\nyou end up with a smaller amount\nin your wallet than when you started.\nThat's kind of like the\nresult that I proved.\n- So why is that result\nlike can you continue down\nthat thread to prove the full conjecture?\n- Well, the problem is\nthat I used arguments\nfrom probability theory,\nand there's always this exceptional event.\nSo in probability we have this law\nof large numbers which\ntells you things like\nif you play a casino with a game,\nat a casino with a losing\nexpectation, over time,\nyou are guaranteed almost\nsurely with probability\nas close to 100% as you wish,\nyou're guaranteed to lose money.\nBut there's always this\nexceptional outlier.\nIt is mathematically possible\nthat even when the game is,\nthe odds are not in your favor,\nyou could just keep\nwinning slightly more often\nthan you lose.\nVery much like how in\nNavier-Stokes there could be most\nof the time your waves can disperse.\nThere could be just one outlier choice\nof initial conditions that\nwould lead you to blow up.\nAnd there could be one outlier choice\nof special number that you stick\nin that shoots off to infinity\nor other numbers crash\nto earth, crash to one.\nIn fact, there's some mathematicians,\nAlex Kontorovich for\ninstance, who've proposed\nthat actually these Collatz iterations\nare like these cellular automata.\nActually if you look at\nwhat they happen in binary,\nthey do actually look a little bit\nlike these Game of Life type patterns.\nAnd in an analogy to how the Game of Life\ncan create these massive\nself-replicating objects\nand so forth, possibly\nyou could create some\nsort of heavier than air flying machine,\na number which is actually\nencoding this machine,\nwhose job it is to encode\nis to create a version\nof itself which is larger.\n- Heavier than air machine\nencoded in a number that flies forever.\n- Yeah, so Conway in fact\nworked on this problem as well.\n- [Lex] Oh, wow.\n- Conway, similar, in fact,\nthat was more inspirations\nfor the Navier-Stokes project\nthat Conway studied generalizations\nof the Collatz problem,\nwhere instead of multiplying by three\nand adding one or dividing by two,\nyou have more complicated branching rules.\nBut instead of having two cases,\nmaybe you have 17 cases\nand you go up and down.\nAnd he showed that once your iteration\ngets complicated enough,\nyou can actually encode Turing machines,\nand you can actually make\nthese problems undecidable\nand do things like this.\nIn fact, he invented\na programming language\nfor these kind of fractional\nlinear transformations.\nHe called it Fractran\nas a play on Fortran,\nand he showed that you can program,\nit was Turing complete.\nYou could make a program that\nif the number you inserted in\nwas encoded as a prime\nit would sync to zero,\nit would go down,\notherwise it would go\nup, and things like that.\nSo the general class of problems is really\nas complicated as all the mathematics.\n- Some of the mystery\nof the cellular automata that we talked\nabout having a mathematical framework\nto say anything about cellular automata,\nmaybe the same kind of\nframework is required\nfor the Collatz conjecture.\n- Yeah, if you want to\ndo it, not statistically,\nbut you really want 100% of\nall inputs for the earth.\nSo what might be feasible is\nstatistically 99%, go to 1.\nBut like everything, that looks hard.\n- What would you say is\nout of these within reach famous problems\nis the hardest problem we have today?\nIs it the Riemann Hypothesis?\n- Riemann is up there.\nP = NP is a good one because\nthat's a meta problem.\nIf you solve that\nin the positive sense that you\ncan find a P = NP algorithm,\nthen potentially this solves a lot\nof other problems as well.\n- And we should mention some\nof the conjectures we've\nbeen talking about.\nA lot of stuff is built\non top of them now.\nThere's ripple effects.\nP = NP has more ripple effects\nthan basically any other.\n- Right, if the Riemann\nhypothesis is disproven,\nthat'd be a big mental shock\nto the number theorists,\nbut it would have follow-on\neffects for cryptography.", "mimetype": "text/plain", "start_char_idx": 157564, "end_char_idx": 161644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee": {"__data__": {"id_": "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af36f640-af93-4630-a6e6-107db2de54cb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ffb8dbd50b46673e603b869d8e5aabe338fbb588f1b91a3d34d4eb5f5f114c19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99c36b9e-1545-4071-a6b7-82e25c631336", "node_type": "1", "metadata": {}, "hash": "94566b7c14a38d80d5c626d2887f2717aac2739b5e161774e6f26f880e230ebd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But like everything, that looks hard.\n- What would you say is\nout of these within reach famous problems\nis the hardest problem we have today?\nIs it the Riemann Hypothesis?\n- Riemann is up there.\nP = NP is a good one because\nthat's a meta problem.\nIf you solve that\nin the positive sense that you\ncan find a P = NP algorithm,\nthen potentially this solves a lot\nof other problems as well.\n- And we should mention some\nof the conjectures we've\nbeen talking about.\nA lot of stuff is built\non top of them now.\nThere's ripple effects.\nP = NP has more ripple effects\nthan basically any other.\n- Right, if the Riemann\nhypothesis is disproven,\nthat'd be a big mental shock\nto the number theorists,\nbut it would have follow-on\neffects for cryptography.\nBecause a lot of cryptography\nuses number theory,\nuses number theory constructions\ninvolving primes and so forth.\nAnd it relies very much\non the intuition that\nnumber theory has built\nover many, many years\nof what operations involving\nprimes behave randomly\nand what ones don't.\nAnd in particular our\nencryption methods are designed\nto turn text written information on it\ninto text which is\nindistinguishable from random noise.\nAnd hence we believe to be\nalmost impossible to crack,\nat least mathematically.\nBut if something\nas core to our beliefs as the\nRiemann hypothesis is wrong,\nit means that there are actual patterns\nof the primes that we're not aware of.\nAnd if there's one, there's\nprobably going to be more.\nAnd suddenly a lot of our\ncrypto systems are in doubt.\n- Yeah, but then how do you\nthen say stuff about the primes?\n- [Terence] Yeah.\n- Like you're going towards\nthe Collatz conjecture again,\nbecause you want it to be random, right?\nYou want it to be random.\n- Yeah, so more broadly, I'm\njust looking for more tools,\nmore ways to show that things are random.\nHow do you prove a\nconspiracy doesn't happen?\n- Is there any chance to you that P = NP?\nCan you imagine a possible universe?\n- It is possible.\nI mean, there's various scenarios.\nI mean, there's one where\nit is technically possible,\nbut in fact it's never\nactually implementable.\nThe evidence is sort of\nslightly pushing in favor of no,\nthat probably P is not equal to NP.\n- I mean, it seems like it's one\nof those cases similar\nto Riemann hypothesis.\nI think the evidence is leaning\npretty heavily on the no.\n- Certainly more on\nthe no than on the yes.\nThe funny thing\nabout P = NP is that we have\nalso a lot more obstructions\nthan we do for almost any other problem.\nSo while there's evidence,\nwe also have a lot of\nresults ruling out many,\nmany types of approaches to the problem.\nThis is the one thing\nthat the computer science has\nactually been very good at.\nIt's actually saying\nthat certain approaches\ncannot work, no-go theorems.\nIt could be undecidable.\nWe don't know.\n- There's a funny story\nI read that when you won the Fields Medal,\nsomebody from the Internet\nwrote you and asked,\nwhat are you going\nto do now that you've won\nthis prestigious award?\nAnd then you just quickly,\nvery humbly said that this\nshiny metal is not going\nto solve any of the problems\nI'm currently working on.\nI'm going to keep working on them.\nIt's just, first of all,\nit's funny to me that\nyou would answer an email\nin that context.\nAnd second of all, it\njust shows your humility.\nBut anyway, maybe you could\nspeak to the Fields Medal.\nBut it's another way for me\nto ask about Grigori Perelman.\nWhat do you think\nabout him famously\ndeclining the Fields Medal\nand the Millennium Prize,\nwhich came with a one million\ndollar of prize money?\nHe stated that I'm not\ninterested in money or fame.\nThe prize is completely irrelevant for me.\nIf the proof is correct,\nthen no other recognition is needed.\n- Yeah, no.\nHe's somewhat of an outlier,\neven among mathematicians who tend\nto have somewhat idealistic views.\nI've never met him.\nI think I'd be interested\nto meet him one day,\nbut I never had the chance.\nI know people who've met him,\nbut he's always had strong\nviews about certain things.", "mimetype": "text/plain", "start_char_idx": 160902, "end_char_idx": 164894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "99c36b9e-1545-4071-a6b7-82e25c631336": {"__data__": {"id_": "99c36b9e-1545-4071-a6b7-82e25c631336", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4028f78-6a5d-4fa1-8894-ef14ec41f4ee", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b0f9b3209272b2424017dd6f79f965231f32d06a2c00b8196604191a8de2214e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f30600b-b14d-4275-95ea-0a138c64c092", "node_type": "1", "metadata": {}, "hash": "a21b1d9eb63a9fee976f027ba657fb8f94bcbe358df0839e27c129abe9914274", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And second of all, it\njust shows your humility.\nBut anyway, maybe you could\nspeak to the Fields Medal.\nBut it's another way for me\nto ask about Grigori Perelman.\nWhat do you think\nabout him famously\ndeclining the Fields Medal\nand the Millennium Prize,\nwhich came with a one million\ndollar of prize money?\nHe stated that I'm not\ninterested in money or fame.\nThe prize is completely irrelevant for me.\nIf the proof is correct,\nthen no other recognition is needed.\n- Yeah, no.\nHe's somewhat of an outlier,\neven among mathematicians who tend\nto have somewhat idealistic views.\nI've never met him.\nI think I'd be interested\nto meet him one day,\nbut I never had the chance.\nI know people who've met him,\nbut he's always had strong\nviews about certain things.\nI mean, it's not like he\nwas completely isolated\nfrom the math community.\nI mean, he would give talks\nand write papers and so forth,\nbut at some point he just decided not\nto engage with the rest of the community.\nHe was disillusioned or\nsomething, I don't know.\nAnd he decided to peace out\nand collect mushrooms in\nSt. Petersburg or something.\nAnd that's fine, you can do that.\nI mean that's another\nsort of flip side to,\nI mean, a lot of our\nproblems that we solve,\nsome of them do have practical\napplication and that's great.\nBut if you stop thinking about a problem,\nso he hasn't published since\nin this field, but that's fine.\nThere's many, many other\npeople who've done so as well.\nYeah, so I guess one thing\nI didn't realize initially\nwith the Fields Medal is that it\nsort of makes you part\nof the establishment.\nSo most mathematicians,\njust career mathematicians,\nyou just focus on\npublishing the next paper,\nmaybe getting one promotion,\none rank, and starting a few projects,\nmaybe taking some students or something.\nBut then suddenly people\nwant your opinion on things\nand you have to think a little bit\nabout things that you might\njust have foolishly say\nbecause you know no one's\ngoing to listen to you.\nIt's more important now.\n- Is it constraining to you?\nAre you able to still have fun\nand be a rebel and try crazy\nstuff and play with ideas?\n- I have a lot less free\ntime than I had previously.\nI mean mostly by choice.\nI mean obviously I have the\noption to sort of decline.\nSo I decline a lot of things.\nI could decline even more,\nor I could acquire a reputation being\nso unreliable that people\ndon't even ask anymore.\n- I love the different algorithms here.\nThis is great.\n- It's always an option.\nBut you know, there are things\nthat are like,\nI mean I don't spend as much time as I do\nas a postdoc just working\non one problem at a time\nor fooling around.\nI still do that a little bit.\nBut yeah, as you advance in your career,\nsome of the more soft skills,\nso math somehow front loads\nall the technical skills\nto the early stages of a career.\nSo as a post office publish or perish,\nyour incentive to advised\nto basically focus\non proving very technical theorems,\nto sort of prove yourself as\nwell as prove the theorems.\nBut then as you get more senior,\nyou have to start mentoring\nand giving interviews\nand trying to shape\ndirection of the field,\nboth research-wise\nand sometimes you have to do\nvarious administrative things,\nand it's kind of the right social contract\nbecause you need to work\nin the trenches to see what\ncan help mathematicians.\n- The other side of the establishment\nsort of the really positive thing\nis that you get to be a light\nthat's an inspiration to a\nlot of young mathematicians\nor young people that are just\ninterested in mathematics.\nIt's like, it's just how\nthe human mind works.\nThis is where I would probably say\nthat I like the Fields Medal,\nthat it does inspire a lot\nof young people somehow.\nThis is just how human brains work.\n- [Terence] Yeah.\n- At the same time,\nI also want to give sort of respect\nto somebody like Grigori Perelman,\nwho is critical of awards in his mind.\nThose are his principles.\nAnd any human that's\nable for their principles\nto do the thing that most\nhumans would not be able to do.\nIt's beautiful to see.", "mimetype": "text/plain", "start_char_idx": 164142, "end_char_idx": 168173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f30600b-b14d-4275-95ea-0a138c64c092": {"__data__": {"id_": "3f30600b-b14d-4275-95ea-0a138c64c092", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99c36b9e-1545-4071-a6b7-82e25c631336", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ea843cd9d483c433480781c4d576b94242adeb88a7612a41d4e0403c7d5710b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a810c62-10a6-4b25-ba45-3ab734e444c3", "node_type": "1", "metadata": {}, "hash": "ef711c43a6b88ffb133ec956f4004000f31763b6935bff140e23e2bf9b6aa6d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The other side of the establishment\nsort of the really positive thing\nis that you get to be a light\nthat's an inspiration to a\nlot of young mathematicians\nor young people that are just\ninterested in mathematics.\nIt's like, it's just how\nthe human mind works.\nThis is where I would probably say\nthat I like the Fields Medal,\nthat it does inspire a lot\nof young people somehow.\nThis is just how human brains work.\n- [Terence] Yeah.\n- At the same time,\nI also want to give sort of respect\nto somebody like Grigori Perelman,\nwho is critical of awards in his mind.\nThose are his principles.\nAnd any human that's\nable for their principles\nto do the thing that most\nhumans would not be able to do.\nIt's beautiful to see.\n- Some recognition is\nnecessary and important.\nBut yeah, it's also important\nto not let these things take\nover your life and only be concerned\nabout getting the next\nbig award or whatever.\nI mean, yeah, so again, you see,\nthese people try to only\nsolve really big math problems\nand not work on things that\nare less sexy, if you wish,\nbut actually still\ninteresting and instructive.\nAs you say,\nthe way the human mind works,\nwe understand things better\nwhen they're attached to humans,\nand also if they're attached\nto a small number of humans.\nThe way our human mind is\nwired, we can comprehend\nand the relationships\nbetween 10 or 20 people.\nBut once you get beyond 100\npeople, there's a limit.\nI think there's a name for it,\nbeyond which it just becomes the other.\nAnd so you have to simplify\nthe whole mass of 99.9% of\nhumanity becomes the other.\nAnd often these models are incorrect\nand this causes all kinds of problems.\nSo to humanize a subject,\nlike if you identify a\nsmall number of people\nand say, \"These are representative\npeople of a subject,\"\nrole models, for example,\nthat has some role,\nbut it can also be,\ntoo much of it can be harmful.\nBecause I'll be the first\nto say that my own career path is not that\nof a typical mathematician.\nI had a very accelerated education.\nI skipped a lot of classes.\nI think I had very fortunate\nmentoring opportunities,\nand I think I was at the\nright place at the right time.\nJust because someone doesn't\nhave my trajectory doesn't mean\nthat they can't be good mathematicians.\nI mean, they may be good mathematicians\nin a very different style.\nAnd we need people with a different style.\nAnd sometimes too much focus is given\non the person who does the\nlast step to complete a project\nin mathematics or elsewhere\nthat's really taken centuries\nor decades with lots\nand lots of building and\nlots of previous work.\nBut that's a story that's difficult\nto tell if you're not an expert,\nbecause it's easier\nto just say, \"One person\ndid this one thing.\"\nIt makes for a much simpler history.\n- I think on the whole it\nis a hugely positive thing\nto talk about Steve Jobs\nas a representative of Apple\nwhen I personally know,\nand of course everybody\nknows, the incredible design,\nthe incredible engineering teams,\njust the individual humans on those teams.\nThey're not a team.\nThey're individual humans on a team.\nAnd there's a lot of brilliance there,\nbut it's just a nice shorthand, like pi.\n- [Terence] Yeah.\n- Steve Jobs, pi.\n- Yeah, yeah.\nAs a starting point, as\na first approximation.\n- And then read some biographies\nand then look into much\ndeeper first approximation.\n- [Terence] Yeah.\n- That's right.\nSo you mentioned you were at Princeton\nto Andrew Wiles at that time.\n- [Terence] Oh, yeah.\n- He was a professor there.\n- It's a funny moment how history\nis just all interconnected.\nAnd at that time he announced\nthat he proved the Fermat's Last Theorem.\nWhat did you think,\nmaybe looking back now with more context\nabout that moment in math history?\n- Yeah, so I was a graduate\nstudent at the time.\nI vaguely remember there\nwas press attention,\nand we all had the same.\nWe had pigeonholes in the same mail room.\nSo we all pitched our mail,\nand suddenly Andrew\nWiles' mailbox exploded\nto be overflowing.\n- (laughs) That's a good metric.\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 167458, "end_char_idx": 171459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a810c62-10a6-4b25-ba45-3ab734e444c3": {"__data__": {"id_": "0a810c62-10a6-4b25-ba45-3ab734e444c3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f30600b-b14d-4275-95ea-0a138c64c092", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4356ddfd3bc6aa18d768a982f631ed903b936f6fb924a35994e99e39477b58d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ce3790a-0f20-4f13-b30b-ce817588fc3a", "node_type": "1", "metadata": {}, "hash": "91a09f551742d31e6109cbea4e01c55a681147cef7c148232a5a6051fce564e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a starting point, as\na first approximation.\n- And then read some biographies\nand then look into much\ndeeper first approximation.\n- [Terence] Yeah.\n- That's right.\nSo you mentioned you were at Princeton\nto Andrew Wiles at that time.\n- [Terence] Oh, yeah.\n- He was a professor there.\n- It's a funny moment how history\nis just all interconnected.\nAnd at that time he announced\nthat he proved the Fermat's Last Theorem.\nWhat did you think,\nmaybe looking back now with more context\nabout that moment in math history?\n- Yeah, so I was a graduate\nstudent at the time.\nI vaguely remember there\nwas press attention,\nand we all had the same.\nWe had pigeonholes in the same mail room.\nSo we all pitched our mail,\nand suddenly Andrew\nWiles' mailbox exploded\nto be overflowing.\n- (laughs) That's a good metric.\n- Yeah.\nSo we all talked about\nit at tea and so forth.\nI mean, we didn't understand,\nmost of us sort of didn't\nunderstand the proof.\nWe understand sort of high-level details.\nIn fact, there's an ongoing\nproject to formalize it in Lean.\nKevin Buzzard is actually.\n- Yeah.\nCan we take that small tangent?\nHow difficult does that,\nbecause as I understand,\nthe proof for Fermat's Last Theorem has\nlike super complicated objects.\n- [Terence] Yeah.\n- Really difficult to formalize, no?\n- Yeah, I guess, yeah, you're right.\nThe objects that they\nuse, you can define them.\nSo they've been defined in Lean.\nSo just defining what\nthey are can be done.\nThat's really not trivial,\nbut it's been done.\nBut there's a lot of really basic facts\nabout these objects that\nhave taken decades to prove\nand that they're in all\nthese different math papers.\nAnd so lots of these have\nto be formalized as well.\nKevin Buzzard's goal, actually,\nhe has a five-year grant to\nformalize Fermat's Last Theorem.\nAnd his aim is that he\ndoesn't think he will be able\nto get all the way down\nto the basic axioms.\nBut he wants to formalize it\nto the point where the only\nthings that he needs to rely on\nas black boxes are things that were known\nby 1980 to number theorists at the time.\nAnd then some other person,\nsome other work would have to\nbe done, to get from there.\nSo it's a different area\nof mathematics than the type\nof mathematics I'm used to.\nIn analysis, which is kind of my area,\nthe objects we study are kind\nof much closer to the ground.\nI study things like prime\nnumbers and functions\nand things that are within scope\nof a high school math\neducation to at least define.\nYeah, but then there's this\nvery advanced algebraic side\nof number theory where people\nhave been building structures\nupon structures for quite a while.\nAnd it's a very sturdy\nstructure at the base at least,\nit's extremely well developed\nin the textbooks and so forth.\nBut it does get\nto the point where if you haven't\ntaken these years of study\nand you want to ask about what is going on\nat level six of this tower,\nyou have to spend quite a bit of time\nbefore they can even get\nto the point where you see\nsomething you recognize.\n- What inspires you about his journey,\nthat was similar as we\ntalked about seven years,\nmostly working in secret?\n- Yeah.\nThat is a romantic,\nso it kind of fits with\nsort of the romantic image\nI think people have of mathematicians\nto the extent that they\nthink of them at all,\nas these kind of eccentric\nwizards or something.\nSo that certainly kind of\naccentuated that perspective.\nI mean, it is a great achievement.\nHis style of solving problems is\nso different from my own, which is great.\nI mean, we need people like that.\n- Can you speak to it,\nin terms of you like the collaborative?\n- I like moving on\nfrom a problem if it's\ngiving too much difficulty.\nBut you need the people\nwho have the tenacity\nand the fearlessness.\nI've collaborated with\npeople like that where I want\nto give up, because the first approach\nthat we tried didn't work,\nand the second one didn't work,\nbut they're convinced and they have third,\nfourth, and the fifth approach works.\nAnd I have to eat my words.", "mimetype": "text/plain", "start_char_idx": 170651, "end_char_idx": 174619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ce3790a-0f20-4f13-b30b-ce817588fc3a": {"__data__": {"id_": "2ce3790a-0f20-4f13-b30b-ce817588fc3a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a810c62-10a6-4b25-ba45-3ab734e444c3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3c0ed59ed9a0108aeaa4b44a8bb5ca8faa477698da05a58d4a7274390ff9c30b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18289b6f-899b-4fce-8ba5-0797e27470e9", "node_type": "1", "metadata": {}, "hash": "3d7fef0af604790d8c611f99bed772a62373ac8e201a40231db70c481454c1b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So that certainly kind of\naccentuated that perspective.\nI mean, it is a great achievement.\nHis style of solving problems is\nso different from my own, which is great.\nI mean, we need people like that.\n- Can you speak to it,\nin terms of you like the collaborative?\n- I like moving on\nfrom a problem if it's\ngiving too much difficulty.\nBut you need the people\nwho have the tenacity\nand the fearlessness.\nI've collaborated with\npeople like that where I want\nto give up, because the first approach\nthat we tried didn't work,\nand the second one didn't work,\nbut they're convinced and they have third,\nfourth, and the fifth approach works.\nAnd I have to eat my words.\nOkay, I didn't think\nthis was going to work,\nbut yes, you were right.\n- And we should say for\npeople who don't know,\nnot only are you known for\nthe brilliance of your work,\nbut the incredible\nproductivity, just the number\nof papers which are all\nof very high quality.\nSo there's something to be\nsaid about being able to jump\nfrom topic to topic.\n- Yeah, it works for me.\nYeah.\nI mean, there are also people\nwho are very productive\nand they focus very deeply on, yeah.\nI think everyone has to\nfind their own workflow.\nOne thing which is a shame\nin mathematics is that\nwe have, mathematics,\nthere's sort of a one\nsize fits all approach\nto teaching mathematics.\nSo we have a certain\ncurriculum and so forth.\nI mean, maybe if you do math\ncompetitions or something,\nyou get a slightly different experience.\nBut I think many people,\nthey don't find their native\nmath language until very late\nor usually too late.\nSo they stop doing mathematics,\nand they have a bad experience\nwith a teacher who's trying\nto teach them one way to do mathematics.\nThey don't like it.\nMy theory is that humans don't come,\nevolution has not given us a math center\nof a brain directly.\nWe have a vision center\nand a language center\nand some other centers\nwhich evolution has honed,\nbut we don't have innate\nsense of mathematics.\nBut our other centers\nare sophisticated enough\nthat different people,\nwe can repurpose our other areas\nof our brain to do mathematics.\nSo some people have figured out how\nto use the visual center\nto do mathematics,\nand so they think very visually\nwhen they do mathematics.\nSome people have repurposed\ntheir language center\nand they think very symbolically.\nSome people, if they are very competitive\nand they like gaming,\nthere's a part of your\nbrain that's very good\nat solving puzzles and games,\nand that can be repurposed.\nBut when I talk to other mathematicians,\nthey don't quite think,\nI can tell that they're using\nsomehow different styles\nof thinking than I am.\nI mean, not disjoint, but\nthey may prefer visual.\nI don't actually prefer visual so much.\nI need lots of visual aids myself.\nMathematics provides a common language,\nso we can still talk to each\nother even if we are thinking\nin different ways.\n- But you can tell\nthere's a difference set\nof subsystems being used\nin the thinking process.\n- Yes, they take different paths,\nthey're very quick at\nthings that I struggle with\nand vice versa, and yet they\nstill get to the same goal.\n- That's beautiful.\n- But I mean, the way we educate,\nunless you have a personalized\ntutor or something, I mean,\neducation sort of just by nature of scale\nhas to be mass produced.\nYou have to teach the 30 kids.\nIf they have 30 different styles,\nyou can't teach 30 different ways.\n- On that topic, what advice\nwould you give to students,\nyoung students who are\nstruggling with math\nbut are interested in it and\nwould like to get better?\nIs there something in this\ncomplicated educational context,\nwhat would you?\n- Yeah, it's a tricky problem.\nOne nice thing is that there\nare now lots of sources\nfor mathematical enrichment\noutside the classroom.\nSo in my day already there\nare math competitions\nand there are also popular\nmath books in the library.\nBut now you have YouTube.\nThere are forums just devoted\nto solving math puzzles.\nAnd math shows up in other places.\nFor example, there are hobbyists\nwho play poker for fun,\nand they, for very specific reasons,\nare interested in very\nspecific probability questions.", "mimetype": "text/plain", "start_char_idx": 173959, "end_char_idx": 178081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18289b6f-899b-4fce-8ba5-0797e27470e9": {"__data__": {"id_": "18289b6f-899b-4fce-8ba5-0797e27470e9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ce3790a-0f20-4f13-b30b-ce817588fc3a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "168b1d0c05cc66a53304fbe67687d74830df612c276831664c7e4ee359005891", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "895982ea-fdb7-419c-8cb7-e7318fd8f4e0", "node_type": "1", "metadata": {}, "hash": "1fcc67390b30e4ff19baf8f50bed3c7ebfa3232f3f3ff9d8042798226713a8c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You have to teach the 30 kids.\nIf they have 30 different styles,\nyou can't teach 30 different ways.\n- On that topic, what advice\nwould you give to students,\nyoung students who are\nstruggling with math\nbut are interested in it and\nwould like to get better?\nIs there something in this\ncomplicated educational context,\nwhat would you?\n- Yeah, it's a tricky problem.\nOne nice thing is that there\nare now lots of sources\nfor mathematical enrichment\noutside the classroom.\nSo in my day already there\nare math competitions\nand there are also popular\nmath books in the library.\nBut now you have YouTube.\nThere are forums just devoted\nto solving math puzzles.\nAnd math shows up in other places.\nFor example, there are hobbyists\nwho play poker for fun,\nand they, for very specific reasons,\nare interested in very\nspecific probability questions.\nAnd there's a community of\namateur probabilists in poker,\nin chess, in baseball.\nI mean, there's math all over the place.\nAnd I'm hoping actually with\nthese new tools for Lean\nand so forth, that actually\nwe can incorporate the broader public\ninto math research projects.\nThis almost doesn't\nhappen at all currently.\nSo in the sciences there is\nsome scope for citizen science,\nlike astronomers looking\nat the amateurs who would discover comets,\nand there's biologists\nthat people who could identify\nbutterflies and so forth.\nThere are a small number\nof activities where amateur mathematicians\ncan discover new primes and so forth.\nBut previously,\nbecause we had to verify\nevery single contribution,\nmost mathematical research projects,\nit would not help to have\ninput from the general public.\nIn fact, it would just be time-consuming,\nbecause just error\nchecking and everything.\nBut one thing about these\nformalization projects\nis that they are bringing in more people.\nSo I'm sure there\nare high school students who've\nalready contributed to some\nof these formalizing projects,\nwho contributed to Mathlib.\nYou don't need to be a\nPhD holder to just work\non one atomic thing.\n- There's something about the\nformalization here that also,\nas a very first step,\nopens it up to the\nprogramming community too,\nthe people who are already\ncomfortable with programming.\nIt seems like programming is somehow,\nmaybe just the feeling,\nbut it feels more accessible\nto folks than math.\nMath is seen as this, like extreme,\nespecially modern mathematics seen\nas this extremely difficult to enter area.\nAnd programming is not.\nSo that could be just an entry point.\n- You can execute code\nand you can get results,\nyou can print out \"Hello\nworld\" pretty quickly.\nIf programming was taught\nas an almost entirely theoretical subject,\nwhere you just taught\nthe computer science,\nthe theory of functions\nand routines and so forth,\nand outside of some very\nspecialized homework assignments,\nyou would not actually program\non the weekend for fun.\nThey would be considered as hard as math.\nSo as I said, there are communities\nof non-mathematicians where\nthey're deploying math\nfor some very specific purpose,\nlike optimizing their\npoker game, and for them,\nand then math becomes fun for them.\n- What advice would you give\nin general to young people\nhow to pick a career,\nhow to find themselves?\n- That's a tough, tough question.\nYeah, so there's a lot of\ncertainty now in the world.\nI mean, there was this period\nafter the war where at least in the West,\nif you came from a good demographic,\nthere was a very stable\npath to a good career.\nYou go to college, you get an education,\nyou pick one profession,\nand you stick to it.\nIt's becoming much more\na thing of the past.\nSo I think you just have to\nbe adaptable and flexible.\nI think people have to get\nskills that are transferable,\nlike learning one specific\nprogramming language\nor one specific subject of mathematics\nor something, that itself is\nnot a super transferable skill,\nbut knowing how to reason\nwith abstract concepts\nor how to problem solve\nwhen things go wrong,\nthese are things which I\nthink we will still need even\nas our tools get better,\nyou'll be working with\nAI, sports, so forth.\n- But actually you're an\ninteresting case study.\nI mean you're like a,\none of the great living\nmathematicians, right?\nAnd then you had a way of doing things,\nand then all of a sudden\nyou start learning,\nfirst of all, you kept\nlearning new fields,\nbut you learn Lean.", "mimetype": "text/plain", "start_char_idx": 177247, "end_char_idx": 181571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "895982ea-fdb7-419c-8cb7-e7318fd8f4e0": {"__data__": {"id_": "895982ea-fdb7-419c-8cb7-e7318fd8f4e0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18289b6f-899b-4fce-8ba5-0797e27470e9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f4f6e888c4eee9b701b4777543ca996f2e494c3aa6f0a3c3d50fdc375815695", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3609c2b-53c2-41eb-8d75-5e8888f306c4", "node_type": "1", "metadata": {}, "hash": "76044e6dbe4e2de5358aa273840aa83ba58142e30d83fec7edb086f340f4c3f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's becoming much more\na thing of the past.\nSo I think you just have to\nbe adaptable and flexible.\nI think people have to get\nskills that are transferable,\nlike learning one specific\nprogramming language\nor one specific subject of mathematics\nor something, that itself is\nnot a super transferable skill,\nbut knowing how to reason\nwith abstract concepts\nor how to problem solve\nwhen things go wrong,\nthese are things which I\nthink we will still need even\nas our tools get better,\nyou'll be working with\nAI, sports, so forth.\n- But actually you're an\ninteresting case study.\nI mean you're like a,\none of the great living\nmathematicians, right?\nAnd then you had a way of doing things,\nand then all of a sudden\nyou start learning,\nfirst of all, you kept\nlearning new fields,\nbut you learn Lean.\nThat's a non-trivial thing to learn.\n- [Terence] Yeah.\n- For a lot of people,\nthat's an extremely uncomfortable\nleap to take, right?\nA lot of mathematicians.\n- First of all,\nI've always been interested\nin new ways to do mathematics.\nI feel like a lot\nof the ways we do things\nright now are inefficient.\nMe and my colleagues who spend a lot\nof time doing very routine\ncomputations or doing things\nthat other mathematicians\nwould instantly know how to do\nand we don't know how to do them,\nand why can't we search\nand get a quick response and so forth.\nSo that's why I've always been interested\nin exploring new workflows.\nAbout four or five years ago,\nI was on a committee where we had to ask\nfor ideas for interesting workshops to run\nat a math institute.\nAnd at the time Peter Scholze\nhad just formalized one\nof his new theorems,\nand there were some other developments\nin computer-assisted proof\nthat looked quite interesting.\nAnd I said, \"Oh,\nwe should run a workshop on this.\nThis would be a good idea.\"\nAnd then I was a bit too\nenthusiastic about this idea.\nSo I got voluntold to actually run it.\nSo I did with a bunch of other people,\nKevin Buzzard and Jordan Ellenberg\nand a bunch of other people,\nand it was a nice success.\nWe brought together a\nbunch of mathematicians\nand computer scientists and other people,\nand we got up to speed\non the state of the art,\nand it was really interesting developments\nthat most mathematicians\ndidn't know was going on\nthat lots of nice proofs\nof concept just saw hints\nof what was going to happen.\nThis was just before\nChatGPT, but there was,\neven then there was one\ntalk about language models\nand the potential capability\nof those in the future.\nSo that got me excited about the subject.\nSo I started giving talks about,\nthis is something more of\nus should start looking at,\nnow that I run this conference.\nAnd then ChatGPT came out, and\nsuddenly AI was everywhere.\nAnd so I got interviewed\na lot about this topic,\nand in particular the\ninteraction between AI\nand formal proof assistants.\nAnd I said, \"Yeah, they\nshould be combined.\nThis is perfect synergy to happen here.\"\nAnd at some point I realized that I have\nto actually not just talk\nthe talk, but walk the walk.\nI don't work in machine learning,\nand I don't work in proof formalization,\nand there's a limit to how much\nI can just rely on authority\nand saying, \"I'm a\nwell-known mathematician.\nJust trust me when I\nsay that this is going\nto change mathematics,\"\nand I'm not doing it any,\nwhen I don't do any of it myself.\nSo I felt like I had\nto actually justify it.\nA lot of what I get into\nactually I don't quite see\nin advance just how much time\nI'm going to spend on it.\nAnd it's only after I'm sort of waist deep\nin a project that I realized\nby that point I'm committed.\n- Well, that's deeply admirable\nthat you're willing to go\ninto the fray, be in some\nsmall way a beginner.\nOr have some of the sort of challenges\nthat a beginner would.\nSo new concepts, new ways\nof thinking, also you know,\nsucking at a thing that others.\nI think in that talk,\nyou could be a Fields\nMedal winning mathematician\nand an undergrad knows\nsomething better than you.\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 180780, "end_char_idx": 184730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3609c2b-53c2-41eb-8d75-5e8888f306c4": {"__data__": {"id_": "e3609c2b-53c2-41eb-8d75-5e8888f306c4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "895982ea-fdb7-419c-8cb7-e7318fd8f4e0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f13042371a5254071f59e5f8ead2ec854fdfe4ee121fbc477d78b0c32750db45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "649468f5-92f6-433a-91ba-f15a267f8eb1", "node_type": "1", "metadata": {}, "hash": "78c3df205cc217ebf160e4b2fba27f35c4e9c5f005b08c6c2f8bbf16f6f427e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just trust me when I\nsay that this is going\nto change mathematics,\"\nand I'm not doing it any,\nwhen I don't do any of it myself.\nSo I felt like I had\nto actually justify it.\nA lot of what I get into\nactually I don't quite see\nin advance just how much time\nI'm going to spend on it.\nAnd it's only after I'm sort of waist deep\nin a project that I realized\nby that point I'm committed.\n- Well, that's deeply admirable\nthat you're willing to go\ninto the fray, be in some\nsmall way a beginner.\nOr have some of the sort of challenges\nthat a beginner would.\nSo new concepts, new ways\nof thinking, also you know,\nsucking at a thing that others.\nI think in that talk,\nyou could be a Fields\nMedal winning mathematician\nand an undergrad knows\nsomething better than you.\n- Yeah.\nI think mathematics inherently,\nI mean mathematics is so huge these days\nthat nobody knows all\nof modern mathematics,\nand inevitably we make mistakes,\nand you can't cover up\nyour mistakes with just\nsort of bravado, because\npeople will ask for your proofs\nand if you don't have the proofs,\nyou don't have the proofs.\n- [Lex] I love math.\n- Yeah.\n(Lex laughs)\nSo it does keep us honest.\nIt's not a perfect panacea,\nbut I think we do have more\nof a culture of admitting error\nbecause we're forced to all the time.\n- Big ridiculous question.\nI'm sorry for it once again.\nWho is the greatest\nmathematician of all time?\n(Terence laughing)\nMaybe one who's no longer with us.\nWho are the candidates?\nEuler, Gauss, Newton, Ramanujan, Hilbert.\n- So first of all, as mentioned before,\nthere's some time dependence.\n- On the day.\n- Yeah.\nIf you cumulatively over\ntime, for example, Euclid,\nsort of is one of the leading contenders.\nAnd then maybe some unnamed\nanonymous mathematicians\nbefore that, whoever came up\nwith the concept of numbers.\n- Do mathematicians today still\nfeel the impact of Hilbert?\n- Oh yeah.\n- Directly of everything that's happened\nin the 20th century?\n- [Terence] Yeah, Hilbert spaces.\nWe have lots of things that\nare named after him, of course.\nJust the arrangement of mathematics\nand just the introduction\nof certain concepts.\nI mean, 23 problems have\nbeen extremely influential.\n- There's some strange power\nto the declaring which\nproblems are hard to solve.\nThe statement of the open problems.\n- Yeah, I mean this is\nbystander effect everywhere.\nIf no one says, \"You should do X,\"\neveryone just will move around waiting\nfor somebody else to do\nsomething and nothing gets done.\nOne thing that actually you\nhave to teach undergraduates\nin mathematics is that you\nshould always try something.\nSo you see a lot of paralysis\nin an undergraduate trying a math problem.\nIf they recognize\nthat there's a certain\ntechnique that can be applied,\nthey will try it.\nBut there are problems\nfor which they see none\nof their standard techniques\nobviously applies.\nAnd the common reaction\nis then just paralysis.\nI don't know what to do.\nI think there's a quote from the Simpsons.\n\"I've tried nothing, and\nI'm all out of ideas.\"\n(Lex laughing)\nSo the next step then is to try anything,\nno matter how stupid.\nAnd in fact, almost the\nstupider the better.\nA technique which is\nalmost guaranteed to fail.\nBut the way it fails is\ngoing to be instructive.\nIt fails because you're not\nat all taking into\naccount this hypothesis.\nOh, this hypothesis must be useful.\nThat's a clue.\n- I think you also suggested\nsomewhere this fascinating\napproach which really stuck\nwith me as they're using it\nand it really works.\nI think you said it's called\nstructured procrastination.\n- [Terence] No, yes.\n- It's when you really don't want\nto do a thing, then you imagine\na thing you don't want to do more.\n- Yes.\n- That's worse than that.\nAnd then in that way you procrastinate\nby not doing the thing that's worse.\n- [Terence] Yeah, yeah.\n- It's a nice hack.\nIt actually works. (chuckles)\n- Yeah, yeah, it is.\nI mean, with anything, I mean,\nlike psychology is really important.", "mimetype": "text/plain", "start_char_idx": 183965, "end_char_idx": 187886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "649468f5-92f6-433a-91ba-f15a267f8eb1": {"__data__": {"id_": "649468f5-92f6-433a-91ba-f15a267f8eb1", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3609c2b-53c2-41eb-8d75-5e8888f306c4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0fc0222a30b4e465b34a40a881db1a356c8b963ec6b5726b4a0dc0acdd157808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61ece166-2a74-4098-9a20-8718fa98a0f4", "node_type": "1", "metadata": {}, "hash": "e249063a4a939d34a5d99c87d3944b9e90873ae1a774592151fbbe8a04d1ce84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But the way it fails is\ngoing to be instructive.\nIt fails because you're not\nat all taking into\naccount this hypothesis.\nOh, this hypothesis must be useful.\nThat's a clue.\n- I think you also suggested\nsomewhere this fascinating\napproach which really stuck\nwith me as they're using it\nand it really works.\nI think you said it's called\nstructured procrastination.\n- [Terence] No, yes.\n- It's when you really don't want\nto do a thing, then you imagine\na thing you don't want to do more.\n- Yes.\n- That's worse than that.\nAnd then in that way you procrastinate\nby not doing the thing that's worse.\n- [Terence] Yeah, yeah.\n- It's a nice hack.\nIt actually works. (chuckles)\n- Yeah, yeah, it is.\nI mean, with anything, I mean,\nlike psychology is really important.\nLike you talk to athletes\nlike marathon runners\nand so forth, and they talk\nabout what's the most important thing,\nis it their training regimen\nor their diet and so forth?\nSo much of it is actually psychology,\njust tricking yourself to\nthink that the form is feasible\nso that you're motivated to do it.\n- Is there something our human mind\nwill never be able to comprehend?\n- Well, as a mathematician, I mean,\nby induction, there must be\nsome sufficient large number\nthat you can't understand.\n(Lex laughing)\nThat was the first\nthing that came to mind.\n- So that, but even broadly, is there,\nis there something about\nour mind that we're going\nto be limited even with\nthe help of mathematics?\n- Well, okay, I mean,\nhow much augmentation are you willing,\nfor example, if I didn't\neven have pen and paper,\nif I had no technology whatsoever,\nso I'm not allowed\nblackboard, pen and paper.\n- You're already much more\nlimited than you would be.\n- Incredibly limited.\nEven language, the English\nlanguage is a technology.\nIt's one that's been very internalized.\n- So you're right,\nthe formulation of the\nproblem is incorrect\nbecause there really is no\nlonger just a solo human.\nWe're already augmented\nin extremely complicated,\nintricate ways, right.\n- Yeah.\n- So we're already like a\ncollective intelligence.\n- Yes, yeah, yes.\nSo humanity, plural,\nhas much more intelligence in\nprinciple on his good days,\nthan the individual humans put together.\nIt can all have less.\nOkay, but yeah, so yeah,\nthe mathematical community, plural,\nis an incredibly super intelligent entity\nthat no single human\nmathematician can come close\nto replicating.\nYou see it a little bit on\nthese question analysis sites.\nSo there's Math Overflow,\nwhich is the math version\nof Stack Overflow.\nAnd sometimes you get\nthese very quick responses\nto very difficult questions\nfrom the community\nand it's a pleasure to watch,\nactually as an expert.\n- I'm a fan spectator of that site.\nJust seeing the brilliance\nof the different people,\nthe depth of knowledge some people have,\nand the willingness to engage in the rigor\nand the nuance of the particular question.\nIt's pretty cool to watch.\nIt's almost like just fun to watch.\nWhat gives you hope\nabout this whole thing we have going on,\nhuman civilization?\n- I think the younger generation\nis always really creative\nand enthusiastic and inventive.\nIt's a pleasure working\nwith young students.\nThe progress of science tells\nus that the problems that used\nto be really difficult can\nbecome trivial to solve.\nI mean it was like navigation.\nJust knowing where you were\non the planet was this horrendous problem.\nPeople died or lost fortunes\nbecause they couldn't navigate.\nAnd we have devices\nin our pockets that do\nthis automatically for us.\nIt's a completely solved problem.\nSo things that seem unfeasible\nfor us now could be maybe\njust homework exercise\nsort of thing. (laughs)\n- Yeah, one of the\nthings I find really sad\nabout the finiteness of life\nis that I won't get\nto see all the cool things\nwe're to going create\nas a civilization, you know?\nThat because in the next\n100 years, 200 years,\njust imagine showing up in 200 years.\n- Yeah, well, already plenty has happened.\nYou know, like if you\ncould go back in time\nand talk to your teenage\nself or something.\nYou know what I mean?", "mimetype": "text/plain", "start_char_idx": 187131, "end_char_idx": 191181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61ece166-2a74-4098-9a20-8718fa98a0f4": {"__data__": {"id_": "61ece166-2a74-4098-9a20-8718fa98a0f4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e50c860-0859-4310-8a53-f13a2d30632f", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "3f765763855bb33373d21a6eac8a0c37de7dfd64f55c585645f4f87e80758e2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "649468f5-92f6-433a-91ba-f15a267f8eb1", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_name": "[English] Terence Tao_ Hardest Problems in Mathematics, Physics & the Future of AI _ Lex Fridman Podcast #472 [DownSub.com] (1).txt", "file_type": "text/plain", "file_size": 192982, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a06f7fb5991532277e8acda2dd9edc2f747dc28c5aaceef0cca28fe4be749d67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean it was like navigation.\nJust knowing where you were\non the planet was this horrendous problem.\nPeople died or lost fortunes\nbecause they couldn't navigate.\nAnd we have devices\nin our pockets that do\nthis automatically for us.\nIt's a completely solved problem.\nSo things that seem unfeasible\nfor us now could be maybe\njust homework exercise\nsort of thing. (laughs)\n- Yeah, one of the\nthings I find really sad\nabout the finiteness of life\nis that I won't get\nto see all the cool things\nwe're to going create\nas a civilization, you know?\nThat because in the next\n100 years, 200 years,\njust imagine showing up in 200 years.\n- Yeah, well, already plenty has happened.\nYou know, like if you\ncould go back in time\nand talk to your teenage\nself or something.\nYou know what I mean?\nJust the Internet and\nnow AI, I mean, again,\nthey're beginning to be\ninternalized and say, \"Yeah,\nof course an AI can understand our voice\nand give reasonable,\nslightly incorrect\nanswers to any question.\"\nBut yeah, this was mind-blowing\neven two years ago.\n- And in the moment,\nit's hilarious to watch\non the Internet and so on,\nthe drama, people take everything\nfor granted very quickly.\nAnd then we humans seem\nto entertain ourselves with drama.\nOut of anything that's created,\nsomebody needs to take one opinion,\nanother person needs to\ntake an opposite opinion,\nargue with each other about it.\nBut when you look at the\narc of things, I mean,\njust even in progress of robotics.\nJust to take a step back and be like, wow,\nthis is beautiful the way\nhumans are able to create this.\n- Yeah, when the infrastructure\nand the culture is healthy,\nthe community of humans can\nbe so much more intelligent\nand mature and rational than\nthe individuals within it.\n- Well, one place I can always count\non rationality is the\ncomments section of your blog,\nwhich I'm a big fan of.\nThere's a lot of really\nsmart people there.\nAnd thank you, of course,\nfor putting those ideas out on the blog.\nAnd I can't tell you how honored I am\nthat you would spend\nyour time with me today.\nI was looking forward\nto this for a long time.\nTerry, I'm a huge fan.\nYou inspire me.\nYou inspire millions of people.\nThank you so much for talking.\n- Thank you, it was a pleasure.\n- Thanks for listening\nto this conversation with Terence Tao.\nTo support this podcast,\nplease check out our sponsors\nin the description or at\nlexfridman.com/sponsors.\nAnd now let me leave you with some words\nfrom Galileo Galilei.\nMathematics is the language\nwith which God has written the universe.\nThank you for listening,\nand I hope to see you next time.", "mimetype": "text/plain", "start_char_idx": 190402, "end_char_idx": 192982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e527aa5-7b7a-4baf-b15a-0bd8016e8ccf": {"__data__": {"id_": "7e527aa5-7b7a-4baf-b15a-0bd8016e8ccf", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7394b63-132b-4609-95d2-c5346c5ba1e9", "node_type": "1", "metadata": {}, "hash": "6fed692af81bb4fa6184a5b10253e4f4bd245c9e12bead01ae947efd746b26b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- I see the danger of this\nconcentration of power\nthrough proprietary AI systems\nas a much bigger danger\nthan everything else.\nWhat works against this\nis people who think that\nfor reasons of security,\nwe should keep AI systems\nunder lock and key\nbecause it's too dangerous\nto put it in the hands of everybody.\nThat would lead to a very bad future\nin which all of our information diet\nis controlled by a small\nnumber of companies\nthrough proprietary systems.\n- I believe that people\nare fundamentally good\nand so if AI, especially open source AI\ncan make them smarter,\nit just empowers the goodness in humans.\n- So I share that feeling.\nOkay?\nI think people are\nfundamentally good. (laughing)\nAnd in fact a lot of doomers are doomers\nbecause they don't think that\npeople are fundamentally good.\n- The following is a\nconversation with Yann LeCun,\nhis third time on this podcast.\nHe is the chief AI scientist at Meta,\nprofessor at NYU,\nTuring Award winner\nand one of the seminal figures\nin the history of artificial intelligence.\nHe and Meta AI\nhave been big proponents of\nopen sourcing AI development,\nand have been walking the walk\nby open sourcing many\nof their biggest models,\nincluding LLaMA 2 and eventually LLaMA 3.\nAlso, Yann has been an outspoken critic\nof those people in the AI community\nwho warn about the looming danger\nand existential threat of AGI.\nHe believes the AGI\nwill be created one day,\nbut it will be good.\nIt will not escape human control\nnor will it dominate and kill all humans.\nAt this moment of rapid AI development,\nthis happens to be somewhat\na controversial position.\nAnd so it's been fun\nseeing Yann get into a lot of intense\nand fascinating discussions online\nas we do in this very conversation.\nThis is the Lex Fridman podcast.\nTo support it,\nplease check out our\nsponsors in the description.\nAnd now, dear friends, here's Yann LeCun.\nYou've had some strong statements,\ntechnical statements\nabout the future of artificial\nintelligence recently,\nthroughout your career\nactually but recently as well.\nYou've said that autoregressive LLMs\nare not the way we're\ngoing to make progress\ntowards superhuman intelligence.\nThese are the large language models\nlike GPT-4, like LLaMA\n2 and 3 soon and so on.\nHow do they work\nand why are they not going\nto take us all the way?\n- For a number of reasons.\nThe first is that there is\na number of characteristics\nof intelligent behavior.\nFor example, the capacity\nto understand the world,\nunderstand the physical world,\nthe ability to remember\nand retrieve things,\npersistent memory,\nthe ability to reason\nand the ability to plan.\nThose are four essential characteristic\nof intelligent systems or entities,\nhumans, animals.\nLLMs can do none of those,\nor they can only do them\nin a very primitive way.\nAnd they don't really\nunderstand the physical world,\nthey don't really have persistent memory,\nthey can't really reason\nand they certainly can't plan.\nAnd so if you expect the\nsystem to become intelligent\njust without having the\npossibility of doing those things,\nyou're making a mistake.\nThat is not to say that\nautoregressive LLMs are not useful,\nthey're certainly useful.\nThat they're not interesting,\nthat we can't build\na whole ecosystem of\napplications around them,\nof course we can.\nBut as a path towards\nhuman level intelligence,\nthey're missing essential components.\nAnd then there is another tidbit or fact\nthat I think is very interesting;\nthose LLMs are trained on\nenormous amounts of text.\nBasically the entirety\nof all publicly available\ntext on the internet, right?\nThat's typically on the\norder of 10 to the 13 tokens.\nEach token is typically two bytes.\nSo that's two 10 to the\n13 bytes as training data.\nIt would take you or me 170,000 years\nto just read through this at\neight hours a day. (laughs)\nSo it seems like an enormous\namount of knowledge, right?\nThat those systems can accumulate.\nBut then you realize it's\nreally not that much data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7394b63-132b-4609-95d2-c5346c5ba1e9": {"__data__": {"id_": "b7394b63-132b-4609-95d2-c5346c5ba1e9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e527aa5-7b7a-4baf-b15a-0bd8016e8ccf", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "41cfb32038ab2e701e9a5ee7c9d39d2de846bbc489090e87076a34bb227c347b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "452ab813-7a58-4442-bd6c-83a15b3782fe", "node_type": "1", "metadata": {}, "hash": "0f4af3b4ecbff5fe5185b828c70fec310c0733cb2f5f1127705ecd13b5657ee4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That they're not interesting,\nthat we can't build\na whole ecosystem of\napplications around them,\nof course we can.\nBut as a path towards\nhuman level intelligence,\nthey're missing essential components.\nAnd then there is another tidbit or fact\nthat I think is very interesting;\nthose LLMs are trained on\nenormous amounts of text.\nBasically the entirety\nof all publicly available\ntext on the internet, right?\nThat's typically on the\norder of 10 to the 13 tokens.\nEach token is typically two bytes.\nSo that's two 10 to the\n13 bytes as training data.\nIt would take you or me 170,000 years\nto just read through this at\neight hours a day. (laughs)\nSo it seems like an enormous\namount of knowledge, right?\nThat those systems can accumulate.\nBut then you realize it's\nreally not that much data.\nIf you talk to\ndevelopmental psychologists,\nand they tell you a 4-year-old\nhas been awake for 16,000\nhours in his or her life,\nand the amount of information\nthat has reached the\nvisual cortex of that child\nin four years\nis about 10 to 15 bytes.\nAnd you can compute this\nby estimating that the optical nerve\ncarry about 20 megabytes\nper second, roughly.\nAnd so 10 to the 15 bytes for a 4-year-old\nversus two times 10 to the 13 bytes\nfor 170,000 years worth of reading.\nWhat that tells you is\nthat through sensory input,\nwe see a lot more information\nthan we do through language.\nAnd that despite our intuition,\nmost of what we learn\nand most of our knowledge\nis through our observation and interaction\nwith the real world,\nnot through language.\nEverything that we learn in\nthe first few years of life,\nand certainly everything\nthat animals learn\nhas nothing to do with language.\n- So it would be good\nto maybe push against\nsome of the intuition\nbehind what you're saying.\nSo it is true there's\nseveral orders of magnitude\nmore data coming into the\nhuman mind, much faster,\nand the human mind is able to\nlearn very quickly from that,\nfilter the data very quickly.\nSomebody might argue\nyour comparison between\nsensory data versus language.\nThat language is already very compressed.\nIt already contains a lot more information\nthan the bytes it takes to store them,\nif you compare it to visual data.\nSo there's a lot of wisdom in language.\nThere's words and the way\nwe stitch them together,\nit already contains a lot of information.\nSo is it possible that language alone\nalready has enough wisdom\nand knowledge in there\nto be able to, from that\nlanguage construct a world model\nand understanding of the world,\nan understanding of the physical world\nthat you're saying LLMs lack?\n- So it's a big debate among philosophers\nand also cognitive scientists,\nlike whether intelligence needs\nto be grounded in reality.\nI'm clearly in the camp\nthat yes, intelligence cannot appear\nwithout some grounding in some reality.\nIt doesn't need to be physical reality,\nit could be simulated\nbut the environment is just much richer\nthan what you can express in language.\nLanguage is a very approximate\nrepresentation or percepts\nand or mental models, right?\nI mean, there's a lot of\ntasks that we accomplish\nwhere we manipulate a mental\nmodel of the situation at hand,\nand that has nothing to do with language.\nEverything that's physical,\nmechanical, whatever,\nwhen we build something,\nwhen we accomplish a task,\na moderate task of grabbing\nsomething, et cetera,\nwe plan our action sequences,\nand we do this\nby essentially imagining the result\nof the outcome of sequence of\nactions that we might imagine.\nAnd that requires mental models\nthat don't have much to do with language.\nAnd that's, I would argue,\nmost of our knowledge\nis derived from that interaction\nwith the physical world.\nSo a lot of my colleagues\nwho are more interested in\nthings like computer vision\nare really on that camp\nthat AI needs to be embodied, essentially.\nAnd then other people\ncoming from the NLP side\nor maybe some other motivation\ndon't necessarily agree with that.\nAnd philosophers are split as well.\nAnd the complexity of the\nworld is hard to imagine.\nIt's hard to represent\nall the complexities\nthat we take completely for\ngranted in the real world\nthat we don't even imagine\nrequire intelligence, right?", "mimetype": "text/plain", "start_char_idx": 3141, "end_char_idx": 7292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "452ab813-7a58-4442-bd6c-83a15b3782fe": {"__data__": {"id_": "452ab813-7a58-4442-bd6c-83a15b3782fe", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7394b63-132b-4609-95d2-c5346c5ba1e9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "5c106a66a87d65c008da993395636abc7c4b285f44270c2f851d9df2dbe0725f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d06beb7f-07d2-40fe-a69a-f320e027778f", "node_type": "1", "metadata": {}, "hash": "57c952ec40e93ac9cc373a7871ef582ccbdef202aa26ab9b59cd7275dacf076c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And that requires mental models\nthat don't have much to do with language.\nAnd that's, I would argue,\nmost of our knowledge\nis derived from that interaction\nwith the physical world.\nSo a lot of my colleagues\nwho are more interested in\nthings like computer vision\nare really on that camp\nthat AI needs to be embodied, essentially.\nAnd then other people\ncoming from the NLP side\nor maybe some other motivation\ndon't necessarily agree with that.\nAnd philosophers are split as well.\nAnd the complexity of the\nworld is hard to imagine.\nIt's hard to represent\nall the complexities\nthat we take completely for\ngranted in the real world\nthat we don't even imagine\nrequire intelligence, right?\nThis is the old Moravec's paradox\nfrom the pioneer of\nrobotics, Hans Moravec,\nwho said, how is it that with computers,\nit seems to be easy to do\nhigh level complex tasks\nlike playing chess and solving integrals\nand doing things like that,\nwhereas the thing we take for\ngranted that we do every day,\nlike, I don't know,\nlearning to drive a car\nor grabbing an object,\nwe can't do with computers. (laughs)\nAnd we have LLMs that\ncan pass the bar exam,\nso they must be smart.\nBut then they can't\nlaunch a drive in 20 hours\nlike any 17-year-old.\nThey can't learn to clear\nout the dinner table\nand fill out the dishwasher\nlike any 10-year-old\ncan learn in one shot.\nWhy is that?\nLike what are we missing?\nWhat type of learning\nor reasoning architecture\nor whatever are we missing\nthat basically prevent us\nfrom having level five self-driving cars\nand domestic robots?\n- Can a large language model\nconstruct a world model\nthat does know how to drive\nand does know how to fill a dishwasher,\nbut just doesn't know\nhow to deal with visual data at this time?\nSo it can operate in a space of concepts.\n- So yeah, that's what a lot\nof people are working on.\nSo the answer,\nthe short answer is no.\nAnd the more complex answer is\nyou can use all kind of tricks\nto get an LLM to basically\ndigest visual representations\nof images or video or\naudio for that matter.\nAnd a classical way of doing this\nis you train a vision system in some way,\nand we have a number of ways\nto train vision systems,\neither supervised,\nunsupervised, self-supervised,\nall kinds of different ways.\nThat will turn any image into\na high level representation.\nBasically, a list of tokens\nthat are really similar\nto the kind of tokens\nthat a typical LLM takes as an input.\nAnd then you just feed that to the LLM\nin addition to the text,\nand you just expect\nthe LLM during training\nto kind of be able to\nuse those representations\nto help make decisions.\nI mean, there's been\nwork along those lines\nfor quite a long time.\nAnd now you see those systems, right?\nI mean, there are LLMs that\nhave some vision extension.\nBut they're basically hacks\nin the sense that those things\nare not like trained to handle,\nto really understand the world.\nThey're not trained\nwith video, for example.\nThey don't really understand\nintuitive physics,\nat least not at the moment.\n- So you don't think\nthere's something special to\nyou about intuitive physics,\nabout sort of common sense reasoning\nabout the physical space,\nabout physical reality?\nThat to you is a giant leap\nthat LLMs are just not able to do?\n- We're not gonna be able to do this\nwith the type of LLMs that\nwe are working with today.\nAnd there's a number of reasons for this,\nbut the main reason is\nthe way LLMs are trained is\nthat you take a piece of text,\nyou remove some of the words\nin that text, you mask them,\nyou replace them by black markers,\nand you train a gigantic neural net\nto predict the words that are missing.\nAnd if you build this neural\nnet in a particular way\nso that it can only look at words\nthat are to the left of the\none it's trying to predict,\nthen what you have is a system\nthat basically is trying to predict\nthe next word in a text, right?\nSo then you can feed it a text, a prompt,\nand you can ask it to\npredict the next word.\nIt can never predict\nthe next word exactly.", "mimetype": "text/plain", "start_char_idx": 6609, "end_char_idx": 10589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d06beb7f-07d2-40fe-a69a-f320e027778f": {"__data__": {"id_": "d06beb7f-07d2-40fe-a69a-f320e027778f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "452ab813-7a58-4442-bd6c-83a15b3782fe", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e7eee594e51a1dfa3fa35bdf149f6f1f15a15ed34f2185bb132d62eb13644f73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cb5d930-be84-4bc0-8756-283e5aa61291", "node_type": "1", "metadata": {}, "hash": "3ca9f9a939c56df99e91eb83c49f347904338536ecb91534f0662279a05c7e5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- We're not gonna be able to do this\nwith the type of LLMs that\nwe are working with today.\nAnd there's a number of reasons for this,\nbut the main reason is\nthe way LLMs are trained is\nthat you take a piece of text,\nyou remove some of the words\nin that text, you mask them,\nyou replace them by black markers,\nand you train a gigantic neural net\nto predict the words that are missing.\nAnd if you build this neural\nnet in a particular way\nso that it can only look at words\nthat are to the left of the\none it's trying to predict,\nthen what you have is a system\nthat basically is trying to predict\nthe next word in a text, right?\nSo then you can feed it a text, a prompt,\nand you can ask it to\npredict the next word.\nIt can never predict\nthe next word exactly.\nAnd so what it's gonna do\nis produce a probability distribution\nof all the possible\nwords in the dictionary.\nIn fact, it doesn't predict words,\nit predicts tokens that\nare kind of subword units.\nAnd so it's easy to handle the uncertainty\nin the prediction there\nbecause there's only a finite number\nof possible words in the dictionary,\nand you can just compute\na distribution over them.\nThen what the system does\nis that it picks a word\nfrom that distribution.\nOf course, there's a higher\nchance of picking words\nthat have a higher probability\nwithin that distribution.\nSo you sample from that distribution\nto actually produce a word,\nand then you shift that\nword into the input.\nAnd so that allows the system now\nto predict the second word, right?\nAnd once you do this,\nyou shift it into the input, et cetera.\nThat's called autoregressive prediction,\nwhich is why those LLMs\nshould be called autoregressive LLMs,\nbut we just call them at LLMs.\nAnd there is a difference\nbetween this kind of process\nand a process by which\nbefore producing a word,\nwhen you talk.\nWhen you and I talk,\nyou and I are bilinguals.\nWe think about what we're gonna say,\nand it's relatively independent\nof the language in which we're gonna say.\nWhen we talk about like, I don't know,\nlet's say a mathematical\nconcept or something.\nThe kind of thinking that we're doing\nand the answer that\nwe're planning to produce\nis not linked to whether\nwe're gonna say it\nin French or Russian or English.\n- Chomsky just rolled his\neyes, but I understand.\nSo you're saying that\nthere's a bigger abstraction\nthat goes before language-\n- [Yann] Yeah.\n- And maps onto language.\n- Right.\nIt's certainly true for a\nlot of thinking that we do.\n- Is that obvious that we don't?\nLike you're saying your\nthinking is same in French\nas it is in English?\n- Yeah, pretty much.\n- Pretty much or is this...\nLike how flexible are you,\nlike if there's a\nprobability distribution?\n(both laugh)\n- Well, it depends what\nkind of thinking, right?\nIf it's like producing puns,\nI get much better in French\nthan English about that (laughs)\nor much worse-\n- Is there an abstract\nrepresentation of puns?\nLike is your humor an abstract...\nLike when you tweet\nand your tweets are\nsometimes a little bit spicy,\nis there an abstract representation\nin your brain of a tweet\nbefore it maps onto English?\n- There is an abstract representation\nof imagining the reaction\nof a reader to that text.\n- Oh, you start with laughter\nand then figure out how\nto make that happen?\n- Figure out like a\nreaction you wanna cause\nand then figure out how to say it\nso that it causes that reaction.\nBut that's like really close to language.\nBut think about like\na mathematical concept\nor imagining something you\nwant to build out of wood\nor something like this, right?\nThe kind of thinking you're doing\nhas absolutely nothing to\ndo with language, really.\nLike it's not like you have necessarily\nlike an internal monologue\nin any particular language.\nYou're imagining mental\nmodels of the thing, right?\nI mean, if I ask you to like imagine\nwhat this water bottle will look like\nif I rotate it 90 degrees,\nthat has nothing to do with language.", "mimetype": "text/plain", "start_char_idx": 9834, "end_char_idx": 13742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cb5d930-be84-4bc0-8756-283e5aa61291": {"__data__": {"id_": "1cb5d930-be84-4bc0-8756-283e5aa61291", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d06beb7f-07d2-40fe-a69a-f320e027778f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "1325117a7921df04bda839a8d5c9a96b1bc171f3e7e0755cca2223f6762312ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f", "node_type": "1", "metadata": {}, "hash": "6d697869234d1e789b5281cc8895a681d96d32495b0c38c2c6f568402eadc302", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- There is an abstract representation\nof imagining the reaction\nof a reader to that text.\n- Oh, you start with laughter\nand then figure out how\nto make that happen?\n- Figure out like a\nreaction you wanna cause\nand then figure out how to say it\nso that it causes that reaction.\nBut that's like really close to language.\nBut think about like\na mathematical concept\nor imagining something you\nwant to build out of wood\nor something like this, right?\nThe kind of thinking you're doing\nhas absolutely nothing to\ndo with language, really.\nLike it's not like you have necessarily\nlike an internal monologue\nin any particular language.\nYou're imagining mental\nmodels of the thing, right?\nI mean, if I ask you to like imagine\nwhat this water bottle will look like\nif I rotate it 90 degrees,\nthat has nothing to do with language.\nAnd so clearly\nthere is a more abstract\nlevel of representation\nin which we do most of our thinking\nand we plan what we're gonna say\nif the output is uttered words\nas opposed to an output\nbeing muscle actions, right?\nWe plan our answer before we produce it.\nAnd LLMs don't do that,\nthey just produce one\nword after the other,\ninstinctively if you want.\nIt's a bit like the subconscious\nactions where you don't...\nLike you're distracted.\nYou're doing something,\nyou're completely concentrated\nand someone comes to you\nand asks you a question.\nAnd you kind of answer the question.\nYou don't have time to\nthink about the answer,\nbut the answer is easy\nso you don't need to pay attention\nand you sort of respond automatically.\nThat's kind of what an LLM does, right?\nIt doesn't think about its answer, really.\nIt retrieves it because it's\naccumulated a lot of knowledge,\nso it can retrieve some things,\nbut it's going to just spit\nout one token after the other\nwithout planning the answer.\n- But you're making it sound\njust one token after the other,\none token at a time generation\nis bound to be simplistic.\nBut if the world model is\nsufficiently sophisticated,\nthat one token at a time,\nthe most likely thing it\ngenerates as a sequence of tokens\nis going to be a deeply profound thing.\n- Okay.\nBut then that assumes that those systems\nactually possess an internal world model.\n- So it really goes to the...\nI think the fundamental question is\ncan you build a really\ncomplete world model?\nNot complete,\nbut one that has a deep\nunderstanding of the world.\n- Yeah.\nSo can you build this\nfirst of all by prediction?\n- [Lex] Right.\n- And the answer is probably yes.\nCan you build it by predicting words?\nAnd the answer is most probably no,\nbecause language is\nvery poor in terms of...\nOr weak or low bandwidth if you want,\nthere's just not enough information there.\nSo building world models\nmeans observing the world\nand understanding why the world\nis evolving the way it is.\nAnd then the extra\ncomponent of a world model\nis something that can predict\nhow the world is going to evolve\nas a consequence of an\naction you might take, right?\nSo one model really is,\nhere is my idea of the state\nof the world at time T,\nhere is an action I might take.\nWhat is the predicted state of the world\nat time T plus one?\nNow, that state of the world\ndoes not need to represent\neverything about the world,\nit just needs to represent\nenough that's relevant for\nthis planning of the action,\nbut not necessarily all the details.\nNow, here is the problem.\nYou're not going to be able to do this\nwith generative models.\nSo a generative model\nthat's trained on video,\nand we've tried to do this for 10 years.\nYou take a video,\nshow a system a piece of video\nand then ask you to predict\nthe reminder of the video.\nBasically predict what's gonna happen.\n- One frame at a time.\nDo the same thing as sort of\nthe autoregressive LLMs do,\nbut for video.\n- Right.\nEither one frame at a time or\na group of frames at a time.\nBut yeah, a large video\nmodel, if you want. (laughing)\nThe idea of doing this\nhas been floating around for a long time.\nAnd at FAIR,\nsome colleagues and I\nhave been trying to do\nthis for about 10 years.", "mimetype": "text/plain", "start_char_idx": 12923, "end_char_idx": 16933, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f": {"__data__": {"id_": "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cb5d930-be84-4bc0-8756-283e5aa61291", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8a479f0e2a208bc0987b90db381157271a419b5b0fc3414e7af1205b5015296b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7", "node_type": "1", "metadata": {}, "hash": "e622dd8b5c479266942097f40b8273719d4c84dac5eb5510dc3f3b7ac7a04a7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, here is the problem.\nYou're not going to be able to do this\nwith generative models.\nSo a generative model\nthat's trained on video,\nand we've tried to do this for 10 years.\nYou take a video,\nshow a system a piece of video\nand then ask you to predict\nthe reminder of the video.\nBasically predict what's gonna happen.\n- One frame at a time.\nDo the same thing as sort of\nthe autoregressive LLMs do,\nbut for video.\n- Right.\nEither one frame at a time or\na group of frames at a time.\nBut yeah, a large video\nmodel, if you want. (laughing)\nThe idea of doing this\nhas been floating around for a long time.\nAnd at FAIR,\nsome colleagues and I\nhave been trying to do\nthis for about 10 years.\nAnd you can't really do the\nsame trick as with LLMs,\nbecause LLMs, as I said,\nyou can't predict exactly\nwhich word is gonna follow\na sequence of words,\nbut you can predict the\ndistribution of the words.\nNow, if you go to video,\nwhat you would have to do\nis predict the distribution\nof all possible frames in a video.\nAnd we don't really know\nhow to do that properly.\nWe do not know how to\nrepresent distributions\nover high dimensional continuous spaces\nin ways that are useful.\nAnd there lies the main issue.\nAnd the reason we can do this\nis because the world\nis incredibly more complicated and richer\nin terms of information than text.\nText is discreet.\nVideo is high dimensional and continuous.\nA lot of details in this.\nSo if I take a video of this room,\nand the video is a camera panning around,\nthere is no way I can predict\neverything that's gonna be\nin the room as I pan around,\nthe system cannot predict\nwhat's gonna be in the room\nas the camera is panning.\nMaybe it's gonna predict,\nthis is a room where there's\na light and there is a wall\nand things like that.\nIt can't predict what the\npainting of the wall looks like\nor what the texture of\nthe couch looks like.\nCertainly not the texture of the carpet.\nSo there's no way it can\npredict all those details.\nSo the way to handle this\nis one way to possibly to handle this,\nwhich we've been working for a long time,\nis to have a model that has\nwhat's called a latent variable.\nAnd the latent variable\nis fed to a neural net,\nand it's supposed to represent\nall the information about the world\nthat you don't perceive yet.\nAnd that you need to augment the system\nfor the prediction to do a\ngood job at predicting pixels,\nincluding the fine texture\nof the carpet and the couch\nand the painting on the wall.\nThat has been a complete\nfailure, essentially.\nAnd we've tried lots of things.\nWe tried just straight neural nets,\nwe tried GANs,\nwe tried VAEs,\nall kinds of regularized auto encoders,\nwe tried many things.\nWe also tried those kind of methods\nto learn good representations\nof images or video\nthat could then be used as input\nfor example, an image\nclassification system.\nAnd that also has basically failed.\nLike all the systems that\nattempt to predict missing parts\nof an image or a video\nfrom a corrupted version of it, basically.\nSo, right, take an image or a video,\ncorrupt it or transform it in some way,\nand then try to reconstruct\nthe complete video or image\nfrom the corrupted version.\nAnd then hope that internally,\nthe system will develop good\nrepresentations of images\nthat you can use for object recognition,\nsegmentation, whatever it is.\nThat has been essentially\na complete failure.\nAnd it works really well for text.\nThat's the principle that\nis used for LLMs, right?\n- So where's the failure exactly?\nIs it that it is very difficult to form\na good representation of an image,\nlike a good embedding\nof all the important\ninformation in the image?\nIs it in terms of the consistency\nof image to image to image to\nimage that forms the video?\nIf we do a highlight reel\nof all the ways you failed.\nWhat's that look like?\n- Okay.\nSo the reason this doesn't work is...\nFirst of all, I have to tell\nyou exactly what doesn't work\nbecause there is something\nelse that does work.", "mimetype": "text/plain", "start_char_idx": 16248, "end_char_idx": 20176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7": {"__data__": {"id_": "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7c1ec7f-7b6d-4b4f-a97a-02b0533d1b9f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "710c074b28175fecfe365ce629fc6ddd63bef4c56887f390d60fcddd2de9a035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b3ec38a-82d6-4146-a8e0-5e74b04234a8", "node_type": "1", "metadata": {}, "hash": "c58f8c9355abba3ed06eb50417d45f0aae21cc850794be07c44b687fd6768428", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then hope that internally,\nthe system will develop good\nrepresentations of images\nthat you can use for object recognition,\nsegmentation, whatever it is.\nThat has been essentially\na complete failure.\nAnd it works really well for text.\nThat's the principle that\nis used for LLMs, right?\n- So where's the failure exactly?\nIs it that it is very difficult to form\na good representation of an image,\nlike a good embedding\nof all the important\ninformation in the image?\nIs it in terms of the consistency\nof image to image to image to\nimage that forms the video?\nIf we do a highlight reel\nof all the ways you failed.\nWhat's that look like?\n- Okay.\nSo the reason this doesn't work is...\nFirst of all, I have to tell\nyou exactly what doesn't work\nbecause there is something\nelse that does work.\nSo the thing that does not work\nis training the system to\nlearn representations of images\nby training it to reconstruct a good image\nfrom a corrupted version of it.\nOkay.\nThat's what doesn't work.\nAnd we have a whole slew\nof techniques for this\nthat are variant of then\nusing auto encoders.\nSomething called MAE,\ndeveloped by some of\nmy colleagues at FAIR,\nmasked autoencoder.\nSo it's basically like the\nLLMs or things like this\nwhere you train the\nsystem by corrupting text,\nexcept you corrupt images.\nYou remove patches from it\nand you train a gigantic\nneural network to reconstruct.\nThe features you get are not good.\nAnd you know they're not good\nbecause if you now train\nthe same architecture,\nbut you train it to\nsupervise with label data,\nwith textual descriptions\nof images, et cetera,\nyou do get good representations.\nAnd the performance on\nrecognition tasks is much better\nthan if you do this self\nsupervised free training.\n- So the architecture is good.\n- The architecture is good.\nThe architecture of the encoder is good.\nOkay?\nBut the fact that you train the\nsystem to reconstruct images\ndoes not lead it to produce\nlong good generic features of images.\n- [Lex] When you train it\nin a self supervised way.\n- Self supervised by reconstruction.\n- [Lex] Yeah, by reconstruction.\n- Okay, so what's the alternative?\n(both laugh)\nThe alternative is joint embedding.\n- What is joint embedding?\nWhat are these architectures\nthat you're so excited about?\n- Okay, so now instead\nof training a system\nto encode the image\nand then training it to\nreconstruct the full image\nfrom a corrupted version,\nyou take the full image,\nyou take the corrupted\nor transformed version,\nyou run them both through encoders,\nwhich in general are\nidentical but not necessarily.\nAnd then you train a predictor\non top of those encoders\nto predict the representation\nof the full input\nfrom the representation\nof the corrupted one.\nOkay?\nSo joint embedding,\nbecause you're taking the full input\nand the corrupted version\nor transformed version,\nrun them both through encoders\nso you get a joint embedding.\nAnd then you're saying\ncan I predict the\nrepresentation of the full one\nfrom the representation\nof the corrupted one?\nOkay?\nAnd I call this a JEPA,\nso that means joint embedding\npredictive architecture\nbecause there's joint embedding\nand there is this predictor\nthat predicts the representation\nof the good guy from the bad guy.\nAnd the big question is\nhow do you train something like this?\nAnd until five years ago or six years ago,\nwe didn't have particularly good answers\nfor how you train those things,\nexcept for one called\ncontrastive learning.\nAnd the idea of contrastive learning\nis you take a pair of images\nthat are, again, an image\nand a corrupted version\nor degraded version somehow\nor transformed version\nof the original one.\nAnd you train the predicted representation\nto be the same as that.\nIf you only do this,\nthis system collapses.\nIt basically completely ignores the input\nand produces representations\nthat are constant.\nSo the contrastive methods avoid this.\nAnd those things have been\naround since the early '90s,\nI had a paper on this in 1993,\nis you also show pairs of images\nthat you know are different\nand then you push away the\nrepresentations from each other.\nSo you say not only do\nrepresentations of things\nthat we know are the same,\nshould be the same or should be similar,\nbut representation of things\nthat we know are different\nshould be different.\nAnd that prevents the collapse,\nbut it has some limitation.", "mimetype": "text/plain", "start_char_idx": 19388, "end_char_idx": 23701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7b3ec38a-82d6-4146-a8e0-5e74b04234a8": {"__data__": {"id_": "7b3ec38a-82d6-4146-a8e0-5e74b04234a8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab103f60-0acd-4606-b3cd-3ff6db6dc7a7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "86857c6534e1e049f933a8f8c62497081dd2920020d0f8df1dcf2eddea892d40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8", "node_type": "1", "metadata": {}, "hash": "7f87e4c0d53f0cd8940219b9e8728d00874b4d0fdaa6a5cb68160330f3e610bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the idea of contrastive learning\nis you take a pair of images\nthat are, again, an image\nand a corrupted version\nor degraded version somehow\nor transformed version\nof the original one.\nAnd you train the predicted representation\nto be the same as that.\nIf you only do this,\nthis system collapses.\nIt basically completely ignores the input\nand produces representations\nthat are constant.\nSo the contrastive methods avoid this.\nAnd those things have been\naround since the early '90s,\nI had a paper on this in 1993,\nis you also show pairs of images\nthat you know are different\nand then you push away the\nrepresentations from each other.\nSo you say not only do\nrepresentations of things\nthat we know are the same,\nshould be the same or should be similar,\nbut representation of things\nthat we know are different\nshould be different.\nAnd that prevents the collapse,\nbut it has some limitation.\nAnd there's a whole bunch of techniques\nthat have appeared over\nthe last six, seven years\nthat can revive this type of method.\nSome of them from FAIR,\nsome of them from Google and other places.\nBut there are limitations to\nthose contrastive methods.\nWhat has changed in the\nlast three, four years\nis now we have methods\nthat are non-contrastive.\nSo they don't require those\nnegative contrastive samples\nof images that we know are different.\nYou train them only with images\nthat are different versions\nor different views of the same thing.\nAnd you rely on some other tweaks\nto prevent the system from collapsing.\nAnd we have half a dozen\ndifferent methods for this now.\n- So what is the fundamental difference\nbetween joint embedding\narchitectures and LLMs?\nSo can JEPA take us to AGI?\nWhether we should say that\nyou don't like the term AGI\nand we'll probably argue,\nI think every single\ntime I've talked to you\nwe've argued about the G in AGI.\n- [Yann] Yes.\n- I get it, I get it, I get it. (laughing)\nWell we'll probably\ncontinue to argue about it.\nIt's great.\nBecause you're like French,\nand ami is I guess friend in French-\n- [Yann] Yes.\n- And AMI stands for advanced\nmachine intelligence-\n- [Yann] Right.\n- But either way, can\nJEPA take us to that,\ntowards that advanced\nmachine intelligence?\n- Well, so it's a first step.\nOkay?\nSo first of all, what's the difference\nwith generative architectures like LLMs?\nSo LLMs or vision systems that\nare trained by reconstruction\ngenerate the inputs, right?\nThey generate the original input\nthat is non-corrupted,\nnon-transformed, right?\nSo you have to predict all the pixels.\nAnd there is a huge amount of\nresources spent in the system\nto actually predict all those\npixels, all the details.\nIn a JEPA, you're not trying\nto predict all the pixels,\nyou're only trying to predict\nan abstract representation\nof the inputs, right?\nAnd that's much easier in many ways.\nSo what the JEPA system\nwhen it's being trained is trying to do,\nis extract as much information\nas possible from the input,\nbut yet only extract information\nthat is relatively easily predictable.\nOkay.\nSo there's a lot of things in the world\nthat we cannot predict.\nLike for example, if you\nhave a self driving car\ndriving down the street or road.\nThere may be trees around the road.\nAnd it could be a windy day,\nso the leaves on the\ntree are kind of moving\nin kind of semi chaotic random ways\nthat you can't predict and you don't care,\nyou don't want to predict.\nSo what you want is your encoder\nto basically eliminate all those details.\nIt'll tell you there's moving leaves,\nbut it's not gonna keep the details\nof exactly what's going on.\nAnd so when you do the prediction\nin representation space,\nyou're not going to have to predict\nevery single pixel of every leaf.\nAnd that not only is a lot simpler,\nbut also it allows the system\nto essentially learn an abstract\nrepresentation of the world\nwhere what can be modeled\nand predicted is preserved\nand the rest is viewed as noise\nand eliminated by the encoder.\nSo it kind of lifts the\nlevel of abstraction\nof the representation.\nIf you think about this,\nthis is something we do\nabsolutely all the time.\nWhenever we describe a phenomenon,\nwe describe it at a particular\nlevel of abstraction.", "mimetype": "text/plain", "start_char_idx": 22812, "end_char_idx": 26951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8": {"__data__": {"id_": "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b3ec38a-82d6-4146-a8e0-5e74b04234a8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "827e73ef9b566e7bad8a8c2cc002cd1667b2c8e54af45f76f2e09926276d79fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f70e098-3d4f-401c-a2d8-5d54e4cd0639", "node_type": "1", "metadata": {}, "hash": "62bb212fcaa72275303ab1a72eac9e298355b6376ebdd6b3c744425f822ff34a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So what you want is your encoder\nto basically eliminate all those details.\nIt'll tell you there's moving leaves,\nbut it's not gonna keep the details\nof exactly what's going on.\nAnd so when you do the prediction\nin representation space,\nyou're not going to have to predict\nevery single pixel of every leaf.\nAnd that not only is a lot simpler,\nbut also it allows the system\nto essentially learn an abstract\nrepresentation of the world\nwhere what can be modeled\nand predicted is preserved\nand the rest is viewed as noise\nand eliminated by the encoder.\nSo it kind of lifts the\nlevel of abstraction\nof the representation.\nIf you think about this,\nthis is something we do\nabsolutely all the time.\nWhenever we describe a phenomenon,\nwe describe it at a particular\nlevel of abstraction.\nAnd we don't always describe\nevery natural phenomenon\nin terms of quantum field theory, right?\nThat would be impossible, right?\nSo we have multiple levels of abstraction\nto describe what happens in the world.\nStarting from quantum field theory\nto like atomic theory and\nmolecules in chemistry,\nmaterials,\nall the way up to kind of\nconcrete objects in the real world\nand things like that.\nSo we can't just only model\neverything at the lowest level.\nAnd that's what the idea\nof JEPA is really about.\nLearn abstract representation\nin a self supervised manner.\nAnd you can do it hierarchically as well.\nSo that I think is an essential component\nof an intelligent system.\nAnd in language, we can\nget away without doing this\nbecause language is already\nto some level abstract\nand already has eliminated\na lot of information\nthat is not predictable.\nAnd so we can get away without\ndoing the joint embedding,\nwithout lifting the abstraction level\nand by directly predicting words.\n- So joint embedding.\nIt's still generative,\nbut it's generative in this\nabstract representation space.\n- [Yann] Yeah.\n- And you're saying language,\nwe were lazy with language\n'cause we already got the\nabstract representation for free\nand now we have to zoom out,\nactually think about\ngenerally intelligent systems,\nwe have to deal with the full mess\nof physical of reality, of reality.\nAnd you do have to do this step\nof jumping from the full,\nrich, detailed reality\nto an abstract representation\nof that reality\nbased on what you can then reason\nand all that kind of stuff.\n- Right.\nAnd the thing is those\nself supervised algorithms\nthat learn by prediction,\neven in representation space,\nthey learn more concept\nif the input data you feed\nthem is more redundant.\nThe more redundancy there is in the data,\nthe more they're able to capture\nsome internal structure of it.\nAnd so there,\nthere is way more\nredundancy in the structure\nin perceptual inputs,\nsensory input like vision,\nthan there is in text,\nwhich is not nearly as redundant.\nThis is back to the\nquestion you were asking\na few minutes ago.\nLanguage might represent\nmore information really\nbecause it's already compressed,\nyou're right about that.\nBut that means it's also less redundant.\nAnd so self supervised\nonly will not work as well.\n- Is it possible to join\nthe self supervised\ntraining on visual data\nand self supervised\ntraining on language data?\nThere is a huge amount of knowledge\neven though you talk down about\nthose 10 to the 13 tokens.\nThose 10 to the 13 tokens\nrepresent the entirety,\na large fraction of what\nus humans have figured out.\nBoth the shit talk on Reddit\nand the contents of all\nthe books and the articles\nand the full spectrum of\nhuman intellectual creation.\nSo is it possible to\njoin those two together?\n- Well, eventually, yes,\nbut I think if we do this too early,\nwe run the risk of being tempted to cheat.\nAnd in fact, that's what\npeople are doing at the moment\nwith vision language model.\nWe're basically cheating.\nWe are using language as a crutch\nto help the deficiencies\nof our vision systems\nto kind of learn good representations\nfrom images and video.\nAnd the problem with this\nis that we might improve our\nvision language system a bit,\nI mean our language models\nby feeding them images.\nBut we're not gonna get to the level\nof even the intelligence\nor level of understanding of the world\nof a cat or a dog which\ndoesn't have language.\nThey don't have language\nand they understand the world\nmuch better than any LLM.", "mimetype": "text/plain", "start_char_idx": 26173, "end_char_idx": 30441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f70e098-3d4f-401c-a2d8-5d54e4cd0639": {"__data__": {"id_": "8f70e098-3d4f-401c-a2d8-5d54e4cd0639", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a67b99e0-d86b-4f5c-8cdd-d7ae2cb77cc8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "723a4d268c8a9234498cc7e3d0a305bb15908e5ca1d7a1ac4d7451067a29b76c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8", "node_type": "1", "metadata": {}, "hash": "fd1f74d33c39eb97a3a47a80bdc9686fb16d65fd6599154a7b0bc640eb570c06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So is it possible to\njoin those two together?\n- Well, eventually, yes,\nbut I think if we do this too early,\nwe run the risk of being tempted to cheat.\nAnd in fact, that's what\npeople are doing at the moment\nwith vision language model.\nWe're basically cheating.\nWe are using language as a crutch\nto help the deficiencies\nof our vision systems\nto kind of learn good representations\nfrom images and video.\nAnd the problem with this\nis that we might improve our\nvision language system a bit,\nI mean our language models\nby feeding them images.\nBut we're not gonna get to the level\nof even the intelligence\nor level of understanding of the world\nof a cat or a dog which\ndoesn't have language.\nThey don't have language\nand they understand the world\nmuch better than any LLM.\nThey can plan really complex actions\nand sort of imagine the\nresult of a bunch of actions.\nHow do we get machines to learn that\nbefore we combine that with language?\nObviously, if we combine\nthis with language,\nthis is gonna be a winner,\nbut before that we have to focus\non like how do we get systems\nto learn how the world works?\n- So this kind of joint embedding\npredictive architecture,\nfor you, that's gonna be able to learn\nsomething like common sense,\nsomething like what a cat uses\nto predict how to mess with\nits owner most optimally\nby knocking over a thing.\n- That's the hope.\nIn fact, the techniques we're\nusing are non-contrastive.\nSo not only is the\narchitecture non-generative,\nthe learning procedures we're\nusing are non-contrastive.\nWe have two sets of techniques.\nOne set is based on distillation\nand there's a number of methods\nthat use this principle.\nOne by DeepMind called BYOL.\nA couple by FAIR,\none called VICReg and\nanother one called I-JEPA.\nAnd VICReg, I should say,\nis not a distillation method actually,\nbut I-JEPA and BYOL certainly are.\nAnd there's another one\nalso called DINO or Dino,\nalso produced at FAIR.\nAnd the idea of those things\nis that you take the full\ninput, let's say an image.\nYou run it through an encoder,\nproduces a representation.\nAnd then you corrupt that\ninput or transform it,\nrun it through essentially what\namounts to the same encoder\nwith some minor differences.\nAnd then train a predictor.\nSometimes a predictor is very simple,\nsometimes it doesn't exist.\nBut train a predictor to\npredict a representation\nof the first uncorrupted input\nfrom the corrupted input.\nBut you only train the second branch.\nYou only train the part of the network\nthat is fed with the corrupted input.\nThe other network, you don't train.\nBut since they share the same weight,\nwhen you modify the first one,\nit also modifies the second one.\nAnd with various tricks,\nyou can prevent the system from collapsing\nwith the collapse of the\ntype I was explaining before\nwhere the system basically\nignores the input.\nSo that works very well.\nThe two techniques\nwe've developed at FAIR,\nDINO and I-JEPA work really well for that.\n- So what kind of data\nare we talking about here?\n- So there's several scenarios.\nOne scenario is you take an image,\nyou corrupt it by changing\nthe cropping, for example,\nchanging the size a little bit,\nmaybe changing the\norientation, blurring it,\nchanging the colors,\ndoing all kinds of horrible things to it-\n- But basic horrible things.\n- Basic horrible things\nthat sort of degrade\nthe quality a little bit\nand change the framing,\ncrop the image.\nAnd in some cases, in the case of I-JEPA,\nyou don't need to do any of this,\nyou just mask some parts of it, right?\nYou just basically remove some regions\nlike a big block, essentially.\nAnd then run through the encoders\nand train the entire system,\nencoder and predictor,\nto predict the representation\nof the good one\nfrom the representation\nof the corrupted one.\nSo that's the I-JEPA.\nIt doesn't need to know that\nit's an image, for example,\nbecause the only thing it needs to know\nis how to do this masking.\nWhereas with DINO,\nyou need to know it's an image\nbecause you need to do things\nlike geometry transformation and blurring\nand things like that that\nare really image specific.\nA more recent version of this\nthat we have is called V-JEPA.", "mimetype": "text/plain", "start_char_idx": 29674, "end_char_idx": 33788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8": {"__data__": {"id_": "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f70e098-3d4f-401c-a2d8-5d54e4cd0639", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6aacd49b1e462476f289d9c1c2abf40806af53051ecfd043bfea8b0a986363a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9081ff6-c98b-4a7f-b5f0-543e167a5201", "node_type": "1", "metadata": {}, "hash": "53d919b6918c836effb4f1a445f9e50b7e61f45c46ef003019ae133680062661", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And in some cases, in the case of I-JEPA,\nyou don't need to do any of this,\nyou just mask some parts of it, right?\nYou just basically remove some regions\nlike a big block, essentially.\nAnd then run through the encoders\nand train the entire system,\nencoder and predictor,\nto predict the representation\nof the good one\nfrom the representation\nof the corrupted one.\nSo that's the I-JEPA.\nIt doesn't need to know that\nit's an image, for example,\nbecause the only thing it needs to know\nis how to do this masking.\nWhereas with DINO,\nyou need to know it's an image\nbecause you need to do things\nlike geometry transformation and blurring\nand things like that that\nare really image specific.\nA more recent version of this\nthat we have is called V-JEPA.\nSo it's basically the same idea as I-JEPA\nexcept it's applied to video.\nSo now you take a whole video\nand you mask a whole chunk of it.\nAnd what we mask is actually\nkind of a temporal tube.\nSo like a whole segment\nof each frame in the video\nover the entire video.\n- And that tube is like\nstatically positioned\nthroughout the frames?\nIt's literally just a straight tube?\n- Throughout the tube, yeah.\nTypically it's 16 frames or something,\nand we mask the same region\nover the entire 16 frames.\nIt's a different one for\nevery video, obviously.\nAnd then again, train that system\nso as to predict the\nrepresentation of the full video\nfrom the partially masked video.\nAnd that works really well.\nIt's the first system that we have\nthat learns good representations of video\nso that when you feed\nthose representations\nto a supervised classifier head,\nit can tell you what action\nis taking place in the video\nwith pretty good accuracy.\nSo it's the first time we get\nsomething of that quality.\n- So that's a good test\nthat a good representation is formed.\nThat means there's something to this.\n- Yeah.\nWe also preliminary result\nthat seem to indicate\nthat the representation\nallows our system to tell\nwhether the video is physically possible\nor completely impossible\nbecause some object disappeared\nor an object suddenly jumped\nfrom one location to another\nor changed shape or something.\n- So it's able to capture\nsome physics based constraints\nabout the reality\nrepresented in the video?\n- [Yann] Yeah.\n- About the appearance and\nthe disappearance of objects?\n- Yeah.\nThat's really new.\n- Okay, but can this actually\nget us to this kind of world model\nthat understands enough about the world\nto be able to drive a car?\n- Possibly.\nAnd this is gonna take a while\nbefore we get to that point.\nAnd there are systems\nalready, robotic systems,\nthat are based on this idea.\nWhat you need for this\nis a slightly modified version of this\nwhere imagine that you have a video,\na complete video,\nand what you're doing to this video\nis that you are either\ntranslating it in time\ntowards the future.\nSo you'll only see the\nbeginning of the video,\nbut you don't see the latter part of it\nthat is in the original one.\nOr you just mask the second\nhalf of the video, for example.\nAnd then you train this I-JEPA system\nor the type I described,\nto predict representation\nof the full video\nfrom the shifted one.\nBut you also feed the\npredictor with an action.\nFor example, the wheel is turned\n10 degrees to the right\nor something, right?\nSo if it's a dash cam in a car\nand you know the angle of the wheel,\nyou should be able to\npredict to some extent\nwhat's going to happen to what you see.\nYou're not gonna be able\nto predict all the details\nof objects that appear\nin the view, obviously,\nbut at an abstract representation level,\nyou can probably predict\nwhat's gonna happen.\nSo now what you have is an internal model\nthat says, here is my idea\nof the state of the world at time T,\nhere is an action I'm taking,\nhere is a prediction\nof the state of the\nworld at time T plus one,\nT plus delta T,\nT plus two seconds, whatever it is.\nIf you have a model of this type,\nyou can use it for planning.", "mimetype": "text/plain", "start_char_idx": 33044, "end_char_idx": 36956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e9081ff6-c98b-4a7f-b5f0-543e167a5201": {"__data__": {"id_": "e9081ff6-c98b-4a7f-b5f0-543e167a5201", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea01a7f5-1af6-448e-a4dd-e21a7322c5c8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "76986e8dbea0c2d82346dc38a62c34c62d81cb968ba698f6f7a174390da30d20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49def48b-1941-470a-812d-9170e20b2412", "node_type": "1", "metadata": {}, "hash": "52972908fd1a44301ba6058040e59625c2e5f385b6459d6226d0dbd62af41d43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But you also feed the\npredictor with an action.\nFor example, the wheel is turned\n10 degrees to the right\nor something, right?\nSo if it's a dash cam in a car\nand you know the angle of the wheel,\nyou should be able to\npredict to some extent\nwhat's going to happen to what you see.\nYou're not gonna be able\nto predict all the details\nof objects that appear\nin the view, obviously,\nbut at an abstract representation level,\nyou can probably predict\nwhat's gonna happen.\nSo now what you have is an internal model\nthat says, here is my idea\nof the state of the world at time T,\nhere is an action I'm taking,\nhere is a prediction\nof the state of the\nworld at time T plus one,\nT plus delta T,\nT plus two seconds, whatever it is.\nIf you have a model of this type,\nyou can use it for planning.\nSo now you can do what LLMs cannot do,\nwhich is planning what you're gonna do\nso as you arrive at a particular outcome\nor satisfy a particular objective, right?\nSo you can have a number\nof objectives, right?\nI can predict that if I have\nan object like this, right?\nAnd I open my hand,\nit's gonna fall, right?\nAnd if I push it with a\nparticular force on the table,\nit's gonna move.\nIf I push the table itself,\nit's probably not gonna\nmove with the same force.\nSo we have this internal model\nof the world in our mind,\nwhich allows us to plan\nsequences of actions\nto arrive at a particular goal.\nAnd so now if you have this world model,\nwe can imagine a sequence of actions,\npredict what the outcome\nof the sequence of action is going to be,\nmeasure to what extent the final state\nsatisfies a particular objective\nlike moving the bottle\nto the left of the table.\nAnd then plan a sequence of actions\nthat will minimize this\nobjective at runtime.\nWe're not talking about learning,\nwe're talking about inference time, right?\nSo this is planning, really.\nAnd in optimal control,\nthis is a very classical thing.\nIt's called model predictive control.\nYou have a model of the\nsystem you want to control\nthat can predict the sequence of states\ncorresponding to a sequence of commands.\nAnd you are planning\na sequence of commands\nso that according to your world model,\nthe end state of the system\nwill satisfy any objectives that you fix.\nThis is the way rocket\ntrajectories have been planned\nsince computers have been around.\nSo since the early '60s, essentially.\n- So yes, for a model predictive control,\nbut you also often talk\nabout hierarchical planning.\n- [Yann] Yeah.\n- Can hierarchical planning\nemerge from this somehow?\n- Well, so no.\nYou will have to build\na specific architecture\nto allow for hierarchical planning.\nSo hierarchical planning\nis absolutely necessary\nif you want to plan complex actions.\nIf I wanna go from, let's\nsay, from New York to Paris,\nthis the example I use all the time.\nAnd I'm sitting in my office at NYU.\nMy objective that I need to minimize\nis my distance to Paris.\nAt a high level,\na very abstract\nrepresentation of my location,\nI would have to decompose\nthis into two sub-goals.\nFirst one is go to the airport,\nsecond one is catch a plane to Paris.\nOkay.\nSo my sub-goal is now\ngoing to the airport.\nMy objective function is\nmy distance to the airport.\nHow do I go to the airport?\nWell, I have to go in the\nstreet and hail a taxi,\nwhich you can do in New York.\nOkay, now I have another sub-goal.\nGo down on the street.\nWell, that means going to the elevator,\ngoing down the elevator,\nwalk out to the street.\nHow do I go to the elevator?\nI have to stand up from my chair,\nopen the door of my office,\ngo to the elevator, push the button.\nHow do I get up for my chair?\nLike you can imagine going\ndown all the way down\nto basically what amounts\nto millisecond by\nmillisecond muscle control.\nOkay?\nAnd obviously you're not\ngoing to plan your entire trip\nfrom New York to Paris\nin terms of millisecond by\nmillisecond muscle control.\nFirst, that would be incredibly expensive,\nbut it will also be completely impossible\nbecause you don't know all the conditions\nof what's gonna happen.\nHow long it's gonna take to catch a taxi\nor to go to the airport with traffic.", "mimetype": "text/plain", "start_char_idx": 36174, "end_char_idx": 40240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49def48b-1941-470a-812d-9170e20b2412": {"__data__": {"id_": "49def48b-1941-470a-812d-9170e20b2412", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9081ff6-c98b-4a7f-b5f0-543e167a5201", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "d6e20716ec45b1096c6a4211962e488d957817f605ce787f5818cb5e8b9a820d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de837f07-74fd-464c-a298-ca010b9dad9d", "node_type": "1", "metadata": {}, "hash": "e59735fbf6a77af83fde2d22eb659fc43d5d43f9242903d8ae5adb54ea3f5813", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay, now I have another sub-goal.\nGo down on the street.\nWell, that means going to the elevator,\ngoing down the elevator,\nwalk out to the street.\nHow do I go to the elevator?\nI have to stand up from my chair,\nopen the door of my office,\ngo to the elevator, push the button.\nHow do I get up for my chair?\nLike you can imagine going\ndown all the way down\nto basically what amounts\nto millisecond by\nmillisecond muscle control.\nOkay?\nAnd obviously you're not\ngoing to plan your entire trip\nfrom New York to Paris\nin terms of millisecond by\nmillisecond muscle control.\nFirst, that would be incredibly expensive,\nbut it will also be completely impossible\nbecause you don't know all the conditions\nof what's gonna happen.\nHow long it's gonna take to catch a taxi\nor to go to the airport with traffic.\nI mean, you would have to know exactly\nthe condition of everything\nto be able to do this planning,\nand you don't have the information.\nSo you have to do this\nhierarchical planning\nso that you can start acting\nand then sort of re-planning as you go.\nAnd nobody really knows\nhow to do this in AI.\nNobody knows how to train a system\nto learn the appropriate\nmultiple levels of representation\nso that hierarchical planning works.\n- Does something like that already emerge?\nSo like can you use an LLM,\nstate-of-the-art LLM,\nto get you from New York to Paris\nby doing exactly the kind of detailed\nset of questions that you just did?\nWhich is can you give me a\nlist of 10 steps I need to do\nto get from New York to Paris?\nAnd then for each of those steps,\ncan you give me a list of 10 steps\nhow I make that step happen?\nAnd for each of those steps,\ncan you give me a list of 10 steps\nto make each one of those,\nuntil you're moving\nyour individual muscles?\nMaybe not.\nWhatever you can actually act upon\nusing your own mind.\n- Right.\nSo there's a lot of questions\nthat are also implied by this, right?\nSo the first thing is LLMs\nwill be able to answer\nsome of those questions\ndown to some level of abstraction.\nUnder the condition that\nthey've been trained\nwith similar scenarios\nin their training set.\n- They would be able to\nanswer all of those questions.\nBut some of them may be hallucinated,\nmeaning non-factual.\n- Yeah, true.\nI mean they'll probably\nproduce some answer.\nExcept they're not gonna be able\nto really kind of produce\nmillisecond by millisecond muscle control\nof how you stand up\nfrom your chair, right?\nBut down to some level of abstraction\nwhere you can describe things by words,\nthey might be able to give you a plan,\nbut only under the condition\nthat they've been trained\nto produce those kind of plans, right?\nThey're not gonna be able\nto plan for situations\nthey never encountered before.\nThey basically are going to\nhave to regurgitate the template\nthat they've been trained on.\n- But where, just for the\nexample of New York to Paris,\nis it gonna start getting into trouble?\nLike at which layer of abstraction\ndo you think you'll start?\nBecause like I can imagine\nalmost every single part of that,\nan LLM will be able to\nanswer somewhat accurately,\nespecially when you're talking\nabout New York and Paris,\nmajor cities.\n- So I mean certainly an LLM\nwould be able to solve that problem\nif you fine tune it for it.\n- [Lex] Sure.\n- And so I can't say that\nan LLM cannot do this,\nit can't do this if you train it for it,\nthere's no question,\ndown to a certain level\nwhere things can be\nformulated in terms of words.\nBut like if you wanna go down\nto like how do you climb down the stairs\nor just stand up from your\nchair in terms of words,\nlike you can't do it.\nThat's one of the reasons you need\nexperience of the physical world,\nwhich is much higher bandwidth\nthan what you can express in words,\nin human language.\n- So everything we've been talking about\non the joint embedding space,\nis it possible that that's what we need\nfor like the interaction\nwith physical reality\non the robotics front?", "mimetype": "text/plain", "start_char_idx": 39445, "end_char_idx": 43348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de837f07-74fd-464c-a298-ca010b9dad9d": {"__data__": {"id_": "de837f07-74fd-464c-a298-ca010b9dad9d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49def48b-1941-470a-812d-9170e20b2412", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e02fd9b6eea06d78bcf8047edff08ec4bd6b3f824fef180fd695455afcf34895", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "128b34cb-c03d-4f9d-8471-eb955a797086", "node_type": "1", "metadata": {}, "hash": "f7ca86b924df9ea5291c43778f13c72069a73bc39c7cb3e072576859c2396f63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- So I mean certainly an LLM\nwould be able to solve that problem\nif you fine tune it for it.\n- [Lex] Sure.\n- And so I can't say that\nan LLM cannot do this,\nit can't do this if you train it for it,\nthere's no question,\ndown to a certain level\nwhere things can be\nformulated in terms of words.\nBut like if you wanna go down\nto like how do you climb down the stairs\nor just stand up from your\nchair in terms of words,\nlike you can't do it.\nThat's one of the reasons you need\nexperience of the physical world,\nwhich is much higher bandwidth\nthan what you can express in words,\nin human language.\n- So everything we've been talking about\non the joint embedding space,\nis it possible that that's what we need\nfor like the interaction\nwith physical reality\non the robotics front?\nAnd then just the LLMs are the\nthing that sits on top of it\nfor the bigger reasoning\nabout like the fact that I\nneed to book a plane ticket\nand I need to know know how to\ngo to the websites and so on.\n- Sure.\nAnd a lot of plans that people know about\nthat are relatively high\nlevel are actually learned.\nMost people don't invent\nthe plans by themselves.\nWe have some ability to do\nthis, of course, obviously,\nbut most plans that people use\nare plans that have been trained on.\nLike they've seen other\npeople use those plans\nor they've been told\nhow to do things, right?\nThat you can't invent how\nyou like take a person\nwho's never heard of airplanes\nand tell them like, how do\nyou go from New York to Paris?\nThey're probably not going to be able\nto kind of deconstruct the whole plan\nunless they've seen\nexamples of that before.\nSo certainly LLMs are\ngonna be able to do this.\nBut then how you link this\nfrom the low level of actions,\nthat needs to be done\nwith things like JEPA,\nthat basically lift the abstraction level\nof the representation\nwithout attempting to reconstruct\nevery detail of the situation.\nThat's why we need JEPAs for.\n- I would love to sort of\nlinger on your skepticism\naround autoregressive LLMs.\nSo one way I would like\nto test that skepticism is\neverything you say makes a lot of sense,\nbut if I apply everything\nyou said today and in general\nto like, I don't know,\n10 years ago, maybe a little bit less.\nNo, let's say three years ago.\nI wouldn't be able to\npredict the success of LLMs.\nSo does it make sense to you\nthat autoregressive LLMs\nare able to be so damn good?\n- [Yann] Yes.\n- Can you explain your intuition?\nBecause if I were to take\nyour wisdom and intuition\nat face value,\nI would say there's no\nway autoregressive LLMs\none token at a time,\nwould be able to do the kind\nof things they're doing.\n- No, there's one thing\nthat autoregressive LLMs\nor that LLMs in general, not\njust the autoregressive ones,\nbut including the BERT\nstyle bidirectional ones,\nare exploiting and its\nself supervised running.\nAnd I've been a very, very strong advocate\nof self supervised running for many years.\nSo those things are an incredibly\nimpressive demonstration\nthat self supervised\nlearning actually works.\nThe idea that started...\nIt didn't start with BERT,\nbut it was really kind of a\ngood demonstration with this.\nSo the idea that you take a\npiece of text, you corrupt it,\nand then you train some\ngigantic neural net\nto reconstruct the parts that are missing.\nThat has been an enormous...\nProduced an enormous amount of benefits.\nIt allowed us to create systems\nthat understand language,\nsystems that can translate\nhundreds of languages in any direction,\nsystems that are multilingual.\nIt's a single system\nthat can be trained to\nunderstand hundreds of languages\nand translate in any direction\nand produce summaries\nand then answer questions\nand produce text.\nAnd then there's a special case of it,\nwhich is the autoregressive trick\nwhere you constrain the system\nto not elaborate a\nrepresentation of the text\nfrom looking at the entire text,\nbut only predicting a word\nfrom the words that have come before.\nRight?\nAnd you do this\nby constraining the\narchitecture of the network.", "mimetype": "text/plain", "start_char_idx": 42576, "end_char_idx": 46551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "128b34cb-c03d-4f9d-8471-eb955a797086": {"__data__": {"id_": "128b34cb-c03d-4f9d-8471-eb955a797086", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de837f07-74fd-464c-a298-ca010b9dad9d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "e0979a941fbaebb38f27fa18eb27e1c9ffd53256ffff22e8a4375326f5653b4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d4bd48d-927e-41ec-a758-12cd2602ca1c", "node_type": "1", "metadata": {}, "hash": "c4a112cd9494d0db677181d3e5f6208183eecec5cb439f45401f261380ed9fa1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So the idea that you take a\npiece of text, you corrupt it,\nand then you train some\ngigantic neural net\nto reconstruct the parts that are missing.\nThat has been an enormous...\nProduced an enormous amount of benefits.\nIt allowed us to create systems\nthat understand language,\nsystems that can translate\nhundreds of languages in any direction,\nsystems that are multilingual.\nIt's a single system\nthat can be trained to\nunderstand hundreds of languages\nand translate in any direction\nand produce summaries\nand then answer questions\nand produce text.\nAnd then there's a special case of it,\nwhich is the autoregressive trick\nwhere you constrain the system\nto not elaborate a\nrepresentation of the text\nfrom looking at the entire text,\nbut only predicting a word\nfrom the words that have come before.\nRight?\nAnd you do this\nby constraining the\narchitecture of the network.\nAnd that's what you can build\nan autoregressive LLM from.\nSo there was a surprise many years ago\nwith what's called decoder only LLM.\nSo systems of this type\nthat are just trying to produce\nwords from the previous one.\nAnd the fact that when you scale them up,\nthey tend to really kind of\nunderstand more about language.\nWhen you train them on lots of data,\nyou make them really big.\nThat was kind of a surprise.\nAnd that surprise occurred\nquite a while back.\nLike with work from Google,\nMeta, OpenAI, et cetera,\ngoing back to the GPT\nkind of general pre-trained transformers.\n- You mean like GPT-2?\nLike there's a certain place\nwhere you start to realize\nscaling might actually keep\ngiving us an emergent benefit.\n- Yeah, I mean there were\nwork from various places,\nbut if you want to kind of\nplace it in the GPT timeline,\nthat would be around GPT-2, yeah.\n- Well, 'cause you said it,\nyou're so charismatic and\nyou said so many words,\nbut self supervised learning, yes.\nBut again, the same\nintuition you're applying\nto saying that autoregressive LLMs\ncannot have a deep\nunderstanding of the world,\nif we just apply that same intuition,\ndoes it make sense to you\nthat they're able to form enough\nof a representation in the world\nto be damn convincing,\nessentially passing the\noriginal Turing test\nwith flying colors.\n- Well, we're fooled by\ntheir fluency, right?\nWe just assume that if a system is fluent\nin manipulating language,\nthen it has all the characteristics\nof human intelligence.\nBut that impression is false.\nWe're really fooled by it.\n- Well, what do you think\nAlan Turing would say?\nWithout understanding anything,\njust hanging out with it-\n- Alan Turing would decide\nthat a Turing test is a really bad test.\n(Lex chuckles)\nOkay.\nThis is what the AI community\nhas decided many years ago\nthat the Turing test was a\nreally bad test of intelligence.\n- What would Hans Moravec say\nabout the large language models?\n- Hans Moravec would say\nthe Moravec's paradox still applies.\n- [Lex] Okay.\n- Okay?\nOkay, we can pass-\n- You don't think he\nwould be really impressed.\n- No, of course everybody\nwould be impressed.\n(laughs)\nBut it is not a question\nof being impressed or not,\nit is a question of knowing\nwhat the limit of those systems can do.\nAgain, they are impressive.\nThey can do a lot of useful things.\nThere's a whole industry that\nis being built around them.\nThey're gonna make progress,\nbut there is a lot of\nthings they cannot do.\nAnd we have to realize what they cannot do\nand then figure out how we get there.\nAnd I'm not saying this...\nI'm saying this from\nbasically 10 years of research\non the idea of self supervised running,\nactually that's going\nback more than 10 years,\nbut the idea of self supervised learning.\nSo basically capturing\nthe internal structure\nof a piece of a set of inputs\nwithout training the system\nfor any particular task, right?\nLearning representations.\nThe conference I co-founded 14 years ago\nis called International Conference\non Learning Representations,\nthat's the entire issue that\ndeep learning is dealing with.\nRight?\nAnd it's been my obsession\nfor almost 40 years now.\nSo learning representation\nis really the thing.\nFor the longest time\nwe could only do this\nwith supervised learning.", "mimetype": "text/plain", "start_char_idx": 45686, "end_char_idx": 49788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2d4bd48d-927e-41ec-a758-12cd2602ca1c": {"__data__": {"id_": "2d4bd48d-927e-41ec-a758-12cd2602ca1c", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "128b34cb-c03d-4f9d-8471-eb955a797086", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a2ca159ab5732fcf756ac4541253e0ad7e67d1e634b40f51ff9528055ea9a779", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4", "node_type": "1", "metadata": {}, "hash": "da90a813934c5987034cf97a598a6d0cf51fac1bcb70b17d66f66f1e49d671ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They're gonna make progress,\nbut there is a lot of\nthings they cannot do.\nAnd we have to realize what they cannot do\nand then figure out how we get there.\nAnd I'm not saying this...\nI'm saying this from\nbasically 10 years of research\non the idea of self supervised running,\nactually that's going\nback more than 10 years,\nbut the idea of self supervised learning.\nSo basically capturing\nthe internal structure\nof a piece of a set of inputs\nwithout training the system\nfor any particular task, right?\nLearning representations.\nThe conference I co-founded 14 years ago\nis called International Conference\non Learning Representations,\nthat's the entire issue that\ndeep learning is dealing with.\nRight?\nAnd it's been my obsession\nfor almost 40 years now.\nSo learning representation\nis really the thing.\nFor the longest time\nwe could only do this\nwith supervised learning.\nAnd then we started working on\nwhat we used to call unsupervised learning\nand sort of revived the idea\nof unsupervised learning\nin the early 2000s with\nYoshua Bengio and Jeff Hinton.\nThen discovered that supervised learning\nactually works pretty well\nif you can collect enough data.\nAnd so the whole idea of\nunsupervised self supervision\ntook a backseat for a bit\nand then I kind of tried\nto revive it in a big way,\nstarting in 2014 basically\nwhen we started FAIR,\nand really pushing for\nlike finding new methods\nto do self supervised running,\nboth for text and for images\nand for video and audio.\nAnd some of that work has\nbeen incredibly successful.\nI mean, the reason why we have\nmultilingual translation system,\nthings to do,\ncontent moderation on Meta,\nfor example, on Facebook\nthat are multilingual,\nthat understand whether piece of text\nis hate speech or not, or something\nis due to their progress\nusing self supervised running for NLP,\ncombining this with\ntransformer architectures\nand blah blah blah.\nBut that's the big success\nof self supervised running.\nWe had similar success\nin speech recognition,\na system called Wav2Vec,\nwhich is also a joint embedding\narchitecture by the way,\ntrained with contrastive learning.\nAnd that system also can produce\nspeech recognition systems\nthat are multilingual\nwith mostly unlabeled data\nand only need a few\nminutes of labeled data\nto actually do speech recognition.\nThat's amazing.\nWe have systems now based on\nthose combination of ideas\nthat can do real time translation\nof hundreds of languages into each other,\nspeech to speech.\n- Speech to speech,\neven including, which is fascinating,\nlanguages that don't have written forms-\n- That's right.\n- They're spoken only.\n- That's right.\nWe don't go through text,\nit goes directly from speech to speech\nusing an internal representation\nof kinda speech units that are discrete.\nBut it's called Textless NLP.\nWe used to call it this way.\nBut yeah.\nI mean incredible success there.\nAnd then for 10 years we\ntried to apply this idea\nto learning representations of images\nby training a system to predict videos,\nlearning intuitive physics\nby training a system to predict\nwhat's gonna happen in the video.\nAnd tried and tried and failed and failed\nwith generative models,\nwith models that predict pixels.\nWe could not get them to learn\ngood representations of images,\nwe could not get them to learn\ngood presentations of videos.\nAnd we tried many times,\nwe published lots of papers on it.\nThey kind of sort of worked,\nbut not really great.\nIt started working,\nwe abandoned this idea\nof predicting every pixel\nand basically just doing the\njoint embedding and predicting\nin representation space.\nThat works.\nSo there's ample evidence\nthat we're not gonna be able\nto learn good representations\nof the real world\nusing generative model.\nSo I'm telling people,\neverybody's talking about generative AI.\nIf you're really interested\nin human level AI,\nabandon the idea of generative AI.\n(Lex laughs)\n- Okay.\nBut you really think it's possible\nto get far with joint\nembedding representation?\nSo like there's common sense reasoning\nand then there's high level reasoning.\nLike I feel like those are two...\nThe kind of reasoning\nthat LLMs are able to do.\nOkay, let me not use the word reasoning,\nbut the kind of stuff\nthat LLMs are able to do\nseems fundamentally different\nthan the common sense reasoning we use\nto navigate the world.\n- [Yann] Yeah.", "mimetype": "text/plain", "start_char_idx": 48923, "end_char_idx": 53218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4": {"__data__": {"id_": "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d4bd48d-927e-41ec-a758-12cd2602ca1c", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "8338571ae5b68725c1670ed080941479259b9318a594d0cf444815b51c2ac16a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5351a29f-f6de-4be1-81cf-d76055352273", "node_type": "1", "metadata": {}, "hash": "6a6ba4cd14dd1cfcff269cdf1ca14198bd026c6e93c268f5d9c8c7ffa458a691", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That works.\nSo there's ample evidence\nthat we're not gonna be able\nto learn good representations\nof the real world\nusing generative model.\nSo I'm telling people,\neverybody's talking about generative AI.\nIf you're really interested\nin human level AI,\nabandon the idea of generative AI.\n(Lex laughs)\n- Okay.\nBut you really think it's possible\nto get far with joint\nembedding representation?\nSo like there's common sense reasoning\nand then there's high level reasoning.\nLike I feel like those are two...\nThe kind of reasoning\nthat LLMs are able to do.\nOkay, let me not use the word reasoning,\nbut the kind of stuff\nthat LLMs are able to do\nseems fundamentally different\nthan the common sense reasoning we use\nto navigate the world.\n- [Yann] Yeah.\n- It seems like we're gonna need both-\n- Sure.\n- Would you be able to get,\nwith the joint embedding which\nis a JEPA type of approach,\nlooking at video, would\nyou be able to learn,\nlet's see,\nwell, how to get from New York to Paris,\nor how to understand the state\nof politics in the world?\n(both laugh)\nRight?\nThese are things where various humans\ngenerate a lot of\nlanguage and opinions on,\nin the space of language,\nbut don't visually represent that\nin any clearly compressible way.\n- Right.\nWell, there's a lot of situations\nthat might be difficult\nfor a purely language\nbased system to know.\nLike, okay, you can probably\nlearn from reading texts,\nthe entirety of the publicly\navailable text in the world\nthat I cannot get from New York to Paris\nby snapping my fingers.\nThat's not gonna work, right?\n- [Lex] Yes.\n- But there's probably\nsort of more complex\nscenarios of this type\nwhich an LLM may never have encountered\nand may not be able to determine\nwhether it's possible or not.\nSo that link from the low\nlevel to the high level...\nThe thing is that the high\nlevel that language expresses\nis based on the common\nexperience of the low level,\nwhich LLMs currently do not have.\nWhen we talk to each other,\nwe know we have a common\nexperience of the world.\nLike a lot of it is similar.\nAnd LLMs don't have that.\n- But see, there it's present.\nYou and I have a common\nexperience of the world\nin terms of the physics\nof how gravity works\nand stuff like this.\nAnd that common knowledge of the world,\nI feel like is there in the language.\nWe don't explicitly express it,\nbut if you have a huge amount of text,\nyou're going to get this stuff\nthat's between the lines.\nIn order to form a consistent world model,\nyou're going to have to\nunderstand how gravity works,\neven if you don't have an\nexplicit explanation of gravity.\nSo even though, in the case of gravity,\nthere is explicit explanation.\nThere's gravity in Wikipedia.\nBut like the stuff that we think of\nas common sense reasoning,\nI feel like to generate\nlanguage correctly,\nyou're going to have to figure that out.\nNow, you could say as you have,\nthere's not enough text-\n- Well, I agree.\n- Sorry.\nOkay, yeah.\n(laughs)\nYou don't think so?\n- No, I agree with what you just said,\nwhich is that to be able to\ndo high level common sense...\nTo have high level common sense,\nyou need to have the\nlow level common sense\nto build on top of.\n- [Lex] Yeah.\nBut that's not there.\n- That's not there in LLMs.\nLLMs are purely trained from text.\nSo then the other statement you made,\nI would not agree\nwith the fact that implicit\nin all languages in the world\nis the underlying reality.\nThere's a lot about underlying reality\nwhich is not expressed in language.\n- Is that obvious to you?\n- Yeah, totally.\n- So like all the conversations we have...\nOkay, there's the dark web,\nmeaning whatever,\nthe private conversations\nlike DMs and stuff like this,\nwhich is much, much larger\nprobably than what's available,\nwhat LLMs are trained on.\n- You don't need to communicate\nthe stuff that is common.\n- But the humor, all of it.\nNo, you do.\nYou don't need to, but it comes through.\nLike if I accidentally knock this over,\nyou'll probably make fun of me.", "mimetype": "text/plain", "start_char_idx": 52475, "end_char_idx": 56405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5351a29f-f6de-4be1-81cf-d76055352273": {"__data__": {"id_": "5351a29f-f6de-4be1-81cf-d76055352273", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f08d3b27-628e-4dcd-8f9a-e0b7d789b3b4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6cb42d2cc7345e5f9de5eced7fc4c69a12cab6083bb40dcdffcdb4c23d694f1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "098ad492-e74e-44e8-9a68-dc99d853cadc", "node_type": "1", "metadata": {}, "hash": "294ff3b27885f59e4ae074bdb551a46b6d0ecbc5a94d67bb6df7de58fba9e8bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Lex] Yeah.\nBut that's not there.\n- That's not there in LLMs.\nLLMs are purely trained from text.\nSo then the other statement you made,\nI would not agree\nwith the fact that implicit\nin all languages in the world\nis the underlying reality.\nThere's a lot about underlying reality\nwhich is not expressed in language.\n- Is that obvious to you?\n- Yeah, totally.\n- So like all the conversations we have...\nOkay, there's the dark web,\nmeaning whatever,\nthe private conversations\nlike DMs and stuff like this,\nwhich is much, much larger\nprobably than what's available,\nwhat LLMs are trained on.\n- You don't need to communicate\nthe stuff that is common.\n- But the humor, all of it.\nNo, you do.\nYou don't need to, but it comes through.\nLike if I accidentally knock this over,\nyou'll probably make fun of me.\nAnd in the content of\nthe you making fun of me\nwill be explanation of\nthe fact that cups fall\nand then gravity works in this way.\nAnd then you'll have some\nvery vague information\nabout what kind of things\nexplode when they hit the ground.\nAnd then maybe you'll\nmake a joke about entropy\nor something like this\nand we will never be able\nto reconstruct this again.\nLike, okay, you'll make\na little joke like this\nand there'll be trillion of other jokes.\nAnd from the jokes,\nyou can piece together the\nfact that gravity works\nand mugs can break and\nall this kind of stuff,\nyou don't need to see...\nIt'll be very inefficient.\nIt's easier for like\nto not knock the thing over. (laughing)\n- [Yann] Yeah.\n- But I feel like it would be there\nif you have enough of that data.\n- I just think that most of\nthe information of this type\nthat we have accumulated\nwhen we were babies\nis just not present in text,\nin any description, essentially.\nAnd the sensory data\nis a much richer source\nfor getting that kind of understanding.\nI mean, that's the 16,000 hours\nof wake time of a 4-year-old.\nAnd tend to do 15 bytes,\ngoing through vision.\nJust vision, right?\nThere is a similar bandwidth of touch\nand a little less through audio.\nAnd then text doesn't...\nLanguage doesn't come in\nuntil like a year in life.\nAnd by the time you are nine years old,\nyou've learned about gravity,\nyou know about inertia,\nyou know about gravity,\nyou know there's stability,\nyou know about the distinction\nbetween animate and inanimate objects.\nBy 18 months,\nyou know about like why\npeople want to do things\nand you help them if they can't.\nI mean there's a lot of\nthings that you learn\nmostly by observation,\nreally not even through interaction.\nIn the first few months of life,\nbabies don't really have\nany influence on the world.\nThey can only observe, right?\nAnd you accumulate like a\ngigantic amount of knowledge\njust from that.\nSo that's what we're missing\nfrom current AI systems.\n- I think in one of your\nslides you have this nice plot\nthat is one of the ways you\nshow that LLMs are limited.\nI wonder if you could\ntalk about hallucinations\nfrom your perspectives.\nWhy hallucinations happen\nfrom large language models,\nand to what degree is\nthat a fundamental flaw\nof large language models.\n- Right.\nSo because of the\nautoregressive prediction,\nevery time an LLM produces\na token or a word,\nthere is some level of\nprobability for that word\nto take you out of the\nset of reasonable answers.\nAnd if you assume,\nwhich is a very strong assumption,\nthat the probability of such error\nis those errors are independent\nacross a sequence of\ntokens being produced.\nWhat that means is that every\ntime you produce a token,\nthe probability\nthat you stay within the set\nof correct answer decreases\nand it decreases exponentially.\n- So there's a strong, like\nyou said, assumption there\nthat if there's a non-zero\nprobability of making a mistake,\nwhich there appears to be,\nthen there's going to be a kind of drift.\n- Yeah.\nAnd that drift is exponential.\nIt's like errors accumulate, right?\nSo the probability that an\nanswer would be nonsensical\nincreases exponentially\nwith the number of tokens.\n- Is that obvious to you by the way?", "mimetype": "text/plain", "start_char_idx": 55607, "end_char_idx": 59594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "098ad492-e74e-44e8-9a68-dc99d853cadc": {"__data__": {"id_": "098ad492-e74e-44e8-9a68-dc99d853cadc", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5351a29f-f6de-4be1-81cf-d76055352273", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c2bce3db93b4405f744fa7260c7b9bf3a94bfe3671054df135591d1035734077", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0384004f-2943-41c9-8d25-42758b2dcac3", "node_type": "1", "metadata": {}, "hash": "9ad369d91a735da0b6ef511a525ac353021fd479027bda18636ca60534355470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And if you assume,\nwhich is a very strong assumption,\nthat the probability of such error\nis those errors are independent\nacross a sequence of\ntokens being produced.\nWhat that means is that every\ntime you produce a token,\nthe probability\nthat you stay within the set\nof correct answer decreases\nand it decreases exponentially.\n- So there's a strong, like\nyou said, assumption there\nthat if there's a non-zero\nprobability of making a mistake,\nwhich there appears to be,\nthen there's going to be a kind of drift.\n- Yeah.\nAnd that drift is exponential.\nIt's like errors accumulate, right?\nSo the probability that an\nanswer would be nonsensical\nincreases exponentially\nwith the number of tokens.\n- Is that obvious to you by the way?\nWell, so mathematically speaking maybe,\nbut like isn't there a\nkind of gravitational pull\ntowards the truth?\nBecause on average, hopefully,\nthe truth is well represented\nin the training set.\n- No, it's basically a struggle\nagainst the curse of dimensionality.\nSo the way you can correct for this\nis that you fine tune the system\nby having it produce answers\nfor all kinds of questions\nthat people might come up with.\nAnd people are people,\nso a lot of the questions that they have\nare very similar to each other.\nSo you can probably cover,\nyou know, 80% or whatever of\nquestions that people will ask\nby collecting data.\nAnd then you fine tune the system\nto produce good answers\nfor all of those things.\nAnd it's probably gonna\nbe able to learn that\nbecause it's got a lot\nof capacity to learn.\nBut then there is the\nenormous set of prompts\nthat you have not covered during training.\nAnd that set is enormous.\nLike within the set of\nall possible prompts,\nthe proportion of prompts that\nhave been used for training\nis absolutely tiny.\nIt's a tiny, tiny, tiny subset\nof all possible prompts.\nAnd so the system will behave properly\non the prompts that it's\nbeen either trained,\npre-trained or fine tuned.\nBut then there is an\nentire space of things\nthat it cannot possibly\nhave been trained on\nbecause it's just the number is gigantic.\nSo whatever training the system\nhas been subject to produce\nappropriate answers,\nyou can break it by finding out a prompt\nthat will be outside of the set of prompts\nit's been trained on\nor things that are similar,\nand then it will just\nspew complete nonsense.\n- When you say prompt,\ndo you mean that exact prompt\nor do you mean a prompt that's like,\nin many parts very different than...\nIs it that easy to ask a question\nor to say a thing that\nhasn't been said before\non the internet?\n- I mean, people have come up with things\nwhere like you put essentially\na random sequence of\ncharacters in a prompt\nand that's enough to kind of\nthrow the system into a mode\nwhere it's gonna answer\nsomething completely different\nthan it would have answered without this.\nSo that's a way to jailbreak\nthe system, basically.\nGo outside of its conditioning, right?\n- So that's a very clear\ndemonstration of it.\nBut of course, that goes outside\nof what it's designed to do, right?\nIf you actually stitch together\nreasonably grammatical sentences,\nis it that easy to break it?\n- Yeah.\nSome people have done things like\nyou write a sentence in English\nor you ask a question in English\nand it produces a perfectly fine answer.\nAnd then you just substitute a few words\nby the same word in another language,\nand all of a sudden the\nanswer is complete nonsense.\n- Yeah.\nSo I guess what I'm saying is like,\nwhich fraction of prompts that\nhumans are likely to generate\nare going to break the system?\n- So the problem is that\nthere is a long tail.\n- [Lex] Yes.\n- This is an issue that a\nlot of people have realized\nin social networks and stuff like that,\nwhich is there's a very, very long tail\nof things that people will ask.\nAnd you can fine tune the system\nfor the 80% or whatever\nof the things that most people will ask.\nAnd then this long tail is so large\nthat you're not gonna be\nable to fine tune the system\nfor all the conditions.\nAnd in the end,\nthe system ends up being\nkind of a giant lookup\ntable, right? (laughing)\nEssentially.\nWhich is not really what you want.", "mimetype": "text/plain", "start_char_idx": 58867, "end_char_idx": 62975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0384004f-2943-41c9-8d25-42758b2dcac3": {"__data__": {"id_": "0384004f-2943-41c9-8d25-42758b2dcac3", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "098ad492-e74e-44e8-9a68-dc99d853cadc", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "86307bf8fa18b318ffa3fe61e862cd80a4d430ca25a2a132cb44472837a24d87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "684c6e3a-36c0-44e6-a75b-ca2c84261e56", "node_type": "1", "metadata": {}, "hash": "268ef28bd4366b2d672015dcd45a5a9876c74298787f39bc25ef96e08d870168", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\nSo I guess what I'm saying is like,\nwhich fraction of prompts that\nhumans are likely to generate\nare going to break the system?\n- So the problem is that\nthere is a long tail.\n- [Lex] Yes.\n- This is an issue that a\nlot of people have realized\nin social networks and stuff like that,\nwhich is there's a very, very long tail\nof things that people will ask.\nAnd you can fine tune the system\nfor the 80% or whatever\nof the things that most people will ask.\nAnd then this long tail is so large\nthat you're not gonna be\nable to fine tune the system\nfor all the conditions.\nAnd in the end,\nthe system ends up being\nkind of a giant lookup\ntable, right? (laughing)\nEssentially.\nWhich is not really what you want.\nYou want systems that can reason,\ncertainly that can plan.\nSo the type of reasoning\nthat takes place in LLM\nis very, very primitive.\nAnd the reason you can tell it's primitive\nis because the amount of computation\nthat is spent per token\nproduced is constant.\nSo if you ask a question\nand that question has an answer\nin a given number of token,\nthe amount of computation\ndevoted to computing that answer\ncan be exactly estimated.\nIt's the size of the prediction network\nwith its 36 layers or 92\nlayers or whatever it is,\nmultiplied by number of tokens.\nThat's it.\nAnd so essentially,\nit doesn't matter if\nthe question being asked\nis simple to answer,\ncomplicated to answer,\nimpossible to answer\nbecause it's decided,\nwell, there's something.\nThe amount of computation\nthe system will be able to\ndevote to the answer is constant\nor is proportional to the\nnumber of token produced\nin the answer, right?\nThis is not the way we work,\nthe way we reason is that\nwhen we are faced with a complex problem\nor a complex question,\nwe spend more time trying to\nsolve it and answer it, right?\nBecause it's more difficult.\n- There's a prediction element,\nthere's an iterative element\nwhere you're like adjusting\nyour understanding of a thing\nby going over and over and over.\nThere's a hierarchical elements on.\nDoes this mean it's a\nfundamental flaw of LLMs-\n- [Yann] Yeah.\n- Or does it mean that... (laughs)\nThere's more part to that question?\n(laughs)\nNow you're just behaving like an LLM.\n(laughs)\nImmediately answering.\nNo, that it's just the\nlow level world model\non top of which we can then build\nsome of these kinds of mechanisms,\nlike you said, persistent\nlong-term memory or reasoning,\nso on.\nBut we need that world model\nthat comes from language.\nMaybe it is not so difficult\nto build this kind of reasoning system\non top of a well constructed world model.\n- Okay.\nWhether it's difficult or not,\nthe near future will say,\nbecause a lot of people\nare working on reasoning\nand planning abilities\nfor dialogue systems.\nI mean, even if we restrict\nourselves to language,\njust having the ability\nto plan your answer before you answer,\nin terms that are not necessarily linked\nwith the language you're gonna\nuse to produce the answer.\nRight?\nSo this idea of this mental model\nthat allows you to plan\nwhat you're gonna say\nbefore you say it.\nThat is very important.\nI think there's going\nto be a lot of systems\nover the next few years\nthat are going to have this capability,\nbut the blueprint of those systems\nwill be extremely different\nfrom autoregressive LLMs.\nSo it's the same difference\nas the difference between\nwhat psychology has called\nsystem one and system two\nin humans, right?\nSo system one is the type of\ntask that you can accomplish\nwithout like deliberately\nconsciously think about\nhow you do them.\nYou just do them.\nYou've done them enough\nthat you can just do it\nsubconsciously, right?\nWithout thinking about them.\nIf you're an experienced driver,\nyou can drive without\nreally thinking about it\nand you can talk to\nsomeone at the same time\nor listen to the radio, right?\nIf you are a very\nexperienced chess player,\nyou can play against a\nnon-experienced chess player\nwithout really thinking either,\nyou just recognize the\npattern and you play, right?\nThat's system one.", "mimetype": "text/plain", "start_char_idx": 62265, "end_char_idx": 66250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "684c6e3a-36c0-44e6-a75b-ca2c84261e56": {"__data__": {"id_": "684c6e3a-36c0-44e6-a75b-ca2c84261e56", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0384004f-2943-41c9-8d25-42758b2dcac3", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "547cd3aa940b0ae41bd72246b9121125cc3305c1dc10b3750db089cb6589c3f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d57ba18-7743-4b02-9f61-aad7cc5dfab4", "node_type": "1", "metadata": {}, "hash": "8ad0f128288469380a676a6822ce74a9e803874bce7832400d124ff9fdc18af8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So it's the same difference\nas the difference between\nwhat psychology has called\nsystem one and system two\nin humans, right?\nSo system one is the type of\ntask that you can accomplish\nwithout like deliberately\nconsciously think about\nhow you do them.\nYou just do them.\nYou've done them enough\nthat you can just do it\nsubconsciously, right?\nWithout thinking about them.\nIf you're an experienced driver,\nyou can drive without\nreally thinking about it\nand you can talk to\nsomeone at the same time\nor listen to the radio, right?\nIf you are a very\nexperienced chess player,\nyou can play against a\nnon-experienced chess player\nwithout really thinking either,\nyou just recognize the\npattern and you play, right?\nThat's system one.\nSo all the things that\nyou do instinctively\nwithout really having to deliberately plan\nand think about it.\nAnd then there is other\ntasks where you need to plan.\nSo if you are a not too\nexperienced chess player\nor you are experienced\nbut you play against another\nexperienced chess player,\nyou think about all\nkinds of options, right?\nYou think about it for a while, right?\nAnd you're much better if you\nhave time to think about it\nthan you are if you play\nblitz with limited time.\nAnd so this type of deliberate planning,\nwhich uses your internal world\nmodel, that's system two,\nthis is what LLMs currently cannot do.\nHow do we get them to do this, right?\nHow do we build a system\nthat can do this kind\nof planning or reasoning\nthat devotes more resources\nto complex problems\nthan to simple problems.\nAnd it's not going to be\nautoregressive prediction of tokens,\nit's going to be more\nsomething akin to inference\nof latent variables\nin what used to be called\nprobabilistic models\nor graphical models and\nthings of that type.\nSo basically the principle is like this.\nThe prompt is like observed variables.\nAnd what the model does\nis that it's basically a measure of...\nIt can measure to what extent an answer\nis a good answer for a prompt.\nOkay?\nSo think of it as some\ngigantic neural net,\nbut it's got only one output.\nAnd that output is a scaler number,\nwhich is let's say zero\nif the answer is a good\nanswer for the question,\nand a large number\nif the answer is not a good\nanswer for the question.\nImagine you had this model.\nIf you had such a model,\nyou could use it to produce good answers.\nThe way you would do is produce the prompt\nand then search through the\nspace of possible answers\nfor one that minimizes that number.\nThat's called an energy based model.\n- But that energy based model\nwould need the model\nconstructed by the LLM.\n- Well, so really what you need to do\nwould be to not search over\npossible strings of text\nthat minimize that energy.\nBut what you would do\nis do this in abstract\nrepresentation space.\nSo in sort of the space\nof abstract thoughts,\nyou would elaborate a thought, right?\nUsing this process of minimizing\nthe output of your model.\nOkay?\nWhich is just a scaler.\nIt's an optimization process, right?\nSo now the way the system\nproduces its answer\nis through optimization\nby minimizing an objective\nfunction basically, right?\nAnd this is, we're\ntalking about inference,\nwe're not talking about training, right?\nThe system has been trained already.\nSo now we have an abstract representation\nof the thought of the answer,\nrepresentation of the answer.\nWe feed that to basically\nan autoregressive decoder,\nwhich can be very simple,\nthat turns this into a text\nthat expresses this thought.\nOkay?\nSo that in my opinion\nis the blueprint of future data systems.\nThey will think about their answer,\nplan their answer by optimization\nbefore turning it into text.\nAnd that is turning complete.\n- Can you explain exactly\nwhat the optimization problem there is?\nLike what's the objective function?\nJust linger on it.\nYou kind of briefly described it,\nbut over what space are you optimizing?\n- The space of representations-\n- Goes abstract representation.\n- That's right.\nSo you have an abstract\nrepresentation inside the system.\nYou have a prompt.\nThe prompt goes through an encoder,\nproduces a representation,\nperhaps goes through a predictor\nthat predicts a\nrepresentation of the answer,\nof the proper answer.\nBut that representation\nmay not be a good answer\nbecause there might be\nsome complicated reasoning\nyou need to do, right?", "mimetype": "text/plain", "start_char_idx": 65528, "end_char_idx": 69804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0d57ba18-7743-4b02-9f61-aad7cc5dfab4": {"__data__": {"id_": "0d57ba18-7743-4b02-9f61-aad7cc5dfab4", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "684c6e3a-36c0-44e6-a75b-ca2c84261e56", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0f49cc0b201b3312a764b6fc95d98b83981b957a6caf2c384261e8d3e90b8da1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb5d9572-b690-45d1-9723-2d06e5668f98", "node_type": "1", "metadata": {}, "hash": "ae0f820e3183e70c91a83d7f75ef7fa4d47120860eca75289829938c1f4dfdaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay?\nSo that in my opinion\nis the blueprint of future data systems.\nThey will think about their answer,\nplan their answer by optimization\nbefore turning it into text.\nAnd that is turning complete.\n- Can you explain exactly\nwhat the optimization problem there is?\nLike what's the objective function?\nJust linger on it.\nYou kind of briefly described it,\nbut over what space are you optimizing?\n- The space of representations-\n- Goes abstract representation.\n- That's right.\nSo you have an abstract\nrepresentation inside the system.\nYou have a prompt.\nThe prompt goes through an encoder,\nproduces a representation,\nperhaps goes through a predictor\nthat predicts a\nrepresentation of the answer,\nof the proper answer.\nBut that representation\nmay not be a good answer\nbecause there might be\nsome complicated reasoning\nyou need to do, right?\nSo then you have another process\nthat takes the representation\nof the answers and modifies it\nso as to minimize a cost function\nthat measures to what extent\nthe answer is a good\nanswer for the question.\nNow we sort of ignore the fact for...\nI mean, the issue for a moment\nof how you train that system\nto measure whether an answer\nis a good answer for sure.\n- But suppose such a\nsystem could be created,\nwhat's the process?\nThis kind of search like process.\n- It's an optimization process.\nYou can do this if the entire\nsystem is differentiable,\nthat scaler output\nis the result of running\nthrough some neural net,\nrunning the answer,\nthe representation of the\nanswer through some neural net.\nThen by gradient descent,\nby back propagating gradients,\nyou can figure out\nlike how to modify the\nrepresentation of the answers\nso as to minimize that.\n- So that's still a gradient based.\n- It's gradient based inference.\nSo now you have a\nrepresentation of the answer\nin abstract space.\nNow you can turn it into text, right?\nAnd the cool thing about this\nis that the representation now\ncan be optimized through gradient descent,\nbut also is independent of the language\nin which you're going\nto express the answer.\n- Right.\nSo you're operating in the\nsubstruct of representation.\nI mean this goes back\nto the joint embedding.\n- [Yann] Right.\n- That it's better to\nwork in the space of...\nI don't know.\nOr to romanticize the notion\nlike space of concepts\nversus the space of concrete\nsensory information.\n- Right.\n- Okay.\nBut can this do something like reasoning,\nwhich is what we're talking about?\n- Well, not really,\nonly in a very simple way.\nI mean basically you can\nthink of those things as doing\nthe kind of optimization\nI was talking about,\nexcept they're optimizing\nthe discrete space\nwhich is the space of\npossible sequences of tokens.\nAnd they do this optimization\nin a horribly inefficient way,\nwhich is generate a lot of hypothesis\nand then select the best ones.\nAnd that's incredibly wasteful\nin terms of competition,\n'cause you basically have to run your LLM\nfor like every possible\ngenerative sequence.\nAnd it's incredibly wasteful.\nSo it's much better to do an optimization\nin continuous space\nwhere you can do gradient descent\nas opposed to like generate tons of things\nand then select the best,\nyou just iteratively refine your answer\nto go towards the best, right?\nThat's much more efficient.\nBut you can only do this\nin continuous spaces\nwith differentiable functions.\n- You're talking about the reasoning,\nlike ability to think\ndeeply or to reason deeply.\nHow do you know what is an answer\nthat's better or worse\nbased on deep reasoning?\n- Right.\nSo then we're asking the question,\nof conceptually, how do you\ntrain an energy based model?\nRight?\nSo energy based model\nis a function with a scaler\noutput, just a number.\nYou give it two inputs, X and Y,\nand it tells you whether Y\nis compatible with X or not.\nX you observe,\nlet's say it's a prompt, an\nimage, a video, whatever.\nAnd Y is a proposal for an answer,\na continuation of video, whatever.\nAnd it tells you whether\nY is compatible with X.\nAnd the way it tells you\nthat Y is compatible with X\nis that the output of that\nfunction would be zero\nif Y is compatible with X,\nit would be a positive number, non-zero\nif Y is not compatible with X.\nOkay.\nHow do you train a system like this?", "mimetype": "text/plain", "start_char_idx": 68969, "end_char_idx": 73156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb5d9572-b690-45d1-9723-2d06e5668f98": {"__data__": {"id_": "fb5d9572-b690-45d1-9723-2d06e5668f98", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d57ba18-7743-4b02-9f61-aad7cc5dfab4", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "764593a76a77879d7f7c7f493bdfecca2e8977b5dc266b00901f4394448098f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8037fe68-a0aa-41a0-b2be-1572abd50b60", "node_type": "1", "metadata": {}, "hash": "9e42fc23a6447dcc1e87cd388c3262d7b5834c957fd6be9c2e37bd5468b72604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "How do you know what is an answer\nthat's better or worse\nbased on deep reasoning?\n- Right.\nSo then we're asking the question,\nof conceptually, how do you\ntrain an energy based model?\nRight?\nSo energy based model\nis a function with a scaler\noutput, just a number.\nYou give it two inputs, X and Y,\nand it tells you whether Y\nis compatible with X or not.\nX you observe,\nlet's say it's a prompt, an\nimage, a video, whatever.\nAnd Y is a proposal for an answer,\na continuation of video, whatever.\nAnd it tells you whether\nY is compatible with X.\nAnd the way it tells you\nthat Y is compatible with X\nis that the output of that\nfunction would be zero\nif Y is compatible with X,\nit would be a positive number, non-zero\nif Y is not compatible with X.\nOkay.\nHow do you train a system like this?\nAt a completely general level,\nis you show it pairs of X\nand Ys that are compatible,\na question and the corresponding answer.\nAnd you train the parameters\nof the big neural net inside\nto produce zero.\nOkay.\nNow that doesn't completely work\nbecause the system might decide,\nwell, I'm just gonna\nsay zero for everything.\nSo now you have to have a process\nto make sure that for a wrong Y,\nthe energy will be larger than zero.\nAnd there you have two options,\none is contrastive methods.\nSo contrastive method is\nyou show an X and a bad Y,\nand you tell the system,\nwell, give a high energy to this.\nLike push up the energy, right?\nChange the weights in the neural\nnet that compute the energy\nso that it goes up.\nSo that's contrasting methods.\nThe problem with this is\nif the space of Y is large,\nthe number of such contrasted samples\nyou're gonna have to show is gigantic.\nBut people do this.\nThey do this when you\ntrain a system with RLHF,\nbasically what you're training\nis what's called a reward model,\nwhich is basically an objective function\nthat tells you whether\nan answer is good or bad.\nAnd that's basically exactly what this is.\nSo we already do this to some extent.\nWe're just not using it for inference,\nwe're just using it for training.\nThere is another set of methods\nwhich are non-contrastive,\nand I prefer those.\nAnd those non-contrastive\nmethod basically say,\nokay, the energy function\nneeds to have low energy on\npairs of XYs that are compatible\nthat come from your training set.\nHow do you make sure that the energy\nis gonna be higher everywhere else?\nAnd the way you do this\nis by having a regularizer, a criterion,\na term in your cost function\nthat basically minimizes\nthe volume of space\nthat can take low energy.\nAnd the precise way to do this,\nthere's all kinds of different\nspecific ways to do this\ndepending on the architecture,\nbut that's the basic principle.\nSo that if you push\ndown the energy function\nfor particular regions in the XY space,\nit will automatically\ngo up in other places\nbecause there's only a\nlimited volume of space\nthat can take low energy.\nOkay?\nBy the construction of the system\nor by the regularizing function.\n- We've been talking very generally,\nbut what is a good X and a good Y?\nWhat is a good representation of X and Y?\nBecause we've been talking about language.\nAnd if you just take language directly,\nthat presumably is not good,\nso there has to be\nsome kind of abstract\nrepresentation of ideas.\n- Yeah.\nI mean you can do this\nwith language directly\nby just, you know, X is a text\nand Y is the continuation of that text.\n- [Lex] Yes.\n- Or X is a question, Y is the answer.\n- But you're saying\nthat's not gonna take it.\nI mean, that's going to\ndo what LLMs are doing.\n- Well, no.\nIt depends on how the internal\nstructure of the system\nis built.\nIf the internal structure of the system\nis built in such a way\nthat inside of the system\nthere is a latent variable,\nlet's called it Z,\nthat you can manipulate\nso as to minimize the output energy,\nthen that Z can be viewed as\nrepresentation of a good answer\nthat you can translate into\na Y that is a good answer.\n- So this kind of system could be trained\nin a very similar way?\n- Very similar way.", "mimetype": "text/plain", "start_char_idx": 72373, "end_char_idx": 76350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8037fe68-a0aa-41a0-b2be-1572abd50b60": {"__data__": {"id_": "8037fe68-a0aa-41a0-b2be-1572abd50b60", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb5d9572-b690-45d1-9723-2d06e5668f98", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "f75c43373c850c070b159a5d0616f00d3c457d26467ea1da48c9d36656fda699", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a95921-3c36-452a-862d-b1c6502d2bd0", "node_type": "1", "metadata": {}, "hash": "3153fe45aa2783330931bd6472ae3fc7138328c0ad42b3e725c4ca60c56e0e46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- Yeah.\nI mean you can do this\nwith language directly\nby just, you know, X is a text\nand Y is the continuation of that text.\n- [Lex] Yes.\n- Or X is a question, Y is the answer.\n- But you're saying\nthat's not gonna take it.\nI mean, that's going to\ndo what LLMs are doing.\n- Well, no.\nIt depends on how the internal\nstructure of the system\nis built.\nIf the internal structure of the system\nis built in such a way\nthat inside of the system\nthere is a latent variable,\nlet's called it Z,\nthat you can manipulate\nso as to minimize the output energy,\nthen that Z can be viewed as\nrepresentation of a good answer\nthat you can translate into\na Y that is a good answer.\n- So this kind of system could be trained\nin a very similar way?\n- Very similar way.\nBut you have to have this\nway of preventing collapse,\nof ensuring that there is high energy\nfor things you don't train it on.\nAnd currently it's very implicit in LLMs.\nIt is done in a way\nthat people don't realize it's being done,\nbut it is being done.\nIt's due to the fact\nthat when you give a high\nprobability to a word,\nautomatically you give low\nprobability to other words\nbecause you only have\na finite amount of probability\nto go around. (laughing)\nRight?\nThey have to sub to one.\nSo when you minimize the\ncross entropy or whatever,\nwhen you train your LLM\nto predict the next word,\nyou are increasing the probability\nyour system will give to the correct word,\nbut you're also decreasing the probability\nit will give to the incorrect words.\nNow, indirectly, that gives\na low probability to...\nA high probability to sequences\nof words that are good\nand low probability two\nsequences of words that are bad,\nbut it's very indirect.\nIt's not obvious why this\nactually works at all,\nbecause you're not doing\nit on a joint probability\nof all the symbols in a sequence,\nyou're just doing it kind of,\nsort of factorized that probability\nin terms of conditional probabilities\nover successive tokens.\n- So how do you do this for visual data?\n- So we've been doing this\nwith all JEPA architectures,\nbasically the-\n- [Lex] The joint embedding?\n- I-JEPA.\nSo there, the compatibility\nbetween two things\nis here's an image or a video,\nhere is a corrupted, shifted\nor transformed version\nof that image or video or masked.\nOkay?\nAnd then the energy of the system\nis the prediction error\nof the representation.\nThe predicted representation\nof the good thing\nversus the actual representation\nof the good thing, right?\nSo you run the corrupted\nimage to the system,\npredict the representation of\nthe good input uncorrupted,\nand then compute the prediction error.\nThat's the energy of the system.\nSo this system will tell you,\nthis is a good image and\nthis is a corrupted version.\nIt will give you zero energy\nif those two things are effectively,\none of them is a corrupted\nversion of the other,\ngive you a high energy\nif the two images are\ncompletely different.\n- And hopefully that whole process\ngives you a really nice\ncompressed representation\nof reality, of visual reality.\n- And we know it does\nbecause then we use those presentations\nas input to a classification\nsystem or something,\nand it works-\n- And then\nthat classification system\nworks really nicely.\nOkay.\nWell, so to summarize,\nyou recommend in a spicy way\nthat only Yann LeCun can,\nyou recommend that we\nabandon generative models\nin favor of joint embedding architectures?\n- [Yann] Yes.\n- Abandon autoregressive generation.\n- [Yann] Yes.\n- Abandon... (laughs)\nThis feels like court testimony.\nAbandon probabilistic models\nin favor of energy based\nmodels, as we talked about.\nAbandon contrastive methods\nin favor of regularized methods.\nAnd let me ask you about this;\nyou've been for a while, a\ncritic of reinforcement learning.\n- [Yann] Yes.\n- So the last recommendation\nis that we abandon RL\nin favor of model predictive control,\nas you were talking about.\nAnd only use RL\nwhen planning doesn't yield\nthe predicted outcome.\nAnd we use RL in that case\nto adjust the world model or the critic.\n- [Yann] Yes.", "mimetype": "text/plain", "start_char_idx": 75605, "end_char_idx": 79607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04a95921-3c36-452a-862d-b1c6502d2bd0": {"__data__": {"id_": "04a95921-3c36-452a-862d-b1c6502d2bd0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8037fe68-a0aa-41a0-b2be-1572abd50b60", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4fcbb592f0d56bf3c683587cfbd736137dcb513702782468bde5294b82423172", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6762b22-01cf-41a9-b6a9-c50d18f1b46a", "node_type": "1", "metadata": {}, "hash": "e72dcddadd9a6dc10c2226c3477941687137090de40ead4141dc35490b364959", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- [Yann] Yes.\n- Abandon autoregressive generation.\n- [Yann] Yes.\n- Abandon... (laughs)\nThis feels like court testimony.\nAbandon probabilistic models\nin favor of energy based\nmodels, as we talked about.\nAbandon contrastive methods\nin favor of regularized methods.\nAnd let me ask you about this;\nyou've been for a while, a\ncritic of reinforcement learning.\n- [Yann] Yes.\n- So the last recommendation\nis that we abandon RL\nin favor of model predictive control,\nas you were talking about.\nAnd only use RL\nwhen planning doesn't yield\nthe predicted outcome.\nAnd we use RL in that case\nto adjust the world model or the critic.\n- [Yann] Yes.\n- So you've mentioned RLHF,\nreinforcement learning\nwith human feedback.\nWhy do you still hate\nreinforcement learning?\n- [Yann] I don't hate\nreinforcement learning,\nand I think it's-\n- So it's all love?\n- I think it should not\nbe abandoned completely,\nbut I think it's use should be minimized\nbecause it's incredibly\ninefficient in terms of samples.\nAnd so the proper way to train a system\nis to first have it learn\ngood representations of\nthe world and world models\nfrom mostly observation,\nmaybe a little bit of interactions.\n- And then steer it based on that.\nIf the representation is good,\nthen the adjustments should be minimal.\n- Yeah.\nNow there's two things.\nIf you've learned the world model,\nyou can use the world model\nto plan a sequence of actions\nto arrive at a particular objective.\nYou don't need RL,\nunless the way you measure\nwhether you succeed\nmight be inexact.\nYour idea of whether you were\ngonna fall from your bike\nmight be wrong,\nor whether the person\nyou're fighting with MMA\nwas gonna do something\nand they do something else. (laughing)\nSo there's two ways you can be wrong.\nEither your objective function\ndoes not reflect\nthe actual objective function\nyou want to optimize,\nor your world model is inaccurate, right?\nSo the prediction you were making\nabout what was gonna happen\nin the world is inaccurate.\nSo if you want to adjust your world model\nwhile you are operating the world\nor your objective function,\nthat is basically in the realm of RL.\nThis is what RL deals with\nto some extent, right?\nSo adjust your world model.\nAnd the way to adjust your\nworld model, even in advance,\nis to explore parts of the\nspace with your world model,\nwhere you know that your\nworld model is inaccurate.\nThat's called curiosity\nbasically, or play, right?\nWhen you play,\nyou kind of explore\npart of the state space\nthat you don't want to do for real\nbecause it might be dangerous,\nbut you can adjust your world model\nwithout killing yourself\nbasically. (laughs)\nSo that's what you want to use RL for.\nWhen it comes time to\nlearning a particular task,\nyou already have all the\ngood representations,\nyou already have your world model,\nbut you need to adjust it\nfor the situation at hand.\nThat's when you use RL.\n- Why do you think RLHF works so well?\nThis enforcement learning\nwith human feedback,\nwhy did it have such a\ntransformational effect\non large language models that came before?\n- So what's had the\ntransformational effect\nis human feedback.\nThere is many ways to use it\nand some of it is just\npurely supervised, actually,\nit's not really reinforcement learning.\n- So it's the HF. (laughing)\n- It's the HF.\nAnd then there is various ways\nto use human feedback, right?\nSo you can ask humans to rate answers,\nmultiple answers that are\nproduced by a world model.\nAnd then what you do is you\ntrain an objective function\nto predict that rating.\nAnd then you can use\nthat objective function\nto predict whether an answer is good,\nand you can back propagate\nreally through this\nto fine tune your system\nso that it only produces\nhighly rated answers.\nOkay?\nSo that's one way.\nSo that's like in RL,\nthat means training what's\ncalled a reward model, right?\nSo something that,\nbasically your small neural net\nthat estimates to what extent\nan answer is good, right?\nIt's very similar to the objective\nI was talking about earlier for planning,\nexcept now it's not used for planning,\nit's used for fine tuning your system.", "mimetype": "text/plain", "start_char_idx": 78974, "end_char_idx": 83032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c6762b22-01cf-41a9-b6a9-c50d18f1b46a": {"__data__": {"id_": "c6762b22-01cf-41a9-b6a9-c50d18f1b46a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a95921-3c36-452a-862d-b1c6502d2bd0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ce67ad8068856e8a079ce6072c9a60b99ab056bc38d81e4126a978abf7547d11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1359fae0-c0ac-4249-9c0c-a410bfd1fecc", "node_type": "1", "metadata": {}, "hash": "2057a1e38626541504c0f6d4c79f385c416c5e88bbfacae8a8dc4f518f389ade", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(laughing)\n- It's the HF.\nAnd then there is various ways\nto use human feedback, right?\nSo you can ask humans to rate answers,\nmultiple answers that are\nproduced by a world model.\nAnd then what you do is you\ntrain an objective function\nto predict that rating.\nAnd then you can use\nthat objective function\nto predict whether an answer is good,\nand you can back propagate\nreally through this\nto fine tune your system\nso that it only produces\nhighly rated answers.\nOkay?\nSo that's one way.\nSo that's like in RL,\nthat means training what's\ncalled a reward model, right?\nSo something that,\nbasically your small neural net\nthat estimates to what extent\nan answer is good, right?\nIt's very similar to the objective\nI was talking about earlier for planning,\nexcept now it's not used for planning,\nit's used for fine tuning your system.\nI think it would be much more efficient\nto use it for planning,\nbut currently it's used\nto fine tune the parameters of the system.\nNow, there's several ways to do this.\nSome of them are supervised.\nYou just ask a human person,\nlike what is a good\nanswer for this, right?\nThen you just type the answer.\nI mean, there's lots of ways\nthat those systems are being adjusted.\n- Now, a lot of people\nhave been very critical\nof the recently released\nGoogle's Gemini 1.5\nfor essentially, in my words,\nI could say super woke.\nWoke in the negative\nconnotation of that word.\nThere is some almost hilariously\nabsurd things that it does,\nlike it modifies history,\nlike generating images of\na black George Washington\nor perhaps more seriously\nsomething that you commented on Twitter,\nwhich is refusing to comment\non or generate images of,\nor even descriptions of\nTiananmen Square or the tank men,\none of the most sort of legendary\nprotest images in history.\nAnd of course, these\nimages are highly censored\nby the Chinese government.\nAnd therefore everybody\nstarted asking questions\nof what is the process\nof designing these LLMs?\nWhat is the role of censorship in these,\nand all that kind of stuff.\nSo you commented on Twitter\nsaying that open source is the answer.\n(laughs)\n- Yeah.\n- Essentially.\nSo can you explain?\n- I actually made that comment\non just about every social network I can.\n(Lex laughs)\nAnd I've made that point\nmultiple times in various forums.\nHere's my point of view on this.\nPeople can complain that\nAI systems are biased,\nand they generally are biased\nby the distribution of the training data\nthat they've been trained on\nthat reflects biases in society.\nAnd that is potentially\noffensive to some people\nor potentially not.\nAnd some techniques to de-bias\nthen become offensive to some people\nbecause of historical\nincorrectness and things like that.\nAnd so you can ask the question.\nYou can ask two questions.\nThe first question is,\nis it possible to produce an\nAI system that is not biased?\nAnd the answer is absolutely not.\nAnd it's not because of\ntechnological challenges,\nalthough there are technological\nchallenges to that.\nIt's because bias is in\nthe eye of the beholder.\nDifferent people may have different ideas\nabout what constitutes\nbias for a lot of things.\nI mean there are facts\nthat are indisputable,\nbut there are a lot of opinions or things\nthat can be expressed in different ways.\nAnd so you cannot have an unbiased system,\nthat's just an impossibility.\nAnd so what's the answer to this?\nAnd the answer is the\nsame answer that we found\nin liberal democracy about the press.\nThe press needs to be free and diverse.\nWe have free speech for a good reason.\nIt's because we don't want\nall of our information\nto come from a unique source,\n'cause that's opposite to\nthe whole idea of democracy\nand progressive ideas\nand even science, right?\nIn science, people have to\nargue for different opinions.\nAnd science makes progress\nwhen people disagree\nand they come up with an answer\nand a consensus forms, right?\nAnd it's true in all\ndemocracies around the world.\nSo there is a future\nwhich is already happening\nwhere every single one of our interaction\nwith the digital world\nwill be mediated by AI systems,\nAI assistance, right?\nWe're gonna have smart glasses.\nYou can already buy them\nfrom Meta, (laughing)\nthe Ray-Ban Meta.", "mimetype": "text/plain", "start_char_idx": 82206, "end_char_idx": 86372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1359fae0-c0ac-4249-9c0c-a410bfd1fecc": {"__data__": {"id_": "1359fae0-c0ac-4249-9c0c-a410bfd1fecc", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6762b22-01cf-41a9-b6a9-c50d18f1b46a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "08263fa614393477780cac115e74a1d199232f1837703680b2ec50e911ae7bbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cc3c5d7-c3b3-48e7-b030-36173754757f", "node_type": "1", "metadata": {}, "hash": "f10c991c9bbbda4c57c0d0e2e843c9408d3b1fc6ab1e8f23911d0f108a0db963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And the answer is the\nsame answer that we found\nin liberal democracy about the press.\nThe press needs to be free and diverse.\nWe have free speech for a good reason.\nIt's because we don't want\nall of our information\nto come from a unique source,\n'cause that's opposite to\nthe whole idea of democracy\nand progressive ideas\nand even science, right?\nIn science, people have to\nargue for different opinions.\nAnd science makes progress\nwhen people disagree\nand they come up with an answer\nand a consensus forms, right?\nAnd it's true in all\ndemocracies around the world.\nSo there is a future\nwhich is already happening\nwhere every single one of our interaction\nwith the digital world\nwill be mediated by AI systems,\nAI assistance, right?\nWe're gonna have smart glasses.\nYou can already buy them\nfrom Meta, (laughing)\nthe Ray-Ban Meta.\nWhere you can talk to them\nand they are connected with an LLM\nand you can get answers\non any question you have.\nOr you can be looking at a monument\nand there is a camera in\nthe system, in the glasses,\nyou can ask it like what can you tell me\nabout this building or this monument?\nYou can be looking at a\nmenu in a foreign language\nand the thing we will\ntranslate it for you.\nWe can do real time translation\nif we speak different languages.\nSo a lot of our interactions\nwith the digital world\nare going to be mediated by those systems\nin the near future.\nIncreasingly, the search\nengines that we're gonna use\nare not gonna be search engines,\nthey're gonna be dialogue systems\nthat we just ask a question,\nand it will answer\nand then point you\nto the perhaps appropriate\nreference for it.\nBut here is the thing,\nwe cannot afford those systems\nto come from a handful of companies\non the west coast of the US\nbecause those systems will constitute\nthe repository of all human knowledge.\nAnd we cannot have that be controlled\nby a small number of people, right?\nIt has to be diverse\nfor the same reason the\npress has to be diverse.\nSo how do we get a diverse\nset of AI assistance?\nIt's very expensive and difficult\nto train a base model, right?\nA base LLM at the moment.\nIn the future might be\nsomething different,\nbut at the moment that's an LLM.\nSo only a few companies\ncan do this properly.\nAnd if some of those\nsubsystems are open source,\nanybody can use them,\nanybody can fine tune them.\nIf we put in place some systems\nthat allows any group of people,\nwhether they are individual citizens,\ngroups of citizens,\ngovernment organizations,\nNGOs, companies, whatever,\nto take those open source\nsystems, AI systems,\nand fine tune them for their\nown purpose on their own data,\nthere we're gonna have\na very large diversity\nof different AI systems\nthat are specialized for\nall of those things, right?\nSo I'll tell you,\nI talked to the French\ngovernment quite a bit\nand the French government will not accept\nthat the digital diet\nof all their citizens\nbe controlled by three companies\non the west coast of the US.\nThat's just not acceptable.\nIt's a danger to democracy.\nRegardless of how well intentioned\nthose companies are, right?\nAnd it's also a danger to local culture,\nto values, to language, right?\nI was talking with the\nfounder of Infosys in India.\nHe's funding a project\nto fine tune LLaMA 2,\nthe open source model produced by Meta.\nSo that LLaMA 2 speaks all 22\nofficial languages in India.\nIt's very important for people in India.\nI was talking to a\nformer colleague of mine,\nMoustapha Cisse,\nwho used to be a scientist at FAIR,\nand then moved back to Africa\nand created a research\nlab for Google in Africa\nand now has a new startup Kera.\nAnd what he's trying to\ndo is basically have LLM\nthat speaks the local languages in Senegal\nso that people can have\naccess to medical information,\n'cause they don't have access to doctors,\nit's a very small number of\ndoctors per capita in Senegal.\nI mean, you can't have any of this\nunless you have open source platforms.", "mimetype": "text/plain", "start_char_idx": 85545, "end_char_idx": 89435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7cc3c5d7-c3b3-48e7-b030-36173754757f": {"__data__": {"id_": "7cc3c5d7-c3b3-48e7-b030-36173754757f", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1359fae0-c0ac-4249-9c0c-a410bfd1fecc", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "29da90e4a2e1281c449d8e722374896c0e11629e4633c87525c2380df892dcb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "669470b0-958c-4c52-9903-cf76af778616", "node_type": "1", "metadata": {}, "hash": "da199f876113a59e541241a51692081077a366b3adc293adcced9beb2f86aa99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I was talking with the\nfounder of Infosys in India.\nHe's funding a project\nto fine tune LLaMA 2,\nthe open source model produced by Meta.\nSo that LLaMA 2 speaks all 22\nofficial languages in India.\nIt's very important for people in India.\nI was talking to a\nformer colleague of mine,\nMoustapha Cisse,\nwho used to be a scientist at FAIR,\nand then moved back to Africa\nand created a research\nlab for Google in Africa\nand now has a new startup Kera.\nAnd what he's trying to\ndo is basically have LLM\nthat speaks the local languages in Senegal\nso that people can have\naccess to medical information,\n'cause they don't have access to doctors,\nit's a very small number of\ndoctors per capita in Senegal.\nI mean, you can't have any of this\nunless you have open source platforms.\nSo with open source platforms,\nyou can have AI systems\nthat are not only diverse in\nterms of political opinions\nor things of that type,\nbut in terms of language,\nculture, value systems,\npolitical opinions, technical\nabilities in various domains.\nAnd you can have an industry,\nan ecosystem of companies\nthat fine tune those open source systems\nfor vertical applications\nin industry, right?\nYou have, I don't know, a\npublisher has thousands of books\nand they want to build a system\nthat allows a customer\nto just ask a question\nabout the content of any of their books.\nYou need to train on their\nproprietary data, right?\nYou have a company,\nwe have one within Meta\nit's called Meta Mate.\nAnd it's basically an LLM\nthat can answer any question\nabout internal stuff\nabout about the company.\nVery useful.\nA lot of companies want this, right?\nA lot of companies want this\nnot just for their employees,\nbut also for their customers,\nto take care of their customers.\nSo the only way you're\ngonna have an AI industry,\nthe only way you're gonna have AI systems\nthat are not uniquely biased,\nis if you have open source platforms\non top of which any group can\nbuild specialized systems.\nSo the inevitable direction of history\nis that the vast majority of AI systems\nwill be built on top of\nopen source platforms.\n- So that's a beautiful vision.\nSo meaning like a company\nlike Meta or Google or so on,\nshould take only minimal fine tuning steps\nafter the building, the\nfoundation, pre-trained model.\nAs few steps as possible.\n- Basically.\n(Lex sighs)\n- Can Meta afford to do that?\n- No.\n- So I don't know if you know this,\nbut companies are supposed\nto make money somehow.\nAnd open source is like giving away...\nI don't know, Mark made a video,\nMark Zuckerberg.\nA very sexy video talking\nabout 350,000 Nvidia H100s.\nThe math of that is,\njust for the GPUs,\nthat's a hundred billion,\nplus the infrastructure\nfor training everything.\nSo I'm no business guy,\nbut how do you make money on that?\nSo the vision you paint\nis a really powerful one,\nbut how is it possible to make money?\n- Okay.\nSo you have several\nbusiness models, right?\nThe business model that\nMeta is built around\nis you offer a service,\nand the financing of that service\nis either through ads or\nthrough business customers.\nSo for example, if you have an LLM\nthat can help a mom-and-pop pizza place\nby talking to their\ncustomers through WhatsApp,\nand so the customers\ncan just order a pizza\nand the system will just ask them,\nlike what topping do you want\nor what size, blah blah, blah.\nThe business will pay for that.\nOkay?\nThat's a model.\nAnd otherwise, if it's a system\nthat is on the more kind\nof classical services,\nit can be ad supported or\nthere's several models.\nBut the point is,\nif you have a big enough\npotential customer base\nand you need to build that\nsystem anyway for them,\nit doesn't hurt you\nto actually distribute it to open source.\n- Again, I'm no business guy,\nbut if you release the open source model,\nthen other people can\ndo the same kind of task\nand compete on it.\nBasically provide fine\ntuned models for businesses,\nis the bet that Meta is making...\nBy the way, I'm a huge fan of all this.\nBut is the bet that Meta is making\nis like, \"we'll do a better job of it?\"\n- Well, no.", "mimetype": "text/plain", "start_char_idx": 88669, "end_char_idx": 92686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "669470b0-958c-4c52-9903-cf76af778616": {"__data__": {"id_": "669470b0-958c-4c52-9903-cf76af778616", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cc3c5d7-c3b3-48e7-b030-36173754757f", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b168bd105996aca3c95d61c93f524479decbc4c31aaa19bcb7881045402c0105", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bc1a505-6730-4f88-9a1f-f83801fc462d", "node_type": "1", "metadata": {}, "hash": "2f0dc63c2ac2ef3335e75b210c5152cbfebb75202816e58b49f753b05b616253", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The business will pay for that.\nOkay?\nThat's a model.\nAnd otherwise, if it's a system\nthat is on the more kind\nof classical services,\nit can be ad supported or\nthere's several models.\nBut the point is,\nif you have a big enough\npotential customer base\nand you need to build that\nsystem anyway for them,\nit doesn't hurt you\nto actually distribute it to open source.\n- Again, I'm no business guy,\nbut if you release the open source model,\nthen other people can\ndo the same kind of task\nand compete on it.\nBasically provide fine\ntuned models for businesses,\nis the bet that Meta is making...\nBy the way, I'm a huge fan of all this.\nBut is the bet that Meta is making\nis like, \"we'll do a better job of it?\"\n- Well, no.\nThe bet is more,\nwe already have a huge user\nbase and customer base.\n- [Lex] Ah, right.\n- Right?\nSo it's gonna be useful to them.\nWhatever we offer them is gonna be useful\nand there is a way to\nderive revenue from this.\n- [Lex] Sure.\n- And it doesn't hurt\nthat we provide that system\nor the base model, right?\nThe foundation model in open source\nfor others to build\napplications on top of it too.\nIf those applications\nturn out to be useful for our customers,\nwe can just buy it for them.\nIt could be that they\nwill improve the platform.\nIn fact, we see this already.\nI mean there is literally\nmillions of downloads of LLaMA 2\nand thousands of people\nwho have provided ideas\nabout how to make it better.\nSo this clearly accelerates progress\nto make the system available\nto sort of a wide community of people.\nAnd there is literally\nthousands of businesses\nwho are building applications with it.\nMeta's ability to derive\nrevenue from this technology\nis not impaired by the distribution\nof base models in open source.\n- The fundamental criticism\nthat Gemini is getting\nis that, as you pointed\nout on the west coast...\nJust to clarify,\nwe're currently in the east coast,\nwhere I would suppose Meta\nAI headquarters would be.\n(laughs)\nSo strong words about the west coast.\nBut I guess the issue that happens is,\nI think it's fair to say\nthat most tech people\nhave a political affiliation\nwith the left wing.\nThey lean left.\nAnd so the problem that people\nare criticizing Gemini with\nis that in that de-biasing\nprocess that you mentioned,\nthat their ideological\nlean becomes obvious.\nIs this something that could be escaped?\nYou're saying open source is the only way?\n- [Yann] Yeah.\n- Have you witnessed this\nkind of ideological lean\nthat makes engineering difficult?\n- No, I don't think it has to do...\nI don't think the issue has to do\nwith the political leaning\nof the people designing those systems.\nIt has to do with the\nacceptability or political leanings\nof their customer base or audience, right?\nSo a big company cannot afford\nto offend too many people.\nSo they're going to make sure\nthat whatever product\nthey put out is \"safe,\"\nwhatever that means.\nAnd it's very possible to overdo it.\nAnd it's also very possible to...\nIt's impossible to do it\nproperly for everyone.\nYou're not going to satisfy everyone.\nSo that's what I said before,\nyou cannot have a system that is unbiased\nand is perceived as unbiased by everyone.\nIt's gonna be,\nyou push it in one way,\none set of people are\ngonna see it as biased.\nAnd then you push it the other way\nand another set of people\nis gonna see it as biased.\nAnd then in addition to this,\nthere's the issue of\nif you push the system\nperhaps a little too far in one direction,\nit's gonna be non-factual, right?\nYou're gonna have black Nazi soldiers in-\n- Yeah.\nSo we should mention image generation\nof black Nazi soldiers,\nwhich is not factually accurate.\n- Right.\nAnd can be offensive for\nsome people as well, right?\nSo it's gonna be impossible\nto kind of produce systems\nthat are unbiased for everyone.\nSo the only solution\nthat I see is diversity.\n- And diversity in full\nmeaning of that word,\ndiversity in every possible way.\n- [Yann] Yeah.\n- Marc Andreessen just tweeted today,\nlet me do a TL;DR.", "mimetype": "text/plain", "start_char_idx": 91972, "end_char_idx": 95926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bc1a505-6730-4f88-9a1f-f83801fc462d": {"__data__": {"id_": "7bc1a505-6730-4f88-9a1f-f83801fc462d", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "669470b0-958c-4c52-9903-cf76af778616", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "c9e8e90bb691162f2e4c98e6e64507abe53dda64e1056e4fbf551ee27b6eb53e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5adda293-a79d-423e-beca-89973e3cfbfe", "node_type": "1", "metadata": {}, "hash": "d5fe44d9b294f9db5c1ee0bf4cdc8e257741a2bcbe5911f15f9ab420003d32ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And then you push it the other way\nand another set of people\nis gonna see it as biased.\nAnd then in addition to this,\nthere's the issue of\nif you push the system\nperhaps a little too far in one direction,\nit's gonna be non-factual, right?\nYou're gonna have black Nazi soldiers in-\n- Yeah.\nSo we should mention image generation\nof black Nazi soldiers,\nwhich is not factually accurate.\n- Right.\nAnd can be offensive for\nsome people as well, right?\nSo it's gonna be impossible\nto kind of produce systems\nthat are unbiased for everyone.\nSo the only solution\nthat I see is diversity.\n- And diversity in full\nmeaning of that word,\ndiversity in every possible way.\n- [Yann] Yeah.\n- Marc Andreessen just tweeted today,\nlet me do a TL;DR.\nThe conclusion is only\nstartups and open source\ncan avoid the issue that he's\nhighlighting with big tech.\nHe's asking,\ncan big tech actually field\ngenerative AI products?\nOne, ever escalating demands\nfrom internal activists,\nemployee mobs, crazed executives,\nbroken boards, pressure groups,\nextremist regulators,\ngovernment agencies, the press,\nin quotes \"experts,\"\nand everything corrupting the output.\nTwo, constant risk of\ngenerating a bad answer\nor drawing a bad picture\nor rendering a bad video.\nWho knows what it's going\nto say or do at any moment?\nThree, legal exposure,\nproduct liability, slander,\nelection law, many other things and so on.\nAnything that makes Congress mad.\nFour, continuous attempts\nto tighten grip on acceptable output,\ndegrade the model,\nlike how good it actually is\nin terms of usable and\npleasant to use and effective\nand all that kind of stuff.\nAnd five, publicity of\nbad text, images, video,\nactual puts those examples\ninto the training data\nfor the next version.\nAnd so on.\nSo he just highlights\nhow difficult this is.\nFrom all kinds of people being unhappy.\nHe just said you can't create a system\nthat makes everybody happy.\n- [Yann] Yes.\n- So if you're going to do\nthe fine tuning yourself\nand keep a close source,\nessentially the problem there\nis then trying to minimize\nthe number of people\nwho are going to be unhappy.\n- [Yann] Yeah.\n- And you're saying like the only...\nThat that's almost\nimpossible to do, right?\nAnd the better way is to do open source.\n- Basically, yeah.\nI mean Marc is right about a\nnumber of things that he lists\nthat indeed scare large companies.\nCertainly, congressional\ninvestigations is one of them.\nLegal liability.\nMaking things\nthat get people to hurt\nthemselves or hurt others.\nLike big companies are really careful\nabout not producing things of this type,\nbecause they have...\nThey don't want to hurt\nanyone, first of all.\nAnd then second, they wanna\npreserve their business.\nSo it's essentially impossible\nfor systems like this\nthat can inevitably\nformulate political opinions\nand opinions about various things\nthat may be political or not,\nbut that people may disagree about.\nAbout, you know, moral issues\nand things about like\nquestions about religion\nand things like that, right?\nOr cultural issues\nthat people from different communities\nwould disagree with in the first place.\nSo there's only kind of a\nrelatively small number of things\nthat people will sort of agree on,\nbasic principles.\nBut beyond that,\nif you want those systems to be useful,\nthey will necessarily have\nto offend a number of people,\ninevitably.\n- And so open source is just better-\n- [Yann] Diversity is better, right?\n- And open source enables diversity.\n- That's right.\nOpen source enables diversity.\n- This can be a fascinating world\nwhere if it's true that\nthe open source world,\nif Meta leads the way\nand creates this kind of open\nsource foundation model world,\nthere's going to be,\nlike governments will have a\nfine tuned model. (laughing)\n- [Yann] Yeah.\n- And then potentially,\npeople that vote left and right\nwill have their own model and preference\nto be able to choose.\nAnd it will potentially\ndivide us even more\nbut that's on us humans.\nWe get to figure out...\nBasically the technology enables humans\nto human more effectively.\nAnd all the difficult ethical\nquestions that humans raise\nwe'll just leave it up\nto us to figure that out.", "mimetype": "text/plain", "start_char_idx": 95197, "end_char_idx": 99318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5adda293-a79d-423e-beca-89973e3cfbfe": {"__data__": {"id_": "5adda293-a79d-423e-beca-89973e3cfbfe", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bc1a505-6730-4f88-9a1f-f83801fc462d", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "a8e2eedc24db62006496b9e14ec8903c2acb1a6a6025e57fedd95e2fd5b0da3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8ec2f21-fa2c-4c70-b847-ad382728a3f6", "node_type": "1", "metadata": {}, "hash": "ad4e92110a7a7d817a22e30bbbf5b181d5c858ed042fb2957104fdd40c2842a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- And so open source is just better-\n- [Yann] Diversity is better, right?\n- And open source enables diversity.\n- That's right.\nOpen source enables diversity.\n- This can be a fascinating world\nwhere if it's true that\nthe open source world,\nif Meta leads the way\nand creates this kind of open\nsource foundation model world,\nthere's going to be,\nlike governments will have a\nfine tuned model. (laughing)\n- [Yann] Yeah.\n- And then potentially,\npeople that vote left and right\nwill have their own model and preference\nto be able to choose.\nAnd it will potentially\ndivide us even more\nbut that's on us humans.\nWe get to figure out...\nBasically the technology enables humans\nto human more effectively.\nAnd all the difficult ethical\nquestions that humans raise\nwe'll just leave it up\nto us to figure that out.\n- Yeah, I mean there are\nsome limits to what...\nThe same way there are\nlimits to free speech,\nthere has to be some\nlimit to the kind of stuff\nthat those systems might\nbe authorized to produce,\nsome guardrails.\nSo I mean, that's one thing\nI've been interested in,\nwhich is in the type of architecture\nthat we were discussing before,\nwhere the output of the system\nis a result of an inference\nto satisfy an objective.\nThat objective can include guardrails.\nAnd we can put guardrails\nin open source systems.\nI mean, if we eventually have systems\nthat are built with this blueprint,\nwe can put guardrails in those systems\nthat guarantee\nthat there is sort of a\nminimum set of guardrails\nthat make the system non-dangerous\nand non-toxic, et cetera.\nBasic things that\neverybody would agree on.\nAnd then the fine tuning\nthat people will add\nor the additional guardrails\nthat people will add\nwill kind of cater to their\ncommunity, whatever it is.\n- And yeah, the fine tuning\nwould be more about the gray\nareas of what is hate speech,\nwhat is dangerous and\nall that kind of stuff.\nI mean, you've-\n- [Yann] Or different value systems.\n- Different value systems.\nBut still even with the objectives\nof how to build a bio weapon, for example,\nI think something you've commented on,\nor at least there's a paper\nwhere a collection of researchers\nis trying to understand the\nsocial impacts of these LLMs.\nAnd I guess one threshold that's nice\nis like does the LLM make it\nany easier than a search would,\nlike a Google search would?\n- Right.\nSo the increasing number\nof studies on this\nseems to point to the\nfact that it doesn't help.\nSo having an LLM doesn't help you\ndesign or build a bio\nweapon or a chemical weapon\nif you already have access to\na search engine and a library.\nAnd so the sort of increased\ninformation you get\nor the ease with which you get\nit doesn't really help you.\nThat's the first thing.\nThe second thing is,\nit's one thing to have\na list of instructions\nof how to make a chemical weapon,\nfor example, a bio weapon.\nIt's another thing to actually build it.\nAnd it's much harder than you might think,\nand then LLM will not help you with that.\nIn fact, nobody in the world,\nnot even like countries use bio weapons\nbecause most of the time they have no idea\nhow to protect their own\npopulations against it.\nSo it's too dangerous\nactually to kind of ever use.\nAnd it's in fact banned\nby international treaties.\nChemical weapons is different.\nIt's also banned by treaties,\nbut it's the same problem.\nIt's difficult to use\nin situations that doesn't\nturn against the perpetrators.\nBut we could ask Elon Musk.\nLike I can give you a very\nprecise list of instructions\nof how you build a rocket engine.\nAnd even if you have\na team of 50 engineers\nthat are really experienced building it,\nyou're still gonna have\nto blow up a dozen of them\nbefore you get one that works.\nAnd it's the same with\nchemical weapons or bio weapons\nor things like this.\nIt requires expertise in the real world\nthat the LLM is not gonna help you with.\n- And it requires even\nthe common sense expertise\nthat we've been talking about,\nwhich is how to take\nlanguage based instructions\nand materialize them in the physical world\nrequires a lot of knowledge\nthat's not in the instructions.\n- Yeah, exactly.", "mimetype": "text/plain", "start_char_idx": 98517, "end_char_idx": 102592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d8ec2f21-fa2c-4c70-b847-ad382728a3f6": {"__data__": {"id_": "d8ec2f21-fa2c-4c70-b847-ad382728a3f6", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5adda293-a79d-423e-beca-89973e3cfbfe", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "39c5f7158b1f417343c60b6a43a73c7a7fa92e74ffd4e1069335fbbbe9d4a42c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a6e98ba-914d-4f8a-82d9-493dddf00598", "node_type": "1", "metadata": {}, "hash": "818cc715b72f4f174760278436088c22cb2b45c8aee2d2e4da23dac7d40858d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chemical weapons is different.\nIt's also banned by treaties,\nbut it's the same problem.\nIt's difficult to use\nin situations that doesn't\nturn against the perpetrators.\nBut we could ask Elon Musk.\nLike I can give you a very\nprecise list of instructions\nof how you build a rocket engine.\nAnd even if you have\na team of 50 engineers\nthat are really experienced building it,\nyou're still gonna have\nto blow up a dozen of them\nbefore you get one that works.\nAnd it's the same with\nchemical weapons or bio weapons\nor things like this.\nIt requires expertise in the real world\nthat the LLM is not gonna help you with.\n- And it requires even\nthe common sense expertise\nthat we've been talking about,\nwhich is how to take\nlanguage based instructions\nand materialize them in the physical world\nrequires a lot of knowledge\nthat's not in the instructions.\n- Yeah, exactly.\nA lot of biologists have\nposted on this actually\nin response to those things\nsaying like do you realize how hard it is\nto actually do the lab work?\nLike this is not trivial.\n- Yeah.\nAnd that's Hans Moravec\ncomes to light once again.\nJust to linger on LLaMA.\nMark announced that LLaMA\n3 is coming out eventually,\nI don't think there's a release date,\nbut what are you most excited about?\nFirst of all, LLaMA 2\nthat's already out there,\nand maybe the future LLaMA 3, 4, 5, 6, 10,\njust the future of the\nopen source under Meta?\n- Well, a number of things.\nSo there's gonna be like\nvarious versions of LLaMA\nthat are improvements of previous LLaMAs.\nBigger, better, multimodal,\nthings like that.\nAnd then in future generations,\nsystems that are capable of planning,\nthat really understand\nhow the world works,\nmaybe are trained from video\nso they have some world model.\nMaybe capable of the type\nof reasoning and planning\nI was talking about earlier.\nLike how long is that gonna take?\nLike when is the research that\nis going in that direction\ngoing to sort of feed into\nthe product line, if you want,\nof LLaMA?\nI don't know, I can't tell you.\nAnd there's a few breakthroughs\nthat we have to basically go through\nbefore we can get there.\nBut you'll be able to monitor our progress\nbecause we publish our research, right?\nSo last week we published the V-JEPA work,\nwhich is sort of a first step\ntowards training systems from video.\nAnd then the next step\nis gonna be world models\nbased on kind of this type of idea,\ntraining from video.\nThere's similar work at\nDeepMind also taking place,\nand also at UC Berkeley\non world models and video.\nA lot of people are working on this.\nI think a lot of good ideas are appearing.\nMy bet is that those systems\nare gonna be JEPA-like,\nthey're not gonna be generative models.\nAnd we'll see what the future will tell.\nThere's really good work at...\nA gentleman called Danijar\nHafner who is now DeepMind,\nwho's worked on kind\nof models of this type\nthat learn representations\nand then use them for\nplanning or learning tasks\nby reinforcement training.\nAnd a lot of work at Berkeley\nby Pieter Abbeel, Sergey Levine,\na bunch of other people of that type.\nI'm collaborating with actually\nin the context of some\ngrants with my NYU hat.\nAnd then collaborations also through Meta,\n'cause the lab at Berkeley\nis associated with Meta\nin some way, with FAIR.\nSo I think it's very exciting.\nI think I'm super excited about...\nI haven't been that excited\nabout like the direction\nof machine learning and AI\nsince 10 years ago when FAIR was started,\nand before that, 30 years ago,\nwhen we were working on,\nsorry 35,\non combination nets and the\nearly days of neural net.\nSo I'm super excited\nbecause I see a path towards\npotentially human level intelligence\nwith systems that can\nunderstand the world,\nremember, plan, reason.\nThere is some set of ideas\nto make progress there\nthat might have a chance of working.\nAnd I'm really excited about this.\nWhat I like is that\nsomewhat we get onto like a good direction\nand perhaps succeed before my\nbrain turns to a white sauce\nor before I need to retire.\n(laughs)\n- Yeah.", "mimetype": "text/plain", "start_char_idx": 101733, "end_char_idx": 105723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a6e98ba-914d-4f8a-82d9-493dddf00598": {"__data__": {"id_": "2a6e98ba-914d-4f8a-82d9-493dddf00598", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8ec2f21-fa2c-4c70-b847-ad382728a3f6", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b2e4177534d3b30c5787b2cc7ee6ea073afe081fadf43a85aabb12655578e71e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c37aab4-0de2-4a66-b058-ffcb33ffeda7", "node_type": "1", "metadata": {}, "hash": "8179e0bb0a2150418022496750d938aeec5e83408a88b2720d026a75e0df988e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So I think it's very exciting.\nI think I'm super excited about...\nI haven't been that excited\nabout like the direction\nof machine learning and AI\nsince 10 years ago when FAIR was started,\nand before that, 30 years ago,\nwhen we were working on,\nsorry 35,\non combination nets and the\nearly days of neural net.\nSo I'm super excited\nbecause I see a path towards\npotentially human level intelligence\nwith systems that can\nunderstand the world,\nremember, plan, reason.\nThere is some set of ideas\nto make progress there\nthat might have a chance of working.\nAnd I'm really excited about this.\nWhat I like is that\nsomewhat we get onto like a good direction\nand perhaps succeed before my\nbrain turns to a white sauce\nor before I need to retire.\n(laughs)\n- Yeah.\nYeah.\nAre you also excited by...\nIs it beautiful to you just\nthe amount of GPUs involved,\nsort of the whole training\nprocess on this much compute?\nJust zooming out,\njust looking at earth and humans together\nhave built these computing devices\nand are able to train this one brain,\nwe then open source.\n(laughs)\nLike giving birth to\nthis open source brain\ntrained on this gigantic compute system.\nThere's just the details\nof how to train on that,\nhow to build the infrastructure\nand the hardware,\nthe cooling, all of this kind of stuff.\nAre you just still the\nmost of your excitement\nis in the theory aspect of it?\nMeaning like the software.\n- Well, I used to be a\nhardware guy many years ago.\n(laughs)\n- Yes, yes, that's right.\n- Decades ago.\n- Hardware has improved a little bit.\nChanged a little bit, yeah.\n- I mean, certainly scale is\nnecessary but not sufficient.\n- [Lex] Absolutely.\n- So we certainly need computation.\nI mean, we're still far\nin terms of compute power\nfrom what we would need\nto match the compute\npower of the human brain.\nThis may occur in the next couple decades,\nbut we're still some ways away.\nAnd certainly in terms\nof power efficiency,\nwe're really far.\nSo a lot of progress to make in hardware.\nAnd right now a lot of\nthe progress is not...\nI mean, there's a bit coming\nfrom Silicon technology,\nbut a lot of it coming from\narchitectural innovation\nand quite a bit coming from\nlike more efficient ways\nof implementing the architectures\nthat have become popular.\nBasically combination of\ntransformers and com net, right?\nAnd so there's still some ways to go\nuntil we are going to saturate.\nWe're gonna have to come up\nwith like new principles,\nnew fabrication technology,\nnew basic components,\nperhaps based on sort\nof different principles\nthan those classical digital CMOS.\n- Interesting.\nSo you think in order to build AmI, ami,\nwe potentially might need\nsome hardware innovation too?\n- Well, if we wanna make it ubiquitous,\nyeah, certainly.\nBecause we're gonna have to\nreduce the power consumption.\nA GPU today, right?\nIs half a kilowatt to a kilowatt.\nHuman brain is about 25 watts.\nAnd the GPU is way below\nthe power of human brain.\nYou need something like a hundred thousand\nor a million to match it.\nSo we are off by a huge factor.\n- You often say that\nAGI is not coming soon.\nMeaning like not this year,\nnot the next few years,\npotentially farther away.\nWhat's your basic intuition behind that?\n- So first of all, it's\nnot to be an event, right?\nThe idea somehow\nwhich is popularized by\nscience fiction in Hollywood\nthat somehow somebody is\ngonna discover the secret,\nthe secret to AGI or\nhuman level AI or AmI,\nwhatever you wanna call it,\nand then turn on a machine\nand then we have AGI.\nThat's just not going to happen.\nIt's not going to be an event.\nIt's gonna be gradual progress.\nAre we gonna have systems\nthat can learn from\nvideo how the world works\nand learn good representations?\nYeah.\nBefore we get them to\nthe scale and performance\nthat we observe in humans,\nit's gonna take quite a while.\nIt's not gonna happen in one day.\nAre we gonna get systems\nthat can have large amount\nof associated memories\nso they can remember stuff?\nYeah.\nBut same, it's not gonna happen tomorrow.\nI mean, there is some basic techniques\nthat need to be developed.", "mimetype": "text/plain", "start_char_idx": 104972, "end_char_idx": 109009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c37aab4-0de2-4a66-b058-ffcb33ffeda7": {"__data__": {"id_": "4c37aab4-0de2-4a66-b058-ffcb33ffeda7", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a6e98ba-914d-4f8a-82d9-493dddf00598", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "369d5a36479ca4a6257b5e6a875fc384b211358aaa5f7dd03d04581abf98f605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1498321-9180-47c2-a360-2082ea4c4100", "node_type": "1", "metadata": {}, "hash": "ebbb7da79fffbc2684fe0489b78cad696fd244d4faa6c4fabe56dd4ead6bb1e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The idea somehow\nwhich is popularized by\nscience fiction in Hollywood\nthat somehow somebody is\ngonna discover the secret,\nthe secret to AGI or\nhuman level AI or AmI,\nwhatever you wanna call it,\nand then turn on a machine\nand then we have AGI.\nThat's just not going to happen.\nIt's not going to be an event.\nIt's gonna be gradual progress.\nAre we gonna have systems\nthat can learn from\nvideo how the world works\nand learn good representations?\nYeah.\nBefore we get them to\nthe scale and performance\nthat we observe in humans,\nit's gonna take quite a while.\nIt's not gonna happen in one day.\nAre we gonna get systems\nthat can have large amount\nof associated memories\nso they can remember stuff?\nYeah.\nBut same, it's not gonna happen tomorrow.\nI mean, there is some basic techniques\nthat need to be developed.\nWe have a lot of them,\nbut like to get this to work\ntogether with a full system\nis another story.\nAre we gonna have systems\nthat can reason and plan,\nperhaps along the lines of\nobjective driven AI architectures\nthat I described before?\nYeah, but like before we\nget this to work properly,\nit's gonna take a while.\nAnd before we get all those\nthings to work together.\nAnd then on top of this,\nhave systems that can learn\nlike hierarchical planning,\nhierarchical representations,\nsystems that can be configured\nfor a lot of different situation at hands\nthe way the human brain can.\nAll of this is gonna\ntake at least a decade,\nprobably much more,\nbecause there are a lot of problems\nthat we're not seeing right now\nthat we have not encountered.\nAnd so we don't know if\nthere is an easy solution\nwithin this framework.\nIt's not just around the corner.\nI mean, I've been hearing\npeople for the last 12, 15 years\nclaiming that AGI is\njust around the corner\nand being systematically wrong.\nAnd I knew they were wrong\nwhen they were saying it.\nI called it bullshit.\n(laughs)\n- Why do you think people\nhave been calling...\nFirst of all, I mean,\nfrom the beginning of,\nfrom the birth of the term\nartificial intelligence,\nthere has been an eternal optimism\nthat's perhaps unlike other technologies.\nIs it Moravec's paradox?\nIs it the explanation\nfor why people are so\noptimistic about AGI?\n- I don't think it's\njust Moravec's paradox.\nMoravec's paradox is a consequence\nof realizing that the world\nis not as easy as we think.\nSo first of all, intelligence\nis not a linear thing\nthat you can measure with a scaler,\nwith a single number.\nCan you say that humans are\nsmarter than orangutans?\nIn some ways, yes,\nbut in some ways orangutans\nare smarter than humans\nin a lot of domains\nthat allows them to survive\nin the forest, (laughing)\nfor example.\n- So IQ is a very limited\nmeasure of intelligence.\nTrue intelligence\nis bigger than what IQ,\nfor example, measures.\n- Well, IQ can measure\napproximately something for humans,\nbut because humans kind of come\nin relatively kind of uniform form, right?\n- [Lex] Yeah.\n- But it only measures one type of ability\nthat may be relevant for\nsome tasks, but not others.\nBut then if you are talking\nabout other intelligent entities\nfor which the basic things\nthat are easy to them\nis very different,\nthen it doesn't mean anything.\nSo intelligence is a collection of skills\nand an ability to acquire\nnew skills efficiently.\nRight?\nAnd the collection of skills\nthat a particular\nintelligent entity possess\nor is capable of learning quickly\nis different from the collection\nof skills of another one.\nAnd because it's a multidimensional thing,\nthe set of skills is a\nhigh dimensional space,\nyou can't measure.\nYou cannot compare two things\nas to whether one is more\nintelligent than the other.\nIt's multidimensional.\n- So you push back against what\nare called AI doomers a lot.\nCan you explain their perspective\nand why you think they're wrong?\n- Okay.\nSo AI doomers imagine all\nkinds of catastrophe scenarios\nof how AI could escape our control\nand basically kill us all. (laughs)\nAnd that relies on a\nwhole bunch of assumptions\nthat are mostly false.\nSo the first assumption\nis that the emergence\nof super intelligence\ncould be an event.", "mimetype": "text/plain", "start_char_idx": 108204, "end_char_idx": 112269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d1498321-9180-47c2-a360-2082ea4c4100": {"__data__": {"id_": "d1498321-9180-47c2-a360-2082ea4c4100", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c37aab4-0de2-4a66-b058-ffcb33ffeda7", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "6f984cb42f319ca5807fca2409becaafebb60d5a2f422fd7c3c5b0195e89e230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae10f68-c4a6-45ce-be29-72ea79e20e7a", "node_type": "1", "metadata": {}, "hash": "2ca3788a144fd055291e0be03c1fc34cb50e325f7c4819c1dfb28d88231e2f2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Right?\nAnd the collection of skills\nthat a particular\nintelligent entity possess\nor is capable of learning quickly\nis different from the collection\nof skills of another one.\nAnd because it's a multidimensional thing,\nthe set of skills is a\nhigh dimensional space,\nyou can't measure.\nYou cannot compare two things\nas to whether one is more\nintelligent than the other.\nIt's multidimensional.\n- So you push back against what\nare called AI doomers a lot.\nCan you explain their perspective\nand why you think they're wrong?\n- Okay.\nSo AI doomers imagine all\nkinds of catastrophe scenarios\nof how AI could escape our control\nand basically kill us all. (laughs)\nAnd that relies on a\nwhole bunch of assumptions\nthat are mostly false.\nSo the first assumption\nis that the emergence\nof super intelligence\ncould be an event.\nThat at some point we're\ngoing to figure out the secret\nand we'll turn on a machine\nthat is super intelligent.\nAnd because we'd never done it before,\nit's gonna take over the\nworld and kill us all.\nThat is false.\nIt's not gonna be an event.\nWe're gonna have systems that\nare like as smart as a cat,\nhave all the characteristics\nof human level intelligence,\nbut their level of intelligence\nwould be like a cat or a\nparrot maybe or something.\nAnd then we're gonna walk our way up\nto kind of make those\nthings more intelligent.\nAnd as we make them more intelligent,\nwe're also gonna put\nsome guardrails in them\nand learn how to kind\nof put some guardrails\nso they behave properly.\nAnd we're not gonna do\nthis with just one...\nIt's not gonna be one effort,\nbut it's gonna be lots of\ndifferent people doing this.\nAnd some of them are gonna succeed\nat making intelligent systems\nthat are controllable and safe\nand have the right guardrails.\nAnd if some other goes rogue,\nthen we can use the good ones\nto go against the rogue ones.\n(laughs)\nSo it's gonna be smart AI\npolice against your rogue AI.\nSo it's not gonna be like\nwe're gonna be exposed\nto like a single rogue AI\nthat's gonna kill us all.\nThat's just not happening.\nNow, there is another fallacy,\nwhich is the fact that because\nthe system is intelligent,\nit necessarily wants to take over.\nAnd there is several arguments\nthat make people scared of this,\nwhich I think are\ncompletely false as well.\nSo one of them is in nature,\nit seems to be that the\nmore intelligent species\nare the ones that end\nup dominating the other.\nAnd even extinguishing the others\nsometimes by design,\nsometimes just by mistake.\nAnd so there is sort of a thinking\nby which you say, well, if AI systems\nare more intelligent than us,\nsurely they're going to eliminate us,\nif not by design,\nsimply because they don't care about us.\nAnd that's just preposterous\nfor a number of reasons.\nFirst reason is they're\nnot going to be a species.\nThey're not gonna be a\nspecies that competes with us.\nThey're not gonna have\nthe desire to dominate\nbecause the desire to dominate\nis something that has to be hardwired\ninto an intelligent system.\nIt is hardwired in humans,\nit is hardwired in baboons,\nin chimpanzees, in wolves,\nnot in orangutans.\nThe species in which this\ndesire to dominate or submit\nor attain status in other ways\nis specific to social species.\nNon-social species like\norangutans don't have it.\nRight?\nAnd they are as smart as we are, almost.\nRight?\n- And to you, there's\nnot significant incentive\nfor humans to encode\nthat into the AI systems.\nAnd to the degree they do,\nthere'll be other AIs that\nsort of punish them for it.\nOut-compete them over-\n- Well, there's all kinds of incentive\nto make AI systems submissive to humans.\nRight?\n- [Lex] Right.\n- I mean, this is the way\nwe're gonna build them, right?\nAnd so then people say,\noh, but look at LLMs.\nLLMs are not controllable.\nAnd they're right,\nLLMs are not controllable.\nBut objective driven AI,\nso systems that derive their answers\nby optimization of an objective\nmeans they have to\noptimize this objective,\nand that objective can include guardrails.\nOne guardrail is obey humans.", "mimetype": "text/plain", "start_char_idx": 111458, "end_char_idx": 115440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ae10f68-c4a6-45ce-be29-72ea79e20e7a": {"__data__": {"id_": "2ae10f68-c4a6-45ce-be29-72ea79e20e7a", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1498321-9180-47c2-a360-2082ea4c4100", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0e66efeab313e4c096588507f4fa0caabe80ec682c2e074cea1a9b506fd6d20a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33007c73-4a9a-484e-abf7-c529794c81a0", "node_type": "1", "metadata": {}, "hash": "db07b35ab45b53f1bc402916dc48d7471abf1b7c8dbb6f57593646212f2b313d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Non-social species like\norangutans don't have it.\nRight?\nAnd they are as smart as we are, almost.\nRight?\n- And to you, there's\nnot significant incentive\nfor humans to encode\nthat into the AI systems.\nAnd to the degree they do,\nthere'll be other AIs that\nsort of punish them for it.\nOut-compete them over-\n- Well, there's all kinds of incentive\nto make AI systems submissive to humans.\nRight?\n- [Lex] Right.\n- I mean, this is the way\nwe're gonna build them, right?\nAnd so then people say,\noh, but look at LLMs.\nLLMs are not controllable.\nAnd they're right,\nLLMs are not controllable.\nBut objective driven AI,\nso systems that derive their answers\nby optimization of an objective\nmeans they have to\noptimize this objective,\nand that objective can include guardrails.\nOne guardrail is obey humans.\nAnother guardrail is don't obey humans\nif it's hurting other humans-\n- I've heard that before\nsomewhere, I don't remember-\n- [Yann] Yes.\n(Lex laughs)\nMaybe in a book. (laughs)\n- Yeah.\nBut speaking of that book,\ncould there be unintended\nconsequences also\nfrom all of this?\n- No, of course.\nSo this is not a simple problem, right?\nI mean designing those guardrails\nso that the system behaves properly\nis not gonna be a simple issue\nfor which there is a silver bullet,\nfor which you have a mathematical proof\nthat the system can be safe.\nIt's gonna be very progressive,\niterative design system\nwhere we put those guardrails\nin such a way that the\nsystem behave properly.\nAnd sometimes they're\ngoing to do something\nthat was unexpected because\nthe guardrail wasn't right,\nand we're gonna correct them\nso that they do it right.\nThe idea somehow that we\ncan't get it slightly wrong,\nbecause if we get it\nslightly wrong we all die,\nis ridiculous.\nWe're just gonna go progressively.\nThe analogy I've used many\ntimes is turbojet design.\nHow did we figure out\nhow to make turbojets so\nunbelievably reliable, right?\nI mean, those are like incredibly\ncomplex pieces of hardware\nthat run at really high temperatures\nfor 20 hours at a time sometimes.\nAnd we can fly halfway around the world\non a two engine jet liner\nat near the speed of sound.\nLike how incredible is this?\nIt is just unbelievable.\nAnd did we do this\nbecause we invented\nlike a general principle\nof how to make turbojets safe?\nNo, it took decades\nto kind of fine tune the\ndesign of those systems\nso that they were safe.\nIs there a separate group\nwithin General Electric\nor Snecma or whatever\nthat is specialized in turbojet safety?\nNo.\nThe design is all about safety.\nBecause a better turbojet\nis also a safer turbojet,\na more reliable one.\nIt's the same for AI.\nLike do you need specific\nprovisions to make AI safe?\nNo, you need to make better AI systems\nand they will be safe\nbecause they are designed\nto be more useful\nand more controllable.\n- So let's imagine a system,\nAI system that's able to\nbe incredibly convincing\nand can convince you of anything.\nI can at least imagine such a system.\nAnd I can see such a\nsystem be weapon-like,\nbecause it can control people's minds,\nwe're pretty gullible.\nWe want to believe a thing.\nAnd you can have an AI\nsystem that controls it\nand you could see governments\nusing that as a weapon.\nSo do you think if you\nimagine such a system,\nthere's any parallel to\nsomething like nuclear weapons?\n- [Yann] No.\n- So why is that technology different?\nSo you're saying there's going\nto be gradual development?\n- [Yann] Yeah.\n- I mean it might be rapid,\nbut they'll be iterative.\nAnd then we'll be able to\nkind of respond and so on.\n- So that AI system designed\nby Vladimir Putin or whatever,\nor his minions (laughing)\nis gonna be like trying\nto talk to every American\nto convince them to vote for-\n- [Lex] Whoever.\n- Whoever pleases Putin or whatever.\nOr rile people up against each other\nas they've been trying to do.\nThey're not gonna be talking to you,\nthey're gonna be talking\nto your AI assistant\nwhich is going to be as\nsmart as theirs, right?", "mimetype": "text/plain", "start_char_idx": 114647, "end_char_idx": 118577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33007c73-4a9a-484e-abf7-c529794c81a0": {"__data__": {"id_": "33007c73-4a9a-484e-abf7-c529794c81a0", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae10f68-c4a6-45ce-be29-72ea79e20e7a", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b577aac24f98499dcae6e2d87a10494aa96fdedefcc3119f62d649875754ca03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e18e7e00-b7c6-4086-a3d0-731676549324", "node_type": "1", "metadata": {}, "hash": "7f8a96ac14fea009fa152c0de5a464088a777ba30e241acac991d2333a7e6027", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So do you think if you\nimagine such a system,\nthere's any parallel to\nsomething like nuclear weapons?\n- [Yann] No.\n- So why is that technology different?\nSo you're saying there's going\nto be gradual development?\n- [Yann] Yeah.\n- I mean it might be rapid,\nbut they'll be iterative.\nAnd then we'll be able to\nkind of respond and so on.\n- So that AI system designed\nby Vladimir Putin or whatever,\nor his minions (laughing)\nis gonna be like trying\nto talk to every American\nto convince them to vote for-\n- [Lex] Whoever.\n- Whoever pleases Putin or whatever.\nOr rile people up against each other\nas they've been trying to do.\nThey're not gonna be talking to you,\nthey're gonna be talking\nto your AI assistant\nwhich is going to be as\nsmart as theirs, right?\nBecause as I said, in the future,\nevery single one of your\ninteraction with the digital world\nwill be mediated by your AI assistant.\nSo the first thing you're\ngonna ask is, is this a scam?\nLike is this thing like\ntelling me the truth?\nLike it's not even going\nto be able to get to you\nbecause it's only going to\ntalk to your AI assistant,\nand your AI is not even going to...\nIt's gonna be like a spam filter, right?\nYou're not even seeing the\nemail, the spam email, right?\nIt's automatically put in a\nfolder that you never see.\nIt's gonna be the same thing.\nThat AI system that tries to\nconvince you of something,\nit's gonna be talking to an AI system\nwhich is gonna be at least as smart as it.\nAnd is gonna say, this is spam. (laughs)\nIt's not even going to\nbring it to your attention.\n- So to you it's very\ndifficult for any one AI system\nto take such a big leap ahead\nto where it can convince\neven the other AI systems?\nSo like there's always going\nto be this kind of race\nwhere nobody's way ahead?\n- That's the history of the world.\nHistory of the world\nis whenever there is a progress someplace,\nthere is a countermeasure.\nAnd it's a cat and mouse game.\n- Mostly yes,\nbut this is why nuclear\nweapons are so interesting\nbecause that was such a powerful weapon\nthat it mattered who got it first.\nThat you could imagine Hitler, Stalin, Mao\ngetting the weapon first\nand that having a different\nkind of impact on the world\nthan the United States\ngetting the weapon first.\nTo you, nuclear weapons is like...\nYou don't imagine a breakthrough discovery\nand then Manhattan project\nlike effort for AI?\n- No.\nAs I said, it's not going to be an event.\nIt's gonna be continuous progress.\nAnd whenever one breakthrough occurs,\nit's gonna be widely\ndisseminated really quickly.\nProbably first within industry.\nI mean, this is not a domain\nwhere government or military organizations\nare particularly innovative,\nand they're in fact way behind.\nAnd so this is gonna come from industry.\nAnd this kind of information\ndisseminates extremely quickly.\nWe've seen this over the\nlast few years, right?\nWhere you have a new...\nLike even take AlphaGo.\nThis was reproduced within three months\neven without like particularly\ndetailed information, right?\n- Yeah.\nThis is an industry that's\nnot good at secrecy.\n(laughs)\n- But even if there is,\njust the fact that you know\nthat something is possible\nmakes you like realize\nthat it's worth investing\nthe time to actually do it.\nYou may be the second person\nto do it but you'll do it.\nSay for all the innovations\nof self supervised running transformers,\ndecoder only architectures, LLMs.\nI mean those things,\nyou don't need to know exactly\nthe details of how they work\nto know that it's possible\nbecause it's deployed and\nthen it's getting reproduced.\nAnd then people who work\nfor those companies move.\nThey go from one company to another.\nAnd the information disseminates.\nWhat makes the success\nof the US tech industry\nand Silicon Valley in\nparticular, is exactly that,\nis because information\ncirculates really, really quickly\nand disseminates very quickly.\nAnd so the whole region sort of is ahead\nbecause of that\ncirculation of information.\n- Maybe just to linger on\nthe psychology of AI doomers.", "mimetype": "text/plain", "start_char_idx": 117826, "end_char_idx": 121801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e18e7e00-b7c6-4086-a3d0-731676549324": {"__data__": {"id_": "e18e7e00-b7c6-4086-a3d0-731676549324", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33007c73-4a9a-484e-abf7-c529794c81a0", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "ef4cafe439be5e157971b03446c021be0e24d2e0ae1bccb9a7a3cac374e3e204", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86bc32e7-7002-4c38-af9e-af9427c5c1e8", "node_type": "1", "metadata": {}, "hash": "cd901dc4cde8fc7dadab90eb2f869c6504ba3f53222e5a324524d382e2b0a5de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You may be the second person\nto do it but you'll do it.\nSay for all the innovations\nof self supervised running transformers,\ndecoder only architectures, LLMs.\nI mean those things,\nyou don't need to know exactly\nthe details of how they work\nto know that it's possible\nbecause it's deployed and\nthen it's getting reproduced.\nAnd then people who work\nfor those companies move.\nThey go from one company to another.\nAnd the information disseminates.\nWhat makes the success\nof the US tech industry\nand Silicon Valley in\nparticular, is exactly that,\nis because information\ncirculates really, really quickly\nand disseminates very quickly.\nAnd so the whole region sort of is ahead\nbecause of that\ncirculation of information.\n- Maybe just to linger on\nthe psychology of AI doomers.\nYou give in the classic Yann LeCun way,\na pretty good example\nof just when a new technology comes to be,\nyou say engineer says,\n\"I invented this new thing,\nI call it a ballpen.\"\nAnd then the TwitterSphere responds,\n\"OMG people could write\nhorrible things with it\nlike misinformation,\npropaganda, hate speech.\nBan it now!\"\nThen writing doomers come in,\nakin to the AI doomers,\n\"imagine if everyone can get a ballpen.\nThis could destroy society.\nThere should be a law\nagainst using ballpen\nto write hate speech,\nregulate ballpens now.\"\nAnd then the pencil industry mogul says,\n\"yeah, ballpens are very dangerous,\nunlike pencil writing which is erasable,\nballpen writing stays forever.\nGovernment should require a\nlicense for a pen manufacturer.\"\nI mean, this does seem to\nbe part of human psychology\nwhen it comes up against new technology.\nWhat deep insights can\nyou speak to about this?\n- Well, there is a natural\nfear of new technology\nand the impact it can have on society.\nAnd people have kind\nof instinctive reaction\nto the world they know\nbeing threatened by major transformations\nthat are either cultural phenomena\nor technological revolutions.\nAnd they fear for their culture,\nthey fear for their job,\nthey fear for the future of their children\nand their way of life, right?\nSo any change is feared.\nAnd you see this along history,\nlike any technological\nrevolution or cultural phenomenon\nwas always accompanied by\ngroups or reaction in the media\nthat basically attributed\nall the problems,\nthe current problems of society\nto that particular change, right?\nElectricity was going to\nkill everyone at some point.\nThe train was going to be a horrible thing\nbecause you can't breathe\npast 50 kilometers an hour.\nAnd so there's a wonderful website\ncalled a Pessimists Archive, right?\nWhich has all those\nnewspaper clips (laughing)\nof all the horrible things\npeople imagined would arrive\nbecause of either technological innovation\nor a cultural phenomenon.\nWonderful examples of jazz or comic books\nbeing blamed for unemployment\nor young people not\nwanting to work anymore\nand things like that, right?\nAnd that has existed for centuries.\nAnd it's knee jerk reactions.\nThe question is do we embrace\nchange or do we resist it?\nAnd what are the real dangers\nas opposed to the imagined imagined ones?\n- So people worry about...\nI think one thing they\nworry about with big tech,\nsomething we've been\ntalking about over and over\nbut I think worth mentioning again,\nthey worry about how powerful AI will be\nand they worry about it\nbeing in the hands of\none centralized power\nof just a handful of central control.\nAnd so that's the\nskepticism with big tech.\nThese companies can make\na huge amount of money\nand control this technology.\nAnd by so doing,\ntake advantage, abuse the\nlittle guy in society.\n- Well, that's exactly why we\nneed open source platforms.\n- Yeah.\nI just wanted to... (laughs)\nNail the point home more and more.\n- [Yann] Yes.\n- So let me ask you on your...\nLike I said, you do get a little bit\nflavorful on the internet.\nJoscha Bach tweeted\nsomething that you LOL'd at\nin reference to HAL 9,000.\nQuote,\n\"I appreciate your argument\nand I fully understand your frustration,\nbut whether the pod bay doors\nshould be opened or closed\nis a complex and nuanced issue.\"\nSo you're at the head of Meta AI.", "mimetype": "text/plain", "start_char_idx": 121030, "end_char_idx": 125109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "86bc32e7-7002-4c38-af9e-af9427c5c1e8": {"__data__": {"id_": "86bc32e7-7002-4c38-af9e-af9427c5c1e8", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e18e7e00-b7c6-4086-a3d0-731676549324", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "b612b83b6465e4826e601b12e09af314e458785e9c4e24ba74712ab01dbf001c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2679fd59-9c0b-4b35-ace8-4152c3b754f9", "node_type": "1", "metadata": {}, "hash": "908cf04ac2aa550dde310d1cd85d654fb22d3ae44df8a78c9e91c4a56aeb325f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And so that's the\nskepticism with big tech.\nThese companies can make\na huge amount of money\nand control this technology.\nAnd by so doing,\ntake advantage, abuse the\nlittle guy in society.\n- Well, that's exactly why we\nneed open source platforms.\n- Yeah.\nI just wanted to... (laughs)\nNail the point home more and more.\n- [Yann] Yes.\n- So let me ask you on your...\nLike I said, you do get a little bit\nflavorful on the internet.\nJoscha Bach tweeted\nsomething that you LOL'd at\nin reference to HAL 9,000.\nQuote,\n\"I appreciate your argument\nand I fully understand your frustration,\nbut whether the pod bay doors\nshould be opened or closed\nis a complex and nuanced issue.\"\nSo you're at the head of Meta AI.\nThis is something that really worries me,\nthat our AI overlords\nwill speak down to us with\ncorporate speak of this nature\nand you sort of resist that\nwith your way of being.\nIs this something you can just comment on\nsort of working at a big company,\nhow you can avoid the\nover fearing, I suppose,\nthe through caution create harm?\n- Yeah.\nAgain, I think the answer to\nthis is open source platforms\nand then enabling a widely\ndiverse set of people\nto build AI assistants\nthat represent the diversity\nof cultures, opinions, languages,\nand value systems across the world.\nSo that you're not bound\nto just be brainwashed\nby a particular way of thinking\nbecause of a single AI entity.\nSo I mean, I think it's a\nreally, really important question\nfor society.\nAnd the problem I'm seeing,\nwhich is why I've been so vocal\nand sometimes a little sardonic about it-\n- Never stop.\nNever stop, Yann.\n(both laugh)\nWe love it.\n- Is because I see the danger\nof this concentration of power\nthrough proprietary AI systems\nas a much bigger danger\nthan everything else.\nThat if we really want\ndiversity of opinion AI systems\nthat in the future\nthat we'll all be interacting\nthrough AI systems,\nwe need those to be diverse\nfor the preservation\nof a diversity of ideas\nand creeds and political\nopinions and whatever,\nand the preservation of democracy.\nAnd what works against this\nis people who think that\nfor reasons of security,\nwe should keep AI systems\nunder lock and key\nbecause it's too dangerous\nto put it in the hands of everybody\nbecause it could be used\nby terrorists or something.\nThat would lead to\npotentially a very bad future\nin which all of our information diet\nis controlled by a small\nnumber of companies\nthrough proprietary systems.\n- So you trust humans with this technology\nto build systems that are on\nthe whole good for humanity?\n- Isn't that what democracy\nand free speech is all about?\n- I think so.\n- Do you trust institutions\nto do the right thing?\nDo you trust people to do the right thing?\nAnd yeah, there's bad people\nwho are gonna do bad things,\nbut they're not going to\nhave superior technology\nto the good people.\nSo then it's gonna be my good\nAI against your bad AI, right?\nI mean it's the examples that\nwe were just talking about\nof maybe some rogue country\nwill build some AI system\nthat's gonna try to convince everybody\nto go into a civil war or something\nor elect a favorable ruler.\nBut then they will have to go\npast our AI systems, right?\n(laughs)\n- An AI system with a\nstrong Russian accent\nwill be trying to convince our-\n- And doesn't put any\narticles in their sentences.\n(both laugh)\n- Well, it'll be at the very\nleast, absurdly comedic.\nOkay.\nSo since we talked about\nsort of the physical reality,\nI'd love to ask your vision\nof the future with robots\nin this physical reality.\nSo many of the kinds of intelligence\nyou've been speaking about\nwould empower robots\nto be more effective\ncollaborators with us humans.\nSo since Tesla's Optimus team\nhas been showing us some\nprogress in humanoid robots,\nI think it really reinvigorated\nthe whole industry\nthat I think Boston\nDynamics has been leading\nfor a very, very long time.\nSo now there's all kinds of companies,\nFigure AI, obviously Boston Dynamics-\n- [Yann] Unitree.\n- Unitree.\nBut there's like a lot of them.\nIt's great.\nIt's great.", "mimetype": "text/plain", "start_char_idx": 124409, "end_char_idx": 128416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2679fd59-9c0b-4b35-ace8-4152c3b754f9": {"__data__": {"id_": "2679fd59-9c0b-4b35-ace8-4152c3b754f9", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86bc32e7-7002-4c38-af9e-af9427c5c1e8", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4c30cbf01003431e13ccbda101d70bfa6451a73535e9410af3439777cab8737a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97efef44-9d6f-466c-9038-ed2fb9861f20", "node_type": "1", "metadata": {}, "hash": "74f5f54139fe056e266e4e1c946a542e69772c65f693e80d6391627f623eda05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(both laugh)\n- Well, it'll be at the very\nleast, absurdly comedic.\nOkay.\nSo since we talked about\nsort of the physical reality,\nI'd love to ask your vision\nof the future with robots\nin this physical reality.\nSo many of the kinds of intelligence\nyou've been speaking about\nwould empower robots\nto be more effective\ncollaborators with us humans.\nSo since Tesla's Optimus team\nhas been showing us some\nprogress in humanoid robots,\nI think it really reinvigorated\nthe whole industry\nthat I think Boston\nDynamics has been leading\nfor a very, very long time.\nSo now there's all kinds of companies,\nFigure AI, obviously Boston Dynamics-\n- [Yann] Unitree.\n- Unitree.\nBut there's like a lot of them.\nIt's great.\nIt's great.\nI mean I love it.\nSo do you think there'll be\nmillions of humanoid robots\nwalking around soon?\n- Not soon, but it's gonna happen.\nLike the next decade\nI think is gonna be really\ninteresting in robots.\nLike the emergence of\nthe robotics industry\nhas been in the waiting for 10, 20 years,\nwithout really emerging\nother than for like kind\nof pre-program behavior\nand stuff like that.\nAnd the main issue is again,\nthe Moravec's paradox.\nLike how do we get the systems\nto understand how the world works\nand kind of plan actions?\nAnd so we can do it for\nreally specialized tasks.\nAnd the way Boston Dynamics goes about it\nis basically with a lot of\nhandcrafted dynamical models\nand careful planning in advance,\nwhich is very classical robotics\nwith a lot of innovation,\na little bit of perception,\nbut it's still not...\nLike they can't build a\ndomestic robot, right?\nAnd we're still some distance away\nfrom completely autonomous\nlevel five driving.\nAnd we're certainly very far away\nfrom having level five autonomous driving\nby a system that can train itself\nby driving 20 hours, like any 17-year-old.\nSo until we have, again, world models,\nsystems that can train themselves\nto understand how the world works,\nwe're not gonna have significant\nprogress in robotics.\nSo a lot of the people\nworking on robotic hardware at the moment\nare betting or banking\non the fact that AI\nis gonna make sufficient\nprogress towards that.\n- And they're hoping to\ndiscover a product in it too-\n- [Yann] Yeah.\n- Before you have a\nreally strong world model,\nthere'll be an almost strong world model.\nAnd people are trying to find a product\nin a clumsy robot, I suppose.\nLike not a perfectly efficient robot.\nSo there's the factory setting\nwhere humanoid robots\ncan help automate some\naspects of the factory.\nI think that's a crazy difficult task\n'cause of all the safety required\nand all this kind of stuff,\nI think in the home is more interesting.\nBut then you start to think...\nI think you mentioned loading\nthe dishwasher, right?\n- [Yann] Yeah.\n- Like I suppose that's\none of the main problems\nyou're working on.\n- I mean there's cleaning up. (laughs)\n- [Lex] Yeah.\n- Cleaning the house,\nclearing up the table after a meal,\nwashing the dishes, all\nthose tasks, cooking.\nI mean all the tasks that in\nprinciple could be automated\nbut are actually incredibly sophisticated,\nreally complicated.\n- But even just basic navigation\naround a space full of uncertainty.\n- That sort of works.\nLike you can sort of do this now.\nNavigation is fine.\n- Well, navigation in a way\nthat's compelling to us humans\nis a different thing.\n- Yeah.\nIt's not gonna be necessarily...\nI mean we have demos actually\n'cause there is a so-called\nembodied AI group at FAIR\nand they've been not\nbuilding their own robots\nbut using commercial robots.\nAnd you can tell the robot\ndog like go to the fridge\nand they can actually open the fridge\nand they can probably pick\nup a can in the fridge\nand stuff like that and bring it to you.\nSo it can navigate,\nit can grab objects\nas long as it's been\ntrained to recognize them,\nwhich vision systems work\npretty well nowadays.\nBut it's not like a\ncompletely general robot\nthat would be sophisticated enough\nto do things like clearing\nup the dinner table.\n(laughs)\n- Yeah, to me that's an exciting future\nof getting humanoid robots.", "mimetype": "text/plain", "start_char_idx": 127702, "end_char_idx": 131733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97efef44-9d6f-466c-9038-ed2fb9861f20": {"__data__": {"id_": "97efef44-9d6f-466c-9038-ed2fb9861f20", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2679fd59-9c0b-4b35-ace8-4152c3b754f9", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "0792a4edd2a49d1f4cd900cb78a3b941a9f8a7fce4906f3ca9744bedd0d68739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29a82e94-1150-43f6-9f29-0fb2e87c5252", "node_type": "1", "metadata": {}, "hash": "920cdefc944a3401d690f7811805dc41a3251862a7b6fac0f57fb2af4b43ae86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Navigation is fine.\n- Well, navigation in a way\nthat's compelling to us humans\nis a different thing.\n- Yeah.\nIt's not gonna be necessarily...\nI mean we have demos actually\n'cause there is a so-called\nembodied AI group at FAIR\nand they've been not\nbuilding their own robots\nbut using commercial robots.\nAnd you can tell the robot\ndog like go to the fridge\nand they can actually open the fridge\nand they can probably pick\nup a can in the fridge\nand stuff like that and bring it to you.\nSo it can navigate,\nit can grab objects\nas long as it's been\ntrained to recognize them,\nwhich vision systems work\npretty well nowadays.\nBut it's not like a\ncompletely general robot\nthat would be sophisticated enough\nto do things like clearing\nup the dinner table.\n(laughs)\n- Yeah, to me that's an exciting future\nof getting humanoid robots.\nRobots in general in\nthe home more and more\nbecause it gets humans\nto really directly\ninteract with AI systems\nin the physical space.\nAnd in so doing it allows us\nto philosophically,\npsychologically explore\nour relationships with robots.\nIt can be really, really interesting.\nSo I hope you make progress\non the whole JEPA thing soon.\n(laughs)\n- Well, I mean, I hope\nthings can work as planned.\nI mean, again, we've been like\nkinda working on this idea\nof self supervised learning\nfrom video for 10 years.\nAnd only made significant\nprogress in the last two or three.\n- And actually you've mentioned\nthat there's a lot of\ninteresting breakthroughs\nthat can happen without having\naccess to a lot of compute.\nSo if you're interested in doing a PhD\nin this kind of stuff,\nthere's a lot of possibilities still\nto do innovative work.\nSo like what advice would you give\nto a undergrad that's\nlooking to go to grad school\nand do a PhD?\n- So basically, I've listed them already.\nThis idea of how do you train\na world model by observation?\nAnd you don't have to train necessarily\non gigantic data sets.\nI mean, it could turn out to be necessary\nto actually train on large data sets\nto have emergent properties\nlike we have with LLMs.\nBut I think there is a lot of\ngood ideas that can be done\nwithout necessarily scaling up.\nThen there is how do you do planning\nwith a learn world model?\nIf the world the system evolves in\nis not the physical world,\nbut is the world of let's say the internet\nor some sort of world\nof where an action consists\nin doing a search in a search engine\nor interrogating a database,\nor running a simulation\nor calling a calculator\nor solving a differential equation,\nhow do you get a system\nto actually plan a sequence of actions\nto give the solution to a problem?\nAnd so the question of planning\nis not just a question of\nplanning physical actions,\nit could be planning actions to use tools\nfor a dialogue system\nor for any kind of intelligence system.\nAnd there's some work on\nthis but not a huge amount.\nSome work at FAIR,\none called Toolformer,\nwhich was a couple years ago\nand some more recent work on planning,\nbut I don't think we\nhave like a good solution\nfor any of that.\nThen there is the question\nof hierarchical planning.\nSo the example I mentioned\nof planning a trip from New York to Paris,\nthat's hierarchical,\nbut almost every action that we take\ninvolves hierarchical\nplanning in some sense.\nAnd we really have absolutely\nno idea how to do this.\nLike there's zero demonstration\nof hierarchical planning in AI,\nwhere the various levels\nof representations\nthat are necessary have been learned.\nWe can do like two level\nhierarchical planning\nwhen we design the two levels.\nSo for example, you have like\na dog legged robot, right?\nYou want it to go from the\nliving room to the kitchen.\nYou can plan a path that\navoids the obstacle.\nAnd then you can send this\nto a lower level planner\nthat figures out how to move the legs\nto kind of follow that\ntrajectories, right?\nSo that works,\nbut that two level planning\nis designed by hand, right?\nWe specify what the proper\nlevels of abstraction,\nthe representation at each\nlevel of abstraction have to be.\nHow do you learn this?\nHow do you learn that\nhierarchical representation\nof action plans, right?", "mimetype": "text/plain", "start_char_idx": 130909, "end_char_idx": 135003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29a82e94-1150-43f6-9f29-0fb2e87c5252": {"__data__": {"id_": "29a82e94-1150-43f6-9f29-0fb2e87c5252", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97efef44-9d6f-466c-9038-ed2fb9861f20", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "02ca099815bfa876d65b869e595426033d47f3d6afc55a2fe7d66ec9f4df5578", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51fa97f9-a208-45c9-bd7f-2740cf91f9bb", "node_type": "1", "metadata": {}, "hash": "b7c6143b87d9cde0d0b30a108048bd2856a4bd719f1f6066de40e1fbb7de7998", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "And we really have absolutely\nno idea how to do this.\nLike there's zero demonstration\nof hierarchical planning in AI,\nwhere the various levels\nof representations\nthat are necessary have been learned.\nWe can do like two level\nhierarchical planning\nwhen we design the two levels.\nSo for example, you have like\na dog legged robot, right?\nYou want it to go from the\nliving room to the kitchen.\nYou can plan a path that\navoids the obstacle.\nAnd then you can send this\nto a lower level planner\nthat figures out how to move the legs\nto kind of follow that\ntrajectories, right?\nSo that works,\nbut that two level planning\nis designed by hand, right?\nWe specify what the proper\nlevels of abstraction,\nthe representation at each\nlevel of abstraction have to be.\nHow do you learn this?\nHow do you learn that\nhierarchical representation\nof action plans, right?\nWith com nets and deep learning,\nwe can train the system\nto learn hierarchical\nrepresentations of percepts.\nWhat is the equivalent\nwhen what you're trying to\nrepresent are action plans?\n- For action plans.\nYeah.\nSo you want basically a\nrobot dog or humanoid robot\nthat turns on and travels\nfrom New York to Paris\nall by itself.\n- [Yann] For example.\n- All right.\nIt might have some trouble at the TSA but-\n- No, but even doing\nsomething fairly simple\nlike a household task.\n- [Lex] Sure.\n- Like cooking or something.\n- Yeah.\nThere's a lot involved.\nIt's a super complex task.\nOnce again, we take it for granted.\nWhat hope do you have for\nthe future of humanity?\nWe're talking about so\nmany exciting technologies,\nso many exciting possibilities.\nWhat gives you hope when you look out\nover the next 10, 20, 50, 100 years?\nIf you look at social media,\nthere's wars going on, there's\ndivision, there's hatred,\nall this kind of stuff\nthat's also part of humanity.\nBut amidst all that, what gives you hope?\n- I love that question.\nWe can make humanity smarter with AI.\nOkay?\nI mean AI basically will\namplify human intelligence.\nIt's as if every one of us\nwill have a staff of smart AI assistants.\nThey might be smarter than us.\nThey'll do our bidding,\nperhaps execute a task\nin ways that are much better\nthan we could do ourselves\nbecause they'd be smarter than us.\nAnd so it's like everyone\nwould be the boss\nof a staff of super smart virtual people.\nSo we shouldn't feel threatened by this\nany more than we should feel threatened\nby being the manager of a group of people,\nsome of whom are more intelligent than us.\nI certainly have a lot\nof experience with this.\n(laughs)\nOf having people working with\nme who are smarter than me.\nThat's actually a wonderful thing.\nSo having machines that\nare smarter than us,\nthat assist us in all of\nour tasks, our daily lives,\nwhether it's professional or personal,\nI think would be an\nabsolutely wonderful thing.\nBecause intelligence is the commodity\nthat is most in demand.\nI mean, all the mistakes\nthat humanity makes\nis because of lack of\nintelligence, really,\nor lack of knowledge, which is related.\nSo making people smarter\nwhich can only be better.\nI mean, for the same reason\nthat public education is a good thing\nand books are a good thing,\nand the internet is also a\ngood thing, intrinsically.\nAnd even social networks are a good thing\nif you run them properly.\n(laughs)\nIt's difficult, but you can.\nBecause it helps the communication\nof information and knowledge\nand the transmission of knowledge.\nSo AI is gonna make humanity smarter.\nAnd the analogy I've been using\nis the fact that perhaps\nan equivalent event\nin the history of humanity\nto what might be provided by\ngeneralization of AI assistant\nis the invention of the printing press.\nIt made everybody smarter.\nThe fact that people could\nhave access to books.\nBooks were a lot cheaper\nthan they were before.\nAnd so a lot more people had\nan incentive to learn to read,\nwhich wasn't the case before.\nAnd people became smarter.\nIt enabled the enlightenment, right?\nThere wouldn't be an enlightenment\nwithout the printing press.\nIt enabled philosophy, rationalism,\nescape from religious doctrine,\ndemocracy, science.\nAnd certainly without this\nthere wouldn't have been\nthe American Revolution\nor the French Revolution.\nAnd so we'll still be under\nfeudal regimes perhaps.", "mimetype": "text/plain", "start_char_idx": 134156, "end_char_idx": 138370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51fa97f9-a208-45c9-bd7f-2740cf91f9bb": {"__data__": {"id_": "51fa97f9-a208-45c9-bd7f-2740cf91f9bb", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a82e94-1150-43f6-9f29-0fb2e87c5252", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "4bf5b7beaad1be91bf95dcb35d26145030d779d510a1e0909a3b3eccdfe47f38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04c5cbf9-c170-457e-b491-3a0e71aefd69", "node_type": "1", "metadata": {}, "hash": "09d1c89e67a566231e2cc15dd3db00581e9aa00d3d1b69e24445a244eded9ebd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because it helps the communication\nof information and knowledge\nand the transmission of knowledge.\nSo AI is gonna make humanity smarter.\nAnd the analogy I've been using\nis the fact that perhaps\nan equivalent event\nin the history of humanity\nto what might be provided by\ngeneralization of AI assistant\nis the invention of the printing press.\nIt made everybody smarter.\nThe fact that people could\nhave access to books.\nBooks were a lot cheaper\nthan they were before.\nAnd so a lot more people had\nan incentive to learn to read,\nwhich wasn't the case before.\nAnd people became smarter.\nIt enabled the enlightenment, right?\nThere wouldn't be an enlightenment\nwithout the printing press.\nIt enabled philosophy, rationalism,\nescape from religious doctrine,\ndemocracy, science.\nAnd certainly without this\nthere wouldn't have been\nthe American Revolution\nor the French Revolution.\nAnd so we'll still be under\nfeudal regimes perhaps.\nAnd so it completely transformed the world\nbecause people became smarter\nand kinda learned about things.\nNow, it also created 200 years\nof essentially religious\nconflicts in Europe, right?\nBecause the first thing that\npeople read was the Bible\nand realized that\nperhaps there was a different\ninterpretation of the Bible\nthan what the priests were telling them.\nAnd so that created\nthe Protestant movement\nand created a rift.\nAnd in fact, the Catholic church\ndidn't like the idea of the printing press\nbut they had no choice.\nAnd so it had some bad\neffects and some good effects.\nI don't think anyone today\nwould say that the invention\nof the printing press\nhad an overall negative effect\ndespite the fact that it created 200 years\nof religious conflicts in Europe.\nNow compare this,\nand I was very proud of myself\nto come up with this analogy,\nbut realized someone else came\nwith the same idea before me.\nCompare this with what\nhappened in the Ottoman Empire.\nThe Ottoman Empire banned the\nprinting press for 200 years.\nAnd it didn't ban it for all languages,\nonly for Arabic.\nYou could actually print books\nin Latin or Hebrew or whatever\nin the Ottoman Empire,\njust not in Arabic.\nAnd I thought it was because\nthe rulers just wanted to preserve\nthe control over the\npopulation and the dogma,\nreligious dogma and everything.\nBut after talking with\nthe UAE Minister of AI,\nOmar Al Olama,\nhe told me no, there was another reason.\nAnd the other reason was that\nit was to preserve the corporation\nof calligraphers, right?\nThere's like an art form\nwhich is writing those\nbeautiful Arabic poems\nor whatever religious text in this thing.\nAnd it was very powerful\ncorporation of scribes basically\nthat kinda run a big chunk of the empire.\nAnd we couldn't put them out of business.\nSo they banned the bridging press\nin part to protect that business.\nNow, what's the analogy for AI today?\nLike who are we protecting by banning AI?\nLike who are the people who\nare asking that AI be regulated\nto protect their jobs?\nAnd of course, it's a real question\nof what is gonna be the effect\nof technological transformation like AI\non the job market and the labor market?\nAnd there are economists\nwho are much more expert\nat this than I am,\nbut when I talk to them,\nthey tell us we're not\ngonna run out of job.\nThis is not gonna cause mass unemployment.\nThis is just gonna be gradual shift\nof different professions.\nThe professions that are gonna be hot\n10 or 15 years from now,\nwe have no idea today\nwhat they're gonna be.\nThe same way if we go\nback 20 years in the past,\nlike who could have thought 20 years ago\nthat like the hottest job,\neven like 5, 10 years ago\nwas mobile app developer?\nLike smartphones weren't invented.\n- Most of the jobs of the future\nmight be in the Metaverse. (laughs)\n- Well, it could be.\nYeah.\n- But the point is you\ncan't possibly predict.\nBut you're right.\nI mean, you've made a\nlot of strong points.\nAnd I believe that people\nare fundamentally good,\nand so if AI, especially open source AI\ncan make them smarter,\nit just empowers the goodness in humans.\n- So I share that feeling.\nOkay?\nI think people are\nfundamentally good. (laughing)\nAnd in fact a lot of doomers are doomers\nbecause they don't think that\npeople are fundamentally good.", "mimetype": "text/plain", "start_char_idx": 137447, "end_char_idx": 141621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04c5cbf9-c170-457e-b491-3a0e71aefd69": {"__data__": {"id_": "04c5cbf9-c170-457e-b491-3a0e71aefd69", "embedding": null, "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a609cc0-d7fd-4e96-b9c9-7465edf4238c", "node_type": "4", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "89bbdb04285d03d969508635f036e4b553dd43b5526cfbf5d3465bcf388d70cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51fa97f9-a208-45c9-bd7f-2740cf91f9bb", "node_type": "1", "metadata": {"file_path": "C:\\podcast-qa\\data\\[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_name": "[English] Yann Lecun_ Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI _ Lex Fridman Podcast #416 [DownSub.com].txt", "file_type": "text/plain", "file_size": 142689, "creation_date": "2025-06-20", "last_modified_date": "2025-06-20"}, "hash": "22451acdff491bb9fb6c1f8181cd1676f2f22ce6511bd12dc1eecf5047288e7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same way if we go\nback 20 years in the past,\nlike who could have thought 20 years ago\nthat like the hottest job,\neven like 5, 10 years ago\nwas mobile app developer?\nLike smartphones weren't invented.\n- Most of the jobs of the future\nmight be in the Metaverse. (laughs)\n- Well, it could be.\nYeah.\n- But the point is you\ncan't possibly predict.\nBut you're right.\nI mean, you've made a\nlot of strong points.\nAnd I believe that people\nare fundamentally good,\nand so if AI, especially open source AI\ncan make them smarter,\nit just empowers the goodness in humans.\n- So I share that feeling.\nOkay?\nI think people are\nfundamentally good. (laughing)\nAnd in fact a lot of doomers are doomers\nbecause they don't think that\npeople are fundamentally good.\nAnd they either don't trust people\nor they don't trust the\ninstitution to do the right thing\nso that people behave properly.\n- Well, I think both you\nand I believe in humanity,\nand I think I speak for a lot of people\nin saying thank you for pushing\nthe open source movement,\npushing to making both\nresearch and AI open source,\nmaking it available to people,\nand also the models themselves,\nmaking that open source also.\nSo thank you for that.\nAnd thank you for speaking your mind\nin such colorful and beautiful\nways on the internet.\nI hope you never stop.\nYou're one of the most fun people I know\nand get to be a fan of.\nSo Yann, thank you for\nspeaking to me once again,\nand thank you for being you.\n- Thank you Lex.\n- Thanks for listening to this\nconversation with Yann LeCun.\nTo support this podcast,\nplease check out our\nsponsors in the description.\nAnd now let me leave you with some words\nfrom Arthur C. Clarke,\n\"the only way to discover\nthe limits of the possible\nis to go beyond them into the impossible.\"\nThank you for listening and\nhope to see you next time.", "mimetype": "text/plain", "start_char_idx": 140874, "end_char_idx": 142689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}